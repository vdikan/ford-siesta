var tipuesearch = {"pages":[{"title":" SIESTA ","text":"SIESTA Note This is an early stage work-in-progress build of documentation for SIESTA code. At the moment it should not by any means be considered a reliable source of information. Please, consult the manual on SIESTA's official repository . Project Dashboard Todo Content Remake the chart for Hamiltonian setup Expand SIESTA calculation flow section in the reference. Developer Info SIESTA Group E. Artacho, J.D. Gale, A. Garcia, J. Junquera, P. Ordejon, D. Sanchez-Portal, J.M. Cela and J.M. Soler","tags":"home","loc":"index.html"},{"title":"state_analysis.F – SIESTA","text":"Files dependent on this one sourcefile~~state_analysis.f~~AfferentGraph sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~state_analysis.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_state_analysis Source Code state_analysis.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_state_analysis use write_subs private public :: state_analysis CONTAINS subroutine state_analysis ( istep ) use siesta_cml use m_born_charge , only : born_charge use parallel , only : IOnode use m_wallclock , only : wallclock use zmatrix , only : lUseZmatrix , iofaZmat , & CartesianForce_to_ZmatForce use atomlist , only : iaorb , iphorb , amass , no_u , lasto use atomlist , only : indxuo use m_spin , only : nspin , SpOrb use m_fixed , only : fixed use sparse_matrices use siesta_geom USE siesta_options use units , only : amu , eV use m_stress use m_energies , only : Etot , FreeE , Eharrs , FreeEHarris , Entropy use m_energies , only : Ebs , Ef use m_ntm use m_forces use m_energies , only : update_FreeE , update_FreeEHarris use m_intramol_pressure , only : remove_intramol_pressure #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_FORCES #endif implicit none integer :: istep integer :: ia , jx , ix real ( dp ) :: volume logical :: eggbox_block = . true . ! Read eggbox info from data file? real ( dp ) :: qspin external :: eggbox , mulliken , moments real ( dp ), external :: volcel !------------------------------------------------------------------------- BEGIN call timer ( 'state_analysis' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_analysis' ) #endif if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'SCF Finalization' ) endif !     Write final Kohn-Sham and Free Energy FreeE = Etot - Temp * Entropy FreeEHarris = Eharrs - Temp * Entropy if ( cml_p ) call cmlStartPropertyList ( mainXML , & title = 'Energies and spin' ) if ( IOnode ) then if ( . not . harrisfun ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS(eV) =        ' , Etot / eV if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = FreeE / eV , & dictref = 'siesta:FreeE' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ebs / eV , & dictref = 'siesta:Ebs' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ef / eV , & dictref = 'siesta:E_Fermi' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif endif !     Substract egg box effect from energy if ( eggbox_block ) then call eggbox ( 'energy' , ucell , na_u , isa , ntm , xa , fa , Etot , & eggbox_block ) FreeE = Etot - Temp * Entropy if ( IOnode ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS - E_eggbox = ' , Etot / eV if ( cml_p ) call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS_egg' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif call update_FreeE ( Temp ) call update_FreeEHarris ( Temp ) call print_spin ( qspin ) if ( cml_p ) call cmlEndPropertyList ( mainXML ) !     Substract egg box effect from the forces if ( eggbox_block ) then call eggbox ( 'forces' , ucell , na_u , isa , ntm , xa , fa , Etot , eggbox_block ) endif if ( IOnode ) call write_raw_efs ( stress , na_u , fa , FreeE ) !     Compute stress without internal molecular pressure call remove_intramol_pressure ( ucell , stress , na_u , xa , fa , mstress ) !     Impose constraints to atomic movements by changing forces ........... if ( RemoveIntraMolecularPressure ) then !        Consider intramolecular pressure-removal as another !        kind of constraint call fixed ( ucell , mstress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) else call fixed ( ucell , stress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) endif #ifdef SIESTA__FLOOK ! We call it right after using the ! geometry constraints. ! In that way we can use both methods on top ! of each other! ! The easy, already implemented methods in fixed, ! and custom ones in Lua :) call slua_call ( LUA , LUA_FORCES ) #endif !     Calculate and output Zmatrix forces if ( lUseZmatrix . and . ( idyn . eq . 0 )) then call CartesianForce_to_ZmatForce ( na_u , xa , fa ) if ( IOnode ) call iofaZmat () endif !     Compute kinetic contribution to stress kin_stress ( 1 : 3 , 1 : 3 ) = 0.0_dp volume = volcel ( ucell ) do ia = 1 , na_u do jx = 1 , 3 do ix = 1 , 3 kin_stress ( ix , jx ) = kin_stress ( ix , jx ) - & amu * amass ( ia ) * va ( ix , ia ) * va ( jx , ia ) / volume enddo enddo enddo !     Add kinetic term to stress tensor tstress = stress + kin_stress !     Force output if ( IOnode ) then call siesta_write_forces ( istep ) call siesta_write_stress_pressure () call wallclock ( '--- end of geometry step' ) endif !     Mulliken population analysis if ( SpOrb ) then call moments ( mullipop , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) else call mulliken ( mullipop , nspin , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) endif ! !     Call the born effective charge routine only in those steps (even) !     in which the dx  is positive. if ( bornz . and . ( mod ( istep , 2 ) . eq . 0 )) then call born_charge () endif !     End the xml module corresponding to the analysis if ( cml_p ) then call cmlEndModule ( mainXML ) endif call timer ( 'state_analysis' , 2 ) !--------------------------------------------------------------------------- END END subroutine state_analysis END MODULE m_state_analysis","tags":"","loc":"sourcefile/state_analysis.f.html"},{"title":"setup_hamiltonian.F – SIESTA","text":"Files dependent on this one sourcefile~~setup_hamiltonian.f~~AfferentGraph sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_setup_hamiltonian Source Code setup_hamiltonian.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_setup_hamiltonian private public :: setup_hamiltonian CONTAINS subroutine setup_hamiltonian ( iscf ) USE siesta_options use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D , H_so_2D use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H , S , Hold use sparse_matrices , only : Dscf , Escf , xijo use class_dSpData1D , only : val use class_dSpData2D , only : val use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , . lastkb , no_s , rmaxv , indxua , iphorb , lasto , . rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use ldau_specs , only : switch_ldau ! This variable determines whether !   the subroutine to compute the !   Hubbard terms should be called !   or not use m_ldau , only : hubbard_term ! Subroutine that compute the !   Hubbard terms use m_dhscf , only : dhscf use m_stress use m_energies use parallel , only : Node use m_steps , only : istp use m_ntm use m_spin , only : spin use m_dipol use alloc , only : re_alloc , de_alloc use m_gamma use m_hsx , only : write_hsx use sys , only : die , bye use m_partial_charges , only : want_partial_charges use files , only : filesOut_t ! derived type for output file names use m_rhog , only : rhog_in , rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none integer , intent ( in ) :: iscf real ( dp ) :: stressl ( 3 , 3 ) real ( dp ), pointer :: fal (:,:) ! Local-node part of atomic F #ifdef MPI real ( dp ) :: buffer1 #endif integer :: io , is , ispin integer :: ifa ! Calc. forces?      0=>no, 1=>yes integer :: istr ! Calc. stress?      0=>no, 1=>yes integer :: ihmat ! Calc. hamiltonian? 0=>no, 1=>yes real ( dp ) :: g2max type ( filesOut_t ) :: filesOut ! blank output file names logical :: use_rhog_in real ( dp ), pointer :: H_vkb (:), H_kin (:), H_ldau (:,:), H_so (:,:) !------------------------------------------------------------------------- BEGIN call timer ( 'setup_H' , 1 ) ! Nullify pointers nullify ( fal ) !$OMP parallel default(shared), private(ispin,io) !     Save present H matrix !$OMP do collapse(2) do ispin = 1 , spin % H do io = 1 , maxnh Hold ( io , ispin ) = H ( io , ispin ) enddo enddo !$OMP end do !$OMP single H_kin => val ( H_kin_1D ) H_vkb => val ( H_vkb_1D ) if ( spin % SO ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so => val ( H_so_2D ) end if !$OMP end single ! keep wait ! We do not need to set the non-spinor components ! For non-colinear they are set down below, ! while for spin-orbit they are set to the H_so initial ! spin-orbit. do ispin = 1 , spin % spinor !$OMP do do io = 1 , maxnh H ( io , ispin ) = H_kin ( io ) + H_vkb ( io ) end do !$OMP end do nowait end do if ( spin % NCol ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = 0._dp end do end do !$OMP end do nowait else if ( spin % SO ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = H_so ( io , ispin - 2 ) end do end do !$OMP end do nowait end if ! .................. ! Non-SCF part of total energy ....................................... ! Note that these will be \"impure\" for a mixed Dscf ! If mixing the charge, Dscf is the previous step's DM_out. Since ! the \"scf\" components of the energy are computed with the (mixed) ! charge, this introduces an inconsistency. In this case the energies ! coming out of this routine need to be corrected. ! !$OMP single Ekin = 0.0_dp Enl = 0.0_dp !$OMP end single ! keep wait !$OMP do collapse(2), reduction(+:Ekin,Enl) do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) end do end do !$OMP end do nowait !$OMP single Eso = 0._dp !$OMP end single if ( spin % SO ) then !$OMP do reduction(+:Eso) do io = 1 , maxnh Eso = Eso + H_so ( io , 1 ) * Dscf ( io , 7 ) + H_so ( io , 2 ) * Dscf ( io , 8 ) . + H_so ( io , 5 ) * Dscf ( io , 3 ) + H_so ( io , 6 ) * Dscf ( io , 4 ) . - H_so ( io , 3 ) * Dscf ( io , 5 ) - H_so ( io , 4 ) * Dscf ( io , 6 ) end do !$OMP end do nowait end if !$OMP end parallel #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 end if #endif !     Non-SCF part of total energy call update_E0 () ! Hubbard term for LDA+U: energy, forces, stress and matrix elements .... if ( switch_ldau ) then if ( spin % NCol ) then call die ( 'LDA+U cannot be used with non-colinear spin.' ) end if if ( spin % SO ) then call die ( 'LDA+U cannot be used with spin-orbit coupling.' ) end if call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) H_ldau => val ( H_ldau_2D ) call hubbard_term ( scell , na_u , na_s , isa , xa , indxua , . maxnh , maxnh , lasto , iphorb , no_u , no_l , . numh , listhptr , listh , numh , listhptr , listh , . spin % spinor , Dscf , Eldau , DEldau , H_ldau , . fal , stressl , H , iscf , . matrix_elements_only = . true .) #ifdef MPI ! Global reduction of energy terms call globalize_sum ( Eldau , buffer1 ) Eldau = buffer1 ! DEldau should not be globalized ! as it is based on globalized occupations #endif Eldau = Eldau + DEldau call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) endif ! .................. ! Add SCF contribution to energy and matrix elements .................. g2max = g2cut call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) ifa = 0 istr = 0 ihmat = 1 if (( hirshpop . or . voropop ) $ . and . partial_charges_at_every_scf_step ) then want_partial_charges = . true . endif use_rhog_in = ( mix_charge . and . iscf > 1 ) call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa , indxua , . ntm , ifa , istr , ihmat , filesOut , . maxnh , numh , listhptr , listh , Dscf , Datm , . maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , . Exc , Dxc , dipol , stress , fal , stressl , . use_rhog_in ) ! This statement will apply to iscf = 1, for example, when ! we do not use rhog_in. Rhog here is always the charge used to ! build H, that is, rhog_in. if ( mix_charge ) rhog_in = rhog want_partial_charges = . false . call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) !  It is wasteful to write over and over H and S, as there are !  no different files. ! Save Hamiltonian and overlap matrices ............................ ! Only in HSX format now.  Use Util/HSX/hsx2hs to generate an HS file if ( savehs . or . write_coop ) then call write_hsx ( gamma , no_u , no_s , spin % H , indxuo , & maxnh , numh , listhptr , listh , H , S , qtot , & temp , xijo ) endif call timer ( 'setup_H' , 2 ) #ifdef SIESTA__PEXSI if ( node == 0 ) call memory_snapshot ( \"after setup_H\" ) #endif if ( h_setup_only ) then call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call bye ( \"H-Setup-Only requested\" ) STOP endif !------------------------------------------------------------------------- END END subroutine setup_hamiltonian END module m_setup_hamiltonian","tags":"","loc":"sourcefile/setup_hamiltonian.f.html"},{"title":"m_mixing_scf.F90 – SIESTA","text":"This file depends on sourcefile~~m_mixing_scf.f90~~EfferentGraph sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_mixing_scf.f90~~AfferentGraph sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~state_init.f state_init.F sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~siesta_forces.f90->sourcefile~state_init.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_mixing_scf Source Code m_mixing_scf.F90 Source Code ! Also the mixing container module m_mixing_scf use class_Fstack_dData1D use m_mixing , only : tMixer implicit none private save type ( tMixer ), pointer :: scf_mixs (:) => null () type ( tMixer ), pointer :: scf_mix => null () ! Default mixing, no discrepancy between spin-components integer , parameter :: MIX_SPIN_ALL = 1 ! Only use spinor components for mixing integer , parameter :: MIX_SPIN_SPINOR = 2 ! Only use spin-sum for mixing (implicit on spinor) integer , parameter :: MIX_SPIN_SUM = 3 ! Use both spin-sum and spin-difference density for mixing (implicit on spinor) integer , parameter :: MIX_SPIN_SUM_DIFF = 4 ! It makes little sense to only mix difference as for spin-polarised ! calculations with no difference it will converge immediately ! How the spin mixing algorthim is chosen integer :: mix_spin = MIX_SPIN_ALL public :: scf_mixs , scf_mix public :: mix_spin public :: MIX_SPIN_ALL , MIX_SPIN_SPINOR , MIX_SPIN_SUM , MIX_SPIN_SUM_DIFF public :: mixers_scf_init public :: mixers_scf_print , mixers_scf_print_block public :: mixers_scf_history_init public :: mixers_scf_reset public :: mixing_scf_converged contains subroutine mixers_scf_init ( nspin , Comm ) use fdf use precision , only : dp #ifdef MPI use mpi_siesta , only : MPI_Comm_World #endif use m_mixing , only : mixers_reset , mixers_init use m_mixing , only : mix_method , mix_method_variant use m_mixing , only : mixer_init use m_mixing , only : mixers_history_init ! The number of spin-components integer , intent ( in ) :: nspin ! The communicator used for the mixer integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf ! Get number of history steps integer :: n_hist , n_kick , n_restart , n_save real ( dp ) :: w , w_kick integer :: n_lin_after real ( dp ) :: w_lin_after logical :: lin_after ! number of history steps saved type ( tMixer ), pointer :: m integer :: nm , im , im2 , tmp logical :: is_broyden character ( len = 70 ) :: method , variant , opt ! If the mixers are denoted by a block, then ! the entire logic *MUST* be defined in the blocks opt = fdf_get ( 'SCF.Mix.Spin' , 'all' ) if ( leqi ( opt , 'all' ) ) then mix_spin = MIX_SPIN_ALL else if ( leqi ( opt , 'spinor' ) ) then mix_spin = MIX_SPIN_SPINOR else if ( leqi ( opt , 'sum' ) ) then mix_spin = MIX_SPIN_SUM else if ( leqi ( opt , 'sum+diff' ) ) then mix_spin = MIX_SPIN_SUM_DIFF else call die ( \"Unknown option given for SCF.Mix.Spin & &all|spinor|sum|sum+diff\" ) end if ! If there is only one spinor we should mix all... if ( nspin == 1 ) mix_spin = MIX_SPIN_ALL ! Initialize to ensure debug stuff read call mixers_init ( 'SCF' , scf_mixs , Comm = Comm ) ! Check for existance of the SCF.Mix block if ( associated ( scf_mixs ) ) then if ( size ( scf_mixs ) > 0 ) then return end if ! Something has gone wrong... ! The user has supplied a block, but ! haven't added any content to the block... ! However, we fall-back to the default mechanism end if ! ensure nullification call mixers_reset ( scf_mixs ) ! >>>*** FIRST ***<<< ! Read in compatibility options ! Figure out if we are dealing with ! Broyden or Pulay n_hist = fdf_get ( 'DM.NumberPulay' , 2 ) tmp = fdf_get ( 'DM.NumberBroyden' , 0 ) is_broyden = tmp > 0 if ( is_broyden ) then n_hist = tmp end if ! Define default mixing weight (used for ! Pulay, Broyden and linear mixing) w = fdf_get ( 'DM.MixingWeight' , 0.25_dp ) ! Default kick-options n_kick = fdf_get ( 'DM.NumberKick' , 0 ) w_kick = fdf_get ( 'DM.KickMixingWeight' , 0.5_dp ) lin_after = fdf_get ( 'SCF.LinearMixingAfterPulay' , . false .) w_lin_after = fdf_get ( 'SCF.MixingWeightAfterPulay' , w ) ! >>>*** END ***<<< ! Read options in new format ! Get history length n_hist = fdf_get ( 'SCF.Mixer.History' , n_hist ) ! update mixing weight and kick mixing weight w = fdf_get ( 'SCF.Mixer.Weight' , w ) n_kick = fdf_get ( 'SCF.Mixer.Kick' , n_kick ) w_kick = fdf_get ( 'SCF.Mixer.Kick.Weight' , w_kick ) ! Restart after this number of iterations n_restart = fdf_get ( 'SCF.Mixer.Restart' , 0 ) n_save = fdf_get ( 'SCF.Mixer.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Get the variant of the mixing method if ( is_broyden ) then method = 'Broyden' else if ( n_hist > 0 ) then method = 'Pulay' else method = 'Linear' end if method = fdf_get ( 'SCF.Mixer.Method' , trim ( method )) variant = fdf_get ( 'SCF.Mixer.Variant' , 'original' ) ! Determine whether linear mixing should be ! performed after the \"advanced\" mixing n_lin_after = fdf_get ( 'SCF.Mixer.Linear.After' , - 1 ) w_lin_after = fdf_get ( 'SCF.Mixer.Linear.After.Weight' , w_lin_after ) ! Determine total number of mixers nm = 1 if ( n_lin_after >= 0 . or . lin_after ) nm = nm + 1 if ( n_kick > 0 ) nm = nm + 1 ! Initiailaze all mixers allocate ( scf_mixs ( nm )) scf_mixs (:)% w = w scf_mixs (:)% n_hist = n_hist scf_mixs (:)% restart = n_restart scf_mixs (:)% restart_save = n_save ! 1. Current mixing index im = 1 ! Store the advanced mixer index (for references to ! later mixers) im2 = im m => scf_mixs ( im ) m % name = method m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! 2. Setup the linear mixing after the actual mixing if ( n_lin_after > 0 . or . lin_after ) then im = im + 1 m => scf_mixs ( im ) ! Signal to switch to this mixer after ! convergence scf_mixs ( im2 )% next_conv => m m % name = 'Linear-After' m % m = mix_method ( 'linear' ) m % w = w_lin_after m % n_itt = n_lin_after ! jump back to previous after having run a ! few iterations m % next => scf_mixs ( im2 ) end if ! In case we have a kick, apply the kick here ! This overrides the \"linear.after\" option if ( n_kick > 0 ) then im = im + 1 m => scf_mixs ( im ) m % name = 'Linear-Kick' m % n_itt = 1 m % n_hist = 0 m % m = mix_method ( 'linear' ) m % w = w_kick m % next => scf_mixs ( im2 ) ! set the default mixer to kick scf_mixs ( im2 )% n_itt = n_kick - 1 scf_mixs ( im2 )% next => m scf_mixs ( im2 )% restart = n_kick - 1 end if ! Correct the input do im = 1 , nm call mixer_init ( scf_mixs ( im ) ) end do ! Initialize the allocation of each mixer call mixers_history_init ( scf_mixs ) #ifdef MPI if ( present ( Comm ) ) then scf_mixs (:)% Comm = Comm else scf_mixs (:)% Comm = MPI_Comm_World end if #endif end subroutine mixers_scf_init subroutine mixers_scf_print ( nspin ) use parallel , only : IONode use m_mixing , only : mixers_print integer , intent ( in ) :: nspin ! Print mixing options call mixers_print ( 'SCF' , scf_mixs ) if ( IONode . and . nspin > 1 ) then select case ( mix_spin ) case ( MIX_SPIN_ALL ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'all' case ( MIX_SPIN_SPINOR ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'spinor' if ( nspin <= 2 ) then call die ( \"SCF.Mixer.Spin spinor option only valid for & &non-collinear and spin-orbit calculations\" ) end if case ( MIX_SPIN_SUM ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum' case ( MIX_SPIN_SUM_DIFF ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum and diff' end select end if end subroutine mixers_scf_print subroutine mixers_scf_print_block ( ) use m_mixing , only : mixers_print_block ! Print mixing options call mixers_print_block ( 'SCF' , scf_mixs ) end subroutine mixers_scf_print_block subroutine mixing_scf_converged ( SCFconverged ) use parallel , only : IONode logical , intent ( inout ) :: SCFconverged integer :: i ! Return if no convergence if ( . not . SCFconverged ) return if ( associated ( scf_mix % next_conv ) ) then ! this means that we skip to the ! following algorithm scf_mix => scf_mix % next_conv SCFconverged = . false . if ( allocated ( scf_mix % stack ) ) then do i = 1 , size ( scf_mix % stack ) ! delete all but one history ! This should be fine call reset ( scf_mix % stack ( i ), - 1 ) end do end if if ( IONode ) then write ( * , '(/,2a)' ) ':!: SCF cycle continuation mixer: ' , & trim ( scf_mix % name ) end if end if end subroutine mixing_scf_converged subroutine mixers_scf_reset () use m_mixing , only : mixers_reset nullify ( scf_mix ) call mixers_reset ( scf_mixs ) end subroutine mixers_scf_reset subroutine mixers_scf_history_init ( ) use m_mixing , only : mixers_history_init call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) end subroutine mixers_scf_history_init end module m_mixing_scf","tags":"","loc":"sourcefile/m_mixing_scf.f90.html"},{"title":"m_mixing.F90 – SIESTA","text":"Files dependent on this one sourcefile~~m_mixing.f90~~AfferentGraph sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~state_init.f state_init.F sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~siesta_forces.f90->sourcefile~state_init.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_mixing Source Code m_mixing.F90 Source Code ! Module for all mixing methods in a standard way ! This module implements mixing of the Pulay and Broyden ! type. ! The Pulay method is implemented in the fast calculation ! setup and in the stable method. ! The stable method is executed if the inversion fails. !  - Stable: G.Kresse and J.Furthmuller, Comp. Mat. Sci. 6, 15, 1996 !  - gr (guarenteed-reduction) : http://arxiv.org/pdf/cond-mat/0005521.pdf ! All implemented methods employ a restart with variable ! history saving. module m_mixing use precision , only : dp #ifdef MPI ! MPI stuff use mpi_siesta #endif ! Intrinsic classes for retaining history use class_dData1D use class_Fstack_dData1D implicit none private save integer , parameter :: MIX_LINEAR = 1 integer , parameter :: MIX_PULAY = 2 integer , parameter :: MIX_BROYDEN = 3 integer , parameter :: MIX_FIRE = 4 ! Action tokens (binary: 0, 1, 2, 4, 8, ...!) integer , parameter :: ACTION_MIX = 0 integer , parameter :: ACTION_RESTART = 1 integer , parameter :: ACTION_NEXT = 2 type tMixer ! Name of mixer character ( len = 24 ) :: name ! The different saved variables per iteration ! and their respective stacks type ( Fstack_dData1D ), allocatable :: stack (:) ! The method of the mixer integer :: m = MIX_PULAY ! In case the mixing method has a variant ! this denote the variant ! This value is thus specific for each method integer :: v = 0 ! The currently reached iteration integer :: cur_itt = 0 , start_itt = 0 ! Different mixers may have different histories integer :: n_hist = 2 ! Number of iterations using this mixer ! There are a couple of signals here !  == 0 : !     only use this mixer until convergence !   > 0 : !     after having runned n_itt step to \"next\" integer :: n_itt = 0 ! When mod(cur_itt,restart_itt) == 0 the history will ! be _reset_ integer :: restart = 0 integer :: restart_save = 0 ! This is an action token specifying the current ! action integer :: action = ACTION_MIX ! The next mixing method following this method type ( tMixer ), pointer :: next => null () ! The next mixing method following this method ! Only used if mixing method achieved convergence ! using this method type ( tMixer ), pointer :: next_conv => null () ! ** Parameters specific for the method: ! The mixing parameter used for this mixer real ( dp ) :: w = 0._dp ! linear array of real variables used specifically ! for this mixing type real ( dp ), pointer :: rv (:) => null () integer , pointer :: iv (:) => null () #ifdef MPI ! In case we have MPI the mixing scheme ! can implement a reduction scheme. ! This can be MPI_Comm_Self to not employ any ! reductions integer :: Comm = MPI_Comm_Self #endif end type tMixer ! Indices for special constanst integer , parameter :: I_PREVIOUS_RES = 0 integer , parameter :: I_P_RESTART = - 1 integer , parameter :: I_P_NEXT = - 2 ! This index should always be the lowest index ! This is used to allocate the correct bounds for the ! additional array of information integer , parameter :: I_SVD_COND = - 3 ! Debug mixing runs logical :: debug_mix = . false . ! In case of parallel mixing this also contains the node number character ( len = 20 ) :: debug_msg = 'mix:' public :: tMixer ! Routines are divided in three sections ! 1. Routines used to construct the mixers !    Routines used to print information regarding !    the mixers public :: mixers_init , mixer_init public :: mixers_print , mixers_print_block public :: mixers_history_init public :: mixers_reset ! 2. Public functions for retrieving information !    from external routines public :: mix_method , mix_method_variant ! 3. Actual mixing methods public :: mixing public :: MIX_LINEAR , MIX_FIRE , MIX_PULAY , MIX_BROYDEN interface mixing module procedure mixing_1d , mixing_2d end interface mixing contains !> Initialize a set of mixers by reading in fdf information. !! @param[in] prefix the fdf-label prefixes !! @param[pointer] mixers the mixers that are to be initialized !! @param[in] Comm @opt optional MPI-communicator subroutine mixers_init ( prefix , mixers , Comm ) use parallel , only : IONode , Node use fdf ! FDF-prefix for searching keywords character ( len =* ), intent ( in ) :: prefix ! The array of mixers (has to be nullified upon entry) type ( tMixer ), pointer :: mixers (:) integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf type ( parsed_line ), pointer :: pline ! number of history steps saved integer :: n_hist , n_restart , n_save real ( dp ) :: w integer :: nm , im , im2 character ( len = 10 ) :: lp character ( len = 70 ) :: method , variant ! Default mixing options... if ( fdf_get ( 'Mixer.Debug' ,. false .) ) then debug_mix = IONode debug_msg = 'mix:' end if if ( fdf_get ( 'Mixer.Debug.MPI' ,. false .) ) then debug_mix = . true . write ( debug_msg , '(a,i0,a)' ) 'mix (' , Node , '):' end if lp = trim ( prefix ) // '.Mixer' ! ensure nullification call mixers_reset ( mixers ) ! Return immediately if the user hasn't defined ! an fdf-block for the mixing options... if ( . not . fdf_block ( trim ( lp ) // 's' , bfdf ) ) return ! update mixing weight and kick mixing weight w = fdf_get ( trim ( lp ) // '.Weight' , 0.1_dp ) ! Get history length n_hist = fdf_get ( trim ( lp ) // '.History' , 6 ) ! Restart after this number of iterations n_restart = fdf_get ( trim ( lp ) // '.Restart' , 0 ) n_save = fdf_get ( trim ( lp ) // '.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Read in the options regarding the mixing options nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 end do if ( nm == 0 ) then call die ( 'mixing: No mixing schemes selected. & &Please at least add one mixer.' ) end if ! Allocate all denoted mixers... allocate ( mixers ( nm )) mixers (:)% w = w mixers (:)% n_hist = n_hist mixers (:)% restart = n_restart mixers (:)% restart_save = n_save ! Rewind to grab names. call fdf_brewind ( bfdf ) nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 mixers ( nm )% name = fdf_bnames ( pline , 1 ) end do ! Now read all mixers for this segment and their options do im = 1 , nm call read_block ( mixers ( im ) ) end do ! Create history stack and associate correct ! stack pointers call mixers_history_init ( mixers ) #ifdef MPI if ( present ( Comm ) ) then mixers (:)% Comm = Comm else mixers (:)% Comm = MPI_Comm_World end if #endif contains subroutine read_block ( m ) type ( tMixer ), intent ( inout ), target :: m character ( len = 64 ) :: opt ! create block string opt = trim ( lp ) // '.' // trim ( m % name ) if ( . not . fdf_block ( opt , bfdf ) ) then call die ( 'Block: ' // trim ( opt ) // ' does not exist!' ) end if ! Default to the pulay method... ! This enables NOT writing this in the block method = 'pulay' variant = ' ' ! read method do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'method' ) ) then method = fdf_bnames ( pline , 2 ) else if ( leqi ( opt , 'variant' ) ) then variant = fdf_bnames ( pline , 2 ) end if end do ! Retrieve the method and the variant m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! Define separate defaults which are ! not part of the default input options select case ( m % m ) case ( MIX_LINEAR ) m % n_hist = 0 end select call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'iterations' ) & . or . leqi ( opt , 'itt' ) ) then m % n_itt = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'history' ) ) then m % n_hist = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'weight' ) . or . leqi ( opt , 'w' ) ) then m % w = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'restart' ) ) then m % restart = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'restart.save' ) ) then m % restart_save = fdf_bintegers ( pline , 1 ) m % restart_save = max ( 0 , m % restart_save ) end if end do ! Initialize the mixer by setting the correct ! standard options and allocate space in the mixers... call mixer_init ( m ) ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'next' ) ) then nullify ( m % next ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next ) ) then call die ( 'mixing: Could not find next mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next , target = m ) ) then call die ( 'mixing: Next *must* not be it-self. & &Please change accordingly.' ) end if else if ( leqi ( opt , 'next.conv' ) ) then nullify ( m % next_conv ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next_conv => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next_conv ) ) then call die ( 'mixing: Could not find next convergence mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next_conv , target = m ) ) then call die ( 'mixing: next.conv *must* not be it-self. & &Please change accordingly.' ) end if end if end do ! Ensure that if a next have not been specified ! it will continue indefinitely. if ( . not . associated ( m % next ) ) then m % n_itt = 0 end if ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) ! skip lines without associated content if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) ! Do options so that a pulay option may refer to ! the actual names of the constants if ( m % m == MIX_PULAY ) then ! The linear mixing weight if ( leqi ( opt , 'weight.linear' ) & . or . leqi ( opt , 'w.linear' ) ) then m % rv ( 1 ) = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'svd.cond' ) ) then ! This is only applicable to the Pulay ! mixing scheme... m % rv ( I_SVD_COND ) = fdf_bvalues ( pline , 1 ) end if end if ! Generic options for all advanced methods... if ( leqi ( opt , 'next.p' ) ) then ! Only allow stepping to the next when ! having a next associated if ( associated ( m % next ) ) then m % rv ( I_P_NEXT ) = fdf_bvalues ( pline , 1 ) end if else if ( leqi ( opt , 'restart.p' ) ) then m % rv ( I_P_RESTART ) = fdf_bvalues ( pline , 1 ) end if end do end subroutine read_block end subroutine mixers_init !> Initialize a single mixer depending on the preset !! options. Useful for external correct setup. !! !! @param[inout] mix mixer to be initialized subroutine mixer_init ( mix ) type ( tMixer ), intent ( inout ) :: mix integer :: n ! Correct amount of history in the mixing. if ( 0 < mix % restart . and . & mix % restart < mix % n_hist ) then ! This is if we restart this scheme, ! then it does not make sense to have a history ! greater than the restart count mix % n_hist = mix % restart end if if ( 0 < mix % n_itt . and . & mix % n_itt < mix % n_hist ) then ! If this only runs for n_itt itterations, ! it makes no sense to have a history greater ! than this. mix % n_hist = mix % n_itt end if select case ( mix % m ) case ( MIX_LINEAR ) allocate ( mix % rv ( I_SVD_COND : 0 )) ! Kill any history settings that do not apply to the ! linear mixer. mix % restart = 0 mix % restart_save = 0 case ( MIX_PULAY ) allocate ( mix % rv ( I_SVD_COND : 1 )) mix % rv ( 1 ) = mix % w ! We allocate the double residual (n_hist-1) mix % n_hist = max ( 2 , mix % n_hist ) if ( mix % v == 1 . or . mix % v == 3 ) then ! The GR method requires an even number ! of restart steps ! And then we ensure the history to be aligned ! with a restart (restart has precedence) mix % restart = mix % restart + mod ( mix % restart , 2 ) end if case ( MIX_BROYDEN ) ! allocate temporary array mix % n_hist = max ( 2 , mix % n_hist ) n = 1 + mix % n_hist allocate ( mix % rv ( I_SVD_COND : n )) mix % rv ( 1 : n ) = mix % w end select if ( mix % restart < 0 ) then call die ( 'mixing: restart count must be positive' ) end if mix % restart_save = min ( mix % n_hist - 1 , mix % restart_save ) mix % restart_save = max ( 0 , mix % restart_save ) ! This is the restart parameter ! I.e. if |f_k / f - 1| < rp ! only works for positive rp mix % rv ( I_PREVIOUS_RES ) = huge ( 1._dp ) mix % rv ( I_P_RESTART ) = - 1._dp mix % rv ( I_P_NEXT ) = - 1._dp mix % rv ( I_SVD_COND ) = 1.e-8_dp end subroutine mixer_init !> Initialize all history for the mixers !! !! Routine for clearing all history and setting up the !! arrays so that they may be used subsequently. !! !! @param[inout] mixers the mixers to be initialized subroutine mixers_history_init ( mixers ) type ( tMixer ), intent ( inout ), target :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns logical :: is_GR do im = 1 , size ( mixers ) m => mixers ( im ) if ( debug_mix . and . current_itt ( m ) >= 1 ) then write ( * , '(a,a)' ) trim ( debug_msg ), & ' resetting history of all mixers' exit end if end do ! Clean up all arrays and reference counted ! objects do im = 1 , size ( mixers ) m => mixers ( im ) ! reset history track m % start_itt = 0 m % cur_itt = 0 ! do not try and de-allocate something not ! allocated if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do ! clean-up deallocate ( m % stack ) end if ! Re-populate select case ( m % m ) case ( MIX_LINEAR ) ! do nothing case ( MIX_PULAY ) is_GR = ( m % v == 1 ) . or . ( m % v == 3 ) if ( . not . is_GR ) then allocate ( m % stack ( 3 )) else allocate ( m % stack ( 2 )) end if ! These arrays contains these informations !   s1 = m%stack(1) !   s2 = m%stack(2) !   s3 = m%stack(3) ! Here <> is input function, x[in], and ! <>' is the corresponding output, x[out]. ! First iteration: !   s1 = { 1' - 1 } !   s3 = { 1' } ! Second iteration !   s2 = { 2' - 2 - (1' - 1) } !   s1 = { 2 - 1 , 2' - 2 } !   s3 = { 2' } ! Third iteration !   s2 = { 2' - 2 - (1' - 1) , 3' - 3 - (2' - 2) } !   s1 = { 2 - 1 , 3 - 2, 3' - 3 } !   s3 = { 3' } ! and so on ! allocate x[i+1] - x[i] call new ( m % stack ( 1 ), m % n_hist ) ! allocate F[i+1] - F[i] call new ( m % stack ( 2 ), m % n_hist - 1 ) if ( . not . is_GR ) then call new ( m % stack ( 3 ), 1 ) end if case ( MIX_BROYDEN ) ! Same as original Pulay allocate ( m % stack ( 3 )) call new ( m % stack ( 1 ), m % n_hist ) call new ( m % stack ( 2 ), m % n_hist - 1 ) call new ( m % stack ( 3 ), 1 ) end select end do end subroutine mixers_history_init !> Reset the mixers, i.e. clean _everything_ !! !! Also deallocates (and nullifies) the input array! !! !! @param[inout] mixers array of mixers to be cleaned subroutine mixers_reset ( mixers ) type ( tMixer ), pointer :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns if ( . not . associated ( mixers ) ) return do im = 1 , size ( mixers ) m => mixers ( im ) if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do deallocate ( m % stack ) end if if ( associated ( m % rv ) ) then deallocate ( m % rv ) nullify ( m % rv ) end if if ( associated ( m % iv ) ) then deallocate ( m % iv ) nullify ( m % iv ) end if end do deallocate ( mixers ) nullify ( mixers ) end subroutine mixers_reset !> Print (to std-out) information regarding the mixers !! !! @param[in] prefix the prefix (fdf) for the mixers !! @param[in] mixers array of mixers allocated subroutine mixers_print ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m character ( len = 50 ) :: fmt logical :: bool integer :: i if ( . not . IONode ) return fmt = 'mix.' // trim ( prefix ) // ':' if ( debug_mix ) then write ( * , '(2a,t50,''= '',l)' ) trim ( fmt ), & ' Debug messages' , debug_mix end if ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Linear mixing' , trim ( m % name ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w if ( m % n_hist > 0 . and . (& associated ( m % next ) & . or . associated ( m % next_conv )) ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Carried history steps' , m % n_hist end if case ( MIX_PULAY ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Pulay mixing' , trim ( m % name ) select case ( m % v ) case ( 0 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable' case ( 1 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR' case ( 2 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable-SVD' case ( 3 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR-SVD' end select write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Linear mixing weight' , m % rv ( 1 ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w write ( * , '(2a,t50,''= '',e10.4)' ) trim ( fmt ), & '    SVD condition' , m % rv ( I_SVD_COND ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_BROYDEN ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Broyden mixing' , trim ( m % name ) !write(*,'(2a,t50,''= '',a)') trim(fmt), & !     '    Variant','original' write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Jacobian weight' , m % w write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Weight prime' , m % rv ( 1 ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_FIRE ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Fire mixing' , trim ( m % name ) end select if ( m % n_itt > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Number of mixing iterations' , m % n_itt if ( associated ( m % next ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer' , trim ( m % next % name ) else call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if end if if ( associated ( m % next_conv ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer upon convergence' , trim ( m % next_conv % name ) end if end do end subroutine mixers_print !> Print (to std-out) the fdf-blocks that recreate the mixer settings !! !! @param[in] prefix the fdf-prefix for reading the blocks !! @param[in] mixers array of mixers that should be printed !!    their fdf-blocks subroutine mixers_print_block ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m logical :: bool integer :: i if ( . not . IONode ) return ! Write block of input write ( * , '(/3a)' ) '%block ' , trim ( prefix ), '.Mixers' do i = 1 , size ( mixers ) m => mixers ( i ) write ( * , '(t3,a)' ) trim ( m % name ) end do write ( * , '(3a)' ) '%endblock ' , trim ( prefix ), '.Mixers' ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) ! Write out this block write ( * , '(/4a)' ) '%block ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) write ( * , '(t3,a)' ) '# Mixing method' ! Write out method select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'linear' case ( MIX_PULAY ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'pulay' select case ( m % v ) case ( 0 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable' case ( 1 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR' case ( 2 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable+SVD' case ( 3 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR+SVD' end select case ( MIX_BROYDEN ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'broyden' ! currently no variants exists end select ! remark write ( * , '(/,t3,a)' ) '# Mixing options' ! Weight ! For Broyden this is the inverse Jacobian write ( * , '(t3,a,f6.4)' ) 'weight ' , m % w select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) write ( * , '(t3,a,f6.4)' ) 'weight.linear ' , m % rv ( 1 ) end select if ( m % n_hist > 0 ) then write ( * , '(t3,a,i0)' ) 'history ' , m % n_hist end if bool = . false . if ( m % restart > 0 ) then write ( * , '(t3,a,i0)' ) 'restart ' , m % restart bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(t3,a,e10.5)' ) 'restart.p ' , m % rv ( I_P_RESTART ) bool = . true . end if end select if ( bool ) then write ( * , '(t3,a,i0)' ) 'restart.save ' , m % restart_save end if ! remark bool = . false . if ( m % n_itt > 0 ) then write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,i0)' ) 'iterations ' , m % n_itt bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,f6.4)' ) 'next.p ' , m % rv ( I_P_NEXT ) bool = . true . end if end select if ( bool . and . associated ( m % next ) ) then write ( * , '(t2,2(tr1,a))' ) 'next' , trim ( m % next % name ) else if ( bool ) then call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if if ( associated ( m % next_conv ) ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t2,2(tr1,a))' ) 'next.conv' , trim ( m % next_conv % name ) end if write ( * , '(4a)' ) '%endblock ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) end do write ( * , * ) ! new-line end subroutine mixers_print_block !> Return the integer specification of the mixing type !! !! @param[in] str the character representation of the mixing type !! @return the integer corresponding to the mixing type function mix_method ( str ) result ( m ) use fdf , only : leqi character ( len =* ), intent ( in ) :: str integer :: m if ( leqi ( str , 'linear' ) ) then m = MIX_LINEAR else if ( leqi ( str , 'pulay' ) . or . & leqi ( str , 'diis' ) . or . & leqi ( str , 'anderson' ) ) then m = MIX_PULAY else if ( leqi ( str , 'broyden' ) ) then m = MIX_BROYDEN else if ( leqi ( str , 'fire' ) ) then m = MIX_FIRE call die ( 'mixing: FIRE currently not supported.' ) else call die ( 'mixing: Unknown mixing variant.' ) end if end function mix_method !> Return the variant of the mixing method !! !! @param[in] m the integer type of the mixing method !! @param[in] str the character specification of the mixing method variant !! @return the variant of the mixing method function mix_method_variant ( m , str ) result ( v ) use fdf , only : leqi integer , intent ( in ) :: m character ( len =* ), intent ( in ) :: str integer :: v v = 0 select case ( m ) case ( MIX_LINEAR ) ! no variants case ( MIX_PULAY ) v = 0 ! We do not implement tho non-stable version ! There is no need to have an inferior Pulay mixer... if ( leqi ( str , 'original' ) . or . & leqi ( str , 'kresse' ) . or . leqi ( str , 'stable' ) ) then ! stable version, will nearly always succeed on inversion v = 0 else if ( leqi ( str , 'original+svd' ) . or . & leqi ( str , 'kresse+svd' ) . or . leqi ( str , 'stable+svd' ) ) then ! stable version, will nearly always succeed on inversion v = 2 else if ( leqi ( str , 'gr' ) . or . & leqi ( str , 'guarenteed-reduction' ) . or . & leqi ( str , 'bowler-gillan' ) ) then ! Guarenteed reduction version v = 1 else if ( leqi ( str , 'gr+svd' ) . or . & leqi ( str , 'guarenteed-reduction+svd' ) . or . & leqi ( str , 'bowler-gillan+svd' ) ) then ! Guarenteed reduction version v = 3 end if case ( MIX_BROYDEN ) ! Currently only one variant v = 0 case ( MIX_FIRE ) ! no variants end select end function mix_method_variant ! The basic mixing procedure is this: ! 1. Initialize the mixing algorithm !    This will typically mean that one needs to !    push input and output matrices ! 2. Calculate the mixing coefficients ! 3. Use coefficients to calculate the !    next (optimized) guess ! 4. Finalize the mixing method ! ! Having the routines split up in this manner ! allows one to skip step 2 and use coefficients ! from another set of input/output to retrieve the ! mixing coefficients. ! Say we may retrieve mixing coefficients from ! the Hamiltonian, but use them for the density-matrix !> Initialize the mixing algorithm !! !! @param[pointer] mix the mixing method !! @param[in] n size of the arrays to be used in the algorithm !! @param[in] xin array of the input variables !! @param[in] xout array of the output variables subroutine mixing_init ( mix , n , xin , F ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n ! In/out of the function real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), pointer :: res (:), rres (:) integer :: i , ns real ( dp ) :: dnorm , dtmp logical :: p_next , p_restart ! Initialize action for mixer mix % action = ACTION_MIX ! Step iterator (so first mixing has cur_itt == 1) mix % cur_itt = mix % cur_itt + 1 ! If we are going to skip to next, we signal it ! before entering if ( mix % n_itt > 0 . and . & mix % n_itt <= current_itt ( mix ) ) then mix % action = IOR ( mix % action , ACTION_NEXT ) end if ! Check whether the residual norm is below a certain ! criteria p_next = mix % rv ( I_P_NEXT ) > 0._dp p_restart = mix % rv ( I_P_RESTART ) > 0._dp ! Check whether a parameter next/restart is required if ( p_restart . or . p_next ) then ! Calculate norm: ||f_k|| dnorm = norm ( n , F , F ) #ifdef MPI dtmp = dnorm call MPI_AllReduce ( dtmp , dnorm , 1 , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) #endif ! Calculate the relative difference dtmp = abs ( dnorm / mix % rv ( I_PREVIOUS_RES ) - 1._dp ) ! We first check for next, that has precedence if ( p_next ) then if ( dtmp < mix % rv ( I_P_NEXT ) ) then ! Signal stepping mixer mix % action = IOR ( mix % action , ACTION_NEXT ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < np  :  ' , & dtmp , ' < ' , mix % rv ( I_P_NEXT ) end if if ( p_restart ) then if ( dtmp < mix % rv ( I_P_RESTART ) ) then ! Signal restart mix % action = IOR ( mix % action , ACTION_RESTART ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < rp  :  ' , & dtmp , ' < ' , mix % rv ( I_P_RESTART ) end if ! Store the new residual norm mix % rv ( I_PREVIOUS_RES ) = dnorm end if ! Push information to the stack select case ( mix % m ) case ( MIX_LINEAR ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' linear' call init_linear () case ( MIX_PULAY ) if ( debug_mix ) then select case ( mix % v ) case ( 0 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay' case ( 1 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay, GR' case ( 2 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD' case ( 3 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD, GR' end select end if call init_pulay () case ( MIX_BROYDEN ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' Broyden' call init_broyden () end select contains subroutine init_linear () ! information for this depends on the ! following method call fake_history_from_linear ( mix % next ) call fake_history_from_linear ( mix % next_conv ) end subroutine init_linear subroutine init_pulay () logical :: GR_linear select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do end if case ( 1 , 3 ) ! Whether this is the linear cycle... GR_linear = mod ( current_itt ( mix ), 2 ) == 1 ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F , mix % rv ( 1 )) ns = n_items ( mix % stack ( 1 )) if ( GR_linear . and . current_itt ( mix ) > 1 . and . & ns > 1 ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = rres ( i ) + res ( i ) end do !$OMP end parallel do else if ( ns > 1 . and . . not . GR_linear ) then ! now we can calculate RRes[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) end if end select end subroutine init_pulay subroutine init_broyden () ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do else ! Store F[x_in] (used to create the input residual) call push_stack_data ( mix % stack ( 3 ), n ) res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if end subroutine init_broyden subroutine fake_history_from_linear ( next ) type ( tMixer ), pointer :: next real ( dp ), pointer :: t1 (:), t2 (:) integer :: ns , nh , i , nhl if ( . not . associated ( next ) ) return ! Reduce to # history of linear nhl = mix % n_hist ! if the number of fake-history steps saved is ! zero we immediately return. ! Only if mix%n_hist > 0 will the below ! occur. if ( nhl == 0 ) return ! Check for the type of following method select case ( next % m ) case ( MIX_PULAY ) ! Here it depends on the variant select case ( next % v ) case ( 0 , 2 ) ! stable pulay mixing ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select case ( MIX_BROYDEN ) ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns >= 2 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select end subroutine fake_history_from_linear end subroutine mixing_init !> Function to retrieve the number of coefficients !! calculated in this iteration. !! This is so external routines can query the size !! of the arrays used. !! !! @param[in] mix the used mixer function mixing_ncoeff ( mix ) result ( n ) type ( tMixer ), intent ( in ) :: mix integer :: n n = 0 select case ( mix % m ) case ( MIX_PULAY ) n = n_items ( mix % stack ( 2 )) case ( MIX_BROYDEN ) n = n_items ( mix % stack ( 2 )) end select end function mixing_ncoeff !> Calculate the mixing coefficients for the !! current mixer !! !! @param[in] mix the current mixer !! @param[in] n the number of elements used to calculate !!           the coefficients !! @param[in] xin the input value !! @param[in] F xout - xin, (residual) !! @param[out] coeff the coefficients subroutine mixing_coeff ( mix , n , xin , F , coeff ) use parallel , only : IONode type ( tMixer ), intent ( inout ) :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), intent ( out ) :: coeff (:) integer :: ncoeff ncoeff = size ( coeff ) if ( ncoeff < mixing_ncoeff ( mix ) ) then write ( * , '(a)' ) 'mix: Error in calculating coefficients' ! Do not allow this... return end if select case ( mix % m ) case ( MIX_LINEAR ) call linear_coeff () case ( MIX_PULAY ) call pulay_coeff () case ( MIX_BROYDEN ) call broyden_coeff () end select contains subroutine linear_coeff () integer :: i do i = 1 , ncoeff coeff ( i ) = 0._dp end do end subroutine linear_coeff subroutine pulay_coeff () integer :: ns , nh , nmax integer :: i , j , info logical :: lreturn ! Calculation quantities real ( dp ) :: dnorm , G real ( dp ), pointer :: res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) lreturn = . false . ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) return ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! Allocate arrays for calculating the ! coefficients allocate ( A ( nh , nh ), Ainv ( nh , nh )) ! Calculate A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = norm(RRes[i],RRes[j]) A ( i , j ) = norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Diagonal A ( i , i ) = norm ( n , rres1 , rres1 ) end do #ifdef MPI ! Global operations, but only for the non-extended entries call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) ! copy over reduced arrays A = Ainv #endif ! Get inverse of matrix select case ( mix % v ) case ( 0 , 1 ) call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if case ( 2 , 3 ) ! We forcefully use the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end select ! NOTE, although mix%stack(1) contains ! the x[i] - x[i-1], the tip of the stack ! contains F[i]! ! res == F[i] res => getstackval ( mix , 1 ) ! Initialize the coefficients do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients on all processors do j = 1 , nh ! res  == F[i] ! rres == F[j+1] - F[j] rres => getstackval ( mix , 2 , j ) dnorm = norm ( n , rres , res ) do i = 1 , nh coeff ( i ) = coeff ( i ) - Ainv ( i , j ) * dnorm end do end do #ifdef MPI ! Reduce the coefficients call MPI_AllReduce ( coeff ( 1 ), A ( 1 , 1 ), nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh coeff ( i ) = A ( i , 1 ) end do #endif else info = 0 ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, SVD failed, > linear' end if ! Clean up memory deallocate ( A , Ainv ) end subroutine pulay_coeff subroutine broyden_coeff () integer :: ns , nh , nmax integer :: i , j , k , info ! Calculation quantities real ( dp ) :: dnorm , dtmp real ( dp ), pointer :: w (:), res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: c (:), A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) ! Easy check for initial step... if ( ns == 1 ) then ! reset coeff = 0._dp return end if ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! This is the modified Broyden algorithm... ! Retrieve the previous weights w => mix % rv ( 2 : 1 + nh ) select case ( mix % v ) case ( 2 ) ! Unity Broyden w ( nh ) = 1._dp case ( 1 ) ! RMS Broyden dnorm = norm ( n , F , F ) #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif w (:) = 1._dp / sqrt ( dnorm ) if ( debug_mix ) & write ( * , '(2(a,e10.4))' ) & trim ( debug_msg ) // ' weight = ' , w ( 1 ), & ' , norm = ' , dnorm case ( 0 ) ! Varying weight dnorm = 0._dp !$OMP parallel do default(shared), private(i), & !$OMP& reduction(max:dnorm) do i = 1 , n dnorm = max ( dnorm , abs ( F ( i )) ) end do !$OMP end parallel do #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif ! Problay 0.2 should be changed to user-defined w ( nh ) = exp ( 1._dp / ( dnorm + 0.2_dp ) ) if ( debug_mix ) & write ( * , '(2a,1000(tr1,e10.4))' ) & trim ( debug_msg ), ' weights = ' , w ( 1 : nh ) end select ! Allocate arrays used allocate ( c ( nh )) allocate ( A ( nh , nh ), Ainv ( nh , nh )) !  < RRes[i] | Res[n] > do i = 1 , nh rres => getstackval ( mix , 2 , i ) c ( i ) = norm ( n , rres , F ) end do #ifdef MPI call MPI_AllReduce ( c ( 1 ), A ( 1 , 1 ), nh , & MPI_Double_Precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh c ( i ) = A ( i , 1 ) end do #endif ! Create A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = dot_product(RRes[i],RRes[j]) A ( i , j ) = w ( i ) * w ( j ) * norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Do the diagonal term A ( i , i ) = w ( i ) * w ( i ) * norm ( n , rres1 , rres1 ) end do #ifdef MPI call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) A = Ainv #endif ! Add the diagonal term ! This should also prevent it from being ! singular (unless mix%w == 0) do i = 1 , nh A ( i , i ) = mix % rv ( 1 ) ** 2 + A ( i , i ) end do ! Calculate the inverse call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients... do i = 1 , nh do j = 1 , nh ! Ainv should be symmetric (A is) coeff ( i ) = coeff ( i ) + w ( j ) * c ( j ) * Ainv ( j , i ) end do ! Calculate correct weight... coeff ( i ) = - w ( i ) * coeff ( i ) end do else ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, SVD failed, > linear' end if deallocate ( A , Ainv ) end subroutine broyden_coeff end subroutine mixing_coeff !> Calculate the guess for the next iteration !! !! Note this gets passed the coefficients. Hence, !! they may be calculated from another set of history !! steps. !! This may be useful in certain situations. !! !! @param[in] mix the current mixer !! @param[in] n the number of elements used to calculate !!           the coefficients !! @param[in] xin the input value !! @param[in] F the xin residual !! @param[out] xnext the input for the following iteration !! @param[in] coeff the coefficients subroutine mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ) real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( out ) :: xnext ( n ) real ( dp ), intent ( in ) :: coeff (:) select case ( mix % m ) case ( MIX_LINEAR ) call mixing_linear () case ( MIX_PULAY ) call mixing_pulay () case ( MIX_BROYDEN ) call mixing_broyden () end select contains subroutine mixing_linear () integer :: i real ( dp ) :: w w = mix % w if ( debug_mix ) write ( * , '(2a,e10.4)' ) & trim ( debug_msg ), ' alpha = ' , w !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + w * F ( i ) end do !$OMP end parallel do end subroutine mixing_linear subroutine mixing_pulay () integer :: ns , nh integer :: i , j logical :: lreturn real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Pulay' xnext = 0._dp return end if ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Pulay (initial), alpha = ' , mix % rv ( 1 ) case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Direct mixing, alpha = ' , mix % rv ( 1 ) end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) then ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ),& ' G = ' , G , ', sum(alpha) = ' , sum ( coeff ), & ', alpha = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_pulay subroutine mixing_broyden () integer :: ns , nh integer :: i , j real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Broyden' xnext = 0._dp return end if if ( ns == 1 ) then if ( debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Broyden (initial), alpha = ' , mix % rv ( 1 ) ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' G = ' , G , & ', sum(coeff) = ' , sum ( coeff ), & ', coeff = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_broyden end subroutine mixing_calc_next !> Finalize the mixing algorithm !! !! @param[inout] mix mixer to be finalized !! @param[in] n size of the input arrays !! @param[in] xin the input for this iteration !! @param[in] F the residual for this iteration !! @param[in] xnext the optimized input for the next iteration subroutine mixing_finalize ( mix , n , xin , F , xnext ) use parallel , only : IONode type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ), xnext ( n ) integer :: rsave select case ( mix % m ) case ( MIX_LINEAR ) call fin_linear () case ( MIX_PULAY ) call fin_pulay () case ( MIX_BROYDEN ) call fin_broyden () end select ! Fix the action to finalize it.. if ( mix % restart > 0 . and . & mod ( current_itt ( mix ), mix % restart ) == 0 ) then mix % action = IOR ( mix % action , ACTION_RESTART ) end if ! Check the actual finalization... ! First check whether we should restart history if ( IAND ( mix % action , ACTION_RESTART ) == ACTION_RESTART ) then ! The user has requested to restart the ! mixing scheme now rsave = mix % restart_save select case ( mix % m ) case ( MIX_PULAY ) if ( IONode ) then write ( * , '(a)' ) 'mix: Pulay -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if case ( MIX_BROYDEN ) if ( IONode ) then write ( * , '(a)' ) 'mix: Broyden -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if end select if ( allocated ( mix % stack ) ) then if ( debug_mix ) & write ( * , '(a,a,i0)' ) trim ( debug_msg ), & ' saved hist = ' , n_items ( mix % stack ( 1 )) end if end if ! check whether we should change the mixer if ( IAND ( mix % action , ACTION_NEXT ) == ACTION_NEXT ) then call mixing_step ( mix ) end if contains subroutine fin_linear () ! do nothing... end subroutine fin_linear subroutine fin_pulay () integer :: ns integer :: i logical :: GR_linear real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) select case ( mix % v ) case ( 0 , 2 ) ! stable Pulay if ( n_items ( mix % stack ( 3 )) == 0 ) then call push_stack_data ( mix % stack ( 3 ), n ) end if res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do case ( 1 , 3 ) ! GR Pulay GR_linear = mod ( current_itt ( mix ), 2 ) == 1 if ( n_items ( mix % stack ( 2 )) > 0 . and . & . not . GR_linear ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  rres == F[i] - F[i-1] rres ( i ) = rres ( i ) - res ( i ) ! Output: !  rres == - F[i-1] end do !$OMP end parallel do call pop ( mix % stack ( 1 )) ! Note that this is Res[i-1] = (F&#94;i-1_out - F&#94;i-1_in) res => getstackval ( mix , 1 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - xin ( i ) + xnext ( i ) end do !$OMP end parallel do end if end select end subroutine fin_pulay subroutine fin_broyden () integer :: ns , nh integer :: i real ( dp ), pointer :: res (:), rres (:) ns = current_itt ( mix ) nh = n_items ( mix % stack ( 2 )) if ( ns >= 2 . and . n_items ( mix % stack ( 3 )) > 0 ) then ! Update the residual to reflect the input residual res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do end if ! Update weights (if necessary) if ( nh > 1 ) then do i = 2 , nh mix % rv ( i ) = mix % rv ( i + 1 ) end do end if end subroutine fin_broyden end subroutine mixing_finalize ! Perform the actual mixing... subroutine mixing_1d ( mix , n , xin , F , xnext , nsub ) ! The current mixing method type ( tMixer ), pointer :: mix ! The current step in the SCF and size of arrays integer , intent ( in ) :: n ! x1 == Input function, ! F1 == Residual from x1 real ( dp ), intent ( in ) :: xin ( n ), F ( n ) ! x2 == Next input function real ( dp ), intent ( inout ) :: xnext ( n ) ! Number of elements used for calculating the mixing ! coefficients integer , intent ( in ), optional :: nsub ! Coefficients integer :: ncoeff real ( dp ), allocatable :: coeff (:) call mixing_init ( mix , n , xin , F ) ncoeff = mixing_ncoeff ( mix ) allocate ( coeff ( ncoeff )) ! Calculate coefficients if ( present ( nsub ) ) then call mixing_coeff ( mix , nsub , xin , F , coeff ) else call mixing_coeff ( mix , n , xin , F , coeff ) end if ! Calculate the following output call mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! Coefficients are not needed anymore... deallocate ( coeff ) ! Finalize the mixer call mixing_finalize ( mix , n , xin , F , xnext ) end subroutine mixing_1d subroutine mixing_2d ( mix , n1 , n2 , xin , F , xnext , nsub ) type ( tMixer ), pointer :: mix integer , intent ( in ) :: n1 , n2 real ( dp ), intent ( in ) :: xin ( n1 , n2 ), F ( n1 , n2 ) real ( dp ), intent ( inout ) :: xnext ( n1 , n2 ) integer , intent ( in ), optional :: nsub ! Simple wrapper for 1D if ( present ( nsub ) ) then call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 ) ,& nsub = n1 * nsub ) else call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 )) end if end subroutine mixing_2d ! Step the mixing object and ensure that ! the old history is either copied over or freed subroutine mixing_step ( mix ) use parallel , only : IONode type ( tMixer ), pointer :: mix type ( tMixer ), pointer :: next => null () type ( dData1D ), pointer :: d1D integer :: i , is , n , init_itt logical :: reset_stack , copy_stack ! First try and next => mix % next if ( associated ( next ) ) then ! Whether or not the two methods are allowed ! to share history copy_stack = mix % m == next % m select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) copy_stack = . true . end select end select copy_stack = copy_stack . and . allocated ( mix % stack ) ! If the two methods are similar if ( copy_stack ) then ! They are similar, copy over the history stack do is = 1 , size ( mix % stack ) ! Get maximum size of the current stack, n = n_items ( mix % stack ( is )) ! Note that this will automatically take care of ! wrap-arounds and delete the unneccesry elements do i = 1 , n d1D => get_pointer ( mix % stack ( is ), i ) call push ( next % stack ( is ), d1D ) end do ! nullify nullify ( d1D ) end do end if end if reset_stack = . true . if ( associated ( next ) ) then if ( associated ( next % next , mix ) . and . & next % n_itt > 0 ) then ! if this is a circular mixing routine ! we should not reset the history... reset_stack = . false . end if end if if ( reset_stack ) then select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) n = size ( mix % stack ) do is = 1 , n call reset ( mix % stack ( is )) end do end select end if if ( associated ( next ) ) then init_itt = 0 ! Set-up the next mixer select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) init_itt = n_items ( next % stack ( 1 )) end select next % start_itt = init_itt next % cur_itt = init_itt if ( IONode ) then write ( * , '(3a)' ) trim ( debug_msg ), ' switching mixer --> ' , & trim ( next % name ) end if mix => mix % next end if end subroutine mixing_step ! Calculate the inverse of a matrix subroutine inverse ( n , A , B , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) integer , intent ( out ) :: info integer :: i , j ! Local arrays real ( dp ) :: pm ( n , n ), work ( n * 4 ), err ! Relative tolerance dependent on the magnitude ! For now we retain the old tolerance real ( dp ), parameter :: etol = 1.e-4_dp integer :: ipiv ( n ) ! initialize info info = 0 ! simple check and fast return if ( n == 1 ) then B ( 1 , 1 ) = 1._dp / A ( 1 , 1 ) return end if call lapack_inv () if ( info /= 0 ) call simple_inv () contains subroutine lapack_inv () B = A call dgetrf ( n , n , B , n , ipiv , info ) if ( info /= 0 ) return call dgetri ( n , B , n , ipiv , work , n * 4 , info ) if ( info /= 0 ) return ! This sets info appropriately call check_inv () end subroutine lapack_inv subroutine simple_inv () real ( dp ) :: x integer :: k ! Copy over A B = A do i = 1 , n if ( B ( i , i ) == 0._dp ) then info = - n return end if x = 1._dp / B ( i , i ) B ( i , i ) = 1._dp do j = 1 , n B ( j , i ) = B ( j , i ) * x end do do k = 1 , n if ( ( k - i ) /= 0 ) then x = B ( i , k ) B ( i , k ) = 0._dp do j = 1 , n B ( j , k ) = B ( j , k ) - B ( j , i ) * x end do end if end do end do ! This sets info appropriately call check_inv () end subroutine simple_inv subroutine check_inv () ! Check correcteness pm = matmul ( A , B ) do j = 1 , n do i = 1 , n if ( i == j ) then err = pm ( i , j ) - 1._dp else err = pm ( i , j ) end if ! This is pretty strict tolerance! if ( abs ( err ) > etol ) then ! Signal failure in inversion info = - n - 1 return end if end do end do end subroutine check_inv end subroutine inverse ! Calculate the svd of a matrix ! With   ||(Ax - b)|| << subroutine svd ( n , A , B , cond , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) real ( dp ), intent ( in ) :: cond integer , intent ( out ) :: info ! Local arrays integer :: rank , i character ( len = 50 ) :: fmt real ( dp ) :: AA ( n , n ), S ( n ), work ( n * 5 ) ! Copy A matrix AA = A ! setup pseudo inverse solution for minimizing ! constraints B = 0._dp do i = 1 , n B ( i , i ) = 1._dp end do call dgelss ( n , n , n , AA , n , B , n , S , cond , rank , work , n * 5 , info ) ! if debugging print out the different variables if ( debug_mix ) then ! also mark the rank if ( rank == n ) then ! complete rank write ( * , '(2a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' SVD singular = ' , S else ! this prints the location of the SVD rank, if not full write ( fmt , '(i0,2a)' ) rank , '(tr1,e10.4),'' >'',100(tr1,e10.4)' write ( * , '(2a,' // trim ( fmt ) // ')' ) & trim ( debug_msg ), ' SVD singular = ' , S end if end if end subroutine svd ! ************************************************* ! *                Helper routines                * ! *                    LOCAL                      * ! ************************************************* ! Returns the value array from the stack(:) ! Returns this array: !    mix%stack(sidx)(hidx) ! defaults to the last item function getstackval ( mix , sidx , hidx ) result ( d1 ) type ( tMixer ), intent ( in ) :: mix integer , intent ( in ) :: sidx integer , intent ( in ), optional :: hidx real ( dp ), pointer :: d1 (:) type ( dData1D ), pointer :: dD1 if ( present ( hidx ) ) then dD1 => get_pointer ( mix % stack ( sidx ), hidx ) else dD1 => get_pointer ( mix % stack ( sidx ), & n_items ( mix % stack ( sidx ))) end if d1 => val ( dD1 ) end function getstackval ! Returns true if the following ! \"advanced\" mixer is 'method' function is_next ( mix , method , next ) result ( bool ) type ( tMixer ), intent ( in ), target :: mix integer , intent ( in ) :: method type ( tMixer ), pointer , optional :: next logical :: bool type ( tMixer ), pointer :: m bool = . false . m => mix % next do while ( associated ( m ) ) if ( m % m == MIX_LINEAR ) then m => m % next else if ( m % m == method ) then bool = . true . exit else ! Quit if it does not do anything exit end if ! this will prevent cyclic combinations if ( associated ( m , mix ) ) exit end do if ( present ( next ) ) then next => m end if end function is_next !> Get current iteration count !! !! This is abstracted because the initial iteration !! and the current iteration may be uniquely defined. function current_itt ( mix ) result ( itt ) type ( tMixer ), intent ( in ) :: mix integer :: itt itt = mix % cur_itt - mix % start_itt end function current_itt ! Stack handling routines function stack_check ( stack , n ) result ( check ) type ( Fstack_dData1D ), intent ( inout ) :: stack integer , intent ( in ) :: n logical :: check ! Local arrays type ( dData1D ), pointer :: dD1 if ( n_items ( stack ) == 0 ) then check = . true . else ! Check that the stack stored arrays are ! of same size... dD1 => get_pointer ( stack , 1 ) check = n == size ( dD1 ) end if end function stack_check subroutine push_stack_data ( s_F , n ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n type ( dData1D ) :: dD1 if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if call newdData1D ( dD1 , n , '(F)' ) ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_stack_data subroutine push_F ( s_F , n , F , fact ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( in ), optional :: fact type ( dData1D ) :: dD1 real ( dp ), pointer :: sF (:) integer :: in , ns integer :: i if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) ns = max_size ( s_F ) if ( in == ns ) then ! we have to cycle the storage call get ( s_F , 1 , dD1 ) else call newdData1D ( dD1 , n , '(F)' ) end if sF => val ( dD1 ) if ( present ( fact ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n sF ( i ) = F ( i ) * fact end do !$OMP end parallel do else call dcopy ( n , F , 1 , sF , 1 ) end if ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_F subroutine update_F ( s_F , n , F ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) type ( dData1D ), pointer :: dD1 real ( dp ), pointer :: FF (:) integer :: in if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) if ( in == 0 ) then ! We need to add it as it does not exist call push_F ( s_F , n , F ) else ! we have an entry, update the latest dD1 => get_pointer ( s_F , in ) FF => val ( dD1 ) call dcopy ( n , F , 1 , FF , 1 ) end if end subroutine update_F subroutine push_diff ( s_rres , s_res , alpha ) type ( Fstack_dData1D ), intent ( inout ) :: s_rres type ( Fstack_dData1D ), intent ( in ) :: s_res real ( dp ), intent ( in ), optional :: alpha type ( dData1D ) :: dD1 type ( dData1D ), pointer :: pD1 real ( dp ), pointer :: res1 (:), res2 (:), rres (:) integer :: in , ns , i , n if ( n_items ( s_res ) < 2 ) then call die ( 'mixing: Residual residuals cannot be calculated, & &inferior residual size.' ) end if in = n_items ( s_res ) ! First get the value of in pD1 => get_pointer ( s_res , in - 1 ) res1 => val ( pD1 ) ! get the value of in pD1 => get_pointer ( s_res , in ) res2 => val ( pD1 ) in = n_items ( s_rres ) ns = max_size ( s_rres ) if ( in == ns ) then ! we have to cycle the storage call get ( s_rres , 1 , dD1 ) else call newdData1D ( dD1 , size ( res1 ), '(res)' ) end if ! Get the residual of the residual rres => val ( dD1 ) n = size ( rres ) if ( present ( alpha ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = ( res2 ( i ) - res1 ( i )) * alpha end do !$OMP end parallel do else !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = res2 ( i ) - res1 ( i ) end do !$OMP end parallel do end if ! Push the data to the stack call push ( s_rres , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_diff !> Calculate the norm of two arrays function norm ( n , x1 , x2 ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: x1 ( n ), x2 ( n ) real ( dp ) :: norm ! Currently we use an external routine integer :: i ! Calculate dot product norm = 0._dp !$OMP parallel do default(shared), private(i) & !$OMP& reduction(+:norm) do i = 1 , n norm = norm + x1 ( i ) * x2 ( i ) end do !$OMP end parallel do end function norm end module m_mixing","tags":"","loc":"sourcefile/m_mixing.f90.html"},{"title":"compute_max_diff.F90 – SIESTA","text":"Files dependent on this one sourcefile~~compute_max_diff.f90~~AfferentGraph sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_max_diff Source Code compute_max_diff.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_compute_max_diff use precision , only : dp !> Temporary for storing the old maximum change real ( dp ), public , save :: dDmax_current interface compute_max_diff module procedure compute_max_diff_1d module procedure compute_max_diff_2d end interface compute_max_diff public :: compute_max_diff contains subroutine compute_max_diff_2d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:,:), X2 (:,:) real ( dp ), intent ( out ) :: max_diff integer :: n1 , n2 integer :: i1 , i2 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) n2 = size ( X1 , 2 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if if ( size ( X2 , 2 ) /= n2 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (2-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i2,i1), & !$OMP& reduction(max:max_diff), collapse(2) do i2 = 1 , n2 do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 , i2 ) - X2 ( i1 , i2 )) ) end do end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_2d subroutine compute_max_diff_1d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:), X2 (:) real ( dp ), intent ( out ) :: max_diff integer :: n1 integer :: i1 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i1), reduction(max:max_diff) do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 ) - X2 ( i1 )) ) end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_1d end module m_compute_max_diff","tags":"","loc":"sourcefile/compute_max_diff.f90.html"},{"title":"setup_H0.F – SIESTA","text":"Files dependent on this one sourcefile~~setup_h0.f~~AfferentGraph sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_setup_H0 Source Code setup_H0.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_setup_H0 private public :: setup_H0 CONTAINS subroutine setup_H0 ( G2max ) !! Computes non-self-consistent part of the Hamiltonian !! and initializes data structures on the grid. USE siesta_options , only : g2cut use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_2D use sparse_matrices , only : Dscf use sparse_matrices , only : listh , listhptr , numh , maxnh use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use m_nlefsm , only : nlefsm use m_kinefsm , only : kinefsm use m_naefs , only : naefs use m_dnaefs , only : dnaefs use m_dhscf , only : dhscf_init use m_energies , only : Eions , Ena , DEna , Emm , Emeta , Eso use m_ntm use m_spin , only : spin use spinorbit , only : spinorb use alloc , only : re_alloc , de_alloc use class_dSpData1D , only : val use class_dSpData2D , only : val #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none real ( dp ), intent ( inout ) :: g2max real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ), dummy_dm ( 1 , 1 ) real ( dp ) :: dummy_E integer :: ia , is real ( dp ), pointer :: H_val (:), H_so (:,:) #ifdef DEBUG call write_debug ( '    PRE setup_H0' ) #endif !----------------------------------------------------------------------BEGIN call timer ( 'Setup_H0' , 1 ) !!Self-energy of isolated ions !!@code Eions = 0.0_dp do ia = 1 , na_u is = isa ( ia ) Eions = Eions + uion ( is ) enddo !!@endcode !! Neutral-atom: energy !!@note !* In these routines, flag is defined !  to tell them NOT to compute !  forces and stresses in this first pass, only energies. !!@endnote !!@code call naefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , Ena , dummy_fa , dummy_stress , & forces_and_stress = . false .) call dnaefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , DEna , dummy_fa , dummy_stress , & forces_and_stress = . false .) Ena = Ena + DEna !!@endcode !! Metadynamics energy: if ( lMetaForce ) then call meta ( xa , na_u , ucell , Emeta , dummy_fa , dummy_stress , $ . false .,. false .) endif !! Add on force field contribution to energy: call twobody ( na_u , xa , isa , ucell , Emm , & ifa = 0 , fa = dummy_fa , istr = 0 , stress = dummy_stress ) !* Now we compute matrix elements of the Kinetic and Non-local !  parts of H !! Kinetic: matrix elements only !!@code H_val => val ( H_kin_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call kinefsm ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , & maxnh , maxnh , lasto , iphorb , isa , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !!@endcode !! Non-local-pseudop:  matrix elements only !!@code H_val => val ( H_vkb_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call nlefsm ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !!@endcode !!@note !* If in the future the spin-orbit routine is able to compute !  forces and stresses, then \"last\" will be needed. If we are not !  computing forces and stresses, calling it in the first iteration !  should be enough. !!@endnote !!@code if ( spin % SO ) then H_so => val ( H_so_2D ) !$OMP parallel workshare default(shared) H_so = 0._dp !$OMP end parallel workshare call spinorb ( no_u , no_l , iaorb , iphorb , isa , indxuo , & maxnh , numh , listhptr , listh , Dscf , H_so , Eso ) else Eso = 0._dp end if !!@endcode !!This will take care of possible changes to the mesh and atomic-related !!mesh structures for geometry changes: !!@code g2max = g2cut call dhscf_init ( spin % Grid , no_s , iaorb , iphorb , & no_l , no_u , na_u , na_s , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnh , numh , listhptr , listh , datm , & dummy_fa , dummy_stress ) !!@endcode call timer ( 'Setup_H0' , 2 ) #ifdef DEBUG call write_debug ( '    POS setup_H0' ) #endif !---------------------------------------------------------------------- END END subroutine setup_H0 END module m_setup_H0","tags":"","loc":"sourcefile/setup_h0.f.html"},{"title":"state_init.F – SIESTA","text":"This file depends on sourcefile~~state_init.f~~EfferentGraph sourcefile~state_init.f state_init.F sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~state_init.f~~AfferentGraph sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~state_init.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_state_init Source Code state_init.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_state_init private public :: state_init CONTAINS subroutine state_init ( istep ) use Kpoint_grid , only : setup_Kpoint_grid , nkpnt use m_os , only : file_exist use m_new_dm , only : new_dm use m_proximity_check , only : proximity_check use siesta_options use units , only : Ang use sparse_matrices , only : maxnh , numh , listh , listhptr use sparse_matrices , only : Dold , Dscf , DM_2D use sparse_matrices , only : Eold , Escf , EDM_2D use sparse_matrices , only : Hold , H , H_2D use sparse_matrices , only : xijo , xij_2D use sparse_matrices , only : S , S_1D use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D , H_so_2D use sparse_matrices , only : sparse_pattern use sparse_matrices , only : block_dist , single_dist use sparse_matrices , only : DM_history use create_Sparsity_SC , only : crtSparsity_SC use m_sparsity_handling , only : SpOrb_to_SpAtom use m_sparsity_handling , only : Sp_to_Spglobal use m_pivot_methods , only : sp2graphviz use siesta_geom use atomlist , only : iphorb , iphkb , indxua , & rmaxo , rmaxkb , rmaxv , rmaxldau , & lastkb , lasto , superc , indxuo , & no_u , no_s , no_l , iza , qtots use alloc , only : re_alloc , de_alloc , alloc_report use m_hsparse , only : hsparse use m_overlap , only : overlap use m_supercell , only : exact_sc_ag use siesta_cml , only : cml_p , cmlStartStep , mainXML use zmatrix , only : lUseZmatrix , write_zmatrix use m_energies , only : Emad use write_subs use m_ioxv , only : ioxv use m_iotdxv , only : iotdxv use m_steps use parallel , only : IOnode , node , nodes , BlockSize use m_spin , only : spin use m_rmaxh use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_normalize_dm , only : normalize_dm use m_eo use m_gamma use files , only : slabel use m_mpi_utils , only : globalize_or use m_mpi_utils , only : globalize_max use domain_decom , only : domainDecom , use_dd , use_dd_perm use ldau_specs , only : switch_ldau , ldau_init use m_sparse , only : xij_offset use m_ts_kpoints , only : setup_ts_kpoint_grid use m_ts_charge , only : TS_RHOCORR_METHOD , TS_RHOCORR_FERMI use m_ts_options , only : BTD_method use m_ts_options , only : TS_Analyze use m_ts_options , only : N_Elec , Elecs , IsVolt use m_ts_electype use m_ts_global_vars , only : TSrun , TSmode , onlyS use sys , only : bye use m_ts_io , only : fname_TSHS , ts_write_tshs use m_ts_sparse , only : ts_sparse_init use m_ts_tri_init , only : ts_tri_init , ts_tri_analyze use files , only : slabel , label_length #ifdef SIESTA__CHESS use m_chess , only : CheSS_init , get_CheSS_parameter #endif #ifdef CDF use iodm_netcdf , only : setup_dm_netcdf_file use iodmhs_netcdf , only : setup_dmhs_netcdf_file #endif use class_Sparsity use class_dSpData1D use class_dSpData2D use class_dData2D #ifdef TEST_IO use m_test_io #endif #ifdef SIESTA__FLOOK use siesta_dicts , only : dict_repopulate_MD #endif implicit none integer :: istep real ( dp ) :: veclen ! Length of a unit-cell vector real ( dp ) :: rmax logical :: cell_can_change integer :: i , ix , iadispl , ixdispl logical :: auxchanged ! Auxiliary supercell changed? logical :: folding , folding1 logical :: foundxv ! dummy for call to ioxv external :: madelung , timer real ( dp ), external :: volcel integer :: ts_kscell_file ( 3 , 3 ) = 0 real ( dp ) :: ts_kdispl_file ( 3 ) = 0.0 logical :: ts_Gamma_file = . true . character ( len = label_length + 6 ) :: fname real ( dp ) :: dummyef = 0.0 , dummyqtot = 0.0 #ifdef SIESTA__CHESS integer :: maxnh_kernel , maxnh_mult , no_l_kernel , no_l_mult integer , dimension (:), allocatable :: listh_kernel , listh_mult integer , dimension (:), allocatable :: numh_kernel , numh_mult real ( dp ) :: chess_value #endif type ( Sparsity ) :: g_Sp character ( len = 256 ) :: oname type ( dData2D ) :: tmp_2D !------------------------------------------------------------------- BEGIN call timer ( 'IterGeom' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_init' ) #endif call timer ( 'state_init' , 1 ) istp = istp + 1 if ( IOnode ) then write ( 6 , '(/,t22,a)' ) repeat ( '=' , 36 ) select case ( idyn ) case ( 0 ) if ( nmove == 0 ) then write ( 6 , '(t25,a)' ) 'Single-point calculation' if ( cml_p ) call cmlStartStep ( mainXML , type = 'Single-Point' , $ index = istp ) else if ( broyden_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin Broyden opt. move = ' , $ istep else if ( fire_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin FIRE opt. move = ' , $ istep else write ( 6 , '(t25,a,i6)' ) 'Begin CG opt. move = ' , $ istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'Geom. Optim' , $ index = istp ) endif !        Print Z-matrix coordinates if ( lUseZmatrix ) then call write_Zmatrix () endif case ( 1 , 3 ) if ( iquench > 0 ) then write ( 6 , '(t25,a,i6)' ) 'Begin MD quenched step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD-quenched' , $ index = istep ) else write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , $ index = istep ) endif case ( 2 , 4 , 5 ) write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , index = istep ) case ( 6 ) write ( 6 , '(t25,a,i6)' ) 'Begin FC step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FC' , index = istep ) if ( istep . eq . 0 ) then write ( 6 , '(t25,a)' ) 'Undisplaced coordinates' else iadispl = ( istep - mod ( istep - 1 , 6 )) / 6 + ia1 ix = mod ( istep - 1 , 6 ) + 1 ixdispl = ( ix - mod ( ix - 1 , 2 ) + 1 ) / 2 write ( 6 , '(t26,a,i0,/,t26,a,i1,a,f10.6,a)' ) 'displace atom ' , & iadispl , 'in direction ' , ixdispl , ' by' , dx / Ang , ' Ang' endif case ( 8 ) write ( 6 , '(t25,a,i6)' ) 'Begin Server step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FS' , index = istep ) case ( 9 ) if ( istep == 0 ) then write ( 6 , '(t25,a,i7)' ) 'Explicit coord. initialization' else write ( 6 , '(t25,a,i7)' ) 'Explicit coord. step =' , istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'ECS' , index = istep ) case ( 10 ) write ( 6 , '(t25,a,i7)' ) 'LUA coord. step =' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'LUA' , index = istep ) end select write ( 6 , '(t22,a)' ) repeat ( '=' , 36 ) !     Print atomic coordinates call outcoor ( ucell , xa , na_u , ' ' , writec ) !     Save structural information in crystallographic format !     (in file SystemLabel.STRUCT_OUT), !     canonical Zmatrix (if applicable), and CML record call siesta_write_positions ( moved = . false .) endif ! IONode ! Write the XV file for single-point calculations, so that ! it is there at the end for those users who rely on it call ioxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , & foundxv ) ! Write TDXV file for TDDFT restart. if ( writetdwf . or . td_elec_dyn ) then call iotdxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , foundxv ) end if !     Actualize things if variable cell auxchanged = . false . cell_can_change = ( varcel . or . & ( idyn . eq . 8 ) ! Force/stress evaluation & ) if ( change_kgrid_in_md ) then cell_can_change = cell_can_change . or . & ( idyn . eq . 3 ) . or . ! Parrinello-Rahman & ( idyn . eq . 4 ) . or . ! Nose-Parrinello-Rahman & ( idyn . eq . 5 ) ! Anneal endif if ( cell_can_change . and . & ( istep . ne . inicoor ) . and . (. not . gamma ) ) then !       Will print k-points also call setup_Kpoint_grid ( ucell ) call setup_ts_kpoint_grid ( ucell ) call re_alloc ( eo , 1 , no_u , 1 , spin % spinor , 1 , nkpnt , 'eo' , & 'state_init' ) call re_alloc ( qo , 1 , no_u , 1 , spin % spinor , 1 , nkpnt , 'qo' , & 'state_init' ) !       Find required supercell if ( gamma ) then nsc ( 1 : 3 ) = 1 else if ( fixauxcell ) then nsc ( 1 : 3 ) = nscold ( 1 : 3 ) else do i = 1 , 3 veclen = sqrt ( ucell ( 1 , i ) ** 2 + ucell ( 2 , i ) ** 2 + ucell ( 3 , i ) ** 2 ) nsc ( i ) = 1 + 2 * ceiling ( rmaxh / veclen ) end do if ( . not . naiveauxcell ) & call exact_sc_ag ( negl , ucell , na_u , isa , xa , nsc ) endif mscell = 0.0_dp do i = 1 , 3 mscell ( i , i ) = nsc ( i ) if ( nsc ( i ) /= nscold ( i )) auxchanged = . true . nscold ( i ) = nsc ( i ) enddo !       Madelung correction for charged systems if ( charnet . ne . 0.0_dp ) then call madelung ( ucell , shape , charnet , Emad ) endif endif !     End variable cell actualization !     Auxiliary supercell !     Do not move from here, as the coordinates might have changed !     even if not the unit cell call superc ( ucell , scell , nsc ) #ifdef SIESTA__FLOOK call dict_repopulate_MD () #endif !     Print unit cell and compute cell volume !     Possible BUG: !     Note that this volume is later used in write_subs and the md output !     routines, even if the cell later changes. if ( IOnode ) call outcell ( ucell ) volume_of_some_cell = volcel ( ucell ) !     Use largest possible range in program, except hsparse... !     2 * rmaxv: Vna overlap !     rmaxo + rmaxkb: Non-local KB action !     2 * (rmaxo + rmaxldau): Interaction through LDAU projector !     2.0_dp * (rmaxo+rmaxkb) : Orbital interaction through KB projectors rmax = max ( 2._dp * rmaxv , 2._dp * ( rmaxo + rmaxldau ), rmaxo + rmaxkb ) if ( . not . negl ) then rmax = max ( rmax , 2.0_dp * ( rmaxo + rmaxkb ) ) endif !     Check if any two atoms are unreasonably close call proximity_check ( rmax ) ! Clear history of mixing parameters call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) ! Ensure sparsity pattern is empty call delete ( sparse_pattern ) ! sadly deleting the sparse pattern does not necessarily ! mean that the arrays are de-associated. ! Remember that the reference counter could (in MD) ! be higher than 1, hence we need to create \"fake\" ! containers and let the new<class> delete the old ! sparsity pattern nullify ( numh , listhptr , listh ) allocate ( numh ( no_l ), listhptr ( no_l )) ! We do not need to allocate listh ! that will be allocated in hsparse #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then !         Calculate a sparsity pattern with some buffers... Only required !         for CheSS chess_value = get_chess_parameter ( 'chess_buffer_kernel' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ buffer = chess_value ) maxnh_kernel = maxnh no_l_kernel = no_l allocate ( listh_kernel ( maxnh_kernel )) allocate ( numh_kernel ( no_l_kernel )) listh_kernel = listh numh_kernel = numh chess_value = get_chess_parameter ( 'chess_buffer_mult' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ buffer = chess_value ) maxnh_mult = maxnh no_l_mult = no_l allocate ( listh_mult ( maxnh_mult )) allocate ( numh_mult ( no_l_mult )) listh_mult = listh numh_mult = numh end if #endif /* CHESS */ !     List of nonzero Hamiltonian matrix elements !     and, if applicable,  vectors between orbital centers !     Listh and xijo are allocated inside hsparse !     Note: We always generate xijo now, for COOP and other !           analyses. call delete ( xij_2D ) ! as xijo will be reallocated nullify ( xijo ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , $ set_xijo = . true ., folding = folding1 ) ! #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then call CheSS_init ( node , nodes , maxnh , maxnh_kernel , maxnh_mult , & no_u , no_l , no_l_kernel , no_l_mult , BlockSize , & spin % spinor , qtots , listh , listh_kernel , listh_mult , & numh , numh_kernel , numh_mult ) deallocate ( listh_kernel ) deallocate ( numh_kernel ) deallocate ( listh_mult ) deallocate ( numh_mult ) end if #endif /* CHESS */ call globalize_or ( folding1 , folding ) if ( folding ) then if ( IOnode ) then print * , \"Folding of H and S is implicitly performed\" endif endif ! ! If using domain decomposition, redistribute orbitals ! for this geometry, based on the hsparse info. ! The first time round, the initial distribution is a ! simple block one (given by preSetOrbitLimits). ! ! Any DM, etc, read from file will be redistributed according ! to the new pattern. ! Inherited DMs from a previous geometry cannot be used if the ! orbital distribution changes. For now, we avoid changing the ! distribution (the variable use_dd_perm is .true. if domain ! decomposition is in effect). Names should be changed... if ( use_dd . and . (. not . use_dd_perm )) then call domainDecom ( no_u , no_l , maxnh ) ! maxnh intent(in) here maxnh = sum ( numh ( 1 : no_l )) ! We still need to re-create Julian Gale's ! indexing for O(N) in parallel. print \"(a5,i3,a20,3i8)\" , $ \"Node: \" , Node , \"no_u, no_l, maxnh: \" , no_u , no_l , maxnh call setup_ordern_indexes ( no_l , no_u , Nodes ) endif ! I would like to skip this alloc/move/dealloc/attach ! by allowing sparsity to have pointer targets. ! However, this poses a problem with intel compilers, ! as it apparently errors out when de-allocating a target pointer write ( oname , \"(a,i0)\" ) \"sparsity for geom step \" , istep call newSparsity ( sparse_pattern , no_l , no_u , maxnh , & numh , listhptr , listh , name = oname ) deallocate ( numh , listhptr , listh ) call attach ( sparse_pattern , & n_col = numh , list_ptr = listhptr , list_col = listh ) ! In case the user requests to create the connectivity graph if ( write_GRAPHVIZ > 0 ) then ! first create the unit-cell sparsity pattern call crtSparsity_SC ( sparse_pattern , g_Sp , UC = . true .) ! next move to global sparsity pattern call Sp_to_Spglobal ( block_dist , g_Sp , g_Sp ) if ( IONode ) then if ( write_GRAPHVIZ /= 2 ) & call sp2graphviz ( trim ( slabel ) // '.ORB.gv' , g_Sp ) ! Convert to atomic if ( write_GRAPHVIZ /= 1 ) then call SpOrb_to_SpAtom ( single_dist , g_Sp , na_u , lasto , g_Sp ) call sp2graphviz ( trim ( slabel ) // '.ATOM.gv' , g_Sp ) end if end if call delete ( g_Sp ) end if ! Copy over xijo array (we can first do it here... :( ) call newdData2D ( tmp_2D , xijo , 'xijo' ) deallocate ( xijo ) write ( oname , \"(a,i0)\" ) \"xijo at geom step \" , istep call newdSpData2D ( sparse_pattern , tmp_2D , block_dist , xij_2D , & name = oname ) call delete ( tmp_2D ) ! decrement container... xijo => val ( xij_2D ) ! Calculate the super-cell offsets... if ( Gamma ) then ! Here we create the super-cell offsets call re_alloc ( isc_off , 1 , 3 , 1 , 1 ) isc_off (:,:) = 0 else call xij_offset ( ucell , nsc , na_u , xa , lasto , & xij_2D , isc_off , & Bcast = . true .) end if ! When the user requests to only do an analyzation, we can call ! appropriate routines and quit if ( TS_Analyze ) then ! Force the creation of the full sparsity pattern call ts_sparse_init ( slabel , IsVolt , N_Elec , Elecs , & ucell , nsc , na_u , xa , lasto , block_dist , sparse_pattern , & Gamma , isc_off ) ! create the tri-diagonal matrix call ts_tri_analyze ( block_dist , sparse_pattern , N_Elec , & Elecs , ucell , na_u , lasto , nsc , isc_off , & BTD_method ) ! Print-out timers call timer ( 'TS-rgn2tri' , 3 ) ! Bye also waits for all processors call bye ( 'transiesta analyzation performed' ) end if write ( oname , \"(a,i0)\" ) \"EDM at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % EDM , block_dist , EDM_2D , & name = oname ) !if (ionode) call print_type(EDM_2D) Escf => val ( EDM_2D ) call re_alloc ( Dold , 1 , maxnh , 1 , spin % DM , name = 'Dold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) call re_alloc ( Hold , 1 , maxnh , 1 , spin % H , name = 'Hold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) if ( converge_EDM ) then call re_alloc ( Eold , 1 , maxnh , 1 , spin % EDM , name = 'Eold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) end if !     Allocate/reallocate storage associated with Hamiltonian/Overlap matrix write ( oname , \"(a,i0)\" ) \"H at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H , block_dist , H_2D , & name = oname ) !if (ionode) call print_type(H_2D) H => val ( H_2D ) write ( oname , \"(a,i0)\" ) \"H_vkb at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_vkb_1D , name = oname ) !if (ionode) call print_type(H_vkb_1D) write ( oname , \"(a,i0)\" ) \"H_kin at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_kin_1D , name = oname ) !if (ionode) call print_type(H_kin_1D) if ( switch_ldau ) then write ( oname , \"(a,i0)\" ) \"H_ldau at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % spinor , & block_dist , H_ldau_2D , name = oname ) ! Initialize to 0, LDA+U may re-calculate !   this matrix sporadically doing the SCF. ! Hence initialization MUST be performed upon ! re-allocation. call init_val ( H_ldau_2D ) if ( inicoor /= istep ) then ! Force initialization of the LDA+U ! when changing geometry ! For the first geometry this is controlled ! by the user via an fdf-key ldau_init = . true . end if end if if ( spin % SO ) then write ( oname , \"(a,i0)\" ) \"H_so at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H - 2 , & block_dist , H_so_2D , name = oname ) end if write ( oname , \"(a,i0)\" ) \"S at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , S_1D , name = oname ) if ( ionode ) call print_type ( S_1D ) S => val ( S_1D ) !     Find overlap matrix call overlap ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , maxnh , & lasto , iphorb , isa , numh , listhptr , listh , S ) ! !     Here we could also read a Hamiltonian, either to proceed to !     the analysis section (with nscf=0) or to start a mix-H scf cycle. ! !     Initialize density matrix ! The resizing of Dscf is done inside new_dm call new_DM ( auxchanged , DM_history , DM_2D , EDM_2D ) Dscf => val ( DM_2D ) Escf => val ( EDM_2D ) ! Initialize energy-density matrix to zero for first call to overfsm ! Only part of Escf is updated in TS, so if it is put as zero here ! a continuation run gives bad forces. if ( . not . TSrun ) then call normalize_DM ( first = . true . ) !$OMP parallel workshare default(shared) Escf (:,:) = 0.0_dp !$OMP end parallel workshare end if #ifdef TEST_IO ! We test the io-performance here call time_io ( spin % H , H_2D ) #endif !     If onlyS, Save overlap matrix and exit if ( onlyS ) then fname = fname_TSHS ( slabel , onlyS = . true . ) ! We include H as S, well-knowing that we only write one of ! them, there is no need to allocate space for no reason! call ts_write_tshs ( fname , & . true ., Gamma , ts_Gamma_file , & ucell , nsc , isc_off , na_u , no_s , spin % H , & ts_kscell_file , ts_kdispl_file , & xa , lasto , & H_2D , S_1D , indxuo , & dummyEf , dummyQtot , Temp , 0 , 0 ) call bye ( 'Save overlap matrix and exit' ) ! Exit siesta endif ! In case the user is requesting a Fermi-correction ! we need to delete the TS_FERMI file after each iteration if ( TSmode . and . TS_RHOCORR_METHOD == TS_RHOCORR_FERMI & . and . IONode ) then ! Delete the TS_FERMI file (enables ! reading it in and improve on the convergence) if ( file_exist ( 'TS_FERMI' ) ) then i = 23455 ! this should just not be used any were... ! Delete the file... open ( unit = i , file = 'TS_FERMI' ) close ( i , status = 'delete' ) end if end if #ifdef CDF if ( writedm_cdf ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh ) endif if ( writedm_cdf_history ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & istep ) endif if ( writedmhs_cdf ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s ) endif if ( writedmhs_cdf_history ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s , & istep ) endif #endif call timer ( 'state_init' , 2 ) END subroutine state_init END module m_state_init","tags":"","loc":"sourcefile/state_init.f.html"},{"title":"siesta_forces.F90 – SIESTA","text":"This file depends on sourcefile~~siesta_forces.f90~~EfferentGraph sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_siesta_forces Source Code siesta_forces.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_siesta_forces implicit none private public :: siesta_forces contains subroutine siesta_forces ( istep ) !! This subroutine represents central SIESTA operation logic. #ifdef MPI use mpi_siesta #endif use units , only : eV , Ang use precision , only : dp use sys , only : bye use files , only : slabel use siesta_cml #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call use flook_siesta , only : LUA_INIT_MD , LUA_SCF_LOOP use siesta_dicts , only : dict_variable_add use m_ts_options , only : ts_scf_mixs use variable , only : cunpack #ifndef NCDF_4 use dictionary , only : assign #endif use m_mixing , only : mixers_history_init #endif use m_state_init use m_setup_hamiltonian use m_setup_H0 use m_compute_dm use m_compute_max_diff use m_scfconvergence_test use m_post_scf_work use m_mixer , only : mixer use m_mixing_scf , only : mixing_scf_converged use m_mixing_scf , only : mixers_scf_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_rhog , only : mix_rhog , compute_charge_diff use siesta_options use parallel , only : IOnode , SIESTA_worker use m_state_analysis use m_steps use m_spin , only : spin use sparse_matrices , only : DM_2D , S_1D use sparse_matrices , only : H , Hold , Dold , Dscf , Eold , Escf , maxnh use m_convergence , only : converger_t use m_convergence , only : reset , set_tolerance use siesta_geom , only : na_u ! Number of atoms in unit cell use m_energies , only : Etot ! Total energy use m_forces , only : fa , cfa ! Forces and constrained forces use m_stress , only : cstress ! Constrained stress tensor use siesta_master , only : forcesToMaster ! Send forces to master prog use siesta_master , only : siesta_server ! Is siesta a server? use m_save_density_matrix , only : save_density_matrix use m_iodm_old , only : write_spmatrix use atomlist , only : no_u , lasto , Qtot use m_dm_charge , only : dm_charge use m_pexsi_solver , only : prevDmax use write_subs , only : siesta_write_forces use write_subs , only : siesta_write_stress_pressure #ifdef NCDF_4 use dictionary use m_ncdf_siesta , only : cdf_init_file , cdf_save_settings use m_ncdf_siesta , only : cdf_save_state , cdf_save_basis #endif use m_compute_energies , only : compute_energies use m_mpi_utils , only : broadcast , barrier use fdf #ifdef SIESTA__PEXSI use m_pexsi , only : pexsi_finalize_scfloop #endif use m_check_walltime use m_energies , only : DE_NEGF use m_ts_options , only : N_Elec use m_ts_method use m_ts_global_vars , only : TSmode , TSinit , TSrun use siesta_geom , only : nsc , xa , ucell , isc_off use sparse_matrices , only : sparse_pattern , block_dist use sparse_matrices , only : S use m_ts_charge , only : ts_get_charges use m_ts_charge , only : TS_RHOCORR_METHOD use m_ts_charge , only : TS_RHOCORR_FERMI use m_ts_charge , only : TS_RHOCORR_FERMI_TOLERANCE use m_transiesta , only : transiesta use kpoint_scf_m , only : gamma_scf use m_energies , only : Ef use m_initwf , only : initwf integer , intent ( inout ) :: istep integer :: iscf logical :: first_scf , SCFconverged real ( dp ) :: dDmax ! Max. change in DM elements real ( dp ) :: dHmax ! Max. change in H elements real ( dp ) :: dEmax ! Max. change in EDM elements real ( dp ) :: drhog ! Max. change in rho(G) (experimental) real ( dp ), target :: G2max ! actually used meshcutoff type ( converger_t ) :: conv_harris , conv_freeE ! For initwf integer :: istpp #ifdef SIESTA__FLOOK ! len=24 from m_mixing.F90 character ( len = 1 ), target :: next_mixer ( 24 ) character ( len = 24 ) :: nnext_mixer integer :: imix #endif logical :: time_is_up character ( len = 40 ) :: tmp_str real ( dp ) :: Qcur #ifdef NCDF_4 type ( dict ) :: d_sav #endif #ifdef MPI integer :: MPIerror #endif external :: die , message #ifdef DEBUG call write_debug ( '    PRE siesta_forces' ) #endif #ifdef SIESTA__PEXSI ! Broadcast relevant things for program logic ! These were set in read_options, called only by \"SIESTA_workers\". call broadcast ( nscf , comm = true_MPI_Comm_World ) #endif !  Initialization tasks for a given geometry: if ( SIESTA_worker ) then call state_init ( istep ) end if #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_init\" ) #endif if ( fdf_get ( \"Sonly\" ,. false .) ) then if ( SIESTA_worker ) then call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) end if call bye ( \"S only\" ) end if Qcur = Qtot #ifdef SIESTA__FLOOK ! Add the iscf constant to the list of variables ! that are available only in this part of the routine. call dict_variable_add ( 'SCF.iteration' , iscf ) call dict_variable_add ( 'SCF.converged' , SCFconverged ) call dict_variable_add ( 'SCF.charge' , Qcur ) call dict_variable_add ( 'SCF.dD' , dDmax ) call dict_variable_add ( 'SCF.dH' , dHmax ) call dict_variable_add ( 'SCF.dE' , dEmax ) call dict_variable_add ( 'SCF.drhoG' , drhog ) ! We have to set the meshcutoff here ! because the asked and required ones are not ! necessarily the same call dict_variable_add ( 'Mesh.Cutoff.Minimum' , G2cut ) call dict_variable_add ( 'Mesh.Cutoff.Used' , G2max ) if ( mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , wmix ) else call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) ! Just to populate the table in the dictionary call dict_variable_add ( 'SCF.Mixer.Switch' , next_mixer ) end if ! Initialize to no switch next_mixer = ' ' #endif !  This call computes the **non-scf** part of  H  and initializes the !  real-space grid structures: if ( SIESTA_worker ) call setup_H0 ( G2max ) !!@todo !* It might be better to split the two, !  putting the grid initialization into **state_init (link!)** and moving the !  calculation of  H_0  to the body of the loop, done `if first_scf=.true.` !  This would suit _analysis_ runs in which **nscf = 0** !!@endtodo #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after setup_H0\" ) #endif #ifdef SIESTA__FLOOK ! Communicate with lua, just before entering the SCF loop ! This is mainly to be able to communicate ! mesh-related quantities (g2max) call slua_call ( LUA , LUA_INIT_MD ) #endif #ifdef NCDF_4 ! Initialize the NC file if ( write_cdf ) then ! Initialize the file... call cdf_init_file ( trim ( slabel ) // '.nc' , is_MD = . false .) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif ! Save the settings call cdf_save_settings ( trim ( slabel ) // '.nc' ) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif d_sav = ( 'sp' . kv . 1 ) // ( 'S' . kv . 1 ) d_sav = d_sav // ( 'nsc' . kv . 1 ) // ( 'xij' . kv . 1 ) d_sav = d_sav // ( 'xa' . kv . 1 ) // ( 'cell' . kv . 1 ) d_sav = d_sav // ( 'isc_off' . kv . 1 ) call cdf_save_state ( trim ( slabel ) // '.nc' , d_sav ) call delete ( d_sav ) ! Save the basis set call cdf_save_basis ( trim ( slabel ) // '.nc' ) end if #endif !* The dHmax variable only has meaning for Hamiltonian !  mixing, or when requiring the Hamiltonian to be converged. dDmax = - 1._dp dHmax = - 1._dp dEmax = - 1._dp drhog = - 1._dp ! Setup convergence criteria: if ( SIESTA_worker ) then if ( converge_Eharr ) then call reset ( conv_harris ) call set_tolerance ( conv_harris , tolerance_Eharr ) end if if ( converge_FreeE ) then call reset ( conv_FreeE ) call set_tolerance ( conv_FreeE , tolerance_FreeE ) end if end if !!# SCF loop !* The current structure of the loop tries to reproduce the !  historical Siesta usage. It should be made more clear. !* Two changes: ! !  1. The number of scf iterations performed is exactly !     equal to the number specified (i.e., the \"forces\" !     phase is not counted as a final scf step) !  2. At the change to a TranSiesta GF run the variable \"first_scf\" !     is implicitly reset to \"true\". ! !!## Start of SCF cycle ! !* Conditions of exit: ! !  * At the top, to catch a non-positive nscf and # of iterations !  * At the bottom, based on convergence ! iscf = 0 do while ( iscf < nscf ) iscf = iscf + 1 !* Note implications for TranSiesta when mixing H. !  Now H will be recomputed instead of simply being !  inherited, however, this is required as if !  we have bias calculations as the electric !  field across the junction needs to be present. first_scf = ( iscf == 1 ) if ( SIESTA_worker ) then ! Check whether we are short of time to continue call check_walltime ( time_is_up ) if ( time_is_up ) then ! Save DM/H if we were not saving it... !   Do any other bookeeping not done by \"die\" call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge' // & ' before wall time exhaustion' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call barrier () ! A non-root node might get first to the 'die' call call die ( \"OUT_OF_TIME: Time is up.\" ) end if call timer ( 'IterSCF' , 1 ) if ( cml_p ) & call cmlStartStep ( xf = mainXML , type = 'SCF' , index = iscf ) if ( mixH ) then if ( first_scf ) then if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then call get_H_from_file () else call setup_hamiltonian ( iscf ) end if end if call compute_DM ( iscf ) ! Maybe set Dold to zero if reading charge or H... call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) else call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) call compute_DM ( iscf ) call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) end if ! This iteration has completed calculating the new DM call compute_energies ( iscf ) if ( mix_charge ) then call compute_charge_diff ( drhog ) end if ! Note: For DM and H convergence checks. At this point: ! If mixing the DM: !        Dscf=DM_out, Dold=DM_in(mixed), H=H_in, Hold=H_in(prev step) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H_in - H_in(prev step)) ! If mixing the Hamiltonian: !        Dscf=DM_out, Dold=DM_in, H=H_(DM_out), Hold=H_in(mixed) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H(DM_out),H_in) call scfconvergence_test ( first_scf , iscf , & dDmax , dHmax , dEmax , & conv_harris , conv_freeE , & SCFconverged ) ! ** Check this heuristic if ( mixH ) then prevDmax = dHmax else prevDmax = dDmax end if ! Calculate current charge based on the density matrix call dm_charge ( spin , DM_2D , S_1D , Qcur ) ! Check whether we should step to the next mixer call mixing_scf_converged ( SCFconverged ) if ( SCFconverged . and . iscf < min_nscf ) then SCFconverged = . false . if ( IONode ) then write ( * , \"(a,i0)\" ) & \"SCF cycle continued for minimum number of iterations: \" , & min_nscf end if end if ! In case the user has requested a Fermi-level correction ! Then we start by correcting the fermi-level if ( TSrun . and . SCFconverged . and . & TS_RHOCORR_METHOD == TS_RHOCORR_FERMI ) then if ( abs ( Qcur - Qtot ) > TS_RHOCORR_FERMI_TOLERANCE ) then ! Call transiesta with fermi-correct call transiesta ( iscf , spin % H , & block_dist , sparse_pattern , Gamma_Scf , ucell , nsc , & isc_off , no_u , na_u , lasto , xa , maxnh , H , S , & Dscf , Escf , Ef , Qtot , . true ., DE_NEGF ) ! We will not have not converged as we have just ! changed the Fermi-level SCFconverged = . false . end if end if if ( monitor_forces_in_scf ) call compute_forces () ! Mix_after_convergence preserves the old behavior of ! the program. if ( (. not . SCFconverged ) . or . mix_after_convergence ) then ! Mix for next step if ( mix_charge ) then call mix_rhog ( iscf ) else call mixer ( iscf ) end if ! Save for possible restarts if ( mixH ) then call write_spmatrix ( H , file = \"H_MIXED\" , when = writeH ) call save_density_matrix ( file = \"DM_OUT\" , when = writeDM ) else call save_density_matrix ( file = \"DM_MIXED\" , when = writeDM ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = writeH ) end if end if call timer ( 'IterSCF' , 2 ) call print_timings ( first_scf , istep == inicoor ) if ( cml_p ) call cmlEndStep ( mainXML ) #ifdef SIESTA__FLOOK ! Communicate with lua call slua_call ( LUA , LUA_SCF_LOOP ) ! Retrieve an easy character string nnext_mixer = cunpack ( next_mixer ) if ( len_trim ( nnext_mixer ) > 0 . and . . not . mix_charge ) then if ( TSrun ) then do imix = 1 , size ( ts_scf_mixs ) if ( ts_scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( ts_scf_mixs ) scf_mix => ts_scf_mixs ( imix ) exit end if end do else do imix = 1 , size ( scf_mixs ) if ( scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( imix ) exit end if end do end if ! Check that we indeed have changed the mixer if ( IONode . and . scf_mix % name /= nnext_mixer ) then write ( * , '(2a)' ) 'siesta-lua: WARNING: trying to change ' , & 'to a non-existing mixer! Not changing anything!' else if ( IONode ) then write ( * , '(2a)' ) 'siesta-lua: Switching mixer method to: ' , & trim ( nnext_mixer ) end if ! Reset for next loop next_mixer = ' ' ! Update the references call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif ! ... except that we might continue for TranSiesta if ( SCFconverged ) then call transiesta_switch () ! might reset SCFconverged and iscf end if else ! non-siesta worker call compute_DM ( iscf ) end if #ifdef SIESTA__PEXSI call broadcast ( iscf , comm = true_MPI_Comm_World ) call broadcast ( SCFconverged , comm = true_MPI_Comm_World ) #endif !  Exit if converged: if ( SCFconverged ) exit end do !! **end of SCF cycle** #ifdef SIESTA__PEXSI if ( isolve == SOLVE_PEXSI ) then call pexsi_finalize_scfloop () end if #endif if ( . not . SIESTA_worker ) return call end_of_cycle_save_operations () if ( . not . SCFconverged ) then if ( SCFMustConverge ) then call message ( 'FATAL' , 'SCF_NOT_CONV: SCF did not converge' // & ' in maximum number of steps (required).' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call barrier () call die ( 'ABNORMAL_TERMINATION' ) else if ( . not . harrisfun ) then call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge  in maximum number of steps.' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) end if end if ! To write the initial wavefunctions to be used in a ! consequent TDDFT run. if ( writetdwf ) then istpp = 0 call initwf ( istpp , totime ) end if if ( TSmode . and . TSinit . and .(. not . SCFConverged ) ) then ! Signal that the DM hasn't converged, so we cannot ! continue to the transiesta routines call die ( 'ABNORMAL_TERMINATION' ) end if ! Clean-up here to limit memory usage call mixers_scf_history_init ( ) ! End of standard SCF loop. ! Do one more pass to compute forces and stresses ! Note that this call will no longer overwrite H while computing the ! final energies, forces and stresses... if ( fdf_get ( \"compute-forces\" ,. true .) ) then call post_scf_work ( istep , iscf , SCFconverged ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after post_scf_work\" ) #endif end if ! ... so H at this point is the latest generator of the DM, except ! if mixing H beyond self-consistency or terminating the scf loop ! without convergence while mixing H call state_analysis ( istep ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_analysis\" ) #endif ! If siesta is running as a subroutine, send forces to master program if ( siesta_server ) & call forcesToMaster ( na_u , Etot , cfa , cstress ) #ifdef DEBUG call write_debug ( '    POS siesta_forces' ) #endif contains ! Read the Hamiltonian from a file subroutine get_H_from_file () use sparse_matrices , only : maxnh , numh , listh , listhptr use atomlist , only : no_l use m_spin , only : spin use m_iodm_old , only : read_spmatrix logical :: found call read_spmatrix ( maxnh , no_l , spin % H , numh , & listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) end subroutine get_H_from_file ! Computes forces and stresses with the current DM_out subroutine compute_forces () use siesta_options , only : recompute_H_after_scf use m_final_H_f_stress , only : final_H_f_stress use write_subs real ( dp ), allocatable :: fa_old (:,:), Hsave (:,:) allocate ( fa_old ( size ( fa , dim = 1 ), size ( fa , dim = 2 ))) fa_old (:,:) = fa (:,:) if ( recompute_H_after_scf ) then allocate ( Hsave ( size ( H , dim = 1 ), size ( H , dim = 2 ))) Hsave (:,:) = H (:,:) end if call final_H_f_stress ( istep , iscf , . false . ) if ( recompute_H_after_scf ) then H (:,:) = Hsave (:,:) deallocate ( Hsave ) end if if ( ionode ) then print * , \"Max diff in force (eV/Ang): \" , & maxval ( abs ( fa - fa_old )) * Ang / eV call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () endif deallocate ( fa_old ) end subroutine compute_forces ! Print out timings of the first SCF loop only subroutine print_timings ( first_scf , first_md ) use timer_options , only : use_tree_timer use m_ts_global_vars , only : TSrun logical , intent ( in ) :: first_scf , first_md character ( len = 20 ) :: routine ! If this is not the first iteration, ! we immediately return. if ( . not . first_scf ) return if ( . not . first_md ) return routine = 'IterSCF' if ( TSrun ) then ! with Green function generation ! The tree-timer requires direct ! children of the routine to be ! queried. ! This is not obeyed in the TS case... :( if ( . not . use_tree_timer ) then routine = 'TS' end if endif call timer ( routine , 3 ) end subroutine print_timings ! Depending on various conditions, save the DMin ! or the DMout, and possibly keep a copy of H ! NOTE: Only if the scf cycle converged before exit it ! is guaranteed that the DM is \"pure out\" and that ! we can recover the right H if mixing H. ! subroutine end_of_cycle_save_operations () logical :: DM_write , H_write ! Depending on the option we should overwrite the ! Hamiltonian if ( mixH . and . . not . mix_after_convergence ) then ! Make sure that we keep the H actually used ! to generate the last DM, if needed. H = Hold end if DM_write = write_DM_at_end_of_cycle . and . & . not . writeDM H_write = write_H_at_end_of_cycle . and . & . not . writeH if ( mix_after_convergence ) then ! If we have been saving them, there is no point in doing ! it one more time if ( mixH ) then call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_MIXED\" , when = H_write ) else call save_density_matrix ( file = \"DM_MIXED\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if else call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if end subroutine end_of_cycle_save_operations subroutine transiesta_switch () use precision , only : dp use parallel , only : IONode use class_dSpData2D use class_Fstack_dData1D use densematrix , only : resetDenseMatrix use siesta_options , only : fire_mix , broyden_maxit use siesta_options , only : dDtol , dHtol use sparse_matrices , only : DM_2D , EDM_2D use atomlist , only : lasto use siesta_geom , only : nsc , isc_off , na_u , xa , ucell use m_energies , only : Ef use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mix , scf_mixs use m_rhog , only : resetRhoG use m_ts_global_vars , only : TSinit , TSrun use m_ts_global_vars , only : ts_print_transiesta use m_ts_method use m_ts_options , only : N_Elec , Elecs use m_ts_options , only : DM_bulk use m_ts_options , only : val_swap use m_ts_options , only : ts_Dtol , ts_Htol use m_ts_options , only : ts_hist_keep use m_ts_options , only : ts_siesta_stop use m_ts_options , only : ts_scf_mixs use m_ts_electype integer :: iEl , na_a integer , allocatable :: allowed_a (:) real ( dp ), pointer :: DM (:,:), EDM (:,:) ! We are done with the initial diagon run ! Now we start the TRANSIESTA (Green functions) run if ( . not . TSmode ) return if ( . not . TSinit ) return ! whether we are in siesta initialization step TSinit = . false . ! whether transiesta is running TSrun = . true . ! If transiesta should stop immediately if ( ts_siesta_stop ) then if ( IONode ) then write ( * , '(a)' ) 'ts: Stopping transiesta (user option)!' end if return end if ! Reduce memory requirements call resetDenseMatrix () ! Signal to continue... ! These two variables are from the top-level ! routine (siesta_forces) SCFconverged = . false . iscf = 0 ! DANGER (when/if going back to the DIAGON run, we should ! re-instantiate the original mixing value) call val_swap ( dDtol , ts_Dtol ) call val_swap ( dHtol , ts_Htol ) ! Clean up mixing history if ( mix_charge ) then call resetRhoG (. true .) else if ( associated ( ts_scf_mixs , target = scf_mixs ) ) then do iel = 1 , size ( scf_mix % stack ) call reset ( scf_mix % stack ( iel ), - ts_hist_keep ) ! Reset iteration count as certain ! mixing schemes require this for consistency scf_mix % cur_itt = n_items ( scf_mix % stack ( iel )) end do else call mixers_history_init ( scf_mixs ) end if end if ! Transfer scf_mixing to the transiesta mixing routine scf_mix => ts_scf_mixs ( 1 ) #ifdef SIESTA__FLOOK if ( . not . mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif call ts_print_transiesta () ! In case of transiesta and DM_bulk. ! In case we ask for initialization of the DM in bulk ! we read in the DM files from the electrodes and ! initialize the bulk to those values if ( DM_bulk > 0 ) then if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Initializing bulk DM in electrodes.' end if na_a = 0 do iEl = 1 , na_u if ( . not . a_isDev ( iEl ) ) na_a = na_a + 1 end do allocate ( allowed_a ( na_a )) na_a = 0 do iEl = 1 , na_u ! We allow the buffer atoms as well (this will even out the ! potential around the back of the electrode) if ( . not . a_isDev ( iEl ) ) then na_a = na_a + 1 allowed_a ( na_a ) = iEl end if end do do iEl = 1 , N_Elec if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Reading in electrode TSDE for ' // & trim ( Elecs ( iEl )% Name ) end if ! Copy over the DM in the lead ! Notice that the EDM matrix that is copied over ! will be equivalent at Ef == 0 call copy_DM ( Elecs ( iEl ), na_u , xa , lasto , nsc , isc_off , & ucell , DM_2D , EDM_2D , na_a , allowed_a ) end do ! Clean-up deallocate ( allowed_a ) if ( IONode ) then write ( * , * ) ! new-line end if ! The electrode EDM is aligned at Ef == 0 ! We need to align the energy matrix DM => val ( DM_2D ) EDM => val ( EDM_2D ) iEl = size ( DM ) call daxpy ( iEl , Ef , DM ( 1 , 1 ), 1 , EDM ( 1 , 1 ), 1 ) end if end subroutine transiesta_switch end subroutine siesta_forces end module m_siesta_forces","tags":"","loc":"sourcefile/siesta_forces.f90.html"},{"title":"siesta_analysis.F – SIESTA","text":"Contents Modules m_siesta_analysis Source Code siesta_analysis.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_siesta_analysis use write_subs private public :: siesta_analysis CONTAINS subroutine siesta_analysis ( relaxd ) USE band , only : nbk , bk , maxbk , bands USE writewave , only : nwk , wfk , wwave USE writewave , only : setup_wfs_list , wfs_filename USE m_ksvinit , only : nkpol , kpol , wgthpol use m_ksv USE m_projected_DOS , only : projected_DOS USE m_local_DOS , only : local_DOS #ifdef SIESTA__PEXSI USE m_pexsi_local_DOS , only : pexsi_local_DOS USE m_pexsi_dos , only : pexsi_dos #endif USE siesta_options use units , only : Debye , eV use sparse_matrices , only : maxnh , listh , listhptr , numh use sparse_matrices , only : H , S , Dscf , xijo use siesta_geom use m_dhscf , only : dhscf use atomlist , only : indxuo , iaorb , lastkb , lasto , datm , no_l , & iphkb , no_u , no_s , iza , iphorb , rmaxo , indxua use atomlist , only : qtot use fdf use writewave , only : wwave use siesta_cml use files , only : slabel use files , only : filesOut_t ! derived type for output file names use zmatrix , only : lUseZmatrix , write_zmatrix use Kpoint_grid use parallel , only : IOnode use parallel , only : SIESTA_worker use files , only : label_length use m_energies use m_steps , only : final use m_ntm use m_spin , only : nspin , spinor_dim , h_spin_dim use m_spin , only : SpOrb , NonCol , SPpol , NoMagn use m_dipol use m_eo use m_forces , only : fa use m_gamma use alloc , only : re_alloc , de_alloc use basis_enthalpy , only : write_basis_enthalpy use m_partial_charges , only : want_partial_charges use m_iodm_old , only : read_spmatrix use m_siesta2wannier90 , only : siesta2wannier90 #ifdef MPI use m_mpi_utils , only : globalize_sum #endif #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_ANALYSIS #endif implicit none logical :: relaxd , getPSI , quenched_MD , found real ( dp ) :: dummy_str ( 3 , 3 ) real ( dp ) :: dummy_strl ( 3 , 3 ) real ( dp ) :: qspin ( 4 ) ! Local real ( dp ) :: polxyz ( 3 , nspin ) ! Autom., small real ( dp ) :: polR ( 3 , nspin ) ! Autom., small real ( dp ) :: qaux real ( dp ), pointer :: ebk (:,:,:) ! Band energies integer :: j , ix , ind , ik , io , ispin integer :: wfs_band_min , wfs_band_max real ( dp ) :: g2max , current_ef #ifdef MPI real ( dp ) :: qtmp ( 4 ) #endif type ( filesOut_t ) :: filesOut ! blank output file names !-----------------------------------------------------------------------BEGIN if ( SIESTA_worker ) call timer ( \"Analysis\" , 1 ) !! Check that we are converged in geometry, !! if strictly required, !! before carrying out any analysis. !!@code quenched_MD = ( ( iquench > 0 ) . and . $ (( idyn . eq . 1 ) . or . ( idyn . eq . 3 )) ) if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( GeometryMustConverge . and . (. not . relaxd )) then call message ( \"FATAL\" , $ \"GEOM_NOT_CONV: Geometry relaxation not converged\" ) call die ( \"ABNORMAL_TERMINATION\" ) endif endif !!@endcode !     All the comments below assume that this compatibility option !     is not used. !     Note also that full compatibility cannot be guaranteed if (. not . compat_pre_v4_dynamics ) then !     This is a sanity safeguard: we reset the geometry (which might !     have been moved by the relaxation or MD routines) to the one used !     in the last computation of the electronic structure. !     See the comments below for explanation !$OMP parallel workshare default(shared) xa ( 1 : 3 , 1 : na_s ) = xa_last ( 1 : 3 , 1 : na_s ) ucell ( 1 : 3 , 1 : 3 ) = ucell_last ( 1 : 3 , 1 : 3 ) scell ( 1 : 3 , 1 : 3 ) = scell_last ( 1 : 3 , 1 : 3 ) !$OMP end parallel workshare endif ! zmatrix info reset?? if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then if ( SIESTA_worker ) then call read_spmatrix ( maxnh , no_l , h_spin_dim , numh , . listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) current_ef = ef ef = fdf_get ( \"Manual-Fermi-Level\" , current_ef , \"Ry\" ) endif endif #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.DOS\" ,. false .)) then call pexsi_dos ( no_u , no_l , spinor_dim , $ maxnh , numh , listhptr , listh , H , S , qtot , ef ) endif #endif ! section done by Siesta subset of nodes if ( SIESTA_worker ) then final = . true . if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'Finalization' ) endif #ifdef SIESTA__FLOOK ! Call lua right before doing the analysis, ! possibly changing some of the variables call slua_call ( LUA , LUA_ANALYSIS ) #endif ! !     NOTE that the geometry output by the following sections !     used to be that \"predicted\" for the next MD or relaxation step. !     This is now changed ! if ( IOnode ) then ! Print atomic coordinates ! This covers CG and MD-quench (verlet, pr), instances in which ! \"relaxd\" is meaningful if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( relaxd ) then ! xa = xa_last ! The \"relaxation\" routines do not update ! the coordinates if relaxed, so this behavior is unchanged call outcoor ( ucell , xa , na_u , 'Relaxed' , . true . ) else ! Since xa = xa_last now, this will just repeat the ! last set of coordinates used, not the predicted ones. call outcoor ( ucell , xa , na_u , 'Final (unrelaxed)' , . true . ) endif endif ! This call will write xa_last to the .STRUCT_OUT file ! (again, since it has already been written by state_init), ! CML records of the latest processed structure, and ! possibly zmatrix info.  *** unmoved?? how? ! Note that the .STRUCT_NEXT_ITER file is produced ! in siesta_move for checkpointing of relaxations and MD runs. ! If all we want are the CML records (to satisfy some expectation ! of appearance in the \"finalization\" section, we might put the ! cml call explicitly and forget about the rest. if ( compat_pre_v4_dynamics ) then call siesta_write_positions ( moved = . true .) else call siesta_write_positions ( moved = . false .) endif ! ??  Clarify Zmatrix behavior **** if ( lUseZmatrix ) call write_Zmatrix ! Print unit cell (cell_last) for variable cell and server operation if ( varcel . or . ( idyn . eq . 8 )) call outcell ( ucell ) !------------------------------------------------------------------ ! It can be argued that these needed the xa_last coordinates ! all along !       Print coordinates in xmol format in a separate file if ( fdf_boolean ( 'WriteCoorXmol' ,. false .)) & call coxmol ( iza , xa , na_u ) !       Print coordinates in cerius format in a separate file if ( fdf_boolean ( 'WriteCoorCerius' ,. false .)) & call coceri ( iza , xa , ucell , na_u , sname ) !       Find interatomic distances (output in file BONDS_FINAL) call bonds ( ucell , na_u , isa , xa , & rmax_bonds , trim ( slabel ) // \".BONDS_FINAL\" ) endif ! IONode !--- end output of geometry information ! ! ! NOTE: In the following sections, wavefunction generation, computation !       of band structure, etc, are carried out using the last Hamiltonian !       generated in the SCF run for the last geometry considered. !   But, if xa /= xa_last, the computation of, say, bands, will use !      H phases which are not the same as those producing the final !      ground-state electronic structure. ! !    Also, since we have removed the replication (superx call) !      of \"moved\" coordinates !      into the auxiliary supercell from 'siesta_move' (recall that it is !      done always in state_init for every new geometry), the \"moved unit !      cell coordinates\" could coexist here with \"unmoved non-unit cell SC coords\", !      which is wrong. !      For all of the above, we should put here a sanity safeguard !        (if we have not done so already at the top of this routine) !        xa(1:3,1:na_s) = xa_last(1:3,1:na_s) !        ucell(1:3,1:3) = ucell_last(1:3,1:3) !        scell(1:3,1:3) = scell_last(1:3,1:3) !        DM and H issues ! !        Some of the routines that follow use H and S, and some use the DM. !        Those which use the DM should work with the final \"out\" DM for !        consistency. !        Those which use H,S should work with the latest diagonalized H,S pair. ! !      If mixing the DM during the scf loop we should avoid mixing it one more time !        after convergence (or restoring Dold) !        If mixing H, we should avoid mixing it one more time !        after convergence (and restoring Hold to have the exact H that generated the !        latest DM, although this is probably too much). !        See the logic in siesta_forces. !     Find and print wavefunctions at selected k-points !   This uses H,S, and xa if ( nwk . gt . 0 ) then wfs_filename = trim ( slabel ) // \".selected.WFSX\" if ( IONode ) print \"(a)\" , $ \"Writing WFSX for selected k-points in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , & nwk , & numh , listhptr , listh , H , S , Ef , xijo , indxuo , & nwk , wfk , no_u , gamma , occtol ) endif !   This uses H,S, and xa if ( write_coop ) then ! Output the wavefunctions for the kpoints in the SCF set ! Note that we allow both a band number and an energy range ! The user is responsible for using appropriate values. wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nkpnt , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . true ., $ use_energy_window = . true .) wfs_filename = trim ( slabel ) // \".fullBZ.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for COOP/COHP in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . nkpnt , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . nkpnt , kpoint , no_u , gamma , occtol ) endif !     Compute bands !   This uses H,S, and xa nullify ( ebk ) call re_alloc ( ebk , 1 , no_u , 1 , spinor_dim , 1 , maxbk , & 'ebk' , 'siesta_analysis' ) if ( nbk . gt . 0 ) then if ( IONode ) print \"(a)\" , \"Computing bands...\" getPSI = fdf_get ( 'WFS.Write.For.Bands' , . false .) if ( getPSI ) then wfs_filename = trim ( slabel ) // \".bands.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for bands in \" $ // trim ( wfs_filename ) wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nbk , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . false ., $ use_energy_window = . false .) endif call bands ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . maxbk , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . . true ., nbk , bk , ebk , occtol , getPSI ) if ( IOnode ) then if ( writbk ) then write ( 6 , '(/,a,/,a4,a12)' ) & 'siesta: Band k vectors (Bohr**-1):' , 'ik' , 'k' do ik = 1 , nbk write ( 6 , '(i4,3f12.6)' ) ik , ( bk ( ix , ik ), ix = 1 , 3 ) enddo endif if ( writb ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Band energies (eV):' , 'ik' , 'is' , 'eps' do ispin = 1 , spinor_dim do ik = 1 , nbk write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin , ( ebk ( io , ispin , ik ) / eV , io = 1 , min ( 10 , no_u )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( ebk ( io , ispin , ik ) / eV , io = 11 , no_u ) enddo enddo endif endif endif !     Print eigenvalues if ( IOnode . and . writeig ) then if (( isolve . eq . SOLVE_DIAGON . or . & (( isolve . eq . SOLVE_MINIM ) . and . minim_calc_eigenvalues )) & . and . no_l . lt . 1000 ) then if ( h_spin_dim <= 2 ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Eigenvalues (eV):' , 'ik' , 'is' , 'eps' do ik = 1 , nkpnt do ispin = 1 , spinor_dim write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin ,( eo ( io , ispin , ik ) / eV , io = 1 , min ( 10 , neigwanted )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( eo ( io , ispin , ik ) / eV , io = 11 , neigwanted ) enddo enddo else write ( 6 , '(/,a)' ) 'siesta: Eigenvalues (eV):' do ik = 1 , nkpnt write ( 6 , '(a,i6)' ) 'ik =' , ik write ( 6 , '(10f7.2)' ) & (( eo ( io , ispin , ik ) / eV , io = 1 , neigwanted ), ispin = 1 , 2 ) enddo endif write ( 6 , '(a,f15.6,a)' ) 'siesta: Fermi energy =' , ef / eV , ' eV' endif endif if ((( isolve . eq . SOLVE_DIAGON ). or . & (( isolve . eq . SOLVE_MINIM ). and . minim_calc_eigenvalues )) & . and . IOnode ) & call ioeig ( eo , ef , neigwanted , nspin , nkpnt , no_u , spinor_dim , & nkpnt , kpoint , kweight ) !   This uses H,S, and xa, as it diagonalizes them again call projected_DOS () !     Print program's energy decomposition and final forces if ( IOnode ) then call siesta_write_energies ( iscf = 0 , dDmax = 0._dp , dHmax = 0._dp ) ! final == .true. which makes the step counter irrelevant call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () call write_basis_enthalpy ( FreeE , FreeEHarris ) endif ! NOTE: Here, the spin polarization is computed using Dscf, which is !       a density matrix obtained after mixing the \"in\" and \"out\" !       DMs of the SCF run for the last geometry considered. !       This can be considered a feature or a bug. call print_spin ( qspin ) ! qspin returned for use below !     This uses the last computed dipole in dhscf during the scf cycle, !     which is in fact derived from the \"in\" DM. !     Perhaps this section should be moved after the call to dhscf below !     AND use the DM_out of the last step (but there might not be a call !     to dhscf if there are no files to output, and the computation of the !     charge density is expensive... !     Print electric dipole if ( shape . ne . 'bulk' ) then if ( IOnode ) then write ( 6 , '(/,a,3f12.6)' ) & 'siesta: Electric dipole (a.u.)  =' , dipol write ( 6 , '(a,3f12.6)' ) & 'siesta: Electric dipole (Debye) =' , & ( dipol ( ix ) / Debye , ix = 1 , 3 ) endif if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = dipol / Debye , & title = 'Electric dipole' , dictref = 'siesta:dipol' , . units = 'siestaUnits:Debye' ) endif !cml_p endif ! NOTE: The use of *_last geometries in the following sections !       guarantees that the analysis of the electronic structure !       is done for the geometry for which it was computed. !  BUT these routines need H,S, so H should not be mixed after !       convergence. !     Calculation of the bulk polarization using the Berry phase !     formulas by King-Smith and Vanderbilt if ( nkpol . gt . 0 . and . . not . bornz ) then if ( NonCol . or . SpOrb ) then if ( IOnode ) then write ( 6 , '(/a)' ) . 'siesta_analysis: bulk polarization implemented only for' write ( 6 , '(/a)' ) . 'siesta_analysis: paramagnetic or collinear spin runs' endif else call KSV_pol ( na_u , na_s , xa_last , rmaxo , scell_last , & ucell_last , no_u , no_l , no_s , nspin , qspin , & maxnh , nkpol , numh , listhptr , listh , & H , S , xijo , indxuo , isa , iphorb , & iaorb , lasto , shape , & nkpol , kpol , wgthpol , polR , polxyz ) endif endif !     Calculation of the optical conductivity call optical ( na_u , na_s , xa_last , scell_last , ucell_last , & no_u , no_l , no_s , nspin , qspin , & maxnh , numh , listhptr , listh , H , S , xijo , $ indxuo , ebk , ef , temp , & isa , iphorb , iphKB , lasto , lastkb , shape ) call de_alloc ( ebk , 'ebk' , 'siesta_analysis' ) !................................... ! !  NOTE: Dscf here might be the mixed one (see above). ! want_partial_charges = ( hirshpop . or . voropop ) . AND . $ (. not . partial_charges_at_every_geometry ) !     Save electron density and potential if ( saverho . or . savedrho . or . saverhoxc . or . & savevh . or . savevt . or . savevna . or . & savepsch . or . savetoch . or . & want_partial_charges ) then if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' g2max = g2cut dummy_str = 0.0 dummy_strl = 0.0 call dhscf ( nspin , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa_last , indxua , & ntm , 0 , 0 , 0 , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_str , fa , dummy_strl ) ! next to last argument is dummy here, ! as no forces are calculated ! todo: make all these optional endif C C     Call the wannier90 interface here, as local_DOS destroys the DM... C if ( w90_processing ) call siesta2wannier90 () C     Find local density of states !  It needs H,S, and xa, as it diagonalizes them again !  NOTE: This call will obliterate Dscf !  It is better to put a explicit out argument for the partial DM computed. call local_DOS () ! In summary, it is better to: ! !   -- Avoid (or warn the user about) doing any analysis if the calculation is not converged !   -- Avoid mixing DM or H after scf convergence !   -- Set xa to xa_last at the top of this file. Write any \"next iter\" coordinate file !      in 'siesta_move' endif ! SIESTA_worker #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.LDOS\" ,. false .)) then call pexsi_local_DOS () endif #endif if ( SIESTA_worker ) call timer ( \"Analysis\" , 2 ) !------------------------------------------------------------------------- END END subroutine siesta_analysis END module m_siesta_analysis","tags":"","loc":"sourcefile/siesta_analysis.f.html"},{"title":"moremeshsubs.F – SIESTA","text":"Contents Modules moreMeshSubs Source Code moremeshsubs.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! #ifdef ASYNCHRONOUS_GRID_COMMS   /* Compile-time option */ # ifdef MPI #   define ASYNCHRONOUS           /* Internal symbol */ # endif #endif C This file contains module moreMeshSubs. It defines and handles C different parallel mesh distributions. C C Written by Rogeli Grima (BSC) Dec.2007 C C Includes the following subroutines connected to the mesh : C C initMeshDistr     = Precompute a new data distribution for the grid mesh C                     to be used in Hamiltonian construction. C setMeshDistr      = Select a mesh distribution and set the grid sizes. C distMeshData      = Move data from one data distribution to another. C allocASynBuffer   = Allocate buffer for asynchronous C                     communications if necessary C MODULE moreMeshSubs use precision , only : grid_p , dp , i8b use parallel , only : node , Nodes , ProcessorY use sys , only : die use alloc , only : re_alloc , de_alloc implicit none PUBLIC :: initMeshDistr , setMeshDistr , allocExtMeshDistr PUBLIC :: allocIpaDistr , distMeshData , resetMeshDistr #ifdef MPI PUBLIC :: initMeshExtencil , distExtMeshData , gathExtMeshData #endif PUBLIC :: allocASynBuffer !     Symbolic names for parallel mesh distributions integer , parameter , public :: UNIFORM = 1 integer , parameter , public :: QUADRATIC = 2 integer , parameter , public :: LINEAR = 3 ! !     Symbolic names for \"reord\"-type operations ! integer , parameter , public :: TO_SEQUENTIAL = + 1 integer , parameter , public :: TO_CLUSTER = - 1 integer , parameter , public :: KEEP = 0 PRIVATE interface distMeshData module procedure distMeshData_rea , distMeshData_int end interface distMeshData !     Private type to hold mesh distribution data TYPE meshDisType integer :: nMesh ( 3 ) ! Number of mesh div. in each axis integer , pointer :: box (:,:,:) ! Mesh box bounds of each node: ! box(1,iAxis,iNode)=lower bounds ! box(2,iAxis,iNode)=upper bounds integer , pointer :: indexp (:) integer , pointer :: idop (:) real ( dp ), pointer :: xdop (:,:) integer , pointer :: ipa (:) END TYPE meshDisType !     Private type to hold communications to move data from one !     distribution to another TYPE meshCommType integer :: ncom ! Number of needed communications integer , pointer :: src (:) ! Sources of communications integer , pointer :: dst (:) ! Destination of communications END TYPE meshCommType C ---------------------------------------------------------------------- C moreMeshSubs variables C ---------------------------------------------------------------------- C character    moduName  : Name of the module C integer      maxDistr  : Maximum number of data distribution that C                          can be handled. C integer      gp        : Alias of the grid precision C meshDisType  meshDistr : Contains information of the several data C                          distributions C meshCommType meshCommu : Contains all the communications to move among C                          the several data distributions C meshCommType exteCommu : Contains all the needed communications to C                          compute the extencil C ---------------------------------------------------------------------- character ( len =* ), parameter :: moduName = 'moreMeshSubs' integer , parameter :: maxDistr = 5 integer , parameter :: gp = grid_p type ( meshDisType ), target , save :: meshDistr ( maxDistr ) type ( meshCommType ), target , save :: & meshCommu (( maxDistr * ( maxDistr - 1 )) / 2 ) type ( meshCommType ), target , save :: exteCommu ( maxDistr , 3 ) #ifdef ASYNCHRONOUS real ( grid_p ), pointer :: tBuff1 (:) real ( grid_p ), pointer :: tBuff2 (:) #endif CONTAINS subroutine initMeshDistr ( iDistr , oDistr , nm , wload ) C ================================================================== C Computes a new data distribution and the communications needed to C move data from/to the current distribution to the existing ones. C The limits of the new distributions are stored in meshDistr(oDistr). C ================================================================== C SUBROUTINE initMeshDistr( iDistr, oDistr, nm, wload ) C C INPUT: C integer iDistr  : Distribution index of the input vector C integer oDistr  : The new data distribution index. C integer nm(3)   : Number of Mesh divisions of each cell vector C integer wload   : Weights of every point of the mesh using the C                   input distribution C C OUTPUT: C The output values are stored in the current module: C        meshDistr(oDistr) C        meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) C C BEHAVIOR: C If this is the first distribution, we split the mesh uniformly among C the several processes (we only split it in dimensions Y and Z). C C For the other data distributions we should split the vector wload. C The subroutine splitwload will return the limits of the new data C distribution. The subroutine compMeshComm will return the communications C needed to move data from/to the current distribution to/from the C previous ones. C C ================================================================== implicit none C     Passed arguments integer , optional , intent ( in ) :: iDistr integer , intent ( in ) :: oDistr integer , intent ( in ) :: nm ( 3 ) integer , optional , intent ( in ) :: wload ( * ) C     Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) !------------------------------------------------------------------------- BEGIN call timer ( 'INITMESH' , 1 ) C     Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) C     Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) C     Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) C     The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else C       In order to compute the other data distributions, we should split C       the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) C       Precompute the communications needed to move data between the new data C       distribution and the previous ones. jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) !--------------------------------------------------------------------------- END end subroutine initMeshDistr C ================================================================== C Allocate memory buffers for asynchronous communications. C It does nothing for synchronous communications. C ================================================================== C SUBROUTINE allocASynBuffer( ndistr ) C C INPUT: C integer ndistr  : Total number of distributions C C OUTPUT: C The output values are stored in the current module: C  real(grid_p) tBuff1(:) :  Buffer for distribution 1 C  real(grid_p) tBuff2(:) :  Buffer for other distributions C C BEHAVIOR: C C C ================================================================== subroutine allocASynBuffer ( ndistr ) use mesh , only : nsm implicit none C     Input variables integer :: ndistr C     Local variables integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters !------------------------------------------------------------------------- BEGIN #ifdef ASYNCHRONOUS C     Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) C     Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo C     Check the size of the intersections between the first data distributions C     and the others data distributions. C     Buffers don't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr ( 1 )% box (:,:, node + 1 ) do ii = 2 , ndistr box2 => meshDistr ( ii )% box (:,:, node + 1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp else lsize = 0 endif imax1 = max ( imax1 , nsize ( 1 ) - lsize ) imax2 = max ( imax2 , nsize ( ii ) - lsize ) enddo C     Deallocate local memory call de_alloc ( nsize , 'nsize' , moduName ) C     Allocate memory for asynchronous communications call re_alloc ( tBuff1 , 1 , imax1 , 'tBuff1' , moduName ) call re_alloc ( tBuff2 , 1 , imax2 , 'tBuff2' , moduName ) #endif !--------------------------------------------------------------------------- END end subroutine allocASynBuffer subroutine allocExtMeshDistr ( iDistr , nep , mop ) use mesh , only : indexp , idop , xdop implicit none C     Input variables integer , intent ( in ) :: iDistr , nep , mop C     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % indexp , 1 , nep , 'distr%indexp' , moduName ) call re_alloc ( distr % idop , 1 , mop , 'distr%idop' , moduName ) call re_alloc ( distr % xdop , 1 , 3 , 1 , mop , 'distr%xdop' , moduName ) indexp => distr % indexp idop => distr % idop xdop => distr % xdop end subroutine allocExtMeshDistr subroutine allocIpaDistr ( iDistr , na ) use mesh , only : ipa implicit none C     Input variables integer , intent ( in ) :: iDistr , na C     Local variables type ( meshDisType ), pointer :: distr !------------------------------------------------------------------------- BEGIN distr => meshDistr ( iDistr ) call re_alloc ( distr % ipa , 1 , na , 'distr%ipa' , moduName ) ipa => meshDistr ( iDistr )% ipa !--------------------------------------------------------------------------- END end subroutine allocIpaDistr subroutine setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) C ================================================================== C Fixes the new data limits and dimensions of the mesh to those of C the data distribution iDistr. C ================================================================== C SUBROUTINE setMeshDistr( iDistr, nsm, nsp, nml, nmpl, ntml, ntpl ) C C INPUT: C integer iDistr  : Distribution index of the input vector C integer nsm     : Number of mesh sub-divisions in each direction C integer nsp     : Number of sub-points of each mesh point C C OUTPUT: C integer nml(3)  : Local number of Mesh divisions in each cell vector C integer nmpl    : Local number of Mesh divisions C integer ntml(3) : Local number of Mesh points in each cell vector C integer ntpl    : Local number of Mesh points C C BEHAVIOR: C Fixes the new data limits and dimensions of the mesh to those of C the data distribution iDistr. C C ================================================================== use mesh , only : meshLim , indexp , ipa , idop , xdop implicit none C     Passed arguments integer , intent ( in ) :: iDistr , nsm , nsp integer , intent ( out ) :: nml ( 3 ), nmpl , ntml ( 3 ), ntpl C     Local variables type ( meshDisType ), pointer :: distr !------------------------------------------------------------------------- BEGIN distr => meshDistr ( iDistr ) meshLim = distr % box ( 1 : 2 , 1 : 3 , node + 1 ) nml ( 1 ) = ( MeshLim ( 2 , 1 ) - MeshLim ( 1 , 1 )) + 1 nml ( 2 ) = ( MeshLim ( 2 , 2 ) - MeshLim ( 1 , 2 )) + 1 nml ( 3 ) = ( MeshLim ( 2 , 3 ) - MeshLim ( 1 , 3 )) + 1 nmpl = nml ( 1 ) * nml ( 2 ) * nml ( 3 ) ntml = nml * nsm ntpl = nmpl * nsp indexp => distr % indexp idop => distr % idop xdop => distr % xdop ipa => distr % ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr subroutine resetMeshDistr ( iDistr ) C ================================================================== C Reset the data of the distribution iDistr C ================================================================== C SUBROUTINE resetMeshDistr( iDistr ) C C INPUT: C integer iDistr   : Distribution index to be reset. C C OUTPUT: C Modify data of the current module. C C BEHAVIOR: C Deallocate associated arrays of the current distribution C ================================================================== implicit none C     Passed arguments integer , optional , intent ( in ) :: iDistr C     Local variables integer :: idis , ini , fin , icom type ( meshDisType ), pointer :: distr type ( meshCommType ), pointer :: mcomm !------------------------------------------------------------------------- BEGIN if ( present ( iDistr )) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis = ini , fin distr => meshDistr ( idis ) distr % nMesh = 0 if ( associated ( distr % box )) then call de_alloc ( distr % box , 'distr%box' , 'moreMeshSubs' ) endif if ( associated ( distr % indexp )) then call de_alloc ( distr % indexp , 'distr%indexp' , & 'moreMeshSubs' ) endif if ( associated ( distr % idop )) then call de_alloc ( distr % idop , 'distr%idop' , & 'moreMeshSubs' ) endif if ( associated ( distr % xdop )) then call de_alloc ( distr % xdop , 'distr%xdop' , & 'moreMeshSubs' ) endif if ( associated ( distr % ipa )) then call de_alloc ( distr % ipa , 'distr%ipa' , & 'moreMeshSubs' ) endif do icom = 1 , 3 mcomm => exteCommu ( idis , icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo do icom = (( idis - 2 ) * ( idis - 1 )) / 2 + 1 , (( idis - 1 ) * idis ) / 2 mcomm => meshCommu ( icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if ( associated ( tBuff1 )) then call de_alloc ( tBuff1 , 'tBuff1' , 'moreMeshSubs' ) endif if ( associated ( tBuff2 )) then call de_alloc ( tBuff2 , 'tBuff2' , 'moreMeshSubs' ) endif #endif !--------------------------------------------------------------------------- END end subroutine resetMeshDistr C ================================================================== C Move data from vector fsrc, that uses distribution iDistr, to vector C fdst, that uses distribution oDistr. It also, re-orders a clustered C data array into a sequential one and viceversa. C If this is a sequencial execution, it only reorders the data. C C NOTE: There are two subroutines: one to deal with real data and C the other with integers. Both are called using the same interface. ! ! AG: NOTE that the integer version does NOT have the exact functionality ! of the real version. In particular, the integer version has no provision ! for a \"serial fallback\", and so this case has been trapped. C ================================================================== C SUBROUTINE distMeshData( iDistr, fsrc, oDistr, fdst, itr ) C C INPUT: C integer      iDistr : Distribution index of the input vector. C real/integer fsrc   : Input vector. C integer      oDistr : Distribution index of the output vector. C integer itr         : TRanslation-direction switch C                       ITR=+1 => From clustered to sequential C                       ITR=-1 => From sequential to clustered C                       ITR=0  => Keep the status C C OUTPUT: C real/integer fdst   : Output vector. C C BEHAVIOR: C Check the communications that this process should do to move data C from iDistr to odistr. We have 3 kind of communications (send, receive C and keep on the same node). We have 3 kind of reorderings (clustered to C sequential, sequential to clustered and keep the same ordering). C C For the sequencial code we call subroutine reord C C ================================================================== #ifdef ASYNCHRONOUS subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none C     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) C     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 C       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then C         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then C         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 C         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else C               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea #else /* SYNCHRONOUS communications */ subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none C     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) C     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & N1 , N2 , N3 , NN , ind , ncom , & icom , NSP , NSRC ( 3 ), NDST ( 3 ), ME , & MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), JS (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters real ( grid_p ), pointer :: TBUF (:) integer :: nsize , nm ( 3 ) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif !----------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) if ( nodes == 1 ) then nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 C       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) NSP = NSM * NSM * NSM ME = Node + 1 nullify ( JS ) call re_alloc ( JS , 1 , NSP , 'JS' , 'moreMeshSubs' ) C       Compute the maximum size of the buffer needed to transfer data C       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo MaxSize = MaxSize * nsp if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then C         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP TBUF ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = TBUF ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then C         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP TBUF ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then C         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else C               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif call de_alloc ( JS , 'JS' , 'moreMeshSubs' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif !------------------------------------------------------------------------ END end subroutine distMeshData_rea #endif subroutine distMeshData_int ( iDistr , fsrc , oDistr , fdst , itr ) #ifdef MPI use mpi_siesta #endif implicit none C     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr integer , intent ( in ) :: fsrc ( * ) integer , intent ( out ) :: fdst ( * ) C     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & ind , ncom , icom , NSRC ( 3 ), NDST ( 3 ), & ME , MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters integer , pointer :: TBUF (:) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif !---------------------------------------------------------------------- BEGIN if ( nodes == 1 ) then call die ( \"Called _int version of distMeshData for n=1\" ) else C       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) ME = Node + 1 C       Compute the maximum size of the buffer needed to transfer data C       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 NSRC ( 2 ) = Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 NSRC ( 3 ) = Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 Dbox => odis % box (:,:, ME ) NDST ( 1 ) = Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 NDST ( 2 ) = Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 NDST ( 3 ) = Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 if ( itr . eq . 0 ) then C         From sequencial to sequencial do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else C               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif endif !--------------------------------------------------------------------------- END end subroutine distMeshData_int C ================================================================== C Checks if there is an intersection between 2 boxes and, if it exist C it returns the resulting box. C ================================================================== C SUBROUTINE boxIntersection( ibox1, ibox2, obox, inters ) C C INPUT: C integer ibox1(2,3)       : Input box 1 C integer ibox2(2,3)       : Input box 2 C C OUTPUT: C integer obox(2,3)        : Intersection between ibox1 and ibox2 C logical inters           : TRUE: There is intersection C                            FALSE: There is not intersection C C BEHAVIOR: C Checks the three axis of the input boxes to see if there is C intersection between the input boxes. C C ================================================================== subroutine boxIntersection ( ibox1 , ibox2 , obox , inters ) implicit none C     Passed arguments integer , intent ( in ) :: ibox1 ( 2 , 3 ), ibox2 ( 2 , 3 ) integer , intent ( out ) :: obox ( 2 , 3 ) logical , intent ( out ) :: inters C     Local variables integer :: iaxis !------------------------------------------------------------------------- BEGIN inters = . true . do iaxis = 1 , 3 obox ( 1 , iaxis ) = max ( ibox1 ( 1 , iaxis ), ibox2 ( 1 , iaxis )) obox ( 2 , iaxis ) = min ( ibox1 ( 2 , iaxis ), ibox2 ( 2 , iaxis )) if ( obox ( 2 , iaxis ). lt . obox ( 1 , iaxis )) inters = . false . enddo !--------------------------------------------------------------------------- END end subroutine boxIntersection #ifdef MPI subroutine initMeshExtencil ( iDistr , nm ) C ================================================================== C Compute the needed communications in order to send/receive the C extencil (when the data is ordered in the distribution iDistr) C ================================================================== C SUBROUTINE initMeshExtencil( iDistr, nm ) C C INPUT: C integer iDistr   : Distribution index to be used. C integer nm(3)    : Number of Mesh divisions in each cell vector C C OUTPUT: C The results are stored in the variable exteCommu(iDistr,1:3) of C the current module. C C BEHAVIOR: C For every dimension of the problem, search all the neightbours that C we have. Given the current data distribution we compute the limits C of our extencil and we check its intersection with all the other C processes. Once we know all our neightbours we call subroutine C scheduleComm in order to minimize the number of communications steps. C C ================================================================== use scheComm implicit none C     Passed arguments integer , intent ( in ) :: iDistr , nm ( 3 ) C     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), Ibox ( 2 , 3 ), & ii , iaxis , ncom , Gcom , Lcom , P1 , P2 integer , pointer :: src (:), dst (:), Dbox (:,:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm type ( COMM_T ) :: comm logical :: inters !------------------------------------------------------------------------- BEGIN idis => meshDistr ( iDistr ) do iaxis = 1 , 3 C       One communication structure for every dimension mcomm => exteCommu ( iDistr , iaxis ) C       Count the number of communications needed to send/receive C       the extencil ncom = 0 do P1 = 1 , Nodes C         Create the extencil boxes for both sides of the current C         partition Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) ncom = ncom + 1 endif enddo enddo Gcom = ncom C       Create a list of communications needed to send/receive C       the extencil if ( Gcom . gt . 0 ) then nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) ncom = 0 do P1 = 1 , Nodes Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif endif enddo enddo comm % np = Nodes C         reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) C         Count the number of communications needed by the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom C         Store the ordered list of communications needed by the current C         process to send/receive the extencil. if ( Lcom . gt . 0 ) then nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , & 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , & 'moreMeshSubs' ) ncom = 0 do P1 = 1 , comm % ncol ii = comm % ind ( P1 , Node + 1 ) if ( ii . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( ii ) mcomm % dst ( ncom ) = dst ( ii ) endif enddo mcomm % ncom = Lcom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) endif call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) endif enddo !--------------------------------------------------------------------------- END end subroutine initMeshExtencil subroutine distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , dens , BDENS ) C ================================================================== C Send/receive the extencil information from the \"dens\" matrix to the C temporal array \"BDENS\". C ================================================================== C SUBROUTINE distExtMeshData( iDistr, iaxis, BS, NSM, NN, NSPIN, C                             maxp, NMeshG, dens, BDENS ) C C INPUT: C integer iDistr    : Distribution index to be used. C integer iaxis     : Axe to be splitted C integer BS        : Dimmension of a plane in the current axe C integer NSM       : Number of mesh sub-divisions in each direction C integer NN        : Size of the extencil C integer NSPIN     : Number of pollarizations C integer MAXP      : Total number of points C integer NMeshG(3) : Number of Mesh points in each cell vector C real    DENS      : electron density matrix C C OUTPUT: C real    BDENS     : Auxiliar arrays to store the extencil from other C                     partitions. C C BEHAVIOR: C Send/receive the extencil information from the \"dens\" matrix to the C temporal array \"BDENS\". C C We have a different code for every axis. We should find if we C intersects with a neightbour node throught the upper, the lower C or both sides. C C ================================================================== use mpi_siesta implicit none C     Passed arguments integer , intent ( in ) :: iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG ( 3 ) real ( gp ), intent ( in ) :: DENS ( maxp , NSPIN ) real ( gp ), intent ( out ) :: BDENS ( BS , 2 * NN , NSPIN ) C     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) !------------------------------------------------------------------------- BEGIN idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM if (. not . associated ( mcomm % dst )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif if (. not . associated ( mcomm % src )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = dimB ( 2 ) - NN + 1 , dimB ( 2 ) uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ) - NN + 1 , dimB ( 3 ) do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) !--------------------------------------------------------------------------- END end subroutine distExtMeshData C ================================================================== C Send/receive the extencil information from the \"BVXC\" temporal array C to the array \"VXC\". C ================================================================== C SUBROUTINE gathExtMeshData( iDistr, iaxis, BS, NSM, NN, NSPIN, C                             maxp, NMeshG, BVXC, VXC ) C C INPUT: C integer iDistr    : Distribution index to be used. C integer iaxis     : Axe to be splitted C integer BS        : Dimmension of a plane in the current axe C integer NSM       : Number of mesh sub-divisions in each direction C integer NN        : Size of the extencil C integer NSPIN     : Number of pollarizations C integer MAXP      : Total number of points C integer NMeshG(3) : Number of Mesh points in each cell vector C real    BVXC      : Auxiliar array that contains the extencil of the C                     exch-corr potential C C OUTPUT: C real    VXC       : exch-corr potential C C BEHAVIOR: C Send/receive the extencil information from the \"BVXC\" temporal array C to the array \"VXC\". C C We have a different code for every axis. We should find if we C intersects with a neightbour node throught the upper, the lower C or both sides. C C ================================================================== subroutine gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , BVXC , VXC ) use mpi_siesta implicit none C     Passed arguments integer , intent ( in ) :: iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG ( 3 ) real ( gp ), intent ( in ) :: BVXC ( BS , 2 * NN , NSPIN ) real ( gp ), intent ( out ) :: VXC ( maxp , NSPIN ) C     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) !------------------------------------------------------------------------- BEGIN idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = NN + 1 , 2 * NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) !--------------------------------------------------------------------------- END end subroutine gathExtMeshData C ================================================================== C Compute the limits of a new distribution, trying to split the load C of the array \"wload\". We use the nested disection algorithm in C order to split the mesh in the 3 dimensions. C ================================================================== C SUBROUTINE splitwload( Nodes, Node, nm, wload, iDistr, oDistr ) C C INPUT: C integer Nodes            : Total number of nodes C integer Node             : current Process ID (from 1 to Node) C integer NM               : Number of mesh sub-divisions in each direction C integer wload            : Weights of every point of the mesh. C type(meshDisType) iDistr : Input distribution C C OUTPUT: C type(meshDisType) oDistr : Onput distribution C C BEHAVIOR: C We use the nested disection algorithm to split the load associated C to the vector wload among all the processes. The problem is that C every process have a different part of wload. Every time that we want C to split a piece of the mesh, we should find which processors have that C information. C wload is a 3D array. In every iteration of the algorithm we should C decide the direction of the cut. Then we should made a reduction of C this 3-D array to a 1-D array (according to the selected direction). C C ================================================================== subroutine splitwload ( Nodes , Node , nm , wload , iDistr , oDistr ) use mpi_siesta implicit none C     Passed arguments integer , intent ( in ) :: Nodes , Node , nm ( 3 ), wload ( * ) type ( meshDisType ), intent ( in ) :: iDistr type ( meshDisType ), intent ( out ) :: oDistr C     Local variables character ( len =* ), parameter :: myName = moduName // 'splitwload ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: PP , Lbox ( 2 , 3 ), Ldim , & QQ , P1 , P2 , POS , ini integer ( i8b ), pointer :: lwload (:), gwload (:), recvB (:) logical :: found , inters integer :: mGdim , mLdim , nAxis , nms ( 3 ) integer ( i8b ) :: h1 , h2 integer , pointer :: PROCS (:) integer :: MPIerror , Status ( MPI_Status_Size ) !------------------------------------------------------------------------- BEGIN call timer ( 'SPLOAD' , 1 ) C     At the begining of the algorithm all the mesh is assigned to the C     first node:  oDistr%box(*,*,1) = nm oDistr % box ( 1 , 1 , 1 ) = 1 oDistr % box ( 2 , 1 , 1 ) = nm ( 1 ) oDistr % box ( 1 , 2 , 1 ) = 1 oDistr % box ( 2 , 2 , 1 ) = nm ( 2 ) oDistr % box ( 1 , 3 , 1 ) = 1 oDistr % box ( 2 , 3 , 1 ) = nm ( 3 ) oDistr % box ( 1 : 2 , 1 : 3 , 2 : Nodes ) = 0 nms = nm C     Array PROCS will contain the number of processes that are associated to C     every box. At the begining all the mesh is assigned to process 1, then C     PROCS(1)=Nodes, while the rest are equal to zero nullify ( PROCS , lwload , gwload , recvB ) call re_alloc ( PROCS , 1 , Nodes , 'PROCS' , 'moreMeshSubs' ) PROCS ( 1 ) = Nodes PROCS ( 2 : Nodes ) = 0 found = . true . do while ( found ) C       Choose the direction to cut the mesh nAxis = 3 if ( nms ( 2 ). gt . nms ( nAxis )) nAxis = 2 if ( nms ( 1 ). gt . nms ( nAxis )) nAxis = 1 nms ( nAxis ) = ( nms ( nAxis ) + 1 ) / 2 C       Check if we still have to keep cutting the mesh found = . false . do PP = Nodes , 1 , - 1 if ( PROCS ( PP ). GT . 1 ) then C           There are more than one processes associated to the mesh C           of process PP. We are going to split the mesh in two parts C           of p1 and p2 processors. p1 = PROCS ( PP ) / 2 p2 = PROCS ( PP ) - p1 found = . true . C           Check if the current partition has intersection with the piece of C           mesh that we want to cut. call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, Node ), & Lbox , inters ) if ( Node . eq . PP ) then mGdim = oDistr % box ( 2 , nAxis , PP ) - oDistr % box ( 1 , nAxis , PP ) + 1 call re_alloc ( gwload , 1 , mGdim , 'gwload' , 'moreMeshSubs' ) call re_alloc ( recvB , 1 , mGdim , 'recvB' , 'moreMeshSubs' ) endif if ( inters ) then C             If there is an intersection I should reduce the intersected part C             from a 3-D array to a 1-D array. mLdim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 call re_alloc ( lwload , 1 , mLdim , 'lwload' , & 'moreMeshSubs' ) call reduce3Dto1D ( nAxis , iDistr % box (:,:, Node ), Lbox , & wload , lwload ) endif if ( Node . eq . PP ) then C             If, I'm the process PP I should receive the information from other C             processes gwload = 0 do QQ = 1 , Nodes call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, QQ ), & Lbox , inters ) if ( inters ) then Ldim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 ini = Lbox ( 1 , nAxis ) - oDistr % box ( 1 , nAxis , PP ) if ( PP . eq . QQ ) then gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + lwload ( 1 : Ldim ) else call mpi_recv ( recvB , Ldim , MPI_INTEGER8 , QQ - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + recvB ( 1 : Ldim ) endif endif enddo call de_alloc ( recvB , 'recvB' , 'moreMeshSubs' ) C             Process PP computes where to cut the mesh call vecBisec ( mGdim , gwload ( 1 : mGdim ), & PROCS ( PP ), POS , h1 , h2 ) call de_alloc ( gwload , 'gwload' , 'moreMeshSubs' ) else if ( inters ) then C             If, I'm not the process PP I should send the information to C             the process PP call MPI_Send ( lwload , mLdim , & MPI_INTEGER8 , PP - 1 , 1 , MPI_Comm_World , & MPIerror ) endif if ( associated ( lwload )) & call de_alloc ( lwload , 'lwload' , 'moreMeshSubs' ) C           Process PP send the position of the cut to the rest of processes call MPI_Bcast ( pos , 1 , MPI_integer , PP - 1 , & MPI_Comm_World , MPIerror ) C           We have splitted the piece of mesh associated to process PP C           in two parts. One would be stored in position PP and the other C           would be stored in position PP+P1 QQ = PP + P1 oDistr % box ( 1 : 2 , 1 : 3 , QQ ) = oDistr % box ( 1 : 2 , 1 : 3 , PP ) pos = oDistr % box ( 1 , naxis , QQ ) + pos oDistr % box ( 1 , naxis , QQ ) = pos oDistr % box ( 2 , naxis , PP ) = pos - 1 C           We should actualize the numbers of processes associated to PP and QQ PROCS ( PP ) = P1 PROCS ( QQ ) = P2 endif enddo enddo call de_alloc ( PROCS , 'PROCS' , 'moreMeshSubs' ) call timer ( 'SPLOAD' , 2 ) !--------------------------------------------------------------------------- END end subroutine splitwload C ================================================================== C Given a 3-D array, \"wload\", we will make a reduction of its values C to one of its dimensions (\"iaxis\"). \"Ibox\" gives the limits of the C input array \"wload\" and \"Lbox\" gives the limits of the part that we C want to reduce. C ================================================================== C SUBROUTINE reduce3Dto1D( iaxis, Ibox, Lbox, wload, lwload ) C C INPUT: C integer   iaxis         : Axe to be reduced C integer   Ibox(2,3)     : Limits of the input array C integer   Lbox(2,3)     : Limits of the intersection that we want to reduce C integer(i8b) wload         : 3-D array that we want to reduce to one of its C                           dimensions C C OUTPUT: C integer(i8b) lwload        : 1-D array. Reduction of the intersected part C                           of wload. C C BEHAVIOR: C First we compute the 3 dimensions of the input array and the C intersection. We accumulate the values of the intersection into a C 1-D array. C C   IF (iaxis=1) lwload(II) = SUM(wload(II,*,*)) C   IF (iaxis=2) lwload(II) = SUM(wload(*,II,*)) C   IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) C C ================================================================== subroutine reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) implicit none C     Passed arguments integer , intent ( in ) :: iaxis , Ibox ( 2 , 3 ), Lbox ( 2 , 3 ), wload ( * ) integer ( i8b ), intent ( out ) :: lwload ( * ) C     Local variables integer :: Idim ( 3 ), Ldim ( 3 ), ind , ind1 , ind2 , ind3 , & I1 , I2 , I3 !------------------------------------------------------------------------- BEGIN C     Dimensions of the input array Idim ( 1 ) = Ibox ( 2 , 1 ) - Ibox ( 1 , 1 ) + 1 Idim ( 2 ) = Ibox ( 2 , 2 ) - Ibox ( 1 , 2 ) + 1 Idim ( 3 ) = Ibox ( 2 , 3 ) - Ibox ( 1 , 3 ) + 1 C     Dimensions of the intersection. Ldim ( 1 ) = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ldim ( 2 ) = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Ldim ( 3 ) = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( iaxis . eq . 1 ) then C       Reduction into the X-axis lwload ( 1 : Ldim ( 1 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I1 ) = lwload ( I1 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else if ( iaxis . eq . 2 ) then C       Reduction into the Y-axis lwload ( 1 : Ldim ( 2 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I2 ) = lwload ( I2 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else C       Reduction into the Z-axis lwload ( 1 : Ldim ( 3 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I3 ) = lwload ( I3 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo endif !--------------------------------------------------------------------------- END end subroutine reduce3Dto1D C ================================================================== C Bisection of the load associated to an array. C ================================================================== C SUBROUTINE vecBisec( nval, values, nparts, pos, h1, h2 ) C C INPUT: C integer   nval         : Dimension of the input array C integer(i8b) values       : Input array C integer   nparts       : Numbers of partitions that we want to make from C                          the input array (in this call we only make one cut). C C OUTPUT: C integer   pos          : Position of the cut C integer(i8b) h1           : Load of the first part C integer(i8b) h2           : Load of the second part C C BEHAVIOR: C We want to split array \"values\" in \"nparts\", but in this call to C vecBisec we are going to make only one cut. First, we split nparts C in two parts: p1=nparts/2 and p2=nparts-p1. Then we compute the total C load of the array \"values\" (\"total\") and the desired load for the C first part: halfG = (total*p1)/nparts. C C Finally, we try to find the position inside \"values\" where we are C nearer of the the desired solution. C C ================================================================== subroutine vecBisec ( nval , values , nparts , pos , h1 , h2 ) implicit none C     Input variables integer , intent ( in ) :: nval , nparts integer ( i8b ), intent ( in ) :: values ( nval ) integer , intent ( out ) :: pos integer ( i8b ), intent ( out ) :: h1 , h2 C     Local variables integer :: p1 , p2 , ii integer ( i8b ) :: total , halfG , halfL !------------------------------------------------------------------------- BEGIN if ( nparts . gt . 1 ) then C       Split the number of parts in 2 p1 = nparts / 2 p2 = nparts - p1 C       Compute the total load of the array total = 0 do ii = 1 , nval total = total + values ( ii ) enddo C       Desired load of the first part halfG = ( total * p1 ) / nparts halfL = 0 pos = 0 C       Loop until we reach the solution do while ( halfL . lt . halfG ) pos = pos + 1 if ( pos . eq . nval + 1 ) STOP 'ERROR in vecBisec' halfL = halfL + values ( pos ) enddo C       Check if the previous position is better than the C       current position if (( halfL - values ( pos ) * p2 / nparts ). gt . halfG ) then halfL = halfL - values ( pos ) pos = pos - 1 endif h1 = halfL h2 = total - halfL endif !--------------------------------------------------------------------------- END end subroutine vecBisec #ifdef REORD1 C ================================================================== C SUBROUTINE reordMeshNumbering( distr1, distr2 ) C Given a new distribution, distr2, reasign each box to the proper C process. We use the following criteria: C   - Minimize the number of communications. Data don't need to C     be communicated if it belongs to the same process in C     different data distributions C C INPUT: C integer distr1  : First distribution C integer distr2  : Second distribution C C OUTPUT: C The output values are stored in the current module: C        meshDistr(distr2)%box(:,:,:) C C BEHAVIOR: C C C ================================================================== subroutine reordMeshNumbering ( distr1 , distr2 ) implicit none C     Passed arguments type ( meshDisType ), intent ( in ) :: distr1 type ( meshDisType ), intent ( out ) :: distr2 C     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters C     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) C     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo C     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 C       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo C       Save the permutation for this node (a negative number means that C       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI C       Change the permutation of those nodes who pretend to use the C       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering #else C ================================================================== C SUBROUTINE reordMeshNumbering( distr1, distr2 ) C Given a new distribution, distr2, reasign each box to the proper C process. We use the following criteria: C   - Minimize the number of communications. Data don't need to C     be communicated if it belongs to the same process in C     different data distributions C   - Distribute memory needs among different NODES (group of processes C     that shares the same memory) C C INPUT: C integer distr1  : First distribution C integer distr2  : Second distribution C integer PROCS_PER_NODE : Number of processes that runs in the same C                          node (sharing the same memory) C C OUTPUT: C The output values are stored in the current module: C        meshDistr(distr2)%box(:,:,:) C C BEHAVIOR: C 1) Compute the size of all the boxes of the second distribution C 2) Reorder the list of boxes according to its size C 3) Create a list of buckets ( C C ================================================================== subroutine reordMeshNumbering ( distr1 , distr2 ) use fdf implicit none C     Passed arguments type ( meshDisType ), intent ( in ) :: distr1 type ( meshDisType ), intent ( out ) :: distr2 C     Local variables integer :: P1 , P2 , I1 , I2 , J1 , J2 , J3 , & K1 , K2 , K3 , NN , NB , NM , SI , & SIMAX , Lbox ( 2 , 3 ) integer , pointer :: Nsiz (:), perm (:), Gprm (:), & chkb (:), box1 (:,:), box2 (:,:), & box (:,:,:) => null () integer :: PROCS_PER_NODE logical :: inters PROCS_PER_NODE = fdf_get ( 'PROCS_PER_NODE' , 4 ) C     Create groups of PROCS_PER_NODE processes NN = nodes + PROCS_PER_NODE - 1 NB = NN / PROCS_PER_NODE ! Number of buckets NM = MOD ( NN , PROCS_PER_NODE ) + 1 ! Size of the last bucket C     Allocate local arrays nullify ( Nsiz , perm , Gprm , chkb ) call re_alloc ( Nsiz , 1 , Nodes , 'Nsiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( Gprm , 1 , Nodes , 'Gprm' , 'moreMeshSubs' ) call re_alloc ( chkb , 1 , NB , 'chkb' , 'moreMeshSubs' ) call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) C     Compute the size of all the boxes of the second distribution do P1 = 1 , Nodes box2 => distr2 % box (:,:, P1 ) Nsiz ( P1 ) = ( box2 ( 2 , 1 ) - box2 ( 1 , 1 ) + 1 ) * & ( box2 ( 2 , 2 ) - box2 ( 1 , 2 ) + 1 ) * & ( box2 ( 2 , 3 ) - box2 ( 1 , 3 ) + 1 ) perm ( P1 ) = P1 enddo C     Reorder the list of boxes according to its size call myQsort ( Nodes , Nsiz , perm ) Gprm ( 1 : Nodes ) = 0 P1 = 0 C     We distribute processes in \"buckets\" of size PROCS_PER_NODE C     We have a total number of NB \"buckets\". DO I1 = 1 , PROCS_PER_NODE C       At every step of loop I1, we assign a box to every bucket if ( I1 . EQ . NM + 1 ) NB = NB - 1 C       Reset chkb. All buckets are empty. chkb ( 1 : NB ) = 0 DO I2 = 1 , NB C         At every step of loop I2, we assign a box to a different bucket P1 = P1 + 1 P2 = perm ( P1 ) ! P2 is the \"P1\"th biggest box box2 => distr2 % box (:,:, P2 ) J2 = 1 J3 = 1 SIMAX = - 1 DO J1 = 1 , Nodes C           J1=node; J2=position inside the bucket; J3=bucket index if ( chkb ( J3 ). eq . 0 . and . Gprm ( J1 ). eq . 0 ) then C             chkb(J3).eq.0 => The current bucket is not in use C             Gprm(J1).eq.0 => The node has not been permuted yet box1 => distr1 % box (:,:, J1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then SI = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) else SI = 0 endif if ( SI . gt . SIMAX ) then C               Save the information of the current node if it has the C               biggest intersection with box P2 SIMAX = SI K1 = J1 K3 = J3 endif endif C           Update information about bucket index and position J2 = J2 + 1 if ( J2 . gt . PROCS_PER_NODE ) then J2 = 1 J3 = J3 + 1 endif ENDDO C         box(P2) will be set to process K1 (that belongs to bucket K3) chkb ( K3 ) = 1 Gprm ( K1 ) = P2 ENDDO ENDDO C     Reorder boxes of the second distribution according to Gprm box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes P2 = Gprm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , P2 ) enddo C     Deallocate local arrays call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( chkb , 'chkb' , 'moreMeshSubs' ) call de_alloc ( Gprm , 'Gprm' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Nsiz , 'Nsiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering #endif C ================================================================== C Find the communications needed to transform one array that uses C distribution \"distr1\" to distribution \"distr2\" C ================================================================== C SUBROUTINE compMeshComm( distr1, distr2, mcomm ) C C INPUT: C meshDisType  distr1    : Source distribution C meshDisType  distr2    : Destiny distribution C C OUTPUT: C meshCommType mcomm     : Communications needed C C BEHAVIOR: C Count the number of intersections between the source distribution C and the destiny distribution. Every intersection represents a C communication. Then we call scheduleComm to optime the order of these C communications. Finally, we save the communications that belongs to C the current process in the variable \"mcomm\" C C ================================================================== subroutine compMeshComm ( distr1 , distr2 , mcomm ) use scheComm implicit none C     Passed arguments type ( meshDisType ), intent ( in ) :: distr1 , distr2 type ( meshCommType ), intent ( out ) :: mcomm C     Local variables integer :: P1 , P2 , ncom , Gcom , Lcom , & Lind , Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:) logical :: inters type ( COMM_T ) :: comm !------------------------------------------------------------------------- BEGIN C     count the number of intersections between Source distribution and C     destiny distribution. Every intersection represents a communication. ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) ncom = ncom + 1 enddo enddo Gcom = ncom C     Allocate local arrays nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) C     Make a list of communications ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif enddo enddo comm % np = Nodes C     reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) C     Count the number of communications of the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom C     Allocate memory to store data of the communications of the C     current process. nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , 'moreMeshSubs' ) C     Save the list of communications for the current process ncom = 0 do P1 = 1 , comm % ncol Lind = comm % ind ( P1 , Node + 1 ) if ( Lind . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( Lind ) mcomm % dst ( ncom ) = dst ( Lind ) endif enddo mcomm % ncom = ncom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) !--------------------------------------------------------------------------- END end subroutine compMeshComm #endif END MODULE moreMeshSubs","tags":"","loc":"sourcefile/moremeshsubs.f.html"},{"title":"compute_dm.F – SIESTA","text":"Files dependent on this one sourcefile~~compute_dm.f~~AfferentGraph sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_dm.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_dm Source Code compute_dm.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_compute_dm private public :: compute_dm logical , public , save :: PreviousCallDiagon = . false . CONTAINS subroutine compute_dm ( iscf ) use precision use units , only : eV USE siesta_options use class_dSpData1D , only : val use sparse_matrices use siesta_geom use atomlist , only : qa , lasto , iphorb , iaorb , no_u , no_s , indxuo , & qtot , Qtots , no_l use sys , only : die , bye use Kpoint_grid use m_energies , only : Ebs , Ecorrec , Entropy , DE_NEGF use m_energies , only : Ef , Efs use m_rmaxh use m_eo use m_spin , only : spin use m_diagon , only : diagon use m_gamma use parallel , only : IONode use parallel , only : SIESTA_worker use m_compute_ebs_shift , only : compute_ebs_shift #ifdef SIESTA__PEXSI use m_pexsi_solver , only : pexsi_solver #endif use m_hsx , only : write_hs_formatted #ifdef MPI use mpi_siesta #endif #ifdef CDF use iodmhs_netcdf , only : write_dmh_netcdf #endif use m_dminim , only : dminim use m_zminim , only : zminim use m_ordern , only : ordern use m_steps , only : istp use m_normalize_dm , only : normalize_dm #ifdef SIESTA__CHESS use m_chess , only : CheSS_wrapper #endif use m_energies , only : DE_NEGF use m_ts_global_vars , only : TSmode , TSinit , TSrun use m_transiesta , only : transiesta implicit none !     Input variables integer , intent ( in ) :: iscf real ( dp ) :: delta_Ebs , delta_Ef logical :: CallDiagon integer :: nnz real ( dp ), pointer :: H_kin (:) ! e1>e2 to signal that we do not want DOS weights real ( dp ), parameter :: e1 = 1.0_dp , e2 = - 1.0_dp real ( dp ) :: buffer1 integer :: mpierr !       character(15)            :: filename, indexstr !       character(15), parameter :: fnameform = '(A,A,A)' !-------------------------------------------------------------------- BEGIN if ( SIESTA_worker ) call timer ( 'compute_dm' , 1 ) #ifdef MPI call MPI_Bcast ( isolve , 1 , MPI_integer , 0 , true_MPI_Comm_World , mpierr ) #endif if ( SIESTA_worker ) then ! Save present density matrix !$OMP parallel default(shared) if ( converge_EDM ) then !$OMP workshare Eold (:,:) = Escf (:,:) Dold (:,:) = Dscf (:,:) !$OMP end workshare else !$OMP workshare Dold (:,:) = Dscf (:,:) !$OMP end workshare end if !$OMP end parallel end if ! Compute shift in Tr(H*DM) for fermi-level bracketting ! Use the current H, the previous iteration H, and the ! previous iteration DM if ( SIESTA_worker ) then if ( iscf > 1 ) then call compute_Ebs_shift ( Dscf , H , Hold , delta_Ebs ) delta_Ef = delta_Ebs / qtot if ( ionode . and . isolve . eq . SOLVE_PEXSI ) then write ( 6 , \"(a,f16.5)\" ) $ \"Estimated change in band-structure energy:\" , $ delta_Ebs / eV , \"Estimated shift in E_fermi: \" , $ delta_Ef / eV endif else delta_Ebs = 0.0_dp delta_Ef = 0.0_dp endif endif #ifdef SIESTA__PEXSI if ( isolve . eq . SOLVE_PEXSI ) then ! This test done in node 0 since NonCol and SpOrb ! are not set for PEXSI-solver-only processes if ( ionode ) then if ( spin % NCol . or . spin % SO ) call die ( $ \"The PEXSI solver does not implement \" // $ \"non-coll spins or Spin-orbit yet\" ) endif call pexsi_solver ( iscf , no_u , no_l , spin % spinor , $ maxnh , numh , listhptr , listh , $ H , S , qtot , Dscf , Escf , $ ef , Entropy , temp , delta_Ef ) endif if (. not . SIESTA_worker ) RETURN #endif ! Here we decide if we want to calculate one or more SCF steps by ! diagonalization before proceeding with the OMM routine CallDiagon = . false . if ( isolve . eq . SOLVE_MINIM ) then if ( istp . eq . 1 ) then if (( iscf . le . call_diagon_first_step ) . or . & ( call_diagon_first_step < 0 )) CallDiagon = . true . else if (( iscf . le . call_diagon_default ) . or . & ( call_diagon_default < 0 )) CallDiagon = . true . endif endif if ( isolve . eq . MATRIX_WRITE ) then !             write(indexstr,'(I15)') iscf !             write(filename,fnameform) 'H_', trim(adjustl(indexstr)), !      &                                '.matrix' !             call write_global_matrix( no_s, no_l, maxnh, numh, listh, !      &           H(1:maxnh,1), filename ) ! !             write(filename,fnameform) 'S_', trim(adjustl(indexstr)), !      &                                '.matrix' !        Note: only one-shot for now call write_hs_formatted ( no_u , spin % H , $ maxnh , numh , listhptr , listh , H , S ) call bye ( \"End of run after writing H.matrix and S.matrix\" ) c$        call write_global_matrix_singlenodewrite( c$     &           no_u, no_s, maxnh, numh, listhptr, listh, c$     &           H(:,1), 'H.matrix') c$ c$        call write_global_matrix_singlenodewrite( c$     &           no_u, no_s, maxnh, numh, listhptr, listh, c$     &           S, 'S.matrix') elseif (( isolve . eq . SOLVE_DIAGON ) . or . ( CallDiagon )) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , nkpnt , kpoint , kweight , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0.0_dp PreviousCallDiagon = . true . elseif ( isolve . eq . SOLVE_ORDERN ) then if (. not . gamma ) call die ( \"Cannot do O(N) with k-points.\" ) if ( spin % NCol . or . spin % SO ) . call die ( \"Cannot do O(N) with non-coll spins or Spin-orbit\" ) call ordern ( usesavelwf , ioptlwf , na_u , no_u , no_l , lasto , & isa , qa , rcoor , rmaxh , ucell , xa , iscf , & istp , ncgmax , etol , eta , qtot , maxnh , numh , & listhptr , listh , H , S , chebef , noeta , rcoorcp , & beta , pmax , Dscf , Escf , Ecorrec , spin % H , qtots ) Entropy = 0.0_dp elseif ( isolve . eq . SOLVE_MINIM ) then if ( spin % NCol . or . spin % SO ) & call die ( ' ERROR : Non - collinear spin calculations & not yet implemented with OMM !') H_kin => val ( H_kin_1D ) if ( gamma ) then call dminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , H , S , H_kin ) else call zminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , no_s , xijo , indxuo , nkpnt , kpoint , & kweight , H , S , H_kin ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #ifdef SIESTA__CHESS elseif ( isolve . eq . SOLVE_CHESS ) then ! FOE solver from the CheSS library if ( gamma ) then call CheSS_wrapper (. false ., PreviousCallDiagon , & iscf , istp , no_l , & spin % spinor , no_u , maxnh , numh , listhptr , listh , & qs , h , s , & Dscf , Escf , Ef ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #endif elseif ( TSmode . and . TSinit ) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , nkpnt , kpoint , kweight , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0._dp else if ( TSrun ) then call transiesta ( iscf , spin % H , block_dist , sparse_pattern , & Gamma , ucell , nsc , isc_off , no_u , na_u , lasto , xa , maxnh , & H , S , Dscf , Escf , Ef , Qtot , . false ., DE_NEGF ) Ecorrec = 0._dp Entropy = 0.0_dp else !call die('siesta: ERROR: wrong solution method') endif #ifdef CDF if ( writedmhs_cdf_history ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf ) else if ( writedmhs_cdf ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf , & overwrite = . true . ) endif #endif !     Print populations at each SCF step if requested before mixing ...... if ( muldeb ) then if ( ionode ) write ( 6 , \"(/a)\" ) & 'siesta: Mulliken populations (DM_out)' if ( spin % SO ) then call moments ( mullipop , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) else call mulliken ( mullipop , spin % DM , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) endif endif ! Write orbital indexes. JMS Dec.2009 if ( IOnode . and . iscf == 1 ) then call write_orb_indx ( na_u , na_s , no_u , no_s , isa , xa , . iaorb , iphorb , indxuo , nsc , ucell ) endif !     Normalize density matrix to exact charge !     Placed here for now to avoid disturbing EHarris if ( . not . TSrun ) then call normalize_dm ( first = . false . ) end if call timer ( 'compute_dm' , 2 ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after compute_DM\" ) #endif !-----------------------------------------------------------------------END END subroutine compute_dm END MODULE m_compute_dm","tags":"","loc":"sourcefile/compute_dm.f.html"},{"title":"tMixer – SIESTA ","text":"type, public :: tMixer Inherits type~~tmixer~~InheritsGraph type~tmixer tMixer type~tmixer->type~tmixer next, next_conv Fstack_dData1D Fstack_dData1D type~tmixer->Fstack_dData1D stack Help × Graph Key Nodes of different colours represent the following: Graph Key Type Type This Page's Entity This Page's Entity Solid arrows point from a derived type to the parent type which it\n    extends. Dashed arrows point from a derived type to the other\n    types it contains as a components, with a label listing the name(s) of\n    said component(s). Contents Variables name stack m v cur_itt start_itt n_hist n_itt restart restart_save action next next_conv w rv iv Source Code tMixer Components Type Visibility Attributes Name Initial character(len=24), public :: name type(Fstack_dData1D), public, allocatable :: stack (:) integer, public :: m = MIX_PULAY integer, public :: v = 0 integer, public :: cur_itt = 0 integer, public :: start_itt = 0 integer, public :: n_hist = 2 integer, public :: n_itt = 0 integer, public :: restart = 0 integer, public :: restart_save = 0 integer, public :: action = ACTION_MIX type( tMixer ), public, pointer :: next => null() type( tMixer ), public, pointer :: next_conv => null() real(kind=dp), public :: w = 0._dp real(kind=dp), public, pointer :: rv (:) => null() integer, public, pointer :: iv (:) => null() Source Code type tMixer ! Name of mixer character ( len = 24 ) :: name ! The different saved variables per iteration ! and their respective stacks type ( Fstack_dData1D ), allocatable :: stack (:) ! The method of the mixer integer :: m = MIX_PULAY ! In case the mixing method has a variant ! this denote the variant ! This value is thus specific for each method integer :: v = 0 ! The currently reached iteration integer :: cur_itt = 0 , start_itt = 0 ! Different mixers may have different histories integer :: n_hist = 2 ! Number of iterations using this mixer ! There are a couple of signals here !  == 0 : !     only use this mixer until convergence !   > 0 : !     after having runned n_itt step to \"next\" integer :: n_itt = 0 ! When mod(cur_itt,restart_itt) == 0 the history will ! be _reset_ integer :: restart = 0 integer :: restart_save = 0 ! This is an action token specifying the current ! action integer :: action = ACTION_MIX ! The next mixing method following this method type ( tMixer ), pointer :: next => null () ! The next mixing method following this method ! Only used if mixing method achieved convergence ! using this method type ( tMixer ), pointer :: next_conv => null () ! ** Parameters specific for the method: ! The mixing parameter used for this mixer real ( dp ) :: w = 0._dp ! linear array of real variables used specifically ! for this mixing type real ( dp ), pointer :: rv (:) => null () integer , pointer :: iv (:) => null () #ifdef MPI ! In case we have MPI the mixing scheme ! can implement a reduction scheme. ! This can be MPI_Comm_Self to not employ any ! reductions integer :: Comm = MPI_Comm_Self #endif end type tMixer","tags":"","loc":"type/tmixer.html"},{"title":"meshDisType – SIESTA ","text":"type, private :: meshDisType Contents Variables nMesh box indexp idop xdop ipa Source Code meshDisType Components Type Visibility Attributes Name Initial integer, public :: nMesh (3) integer, public, pointer :: box (:,:,:) integer, public, pointer :: indexp (:) integer, public, pointer :: idop (:) real(kind=dp), public, pointer :: xdop (:,:) integer, public, pointer :: ipa (:) Source Code TYPE meshDisType integer :: nMesh ( 3 ) ! Number of mesh div. in each axis integer , pointer :: box (:,:,:) ! Mesh box bounds of each node: ! box(1,iAxis,iNode)=lower bounds ! box(2,iAxis,iNode)=upper bounds integer , pointer :: indexp (:) integer , pointer :: idop (:) real ( dp ), pointer :: xdop (:,:) integer , pointer :: ipa (:) END TYPE meshDisType","tags":"","loc":"type/meshdistype.html"},{"title":"meshCommType – SIESTA ","text":"type, private :: meshCommType Contents Variables ncom src dst Source Code meshCommType Components Type Visibility Attributes Name Initial integer, public :: ncom integer, public, pointer :: src (:) integer, public, pointer :: dst (:) Source Code TYPE meshCommType integer :: ncom ! Number of needed communications integer , pointer :: src (:) ! Sources of communications integer , pointer :: dst (:) ! Destination of communications END TYPE meshCommType","tags":"","loc":"type/meshcommtype.html"},{"title":"state_analysis – SIESTA","text":"public subroutine state_analysis(istep) Uses siesta_cml m_born_charge parallel m_wallclock zmatrix atomlist atomlist m_spin m_fixed sparse_matrices siesta_geom siesta_options units m_stress m_energies m_energies m_ntm m_forces m_energies m_intramol_pressure flook_siesta proc~~state_analysis~~UsesGraph proc~state_analysis state_analysis atomlist atomlist proc~state_analysis->atomlist m_fixed m_fixed proc~state_analysis->m_fixed parallel parallel proc~state_analysis->parallel sparse_matrices sparse_matrices proc~state_analysis->sparse_matrices units units proc~state_analysis->units m_stress m_stress proc~state_analysis->m_stress siesta_geom siesta_geom proc~state_analysis->siesta_geom m_wallclock m_wallclock proc~state_analysis->m_wallclock m_spin m_spin proc~state_analysis->m_spin siesta_options siesta_options proc~state_analysis->siesta_options m_born_charge m_born_charge proc~state_analysis->m_born_charge m_energies m_energies proc~state_analysis->m_energies flook_siesta flook_siesta proc~state_analysis->flook_siesta siesta_cml siesta_cml proc~state_analysis->siesta_cml zmatrix zmatrix proc~state_analysis->zmatrix m_ntm m_ntm proc~state_analysis->m_ntm m_intramol_pressure m_intramol_pressure proc~state_analysis->m_intramol_pressure m_forces m_forces proc~state_analysis->m_forces Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: istep Calls proc~~state_analysis~~CallsGraph proc~state_analysis state_analysis update_freee update_freee proc~state_analysis->update_freee volcel volcel proc~state_analysis->volcel update_freeeharris update_freeeharris proc~state_analysis->update_freeeharris cmlstartmodule cmlstartmodule proc~state_analysis->cmlstartmodule siesta_write_forces siesta_write_forces proc~state_analysis->siesta_write_forces cartesianforce_to_zmatforce cartesianforce_to_zmatforce proc~state_analysis->cartesianforce_to_zmatforce cmladdproperty cmladdproperty proc~state_analysis->cmladdproperty write_debug write_debug proc~state_analysis->write_debug slua_call slua_call proc~state_analysis->slua_call born_charge born_charge proc~state_analysis->born_charge va va proc~state_analysis->va timer timer proc~state_analysis->timer remove_intramol_pressure remove_intramol_pressure proc~state_analysis->remove_intramol_pressure fixed fixed proc~state_analysis->fixed print_spin print_spin proc~state_analysis->print_spin mulliken mulliken proc~state_analysis->mulliken kin_stress kin_stress proc~state_analysis->kin_stress amass amass proc~state_analysis->amass moments moments proc~state_analysis->moments eggbox eggbox proc~state_analysis->eggbox wallclock wallclock proc~state_analysis->wallclock cmlendmodule cmlendmodule proc~state_analysis->cmlendmodule siesta_write_stress_pressure siesta_write_stress_pressure proc~state_analysis->siesta_write_stress_pressure Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~state_analysis~~CalledByGraph proc~state_analysis state_analysis proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_analysis Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code state_analysis Source Code subroutine state_analysis ( istep ) use siesta_cml use m_born_charge , only : born_charge use parallel , only : IOnode use m_wallclock , only : wallclock use zmatrix , only : lUseZmatrix , iofaZmat , & CartesianForce_to_ZmatForce use atomlist , only : iaorb , iphorb , amass , no_u , lasto use atomlist , only : indxuo use m_spin , only : nspin , SpOrb use m_fixed , only : fixed use sparse_matrices use siesta_geom USE siesta_options use units , only : amu , eV use m_stress use m_energies , only : Etot , FreeE , Eharrs , FreeEHarris , Entropy use m_energies , only : Ebs , Ef use m_ntm use m_forces use m_energies , only : update_FreeE , update_FreeEHarris use m_intramol_pressure , only : remove_intramol_pressure #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_FORCES #endif implicit none integer :: istep integer :: ia , jx , ix real ( dp ) :: volume logical :: eggbox_block = . true . ! Read eggbox info from data file? real ( dp ) :: qspin external :: eggbox , mulliken , moments real ( dp ), external :: volcel !------------------------------------------------------------------------- BEGIN call timer ( 'state_analysis' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_analysis' ) #endif if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'SCF Finalization' ) endif !     Write final Kohn-Sham and Free Energy FreeE = Etot - Temp * Entropy FreeEHarris = Eharrs - Temp * Entropy if ( cml_p ) call cmlStartPropertyList ( mainXML , & title = 'Energies and spin' ) if ( IOnode ) then if ( . not . harrisfun ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS(eV) =        ' , Etot / eV if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = FreeE / eV , & dictref = 'siesta:FreeE' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ebs / eV , & dictref = 'siesta:Ebs' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ef / eV , & dictref = 'siesta:E_Fermi' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif endif !     Substract egg box effect from energy if ( eggbox_block ) then call eggbox ( 'energy' , ucell , na_u , isa , ntm , xa , fa , Etot , & eggbox_block ) FreeE = Etot - Temp * Entropy if ( IOnode ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS - E_eggbox = ' , Etot / eV if ( cml_p ) call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS_egg' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif call update_FreeE ( Temp ) call update_FreeEHarris ( Temp ) call print_spin ( qspin ) if ( cml_p ) call cmlEndPropertyList ( mainXML ) !     Substract egg box effect from the forces if ( eggbox_block ) then call eggbox ( 'forces' , ucell , na_u , isa , ntm , xa , fa , Etot , eggbox_block ) endif if ( IOnode ) call write_raw_efs ( stress , na_u , fa , FreeE ) !     Compute stress without internal molecular pressure call remove_intramol_pressure ( ucell , stress , na_u , xa , fa , mstress ) !     Impose constraints to atomic movements by changing forces ........... if ( RemoveIntraMolecularPressure ) then !        Consider intramolecular pressure-removal as another !        kind of constraint call fixed ( ucell , mstress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) else call fixed ( ucell , stress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) endif #ifdef SIESTA__FLOOK ! We call it right after using the ! geometry constraints. ! In that way we can use both methods on top ! of each other! ! The easy, already implemented methods in fixed, ! and custom ones in Lua :) call slua_call ( LUA , LUA_FORCES ) #endif !     Calculate and output Zmatrix forces if ( lUseZmatrix . and . ( idyn . eq . 0 )) then call CartesianForce_to_ZmatForce ( na_u , xa , fa ) if ( IOnode ) call iofaZmat () endif !     Compute kinetic contribution to stress kin_stress ( 1 : 3 , 1 : 3 ) = 0.0_dp volume = volcel ( ucell ) do ia = 1 , na_u do jx = 1 , 3 do ix = 1 , 3 kin_stress ( ix , jx ) = kin_stress ( ix , jx ) - & amu * amass ( ia ) * va ( ix , ia ) * va ( jx , ia ) / volume enddo enddo enddo !     Add kinetic term to stress tensor tstress = stress + kin_stress !     Force output if ( IOnode ) then call siesta_write_forces ( istep ) call siesta_write_stress_pressure () call wallclock ( '--- end of geometry step' ) endif !     Mulliken population analysis if ( SpOrb ) then call moments ( mullipop , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) else call mulliken ( mullipop , nspin , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) endif ! !     Call the born effective charge routine only in those steps (even) !     in which the dx  is positive. if ( bornz . and . ( mod ( istep , 2 ) . eq . 0 )) then call born_charge () endif !     End the xml module corresponding to the analysis if ( cml_p ) then call cmlEndModule ( mainXML ) endif call timer ( 'state_analysis' , 2 ) !--------------------------------------------------------------------------- END END subroutine state_analysis","tags":"","loc":"proc/state_analysis.html"},{"title":"setup_hamiltonian – SIESTA","text":"public subroutine setup_hamiltonian(iscf) Uses siesta_options sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices class_dSpData1D class_dSpData2D siesta_geom atmfuncs atomlist metaforce molecularmechanics ldau_specs m_ldau m_dhscf m_stress m_energies parallel m_steps m_ntm m_spin m_dipol alloc m_gamma m_hsx sys m_partial_charges files m_rhog m_mpi_utils proc~~setup_hamiltonian~~UsesGraph proc~setup_hamiltonian setup_hamiltonian atomlist atomlist proc~setup_hamiltonian->atomlist parallel parallel proc~setup_hamiltonian->parallel sys sys proc~setup_hamiltonian->sys ldau_specs ldau_specs proc~setup_hamiltonian->ldau_specs class_dSpData1D class_dSpData1D proc~setup_hamiltonian->class_dSpData1D m_hsx m_hsx proc~setup_hamiltonian->m_hsx atmfuncs atmfuncs proc~setup_hamiltonian->atmfuncs m_ntm m_ntm proc~setup_hamiltonian->m_ntm m_dhscf m_dhscf proc~setup_hamiltonian->m_dhscf m_stress m_stress proc~setup_hamiltonian->m_stress sparse_matrices sparse_matrices proc~setup_hamiltonian->sparse_matrices files files proc~setup_hamiltonian->files siesta_options siesta_options proc~setup_hamiltonian->siesta_options alloc alloc proc~setup_hamiltonian->alloc m_partial_charges m_partial_charges proc~setup_hamiltonian->m_partial_charges metaforce metaforce proc~setup_hamiltonian->metaforce m_energies m_energies proc~setup_hamiltonian->m_energies m_rhog m_rhog proc~setup_hamiltonian->m_rhog m_dipol m_dipol proc~setup_hamiltonian->m_dipol m_ldau m_ldau proc~setup_hamiltonian->m_ldau m_gamma m_gamma proc~setup_hamiltonian->m_gamma m_steps m_steps proc~setup_hamiltonian->m_steps siesta_geom siesta_geom proc~setup_hamiltonian->siesta_geom m_mpi_utils m_mpi_utils proc~setup_hamiltonian->m_mpi_utils class_dSpData2D class_dSpData2D proc~setup_hamiltonian->class_dSpData2D m_spin m_spin proc~setup_hamiltonian->m_spin molecularmechanics molecularmechanics proc~setup_hamiltonian->molecularmechanics Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~setup_hamiltonian~~CallsGraph proc~setup_hamiltonian setup_hamiltonian update_e0 update_e0 proc~setup_hamiltonian->update_e0 dscf dscf proc~setup_hamiltonian->dscf globalize_sum globalize_sum proc~setup_hamiltonian->globalize_sum hubbard_term hubbard_term proc~setup_hamiltonian->hubbard_term re_alloc re_alloc proc~setup_hamiltonian->re_alloc bye bye proc~setup_hamiltonian->bye dhscf dhscf proc~setup_hamiltonian->dhscf timer timer proc~setup_hamiltonian->timer h h proc~setup_hamiltonian->h de_alloc de_alloc proc~setup_hamiltonian->de_alloc hold hold proc~setup_hamiltonian->hold write_hsx write_hsx proc~setup_hamiltonian->write_hsx die die proc~setup_hamiltonian->die val val proc~setup_hamiltonian->val Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~setup_hamiltonian~~CalledByGraph proc~setup_hamiltonian setup_hamiltonian proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setup_hamiltonian Source Code subroutine setup_hamiltonian ( iscf ) USE siesta_options use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D , H_so_2D use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H , S , Hold use sparse_matrices , only : Dscf , Escf , xijo use class_dSpData1D , only : val use class_dSpData2D , only : val use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , . lastkb , no_s , rmaxv , indxua , iphorb , lasto , . rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use ldau_specs , only : switch_ldau ! This variable determines whether !   the subroutine to compute the !   Hubbard terms should be called !   or not use m_ldau , only : hubbard_term ! Subroutine that compute the !   Hubbard terms use m_dhscf , only : dhscf use m_stress use m_energies use parallel , only : Node use m_steps , only : istp use m_ntm use m_spin , only : spin use m_dipol use alloc , only : re_alloc , de_alloc use m_gamma use m_hsx , only : write_hsx use sys , only : die , bye use m_partial_charges , only : want_partial_charges use files , only : filesOut_t ! derived type for output file names use m_rhog , only : rhog_in , rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none integer , intent ( in ) :: iscf real ( dp ) :: stressl ( 3 , 3 ) real ( dp ), pointer :: fal (:,:) ! Local-node part of atomic F #ifdef MPI real ( dp ) :: buffer1 #endif integer :: io , is , ispin integer :: ifa ! Calc. forces?      0=>no, 1=>yes integer :: istr ! Calc. stress?      0=>no, 1=>yes integer :: ihmat ! Calc. hamiltonian? 0=>no, 1=>yes real ( dp ) :: g2max type ( filesOut_t ) :: filesOut ! blank output file names logical :: use_rhog_in real ( dp ), pointer :: H_vkb (:), H_kin (:), H_ldau (:,:), H_so (:,:) !------------------------------------------------------------------------- BEGIN call timer ( 'setup_H' , 1 ) ! Nullify pointers nullify ( fal ) !$OMP parallel default(shared), private(ispin,io) !     Save present H matrix !$OMP do collapse(2) do ispin = 1 , spin % H do io = 1 , maxnh Hold ( io , ispin ) = H ( io , ispin ) enddo enddo !$OMP end do !$OMP single H_kin => val ( H_kin_1D ) H_vkb => val ( H_vkb_1D ) if ( spin % SO ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so => val ( H_so_2D ) end if !$OMP end single ! keep wait ! We do not need to set the non-spinor components ! For non-colinear they are set down below, ! while for spin-orbit they are set to the H_so initial ! spin-orbit. do ispin = 1 , spin % spinor !$OMP do do io = 1 , maxnh H ( io , ispin ) = H_kin ( io ) + H_vkb ( io ) end do !$OMP end do nowait end do if ( spin % NCol ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = 0._dp end do end do !$OMP end do nowait else if ( spin % SO ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = H_so ( io , ispin - 2 ) end do end do !$OMP end do nowait end if ! .................. ! Non-SCF part of total energy ....................................... ! Note that these will be \"impure\" for a mixed Dscf ! If mixing the charge, Dscf is the previous step's DM_out. Since ! the \"scf\" components of the energy are computed with the (mixed) ! charge, this introduces an inconsistency. In this case the energies ! coming out of this routine need to be corrected. ! !$OMP single Ekin = 0.0_dp Enl = 0.0_dp !$OMP end single ! keep wait !$OMP do collapse(2), reduction(+:Ekin,Enl) do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) end do end do !$OMP end do nowait !$OMP single Eso = 0._dp !$OMP end single if ( spin % SO ) then !$OMP do reduction(+:Eso) do io = 1 , maxnh Eso = Eso + H_so ( io , 1 ) * Dscf ( io , 7 ) + H_so ( io , 2 ) * Dscf ( io , 8 ) . + H_so ( io , 5 ) * Dscf ( io , 3 ) + H_so ( io , 6 ) * Dscf ( io , 4 ) . - H_so ( io , 3 ) * Dscf ( io , 5 ) - H_so ( io , 4 ) * Dscf ( io , 6 ) end do !$OMP end do nowait end if !$OMP end parallel #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 end if #endif !     Non-SCF part of total energy call update_E0 () ! Hubbard term for LDA+U: energy, forces, stress and matrix elements .... if ( switch_ldau ) then if ( spin % NCol ) then call die ( 'LDA+U cannot be used with non-colinear spin.' ) end if if ( spin % SO ) then call die ( 'LDA+U cannot be used with spin-orbit coupling.' ) end if call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) H_ldau => val ( H_ldau_2D ) call hubbard_term ( scell , na_u , na_s , isa , xa , indxua , . maxnh , maxnh , lasto , iphorb , no_u , no_l , . numh , listhptr , listh , numh , listhptr , listh , . spin % spinor , Dscf , Eldau , DEldau , H_ldau , . fal , stressl , H , iscf , . matrix_elements_only = . true .) #ifdef MPI ! Global reduction of energy terms call globalize_sum ( Eldau , buffer1 ) Eldau = buffer1 ! DEldau should not be globalized ! as it is based on globalized occupations #endif Eldau = Eldau + DEldau call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) endif ! .................. ! Add SCF contribution to energy and matrix elements .................. g2max = g2cut call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) ifa = 0 istr = 0 ihmat = 1 if (( hirshpop . or . voropop ) $ . and . partial_charges_at_every_scf_step ) then want_partial_charges = . true . endif use_rhog_in = ( mix_charge . and . iscf > 1 ) call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa , indxua , . ntm , ifa , istr , ihmat , filesOut , . maxnh , numh , listhptr , listh , Dscf , Datm , . maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , . Exc , Dxc , dipol , stress , fal , stressl , . use_rhog_in ) ! This statement will apply to iscf = 1, for example, when ! we do not use rhog_in. Rhog here is always the charge used to ! build H, that is, rhog_in. if ( mix_charge ) rhog_in = rhog want_partial_charges = . false . call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) !  It is wasteful to write over and over H and S, as there are !  no different files. ! Save Hamiltonian and overlap matrices ............................ ! Only in HSX format now.  Use Util/HSX/hsx2hs to generate an HS file if ( savehs . or . write_coop ) then call write_hsx ( gamma , no_u , no_s , spin % H , indxuo , & maxnh , numh , listhptr , listh , H , S , qtot , & temp , xijo ) endif call timer ( 'setup_H' , 2 ) #ifdef SIESTA__PEXSI if ( node == 0 ) call memory_snapshot ( \"after setup_H\" ) #endif if ( h_setup_only ) then call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call bye ( \"H-Setup-Only requested\" ) STOP endif !------------------------------------------------------------------------- END END subroutine setup_hamiltonian","tags":"","loc":"proc/setup_hamiltonian.html"},{"title":"mixers_scf_init – SIESTA","text":"public subroutine mixers_scf_init(nspin, Comm) Uses fdf precision m_mixing m_mixing m_mixing m_mixing proc~~mixers_scf_init~~UsesGraph proc~mixers_scf_init mixers_scf_init precision precision proc~mixers_scf_init->precision fdf fdf proc~mixers_scf_init->fdf module~m_mixing m_mixing proc~mixers_scf_init->module~m_mixing module~m_mixing->precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in), optional :: Comm Calls proc~~mixers_scf_init~~CallsGraph proc~mixers_scf_init mixers_scf_init proc~mixers_reset mixers_reset proc~mixers_scf_init->proc~mixers_reset leqi leqi proc~mixers_scf_init->leqi proc~mix_method mix_method proc~mixers_scf_init->proc~mix_method fdf_get fdf_get proc~mixers_scf_init->fdf_get proc~mixers_history_init mixers_history_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_init mixers_init proc~mixers_scf_init->proc~mixers_init proc~mix_method_variant mix_method_variant proc~mixers_scf_init->proc~mix_method_variant die die proc~mixers_scf_init->die delete delete proc~mixers_reset->delete proc~mix_method->leqi proc~mix_method->die proc~mixers_history_init->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new proc~mixers_init->proc~mixers_reset proc~mixers_init->fdf_get proc~mixers_init->proc~mixers_history_init proc~mixers_init->die fdf_bnames fdf_bnames proc~mixers_init->fdf_bnames fdf_bline fdf_bline proc~mixers_init->fdf_bline fdf_block fdf_block proc~mixers_init->fdf_block fdf_brewind fdf_brewind proc~mixers_init->fdf_brewind fdf_bnnames fdf_bnnames proc~mixers_init->fdf_bnnames proc~mix_method_variant->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_init Source Code subroutine mixers_scf_init ( nspin , Comm ) use fdf use precision , only : dp #ifdef MPI use mpi_siesta , only : MPI_Comm_World #endif use m_mixing , only : mixers_reset , mixers_init use m_mixing , only : mix_method , mix_method_variant use m_mixing , only : mixer_init use m_mixing , only : mixers_history_init ! The number of spin-components integer , intent ( in ) :: nspin ! The communicator used for the mixer integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf ! Get number of history steps integer :: n_hist , n_kick , n_restart , n_save real ( dp ) :: w , w_kick integer :: n_lin_after real ( dp ) :: w_lin_after logical :: lin_after ! number of history steps saved type ( tMixer ), pointer :: m integer :: nm , im , im2 , tmp logical :: is_broyden character ( len = 70 ) :: method , variant , opt ! If the mixers are denoted by a block, then ! the entire logic *MUST* be defined in the blocks opt = fdf_get ( 'SCF.Mix.Spin' , 'all' ) if ( leqi ( opt , 'all' ) ) then mix_spin = MIX_SPIN_ALL else if ( leqi ( opt , 'spinor' ) ) then mix_spin = MIX_SPIN_SPINOR else if ( leqi ( opt , 'sum' ) ) then mix_spin = MIX_SPIN_SUM else if ( leqi ( opt , 'sum+diff' ) ) then mix_spin = MIX_SPIN_SUM_DIFF else call die ( \"Unknown option given for SCF.Mix.Spin & &all|spinor|sum|sum+diff\" ) end if ! If there is only one spinor we should mix all... if ( nspin == 1 ) mix_spin = MIX_SPIN_ALL ! Initialize to ensure debug stuff read call mixers_init ( 'SCF' , scf_mixs , Comm = Comm ) ! Check for existance of the SCF.Mix block if ( associated ( scf_mixs ) ) then if ( size ( scf_mixs ) > 0 ) then return end if ! Something has gone wrong... ! The user has supplied a block, but ! haven't added any content to the block... ! However, we fall-back to the default mechanism end if ! ensure nullification call mixers_reset ( scf_mixs ) ! >>>*** FIRST ***<<< ! Read in compatibility options ! Figure out if we are dealing with ! Broyden or Pulay n_hist = fdf_get ( 'DM.NumberPulay' , 2 ) tmp = fdf_get ( 'DM.NumberBroyden' , 0 ) is_broyden = tmp > 0 if ( is_broyden ) then n_hist = tmp end if ! Define default mixing weight (used for ! Pulay, Broyden and linear mixing) w = fdf_get ( 'DM.MixingWeight' , 0.25_dp ) ! Default kick-options n_kick = fdf_get ( 'DM.NumberKick' , 0 ) w_kick = fdf_get ( 'DM.KickMixingWeight' , 0.5_dp ) lin_after = fdf_get ( 'SCF.LinearMixingAfterPulay' , . false .) w_lin_after = fdf_get ( 'SCF.MixingWeightAfterPulay' , w ) ! >>>*** END ***<<< ! Read options in new format ! Get history length n_hist = fdf_get ( 'SCF.Mixer.History' , n_hist ) ! update mixing weight and kick mixing weight w = fdf_get ( 'SCF.Mixer.Weight' , w ) n_kick = fdf_get ( 'SCF.Mixer.Kick' , n_kick ) w_kick = fdf_get ( 'SCF.Mixer.Kick.Weight' , w_kick ) ! Restart after this number of iterations n_restart = fdf_get ( 'SCF.Mixer.Restart' , 0 ) n_save = fdf_get ( 'SCF.Mixer.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Get the variant of the mixing method if ( is_broyden ) then method = 'Broyden' else if ( n_hist > 0 ) then method = 'Pulay' else method = 'Linear' end if method = fdf_get ( 'SCF.Mixer.Method' , trim ( method )) variant = fdf_get ( 'SCF.Mixer.Variant' , 'original' ) ! Determine whether linear mixing should be ! performed after the \"advanced\" mixing n_lin_after = fdf_get ( 'SCF.Mixer.Linear.After' , - 1 ) w_lin_after = fdf_get ( 'SCF.Mixer.Linear.After.Weight' , w_lin_after ) ! Determine total number of mixers nm = 1 if ( n_lin_after >= 0 . or . lin_after ) nm = nm + 1 if ( n_kick > 0 ) nm = nm + 1 ! Initiailaze all mixers allocate ( scf_mixs ( nm )) scf_mixs (:)% w = w scf_mixs (:)% n_hist = n_hist scf_mixs (:)% restart = n_restart scf_mixs (:)% restart_save = n_save ! 1. Current mixing index im = 1 ! Store the advanced mixer index (for references to ! later mixers) im2 = im m => scf_mixs ( im ) m % name = method m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! 2. Setup the linear mixing after the actual mixing if ( n_lin_after > 0 . or . lin_after ) then im = im + 1 m => scf_mixs ( im ) ! Signal to switch to this mixer after ! convergence scf_mixs ( im2 )% next_conv => m m % name = 'Linear-After' m % m = mix_method ( 'linear' ) m % w = w_lin_after m % n_itt = n_lin_after ! jump back to previous after having run a ! few iterations m % next => scf_mixs ( im2 ) end if ! In case we have a kick, apply the kick here ! This overrides the \"linear.after\" option if ( n_kick > 0 ) then im = im + 1 m => scf_mixs ( im ) m % name = 'Linear-Kick' m % n_itt = 1 m % n_hist = 0 m % m = mix_method ( 'linear' ) m % w = w_kick m % next => scf_mixs ( im2 ) ! set the default mixer to kick scf_mixs ( im2 )% n_itt = n_kick - 1 scf_mixs ( im2 )% next => m scf_mixs ( im2 )% restart = n_kick - 1 end if ! Correct the input do im = 1 , nm call mixer_init ( scf_mixs ( im ) ) end do ! Initialize the allocation of each mixer call mixers_history_init ( scf_mixs ) #ifdef MPI if ( present ( Comm ) ) then scf_mixs (:)% Comm = Comm else scf_mixs (:)% Comm = MPI_Comm_World end if #endif end subroutine mixers_scf_init","tags":"","loc":"proc/mixers_scf_init.html"},{"title":"mixers_scf_print – SIESTA","text":"public subroutine mixers_scf_print(nspin) Uses parallel m_mixing proc~~mixers_scf_print~~UsesGraph proc~mixers_scf_print mixers_scf_print parallel parallel proc~mixers_scf_print->parallel module~m_mixing m_mixing proc~mixers_scf_print->module~m_mixing precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Calls proc~~mixers_scf_print~~CallsGraph proc~mixers_scf_print mixers_scf_print die die proc~mixers_scf_print->die proc~mixers_print mixers_print proc~mixers_scf_print->proc~mixers_print proc~mixers_print->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_print Source Code subroutine mixers_scf_print ( nspin ) use parallel , only : IONode use m_mixing , only : mixers_print integer , intent ( in ) :: nspin ! Print mixing options call mixers_print ( 'SCF' , scf_mixs ) if ( IONode . and . nspin > 1 ) then select case ( mix_spin ) case ( MIX_SPIN_ALL ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'all' case ( MIX_SPIN_SPINOR ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'spinor' if ( nspin <= 2 ) then call die ( \"SCF.Mixer.Spin spinor option only valid for & &non-collinear and spin-orbit calculations\" ) end if case ( MIX_SPIN_SUM ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum' case ( MIX_SPIN_SUM_DIFF ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum and diff' end select end if end subroutine mixers_scf_print","tags":"","loc":"proc/mixers_scf_print.html"},{"title":"mixers_scf_print_block – SIESTA","text":"public subroutine mixers_scf_print_block() Uses m_mixing proc~~mixers_scf_print_block~~UsesGraph proc~mixers_scf_print_block mixers_scf_print_block module~m_mixing m_mixing proc~mixers_scf_print_block->module~m_mixing precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_print_block~~CallsGraph proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_print_block mixers_print_block proc~mixers_scf_print_block->proc~mixers_print_block die die proc~mixers_print_block->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_print_block Source Code subroutine mixers_scf_print_block ( ) use m_mixing , only : mixers_print_block ! Print mixing options call mixers_print_block ( 'SCF' , scf_mixs ) end subroutine mixers_scf_print_block","tags":"","loc":"proc/mixers_scf_print_block.html"},{"title":"mixing_scf_converged – SIESTA","text":"public subroutine mixing_scf_converged(SCFconverged) Uses parallel proc~~mixing_scf_converged~~UsesGraph proc~mixing_scf_converged mixing_scf_converged parallel parallel proc~mixing_scf_converged->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name logical, intent(inout) :: SCFconverged Calls proc~~mixing_scf_converged~~CallsGraph proc~mixing_scf_converged mixing_scf_converged reset reset proc~mixing_scf_converged->reset Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_scf_converged~~CalledByGraph proc~mixing_scf_converged mixing_scf_converged proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixing_scf_converged Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_scf_converged Source Code subroutine mixing_scf_converged ( SCFconverged ) use parallel , only : IONode logical , intent ( inout ) :: SCFconverged integer :: i ! Return if no convergence if ( . not . SCFconverged ) return if ( associated ( scf_mix % next_conv ) ) then ! this means that we skip to the ! following algorithm scf_mix => scf_mix % next_conv SCFconverged = . false . if ( allocated ( scf_mix % stack ) ) then do i = 1 , size ( scf_mix % stack ) ! delete all but one history ! This should be fine call reset ( scf_mix % stack ( i ), - 1 ) end do end if if ( IONode ) then write ( * , '(/,2a)' ) ':!: SCF cycle continuation mixer: ' , & trim ( scf_mix % name ) end if end if end subroutine mixing_scf_converged","tags":"","loc":"proc/mixing_scf_converged.html"},{"title":"mixers_scf_reset – SIESTA","text":"public subroutine mixers_scf_reset() Uses m_mixing proc~~mixers_scf_reset~~UsesGraph proc~mixers_scf_reset mixers_scf_reset module~m_mixing m_mixing proc~mixers_scf_reset->module~m_mixing precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_reset~~CallsGraph proc~mixers_scf_reset mixers_scf_reset proc~mixers_reset mixers_reset proc~mixers_scf_reset->proc~mixers_reset delete delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_reset Source Code subroutine mixers_scf_reset () use m_mixing , only : mixers_reset nullify ( scf_mix ) call mixers_reset ( scf_mixs ) end subroutine mixers_scf_reset","tags":"","loc":"proc/mixers_scf_reset.html"},{"title":"mixers_scf_history_init – SIESTA","text":"public subroutine mixers_scf_history_init() Uses m_mixing proc~~mixers_scf_history_init~~UsesGraph proc~mixers_scf_history_init mixers_scf_history_init module~m_mixing m_mixing proc~mixers_scf_history_init->module~m_mixing precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_history_init~~CallsGraph proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_history_init mixers_history_init proc~mixers_scf_history_init->proc~mixers_history_init proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new delete delete proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_scf_history_init~~CalledByGraph proc~mixers_scf_history_init mixers_scf_history_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_history_init Source Code subroutine mixers_scf_history_init ( ) use m_mixing , only : mixers_history_init call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) end subroutine mixers_scf_history_init","tags":"","loc":"proc/mixers_scf_history_init.html"},{"title":"mix_method – SIESTA","text":"public function mix_method(str) result(m) Uses fdf proc~~mix_method~~UsesGraph proc~mix_method mix_method fdf fdf proc~mix_method->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Return the integer specification of the mixing type @param[in] str the character representation of the mixing type\n @return the integer corresponding to the mixing type Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: str Return Value integer Calls proc~~mix_method~~CallsGraph proc~mix_method mix_method die die proc~mix_method->die leqi leqi proc~mix_method->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_method~~CalledByGraph proc~mix_method mix_method proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mix_method Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_method Source Code function mix_method ( str ) result ( m ) use fdf , only : leqi character ( len =* ), intent ( in ) :: str integer :: m if ( leqi ( str , 'linear' ) ) then m = MIX_LINEAR else if ( leqi ( str , 'pulay' ) . or . & leqi ( str , 'diis' ) . or . & leqi ( str , 'anderson' ) ) then m = MIX_PULAY else if ( leqi ( str , 'broyden' ) ) then m = MIX_BROYDEN else if ( leqi ( str , 'fire' ) ) then m = MIX_FIRE call die ( 'mixing: FIRE currently not supported.' ) else call die ( 'mixing: Unknown mixing variant.' ) end if end function mix_method","tags":"","loc":"proc/mix_method.html"},{"title":"mix_method_variant – SIESTA","text":"public function mix_method_variant(m, str) result(v) Uses fdf proc~~mix_method_variant~~UsesGraph proc~mix_method_variant mix_method_variant fdf fdf proc~mix_method_variant->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Return the variant of the mixing method @param[in] m the integer type of the mixing method\n @param[in] str the character specification of the mixing method variant\n @return the variant of the mixing method Arguments Type Intent Optional Attributes Name integer, intent(in) :: m character(len=*), intent(in) :: str Return Value integer Calls proc~~mix_method_variant~~CallsGraph proc~mix_method_variant mix_method_variant leqi leqi proc~mix_method_variant->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_method_variant~~CalledByGraph proc~mix_method_variant mix_method_variant proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mix_method_variant Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_method_variant Source Code function mix_method_variant ( m , str ) result ( v ) use fdf , only : leqi integer , intent ( in ) :: m character ( len =* ), intent ( in ) :: str integer :: v v = 0 select case ( m ) case ( MIX_LINEAR ) ! no variants case ( MIX_PULAY ) v = 0 ! We do not implement tho non-stable version ! There is no need to have an inferior Pulay mixer... if ( leqi ( str , 'original' ) . or . & leqi ( str , 'kresse' ) . or . leqi ( str , 'stable' ) ) then ! stable version, will nearly always succeed on inversion v = 0 else if ( leqi ( str , 'original+svd' ) . or . & leqi ( str , 'kresse+svd' ) . or . leqi ( str , 'stable+svd' ) ) then ! stable version, will nearly always succeed on inversion v = 2 else if ( leqi ( str , 'gr' ) . or . & leqi ( str , 'guarenteed-reduction' ) . or . & leqi ( str , 'bowler-gillan' ) ) then ! Guarenteed reduction version v = 1 else if ( leqi ( str , 'gr+svd' ) . or . & leqi ( str , 'guarenteed-reduction+svd' ) . or . & leqi ( str , 'bowler-gillan+svd' ) ) then ! Guarenteed reduction version v = 3 end if case ( MIX_BROYDEN ) ! Currently only one variant v = 0 case ( MIX_FIRE ) ! no variants end select end function mix_method_variant","tags":"","loc":"proc/mix_method_variant.html"},{"title":"mixing_ncoeff – SIESTA","text":"private function mixing_ncoeff(mix) result(n) Function to retrieve the number of coefficients\n calculated in this iteration.\n This is so external routines can query the size\n of the arrays used. @param[in] mix the used mixer Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer Calls proc~~mixing_ncoeff~~CallsGraph proc~mixing_ncoeff mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_ncoeff~~CalledByGraph proc~mixing_ncoeff mixing_ncoeff proc~mixing_coeff mixing_coeff proc~mixing_coeff->proc~mixing_ncoeff proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_ncoeff proc~mixing_1d->proc~mixing_coeff interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_ncoeff Source Code function mixing_ncoeff ( mix ) result ( n ) type ( tMixer ), intent ( in ) :: mix integer :: n n = 0 select case ( mix % m ) case ( MIX_PULAY ) n = n_items ( mix % stack ( 2 )) case ( MIX_BROYDEN ) n = n_items ( mix % stack ( 2 )) end select end function mixing_ncoeff","tags":"","loc":"proc/mixing_ncoeff.html"},{"title":"getstackval – SIESTA","text":"private function getstackval(mix, sidx, hidx) result(d1) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix integer, intent(in) :: sidx integer, intent(in), optional :: hidx Return Value real(kind=dp),\n  pointer,(:) Calls proc~~getstackval~~CallsGraph proc~getstackval getstackval get_pointer get_pointer proc~getstackval->get_pointer val val proc~getstackval->val n_items n_items proc~getstackval->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code getstackval Source Code function getstackval ( mix , sidx , hidx ) result ( d1 ) type ( tMixer ), intent ( in ) :: mix integer , intent ( in ) :: sidx integer , intent ( in ), optional :: hidx real ( dp ), pointer :: d1 (:) type ( dData1D ), pointer :: dD1 if ( present ( hidx ) ) then dD1 => get_pointer ( mix % stack ( sidx ), hidx ) else dD1 => get_pointer ( mix % stack ( sidx ), & n_items ( mix % stack ( sidx ))) end if d1 => val ( dD1 ) end function getstackval","tags":"","loc":"proc/getstackval.html"},{"title":"is_next – SIESTA","text":"private function is_next(mix, method, next) result(bool) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in), target :: mix integer, intent(in) :: method type( tMixer ), optional pointer :: next Return Value logical Contents Source Code is_next Source Code function is_next ( mix , method , next ) result ( bool ) type ( tMixer ), intent ( in ), target :: mix integer , intent ( in ) :: method type ( tMixer ), pointer , optional :: next logical :: bool type ( tMixer ), pointer :: m bool = . false . m => mix % next do while ( associated ( m ) ) if ( m % m == MIX_LINEAR ) then m => m % next else if ( m % m == method ) then bool = . true . exit else ! Quit if it does not do anything exit end if ! this will prevent cyclic combinations if ( associated ( m , mix ) ) exit end do if ( present ( next ) ) then next => m end if end function is_next","tags":"","loc":"proc/is_next.html"},{"title":"current_itt – SIESTA","text":"private function current_itt(mix) result(itt) Get current iteration count This is abstracted because the initial iteration\n and the current iteration may be uniquely defined. Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer Called by proc~~current_itt~~CalledByGraph proc~current_itt current_itt proc~mixers_history_init mixers_history_init proc~mixers_history_init->proc~current_itt proc~mixing_finalize mixing_finalize proc~mixing_finalize->proc~current_itt proc~mixing_init mixing_init proc~mixing_init->proc~current_itt proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->proc~mixers_history_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_init mixers_init proc~mixers_scf_init->proc~mixers_init proc~state_init state_init proc~state_init->proc~mixers_history_init proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize proc~mixing_1d->proc~mixing_init proc~mixers_init->proc~mixers_history_init interface~mixing mixing interface~mixing->proc~mixing_1d proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixers_scf_history_init proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code current_itt Source Code function current_itt ( mix ) result ( itt ) type ( tMixer ), intent ( in ) :: mix integer :: itt itt = mix % cur_itt - mix % start_itt end function current_itt","tags":"","loc":"proc/current_itt.html"},{"title":"stack_check – SIESTA","text":"private function stack_check(stack, n) result(check) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: stack integer, intent(in) :: n Return Value logical Calls proc~~stack_check~~CallsGraph proc~stack_check stack_check get_pointer get_pointer proc~stack_check->get_pointer n_items n_items proc~stack_check->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~stack_check~~CalledByGraph proc~stack_check stack_check proc~update_f update_F proc~update_f->proc~stack_check proc~push_f push_F proc~update_f->proc~push_f proc~push_stack_data push_stack_data proc~push_stack_data->proc~stack_check proc~push_f->proc~stack_check Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code stack_check Source Code function stack_check ( stack , n ) result ( check ) type ( Fstack_dData1D ), intent ( inout ) :: stack integer , intent ( in ) :: n logical :: check ! Local arrays type ( dData1D ), pointer :: dD1 if ( n_items ( stack ) == 0 ) then check = . true . else ! Check that the stack stored arrays are ! of same size... dD1 => get_pointer ( stack , 1 ) check = n == size ( dD1 ) end if end function stack_check","tags":"","loc":"proc/stack_check.html"},{"title":"norm – SIESTA","text":"private function norm(n, x1, x2) Calculate the norm of two arrays Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: x1 (n) real(kind=dp), intent(in) :: x2 (n) Return Value real(kind=dp) Called by proc~~norm~~CalledByGraph proc~norm norm proc~mixing_init mixing_init proc~mixing_init->proc~norm proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code norm Source Code function norm ( n , x1 , x2 ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: x1 ( n ), x2 ( n ) real ( dp ) :: norm ! Currently we use an external routine integer :: i ! Calculate dot product norm = 0._dp !$OMP parallel do default(shared), private(i) & !$OMP& reduction(+:norm) do i = 1 , n norm = norm + x1 ( i ) * x2 ( i ) end do !$OMP end parallel do end function norm","tags":"","loc":"proc/norm.html"},{"title":"mixers_init – SIESTA","text":"public subroutine mixers_init(prefix, mixers, Comm) Uses parallel fdf proc~~mixers_init~~UsesGraph proc~mixers_init mixers_init parallel parallel proc~mixers_init->parallel fdf fdf proc~mixers_init->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Initialize a set of mixers by reading in fdf information.\n @param[in] prefix the fdf-label prefixes\n @param[pointer] mixers the mixers that are to be initialized\n @param[in] Comm @opt optional MPI-communicator Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), pointer :: mixers (:) integer, intent(in), optional :: Comm Calls proc~~mixers_init~~CallsGraph proc~mixers_init mixers_init fdf_bnames fdf_bnames proc~mixers_init->fdf_bnames proc~mixers_reset mixers_reset proc~mixers_init->proc~mixers_reset fdf_bline fdf_bline proc~mixers_init->fdf_bline fdf_get fdf_get proc~mixers_init->fdf_get fdf_block fdf_block proc~mixers_init->fdf_block fdf_brewind fdf_brewind proc~mixers_init->fdf_brewind proc~mixers_history_init mixers_history_init proc~mixers_init->proc~mixers_history_init fdf_bnnames fdf_bnnames proc~mixers_init->fdf_bnnames die die proc~mixers_init->die delete delete proc~mixers_reset->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_init~~CalledByGraph proc~mixers_init mixers_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_init Source Code subroutine mixers_init ( prefix , mixers , Comm ) use parallel , only : IONode , Node use fdf ! FDF-prefix for searching keywords character ( len =* ), intent ( in ) :: prefix ! The array of mixers (has to be nullified upon entry) type ( tMixer ), pointer :: mixers (:) integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf type ( parsed_line ), pointer :: pline ! number of history steps saved integer :: n_hist , n_restart , n_save real ( dp ) :: w integer :: nm , im , im2 character ( len = 10 ) :: lp character ( len = 70 ) :: method , variant ! Default mixing options... if ( fdf_get ( 'Mixer.Debug' ,. false .) ) then debug_mix = IONode debug_msg = 'mix:' end if if ( fdf_get ( 'Mixer.Debug.MPI' ,. false .) ) then debug_mix = . true . write ( debug_msg , '(a,i0,a)' ) 'mix (' , Node , '):' end if lp = trim ( prefix ) // '.Mixer' ! ensure nullification call mixers_reset ( mixers ) ! Return immediately if the user hasn't defined ! an fdf-block for the mixing options... if ( . not . fdf_block ( trim ( lp ) // 's' , bfdf ) ) return ! update mixing weight and kick mixing weight w = fdf_get ( trim ( lp ) // '.Weight' , 0.1_dp ) ! Get history length n_hist = fdf_get ( trim ( lp ) // '.History' , 6 ) ! Restart after this number of iterations n_restart = fdf_get ( trim ( lp ) // '.Restart' , 0 ) n_save = fdf_get ( trim ( lp ) // '.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Read in the options regarding the mixing options nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 end do if ( nm == 0 ) then call die ( 'mixing: No mixing schemes selected. & &Please at least add one mixer.' ) end if ! Allocate all denoted mixers... allocate ( mixers ( nm )) mixers (:)% w = w mixers (:)% n_hist = n_hist mixers (:)% restart = n_restart mixers (:)% restart_save = n_save ! Rewind to grab names. call fdf_brewind ( bfdf ) nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 mixers ( nm )% name = fdf_bnames ( pline , 1 ) end do ! Now read all mixers for this segment and their options do im = 1 , nm call read_block ( mixers ( im ) ) end do ! Create history stack and associate correct ! stack pointers call mixers_history_init ( mixers ) #ifdef MPI if ( present ( Comm ) ) then mixers (:)% Comm = Comm else mixers (:)% Comm = MPI_Comm_World end if #endif contains subroutine read_block ( m ) type ( tMixer ), intent ( inout ), target :: m character ( len = 64 ) :: opt ! create block string opt = trim ( lp ) // '.' // trim ( m % name ) if ( . not . fdf_block ( opt , bfdf ) ) then call die ( 'Block: ' // trim ( opt ) // ' does not exist!' ) end if ! Default to the pulay method... ! This enables NOT writing this in the block method = 'pulay' variant = ' ' ! read method do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'method' ) ) then method = fdf_bnames ( pline , 2 ) else if ( leqi ( opt , 'variant' ) ) then variant = fdf_bnames ( pline , 2 ) end if end do ! Retrieve the method and the variant m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! Define separate defaults which are ! not part of the default input options select case ( m % m ) case ( MIX_LINEAR ) m % n_hist = 0 end select call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'iterations' ) & . or . leqi ( opt , 'itt' ) ) then m % n_itt = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'history' ) ) then m % n_hist = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'weight' ) . or . leqi ( opt , 'w' ) ) then m % w = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'restart' ) ) then m % restart = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'restart.save' ) ) then m % restart_save = fdf_bintegers ( pline , 1 ) m % restart_save = max ( 0 , m % restart_save ) end if end do ! Initialize the mixer by setting the correct ! standard options and allocate space in the mixers... call mixer_init ( m ) ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'next' ) ) then nullify ( m % next ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next ) ) then call die ( 'mixing: Could not find next mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next , target = m ) ) then call die ( 'mixing: Next *must* not be it-self. & &Please change accordingly.' ) end if else if ( leqi ( opt , 'next.conv' ) ) then nullify ( m % next_conv ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next_conv => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next_conv ) ) then call die ( 'mixing: Could not find next convergence mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next_conv , target = m ) ) then call die ( 'mixing: next.conv *must* not be it-self. & &Please change accordingly.' ) end if end if end do ! Ensure that if a next have not been specified ! it will continue indefinitely. if ( . not . associated ( m % next ) ) then m % n_itt = 0 end if ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) ! skip lines without associated content if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) ! Do options so that a pulay option may refer to ! the actual names of the constants if ( m % m == MIX_PULAY ) then ! The linear mixing weight if ( leqi ( opt , 'weight.linear' ) & . or . leqi ( opt , 'w.linear' ) ) then m % rv ( 1 ) = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'svd.cond' ) ) then ! This is only applicable to the Pulay ! mixing scheme... m % rv ( I_SVD_COND ) = fdf_bvalues ( pline , 1 ) end if end if ! Generic options for all advanced methods... if ( leqi ( opt , 'next.p' ) ) then ! Only allow stepping to the next when ! having a next associated if ( associated ( m % next ) ) then m % rv ( I_P_NEXT ) = fdf_bvalues ( pline , 1 ) end if else if ( leqi ( opt , 'restart.p' ) ) then m % rv ( I_P_RESTART ) = fdf_bvalues ( pline , 1 ) end if end do end subroutine read_block end subroutine mixers_init","tags":"","loc":"proc/mixers_init.html"},{"title":"mixer_init – SIESTA","text":"public subroutine mixer_init(mix) Initialize a single mixer depending on the preset\n options. Useful for external correct setup. @param[inout] mix mixer to be initialized Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix Calls proc~~mixer_init~~CallsGraph proc~mixer_init mixer_init die die proc~mixer_init->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixer_init Source Code subroutine mixer_init ( mix ) type ( tMixer ), intent ( inout ) :: mix integer :: n ! Correct amount of history in the mixing. if ( 0 < mix % restart . and . & mix % restart < mix % n_hist ) then ! This is if we restart this scheme, ! then it does not make sense to have a history ! greater than the restart count mix % n_hist = mix % restart end if if ( 0 < mix % n_itt . and . & mix % n_itt < mix % n_hist ) then ! If this only runs for n_itt itterations, ! it makes no sense to have a history greater ! than this. mix % n_hist = mix % n_itt end if select case ( mix % m ) case ( MIX_LINEAR ) allocate ( mix % rv ( I_SVD_COND : 0 )) ! Kill any history settings that do not apply to the ! linear mixer. mix % restart = 0 mix % restart_save = 0 case ( MIX_PULAY ) allocate ( mix % rv ( I_SVD_COND : 1 )) mix % rv ( 1 ) = mix % w ! We allocate the double residual (n_hist-1) mix % n_hist = max ( 2 , mix % n_hist ) if ( mix % v == 1 . or . mix % v == 3 ) then ! The GR method requires an even number ! of restart steps ! And then we ensure the history to be aligned ! with a restart (restart has precedence) mix % restart = mix % restart + mod ( mix % restart , 2 ) end if case ( MIX_BROYDEN ) ! allocate temporary array mix % n_hist = max ( 2 , mix % n_hist ) n = 1 + mix % n_hist allocate ( mix % rv ( I_SVD_COND : n )) mix % rv ( 1 : n ) = mix % w end select if ( mix % restart < 0 ) then call die ( 'mixing: restart count must be positive' ) end if mix % restart_save = min ( mix % n_hist - 1 , mix % restart_save ) mix % restart_save = max ( 0 , mix % restart_save ) ! This is the restart parameter ! I.e. if |f_k / f - 1| < rp ! only works for positive rp mix % rv ( I_PREVIOUS_RES ) = huge ( 1._dp ) mix % rv ( I_P_RESTART ) = - 1._dp mix % rv ( I_P_NEXT ) = - 1._dp mix % rv ( I_SVD_COND ) = 1.e-8_dp end subroutine mixer_init","tags":"","loc":"proc/mixer_init.html"},{"title":"mixers_history_init – SIESTA","text":"public subroutine mixers_history_init(mixers) Initialize all history for the mixers Routine for clearing all history and setting up the\n arrays so that they may be used subsequently. @param[inout] mixers the mixers to be initialized Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout), target :: mixers (:) Calls proc~~mixers_history_init~~CallsGraph proc~mixers_history_init mixers_history_init proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new delete delete proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_history_init~~CalledByGraph proc~mixers_history_init mixers_history_init proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->proc~mixers_history_init proc~mixers_init mixers_init proc~mixers_init->proc~mixers_history_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_scf_init->proc~mixers_init proc~state_init state_init proc~state_init->proc~mixers_history_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixers_scf_history_init proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_history_init Source Code subroutine mixers_history_init ( mixers ) type ( tMixer ), intent ( inout ), target :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns logical :: is_GR do im = 1 , size ( mixers ) m => mixers ( im ) if ( debug_mix . and . current_itt ( m ) >= 1 ) then write ( * , '(a,a)' ) trim ( debug_msg ), & ' resetting history of all mixers' exit end if end do ! Clean up all arrays and reference counted ! objects do im = 1 , size ( mixers ) m => mixers ( im ) ! reset history track m % start_itt = 0 m % cur_itt = 0 ! do not try and de-allocate something not ! allocated if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do ! clean-up deallocate ( m % stack ) end if ! Re-populate select case ( m % m ) case ( MIX_LINEAR ) ! do nothing case ( MIX_PULAY ) is_GR = ( m % v == 1 ) . or . ( m % v == 3 ) if ( . not . is_GR ) then allocate ( m % stack ( 3 )) else allocate ( m % stack ( 2 )) end if ! These arrays contains these informations !   s1 = m%stack(1) !   s2 = m%stack(2) !   s3 = m%stack(3) ! Here <> is input function, x[in], and ! <>' is the corresponding output, x[out]. ! First iteration: !   s1 = { 1' - 1 } !   s3 = { 1' } ! Second iteration !   s2 = { 2' - 2 - (1' - 1) } !   s1 = { 2 - 1 , 2' - 2 } !   s3 = { 2' } ! Third iteration !   s2 = { 2' - 2 - (1' - 1) , 3' - 3 - (2' - 2) } !   s1 = { 2 - 1 , 3 - 2, 3' - 3 } !   s3 = { 3' } ! and so on ! allocate x[i+1] - x[i] call new ( m % stack ( 1 ), m % n_hist ) ! allocate F[i+1] - F[i] call new ( m % stack ( 2 ), m % n_hist - 1 ) if ( . not . is_GR ) then call new ( m % stack ( 3 ), 1 ) end if case ( MIX_BROYDEN ) ! Same as original Pulay allocate ( m % stack ( 3 )) call new ( m % stack ( 1 ), m % n_hist ) call new ( m % stack ( 2 ), m % n_hist - 1 ) call new ( m % stack ( 3 ), 1 ) end select end do end subroutine mixers_history_init","tags":"","loc":"proc/mixers_history_init.html"},{"title":"mixers_reset – SIESTA","text":"public subroutine mixers_reset(mixers) Reset the mixers, i.e. clean everything Also deallocates (and nullifies) the input array! @param[inout] mixers array of mixers to be cleaned Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mixers (:) Calls proc~~mixers_reset~~CallsGraph proc~mixers_reset mixers_reset delete delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_reset~~CalledByGraph proc~mixers_reset mixers_reset proc~mixers_init mixers_init proc~mixers_init->proc~mixers_reset proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_reset proc~mixers_scf_init->proc~mixers_init proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->proc~mixers_reset Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_reset Source Code subroutine mixers_reset ( mixers ) type ( tMixer ), pointer :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns if ( . not . associated ( mixers ) ) return do im = 1 , size ( mixers ) m => mixers ( im ) if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do deallocate ( m % stack ) end if if ( associated ( m % rv ) ) then deallocate ( m % rv ) nullify ( m % rv ) end if if ( associated ( m % iv ) ) then deallocate ( m % iv ) nullify ( m % iv ) end if end do deallocate ( mixers ) nullify ( mixers ) end subroutine mixers_reset","tags":"","loc":"proc/mixers_reset.html"},{"title":"mixers_print – SIESTA","text":"public subroutine mixers_print(prefix, mixers) Uses parallel proc~~mixers_print~~UsesGraph proc~mixers_print mixers_print parallel parallel proc~mixers_print->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Print (to std-out) information regarding the mixers @param[in] prefix the prefix (fdf) for the mixers\n @param[in] mixers array of mixers allocated Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) Calls proc~~mixers_print~~CallsGraph proc~mixers_print mixers_print die die proc~mixers_print->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_print~~CalledByGraph proc~mixers_print mixers_print proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->proc~mixers_print Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_print Source Code subroutine mixers_print ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m character ( len = 50 ) :: fmt logical :: bool integer :: i if ( . not . IONode ) return fmt = 'mix.' // trim ( prefix ) // ':' if ( debug_mix ) then write ( * , '(2a,t50,''= '',l)' ) trim ( fmt ), & ' Debug messages' , debug_mix end if ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Linear mixing' , trim ( m % name ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w if ( m % n_hist > 0 . and . (& associated ( m % next ) & . or . associated ( m % next_conv )) ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Carried history steps' , m % n_hist end if case ( MIX_PULAY ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Pulay mixing' , trim ( m % name ) select case ( m % v ) case ( 0 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable' case ( 1 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR' case ( 2 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable-SVD' case ( 3 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR-SVD' end select write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Linear mixing weight' , m % rv ( 1 ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w write ( * , '(2a,t50,''= '',e10.4)' ) trim ( fmt ), & '    SVD condition' , m % rv ( I_SVD_COND ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_BROYDEN ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Broyden mixing' , trim ( m % name ) !write(*,'(2a,t50,''= '',a)') trim(fmt), & !     '    Variant','original' write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Jacobian weight' , m % w write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Weight prime' , m % rv ( 1 ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_FIRE ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Fire mixing' , trim ( m % name ) end select if ( m % n_itt > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Number of mixing iterations' , m % n_itt if ( associated ( m % next ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer' , trim ( m % next % name ) else call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if end if if ( associated ( m % next_conv ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer upon convergence' , trim ( m % next_conv % name ) end if end do end subroutine mixers_print","tags":"","loc":"proc/mixers_print.html"},{"title":"mixers_print_block – SIESTA","text":"public subroutine mixers_print_block(prefix, mixers) Uses parallel proc~~mixers_print_block~~UsesGraph proc~mixers_print_block mixers_print_block parallel parallel proc~mixers_print_block->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Print (to std-out) the fdf-blocks that recreate the mixer settings @param[in] prefix the fdf-prefix for reading the blocks\n @param[in] mixers array of mixers that should be printed\n    their fdf-blocks Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) Calls proc~~mixers_print_block~~CallsGraph proc~mixers_print_block mixers_print_block die die proc~mixers_print_block->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_print_block~~CalledByGraph proc~mixers_print_block mixers_print_block proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->proc~mixers_print_block Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_print_block Source Code subroutine mixers_print_block ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m logical :: bool integer :: i if ( . not . IONode ) return ! Write block of input write ( * , '(/3a)' ) '%block ' , trim ( prefix ), '.Mixers' do i = 1 , size ( mixers ) m => mixers ( i ) write ( * , '(t3,a)' ) trim ( m % name ) end do write ( * , '(3a)' ) '%endblock ' , trim ( prefix ), '.Mixers' ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) ! Write out this block write ( * , '(/4a)' ) '%block ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) write ( * , '(t3,a)' ) '# Mixing method' ! Write out method select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'linear' case ( MIX_PULAY ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'pulay' select case ( m % v ) case ( 0 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable' case ( 1 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR' case ( 2 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable+SVD' case ( 3 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR+SVD' end select case ( MIX_BROYDEN ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'broyden' ! currently no variants exists end select ! remark write ( * , '(/,t3,a)' ) '# Mixing options' ! Weight ! For Broyden this is the inverse Jacobian write ( * , '(t3,a,f6.4)' ) 'weight ' , m % w select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) write ( * , '(t3,a,f6.4)' ) 'weight.linear ' , m % rv ( 1 ) end select if ( m % n_hist > 0 ) then write ( * , '(t3,a,i0)' ) 'history ' , m % n_hist end if bool = . false . if ( m % restart > 0 ) then write ( * , '(t3,a,i0)' ) 'restart ' , m % restart bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(t3,a,e10.5)' ) 'restart.p ' , m % rv ( I_P_RESTART ) bool = . true . end if end select if ( bool ) then write ( * , '(t3,a,i0)' ) 'restart.save ' , m % restart_save end if ! remark bool = . false . if ( m % n_itt > 0 ) then write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,i0)' ) 'iterations ' , m % n_itt bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,f6.4)' ) 'next.p ' , m % rv ( I_P_NEXT ) bool = . true . end if end select if ( bool . and . associated ( m % next ) ) then write ( * , '(t2,2(tr1,a))' ) 'next' , trim ( m % next % name ) else if ( bool ) then call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if if ( associated ( m % next_conv ) ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t2,2(tr1,a))' ) 'next.conv' , trim ( m % next_conv % name ) end if write ( * , '(4a)' ) '%endblock ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) end do write ( * , * ) ! new-line end subroutine mixers_print_block","tags":"","loc":"proc/mixers_print_block.html"},{"title":"mixing_init – SIESTA","text":"private subroutine mixing_init(mix, n, xin, F) Initialize the mixing algorithm @param[pointer] mix the mixing method\n @param[in] n size of the arrays to be used in the algorithm\n @param[in] xin array of the input variables\n @param[in] xout array of the output variables Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) Calls proc~~mixing_init~~CallsGraph proc~mixing_init mixing_init proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_init~~CalledByGraph proc~mixing_init mixing_init proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_init Source Code subroutine mixing_init ( mix , n , xin , F ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n ! In/out of the function real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), pointer :: res (:), rres (:) integer :: i , ns real ( dp ) :: dnorm , dtmp logical :: p_next , p_restart ! Initialize action for mixer mix % action = ACTION_MIX ! Step iterator (so first mixing has cur_itt == 1) mix % cur_itt = mix % cur_itt + 1 ! If we are going to skip to next, we signal it ! before entering if ( mix % n_itt > 0 . and . & mix % n_itt <= current_itt ( mix ) ) then mix % action = IOR ( mix % action , ACTION_NEXT ) end if ! Check whether the residual norm is below a certain ! criteria p_next = mix % rv ( I_P_NEXT ) > 0._dp p_restart = mix % rv ( I_P_RESTART ) > 0._dp ! Check whether a parameter next/restart is required if ( p_restart . or . p_next ) then ! Calculate norm: ||f_k|| dnorm = norm ( n , F , F ) #ifdef MPI dtmp = dnorm call MPI_AllReduce ( dtmp , dnorm , 1 , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) #endif ! Calculate the relative difference dtmp = abs ( dnorm / mix % rv ( I_PREVIOUS_RES ) - 1._dp ) ! We first check for next, that has precedence if ( p_next ) then if ( dtmp < mix % rv ( I_P_NEXT ) ) then ! Signal stepping mixer mix % action = IOR ( mix % action , ACTION_NEXT ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < np  :  ' , & dtmp , ' < ' , mix % rv ( I_P_NEXT ) end if if ( p_restart ) then if ( dtmp < mix % rv ( I_P_RESTART ) ) then ! Signal restart mix % action = IOR ( mix % action , ACTION_RESTART ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < rp  :  ' , & dtmp , ' < ' , mix % rv ( I_P_RESTART ) end if ! Store the new residual norm mix % rv ( I_PREVIOUS_RES ) = dnorm end if ! Push information to the stack select case ( mix % m ) case ( MIX_LINEAR ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' linear' call init_linear () case ( MIX_PULAY ) if ( debug_mix ) then select case ( mix % v ) case ( 0 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay' case ( 1 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay, GR' case ( 2 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD' case ( 3 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD, GR' end select end if call init_pulay () case ( MIX_BROYDEN ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' Broyden' call init_broyden () end select contains subroutine init_linear () ! information for this depends on the ! following method call fake_history_from_linear ( mix % next ) call fake_history_from_linear ( mix % next_conv ) end subroutine init_linear subroutine init_pulay () logical :: GR_linear select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do end if case ( 1 , 3 ) ! Whether this is the linear cycle... GR_linear = mod ( current_itt ( mix ), 2 ) == 1 ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F , mix % rv ( 1 )) ns = n_items ( mix % stack ( 1 )) if ( GR_linear . and . current_itt ( mix ) > 1 . and . & ns > 1 ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = rres ( i ) + res ( i ) end do !$OMP end parallel do else if ( ns > 1 . and . . not . GR_linear ) then ! now we can calculate RRes[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) end if end select end subroutine init_pulay subroutine init_broyden () ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do else ! Store F[x_in] (used to create the input residual) call push_stack_data ( mix % stack ( 3 ), n ) res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if end subroutine init_broyden subroutine fake_history_from_linear ( next ) type ( tMixer ), pointer :: next real ( dp ), pointer :: t1 (:), t2 (:) integer :: ns , nh , i , nhl if ( . not . associated ( next ) ) return ! Reduce to # history of linear nhl = mix % n_hist ! if the number of fake-history steps saved is ! zero we immediately return. ! Only if mix%n_hist > 0 will the below ! occur. if ( nhl == 0 ) return ! Check for the type of following method select case ( next % m ) case ( MIX_PULAY ) ! Here it depends on the variant select case ( next % v ) case ( 0 , 2 ) ! stable pulay mixing ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select case ( MIX_BROYDEN ) ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns >= 2 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select end subroutine fake_history_from_linear end subroutine mixing_init","tags":"","loc":"proc/mixing_init.html"},{"title":"mixing_coeff – SIESTA","text":"private subroutine mixing_coeff(mix, n, xin, F, coeff) Uses parallel proc~~mixing_coeff~~UsesGraph proc~mixing_coeff mixing_coeff parallel parallel proc~mixing_coeff->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Calculate the mixing coefficients for the\n current mixer @param[in] mix the current mixer\n @param[in] n the number of elements used to calculate\n           the coefficients\n @param[in] xin the input value\n @param[in] F xout - xin, (residual)\n @param[out] coeff the coefficients Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: coeff (:) Calls proc~~mixing_coeff~~CallsGraph proc~mixing_coeff mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_coeff~~CalledByGraph proc~mixing_coeff mixing_coeff proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_coeff interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_coeff Source Code subroutine mixing_coeff ( mix , n , xin , F , coeff ) use parallel , only : IONode type ( tMixer ), intent ( inout ) :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), intent ( out ) :: coeff (:) integer :: ncoeff ncoeff = size ( coeff ) if ( ncoeff < mixing_ncoeff ( mix ) ) then write ( * , '(a)' ) 'mix: Error in calculating coefficients' ! Do not allow this... return end if select case ( mix % m ) case ( MIX_LINEAR ) call linear_coeff () case ( MIX_PULAY ) call pulay_coeff () case ( MIX_BROYDEN ) call broyden_coeff () end select contains subroutine linear_coeff () integer :: i do i = 1 , ncoeff coeff ( i ) = 0._dp end do end subroutine linear_coeff subroutine pulay_coeff () integer :: ns , nh , nmax integer :: i , j , info logical :: lreturn ! Calculation quantities real ( dp ) :: dnorm , G real ( dp ), pointer :: res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) lreturn = . false . ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) return ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! Allocate arrays for calculating the ! coefficients allocate ( A ( nh , nh ), Ainv ( nh , nh )) ! Calculate A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = norm(RRes[i],RRes[j]) A ( i , j ) = norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Diagonal A ( i , i ) = norm ( n , rres1 , rres1 ) end do #ifdef MPI ! Global operations, but only for the non-extended entries call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) ! copy over reduced arrays A = Ainv #endif ! Get inverse of matrix select case ( mix % v ) case ( 0 , 1 ) call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if case ( 2 , 3 ) ! We forcefully use the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end select ! NOTE, although mix%stack(1) contains ! the x[i] - x[i-1], the tip of the stack ! contains F[i]! ! res == F[i] res => getstackval ( mix , 1 ) ! Initialize the coefficients do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients on all processors do j = 1 , nh ! res  == F[i] ! rres == F[j+1] - F[j] rres => getstackval ( mix , 2 , j ) dnorm = norm ( n , rres , res ) do i = 1 , nh coeff ( i ) = coeff ( i ) - Ainv ( i , j ) * dnorm end do end do #ifdef MPI ! Reduce the coefficients call MPI_AllReduce ( coeff ( 1 ), A ( 1 , 1 ), nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh coeff ( i ) = A ( i , 1 ) end do #endif else info = 0 ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, SVD failed, > linear' end if ! Clean up memory deallocate ( A , Ainv ) end subroutine pulay_coeff subroutine broyden_coeff () integer :: ns , nh , nmax integer :: i , j , k , info ! Calculation quantities real ( dp ) :: dnorm , dtmp real ( dp ), pointer :: w (:), res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: c (:), A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) ! Easy check for initial step... if ( ns == 1 ) then ! reset coeff = 0._dp return end if ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! This is the modified Broyden algorithm... ! Retrieve the previous weights w => mix % rv ( 2 : 1 + nh ) select case ( mix % v ) case ( 2 ) ! Unity Broyden w ( nh ) = 1._dp case ( 1 ) ! RMS Broyden dnorm = norm ( n , F , F ) #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif w (:) = 1._dp / sqrt ( dnorm ) if ( debug_mix ) & write ( * , '(2(a,e10.4))' ) & trim ( debug_msg ) // ' weight = ' , w ( 1 ), & ' , norm = ' , dnorm case ( 0 ) ! Varying weight dnorm = 0._dp !$OMP parallel do default(shared), private(i), & !$OMP& reduction(max:dnorm) do i = 1 , n dnorm = max ( dnorm , abs ( F ( i )) ) end do !$OMP end parallel do #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif ! Problay 0.2 should be changed to user-defined w ( nh ) = exp ( 1._dp / ( dnorm + 0.2_dp ) ) if ( debug_mix ) & write ( * , '(2a,1000(tr1,e10.4))' ) & trim ( debug_msg ), ' weights = ' , w ( 1 : nh ) end select ! Allocate arrays used allocate ( c ( nh )) allocate ( A ( nh , nh ), Ainv ( nh , nh )) !  < RRes[i] | Res[n] > do i = 1 , nh rres => getstackval ( mix , 2 , i ) c ( i ) = norm ( n , rres , F ) end do #ifdef MPI call MPI_AllReduce ( c ( 1 ), A ( 1 , 1 ), nh , & MPI_Double_Precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh c ( i ) = A ( i , 1 ) end do #endif ! Create A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = dot_product(RRes[i],RRes[j]) A ( i , j ) = w ( i ) * w ( j ) * norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Do the diagonal term A ( i , i ) = w ( i ) * w ( i ) * norm ( n , rres1 , rres1 ) end do #ifdef MPI call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) A = Ainv #endif ! Add the diagonal term ! This should also prevent it from being ! singular (unless mix%w == 0) do i = 1 , nh A ( i , i ) = mix % rv ( 1 ) ** 2 + A ( i , i ) end do ! Calculate the inverse call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients... do i = 1 , nh do j = 1 , nh ! Ainv should be symmetric (A is) coeff ( i ) = coeff ( i ) + w ( j ) * c ( j ) * Ainv ( j , i ) end do ! Calculate correct weight... coeff ( i ) = - w ( i ) * coeff ( i ) end do else ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, SVD failed, > linear' end if deallocate ( A , Ainv ) end subroutine broyden_coeff end subroutine mixing_coeff","tags":"","loc":"proc/mixing_coeff.html"},{"title":"mixing_calc_next – SIESTA","text":"private subroutine mixing_calc_next(mix, n, xin, F, xnext, coeff) Calculate the guess for the next iteration Note this gets passed the coefficients. Hence,\n they may be calculated from another set of history\n steps.\n This may be useful in certain situations. @param[in] mix the current mixer\n @param[in] n the number of elements used to calculate\n           the coefficients\n @param[in] xin the input value\n @param[in] F the xin residual\n @param[out] xnext the input for the following iteration\n @param[in] coeff the coefficients Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: xnext (n) real(kind=dp), intent(in) :: coeff (:) Called by proc~~mixing_calc_next~~CalledByGraph proc~mixing_calc_next mixing_calc_next proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_calc_next interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_calc_next Source Code subroutine mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ) real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( out ) :: xnext ( n ) real ( dp ), intent ( in ) :: coeff (:) select case ( mix % m ) case ( MIX_LINEAR ) call mixing_linear () case ( MIX_PULAY ) call mixing_pulay () case ( MIX_BROYDEN ) call mixing_broyden () end select contains subroutine mixing_linear () integer :: i real ( dp ) :: w w = mix % w if ( debug_mix ) write ( * , '(2a,e10.4)' ) & trim ( debug_msg ), ' alpha = ' , w !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + w * F ( i ) end do !$OMP end parallel do end subroutine mixing_linear subroutine mixing_pulay () integer :: ns , nh integer :: i , j logical :: lreturn real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Pulay' xnext = 0._dp return end if ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Pulay (initial), alpha = ' , mix % rv ( 1 ) case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Direct mixing, alpha = ' , mix % rv ( 1 ) end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) then ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ),& ' G = ' , G , ', sum(alpha) = ' , sum ( coeff ), & ', alpha = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_pulay subroutine mixing_broyden () integer :: ns , nh integer :: i , j real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Broyden' xnext = 0._dp return end if if ( ns == 1 ) then if ( debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Broyden (initial), alpha = ' , mix % rv ( 1 ) ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' G = ' , G , & ', sum(coeff) = ' , sum ( coeff ), & ', coeff = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_broyden end subroutine mixing_calc_next","tags":"","loc":"proc/mixing_calc_next.html"},{"title":"mixing_finalize – SIESTA","text":"private subroutine mixing_finalize(mix, n, xin, F, xnext) Uses parallel proc~~mixing_finalize~~UsesGraph proc~mixing_finalize mixing_finalize parallel parallel proc~mixing_finalize->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finalize the mixing algorithm @param[inout] mix mixer to be finalized\n @param[in] n size of the input arrays\n @param[in] xin the input for this iteration\n @param[in] F the residual for this iteration\n @param[in] xnext the optimized input for the next iteration Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in) :: xnext (n) Calls proc~~mixing_finalize~~CallsGraph proc~mixing_finalize mixing_finalize proc~current_itt current_itt proc~mixing_finalize->proc~current_itt reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step n_items n_items proc~mixing_finalize->n_items proc~mixing_step->reset proc~mixing_step->n_items get_pointer get_pointer proc~mixing_step->get_pointer push push proc~mixing_step->push Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_finalize~~CalledByGraph proc~mixing_finalize mixing_finalize proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_finalize Source Code subroutine mixing_finalize ( mix , n , xin , F , xnext ) use parallel , only : IONode type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ), xnext ( n ) integer :: rsave select case ( mix % m ) case ( MIX_LINEAR ) call fin_linear () case ( MIX_PULAY ) call fin_pulay () case ( MIX_BROYDEN ) call fin_broyden () end select ! Fix the action to finalize it.. if ( mix % restart > 0 . and . & mod ( current_itt ( mix ), mix % restart ) == 0 ) then mix % action = IOR ( mix % action , ACTION_RESTART ) end if ! Check the actual finalization... ! First check whether we should restart history if ( IAND ( mix % action , ACTION_RESTART ) == ACTION_RESTART ) then ! The user has requested to restart the ! mixing scheme now rsave = mix % restart_save select case ( mix % m ) case ( MIX_PULAY ) if ( IONode ) then write ( * , '(a)' ) 'mix: Pulay -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if case ( MIX_BROYDEN ) if ( IONode ) then write ( * , '(a)' ) 'mix: Broyden -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if end select if ( allocated ( mix % stack ) ) then if ( debug_mix ) & write ( * , '(a,a,i0)' ) trim ( debug_msg ), & ' saved hist = ' , n_items ( mix % stack ( 1 )) end if end if ! check whether we should change the mixer if ( IAND ( mix % action , ACTION_NEXT ) == ACTION_NEXT ) then call mixing_step ( mix ) end if contains subroutine fin_linear () ! do nothing... end subroutine fin_linear subroutine fin_pulay () integer :: ns integer :: i logical :: GR_linear real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) select case ( mix % v ) case ( 0 , 2 ) ! stable Pulay if ( n_items ( mix % stack ( 3 )) == 0 ) then call push_stack_data ( mix % stack ( 3 ), n ) end if res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do case ( 1 , 3 ) ! GR Pulay GR_linear = mod ( current_itt ( mix ), 2 ) == 1 if ( n_items ( mix % stack ( 2 )) > 0 . and . & . not . GR_linear ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  rres == F[i] - F[i-1] rres ( i ) = rres ( i ) - res ( i ) ! Output: !  rres == - F[i-1] end do !$OMP end parallel do call pop ( mix % stack ( 1 )) ! Note that this is Res[i-1] = (F&#94;i-1_out - F&#94;i-1_in) res => getstackval ( mix , 1 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - xin ( i ) + xnext ( i ) end do !$OMP end parallel do end if end select end subroutine fin_pulay subroutine fin_broyden () integer :: ns , nh integer :: i real ( dp ), pointer :: res (:), rres (:) ns = current_itt ( mix ) nh = n_items ( mix % stack ( 2 )) if ( ns >= 2 . and . n_items ( mix % stack ( 3 )) > 0 ) then ! Update the residual to reflect the input residual res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do end if ! Update weights (if necessary) if ( nh > 1 ) then do i = 2 , nh mix % rv ( i ) = mix % rv ( i + 1 ) end do end if end subroutine fin_broyden end subroutine mixing_finalize","tags":"","loc":"proc/mixing_finalize.html"},{"title":"mixing_1d – SIESTA","text":"private subroutine mixing_1d(mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub Calls proc~~mixing_1d~~CallsGraph proc~mixing_1d mixing_1d proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d->proc~mixing_ncoeff proc~mixing_calc_next mixing_calc_next proc~mixing_1d->proc~mixing_calc_next proc~mixing_init mixing_init proc~mixing_1d->proc~mixing_init proc~mixing_finalize mixing_finalize proc~mixing_1d->proc~mixing_finalize proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items proc~norm norm proc~mixing_init->proc~norm proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step proc~mixing_finalize->n_items proc~mixing_finalize->proc~current_itt reset reset proc~mixing_finalize->reset proc~mixing_step->n_items proc~mixing_step->reset get_pointer get_pointer proc~mixing_step->get_pointer push push proc~mixing_step->push Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_1d~~CalledByGraph proc~mixing_1d mixing_1d interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_1d Source Code subroutine mixing_1d ( mix , n , xin , F , xnext , nsub ) ! The current mixing method type ( tMixer ), pointer :: mix ! The current step in the SCF and size of arrays integer , intent ( in ) :: n ! x1 == Input function, ! F1 == Residual from x1 real ( dp ), intent ( in ) :: xin ( n ), F ( n ) ! x2 == Next input function real ( dp ), intent ( inout ) :: xnext ( n ) ! Number of elements used for calculating the mixing ! coefficients integer , intent ( in ), optional :: nsub ! Coefficients integer :: ncoeff real ( dp ), allocatable :: coeff (:) call mixing_init ( mix , n , xin , F ) ncoeff = mixing_ncoeff ( mix ) allocate ( coeff ( ncoeff )) ! Calculate coefficients if ( present ( nsub ) ) then call mixing_coeff ( mix , nsub , xin , F , coeff ) else call mixing_coeff ( mix , n , xin , F , coeff ) end if ! Calculate the following output call mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! Coefficients are not needed anymore... deallocate ( coeff ) ! Finalize the mixer call mixing_finalize ( mix , n , xin , F , xnext ) end subroutine mixing_1d","tags":"","loc":"proc/mixing_1d.html"},{"title":"mixing_2d – SIESTA","text":"private subroutine mixing_2d(mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub Called by proc~~mixing_2d~~CalledByGraph proc~mixing_2d mixing_2d interface~mixing mixing interface~mixing->proc~mixing_2d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_2d Source Code subroutine mixing_2d ( mix , n1 , n2 , xin , F , xnext , nsub ) type ( tMixer ), pointer :: mix integer , intent ( in ) :: n1 , n2 real ( dp ), intent ( in ) :: xin ( n1 , n2 ), F ( n1 , n2 ) real ( dp ), intent ( inout ) :: xnext ( n1 , n2 ) integer , intent ( in ), optional :: nsub ! Simple wrapper for 1D if ( present ( nsub ) ) then call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 ) ,& nsub = n1 * nsub ) else call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 )) end if end subroutine mixing_2d","tags":"","loc":"proc/mixing_2d.html"},{"title":"mixing_step – SIESTA","text":"private subroutine mixing_step(mix) Uses parallel proc~~mixing_step~~UsesGraph proc~mixing_step mixing_step parallel parallel proc~mixing_step->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix Calls proc~~mixing_step~~CallsGraph proc~mixing_step mixing_step get_pointer get_pointer proc~mixing_step->get_pointer push push proc~mixing_step->push reset reset proc~mixing_step->reset n_items n_items proc~mixing_step->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_step~~CalledByGraph proc~mixing_step mixing_step proc~mixing_finalize mixing_finalize proc~mixing_finalize->proc~mixing_step proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_step Source Code subroutine mixing_step ( mix ) use parallel , only : IONode type ( tMixer ), pointer :: mix type ( tMixer ), pointer :: next => null () type ( dData1D ), pointer :: d1D integer :: i , is , n , init_itt logical :: reset_stack , copy_stack ! First try and next => mix % next if ( associated ( next ) ) then ! Whether or not the two methods are allowed ! to share history copy_stack = mix % m == next % m select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) copy_stack = . true . end select end select copy_stack = copy_stack . and . allocated ( mix % stack ) ! If the two methods are similar if ( copy_stack ) then ! They are similar, copy over the history stack do is = 1 , size ( mix % stack ) ! Get maximum size of the current stack, n = n_items ( mix % stack ( is )) ! Note that this will automatically take care of ! wrap-arounds and delete the unneccesry elements do i = 1 , n d1D => get_pointer ( mix % stack ( is ), i ) call push ( next % stack ( is ), d1D ) end do ! nullify nullify ( d1D ) end do end if end if reset_stack = . true . if ( associated ( next ) ) then if ( associated ( next % next , mix ) . and . & next % n_itt > 0 ) then ! if this is a circular mixing routine ! we should not reset the history... reset_stack = . false . end if end if if ( reset_stack ) then select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) n = size ( mix % stack ) do is = 1 , n call reset ( mix % stack ( is )) end do end select end if if ( associated ( next ) ) then init_itt = 0 ! Set-up the next mixer select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) init_itt = n_items ( next % stack ( 1 )) end select next % start_itt = init_itt next % cur_itt = init_itt if ( IONode ) then write ( * , '(3a)' ) trim ( debug_msg ), ' switching mixer --> ' , & trim ( next % name ) end if mix => mix % next end if end subroutine mixing_step","tags":"","loc":"proc/mixing_step.html"},{"title":"inverse – SIESTA","text":"private subroutine inverse(n, A, B, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) integer, intent(out) :: info Contents Source Code inverse Source Code subroutine inverse ( n , A , B , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) integer , intent ( out ) :: info integer :: i , j ! Local arrays real ( dp ) :: pm ( n , n ), work ( n * 4 ), err ! Relative tolerance dependent on the magnitude ! For now we retain the old tolerance real ( dp ), parameter :: etol = 1.e-4_dp integer :: ipiv ( n ) ! initialize info info = 0 ! simple check and fast return if ( n == 1 ) then B ( 1 , 1 ) = 1._dp / A ( 1 , 1 ) return end if call lapack_inv () if ( info /= 0 ) call simple_inv () contains subroutine lapack_inv () B = A call dgetrf ( n , n , B , n , ipiv , info ) if ( info /= 0 ) return call dgetri ( n , B , n , ipiv , work , n * 4 , info ) if ( info /= 0 ) return ! This sets info appropriately call check_inv () end subroutine lapack_inv subroutine simple_inv () real ( dp ) :: x integer :: k ! Copy over A B = A do i = 1 , n if ( B ( i , i ) == 0._dp ) then info = - n return end if x = 1._dp / B ( i , i ) B ( i , i ) = 1._dp do j = 1 , n B ( j , i ) = B ( j , i ) * x end do do k = 1 , n if ( ( k - i ) /= 0 ) then x = B ( i , k ) B ( i , k ) = 0._dp do j = 1 , n B ( j , k ) = B ( j , k ) - B ( j , i ) * x end do end if end do end do ! This sets info appropriately call check_inv () end subroutine simple_inv subroutine check_inv () ! Check correcteness pm = matmul ( A , B ) do j = 1 , n do i = 1 , n if ( i == j ) then err = pm ( i , j ) - 1._dp else err = pm ( i , j ) end if ! This is pretty strict tolerance! if ( abs ( err ) > etol ) then ! Signal failure in inversion info = - n - 1 return end if end do end do end subroutine check_inv end subroutine inverse","tags":"","loc":"proc/inverse.html"},{"title":"svd – SIESTA","text":"private subroutine svd(n, A, B, cond, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) real(kind=dp), intent(in) :: cond integer, intent(out) :: info Calls proc~~svd~~CallsGraph proc~svd svd dgelss dgelss proc~svd->dgelss Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code svd Source Code subroutine svd ( n , A , B , cond , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) real ( dp ), intent ( in ) :: cond integer , intent ( out ) :: info ! Local arrays integer :: rank , i character ( len = 50 ) :: fmt real ( dp ) :: AA ( n , n ), S ( n ), work ( n * 5 ) ! Copy A matrix AA = A ! setup pseudo inverse solution for minimizing ! constraints B = 0._dp do i = 1 , n B ( i , i ) = 1._dp end do call dgelss ( n , n , n , AA , n , B , n , S , cond , rank , work , n * 5 , info ) ! if debugging print out the different variables if ( debug_mix ) then ! also mark the rank if ( rank == n ) then ! complete rank write ( * , '(2a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' SVD singular = ' , S else ! this prints the location of the SVD rank, if not full write ( fmt , '(i0,2a)' ) rank , '(tr1,e10.4),'' >'',100(tr1,e10.4)' write ( * , '(2a,' // trim ( fmt ) // ')' ) & trim ( debug_msg ), ' SVD singular = ' , S end if end if end subroutine svd","tags":"","loc":"proc/svd.html"},{"title":"push_stack_data – SIESTA","text":"private subroutine push_stack_data(s_F, n) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n Calls proc~~push_stack_data~~CallsGraph proc~push_stack_data push_stack_data push push proc~push_stack_data->push proc~stack_check stack_check proc~push_stack_data->proc~stack_check delete delete proc~push_stack_data->delete newddata1d newddata1d proc~push_stack_data->newddata1d die die proc~push_stack_data->die get_pointer get_pointer proc~stack_check->get_pointer n_items n_items proc~stack_check->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_stack_data Source Code subroutine push_stack_data ( s_F , n ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n type ( dData1D ) :: dD1 if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if call newdData1D ( dD1 , n , '(F)' ) ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_stack_data","tags":"","loc":"proc/push_stack_data.html"},{"title":"push_F – SIESTA","text":"private subroutine push_F(s_F, n, F, fact) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in), optional :: fact Calls proc~~push_f~~CallsGraph proc~push_f push_F push push proc~push_f->push n_items n_items proc~push_f->n_items delete delete proc~push_f->delete proc~stack_check stack_check proc~push_f->proc~stack_check max_size max_size proc~push_f->max_size get get proc~push_f->get dcopy dcopy proc~push_f->dcopy newddata1d newddata1d proc~push_f->newddata1d die die proc~push_f->die val val proc~push_f->val proc~stack_check->n_items get_pointer get_pointer proc~stack_check->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~push_f~~CalledByGraph proc~push_f push_F proc~update_f update_F proc~update_f->proc~push_f Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_F Source Code subroutine push_F ( s_F , n , F , fact ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( in ), optional :: fact type ( dData1D ) :: dD1 real ( dp ), pointer :: sF (:) integer :: in , ns integer :: i if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) ns = max_size ( s_F ) if ( in == ns ) then ! we have to cycle the storage call get ( s_F , 1 , dD1 ) else call newdData1D ( dD1 , n , '(F)' ) end if sF => val ( dD1 ) if ( present ( fact ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n sF ( i ) = F ( i ) * fact end do !$OMP end parallel do else call dcopy ( n , F , 1 , sF , 1 ) end if ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_F","tags":"","loc":"proc/push_f.html"},{"title":"update_F – SIESTA","text":"private subroutine update_F(s_F, n, F) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) Calls proc~~update_f~~CallsGraph proc~update_f update_F get_pointer get_pointer proc~update_f->get_pointer n_items n_items proc~update_f->n_items proc~stack_check stack_check proc~update_f->proc~stack_check dcopy dcopy proc~update_f->dcopy die die proc~update_f->die val val proc~update_f->val proc~push_f push_F proc~update_f->proc~push_f proc~stack_check->get_pointer proc~stack_check->n_items proc~push_f->n_items proc~push_f->proc~stack_check proc~push_f->dcopy proc~push_f->die proc~push_f->val push push proc~push_f->push delete delete proc~push_f->delete max_size max_size proc~push_f->max_size get get proc~push_f->get newddata1d newddata1d proc~push_f->newddata1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code update_F Source Code subroutine update_F ( s_F , n , F ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) type ( dData1D ), pointer :: dD1 real ( dp ), pointer :: FF (:) integer :: in if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) if ( in == 0 ) then ! We need to add it as it does not exist call push_F ( s_F , n , F ) else ! we have an entry, update the latest dD1 => get_pointer ( s_F , in ) FF => val ( dD1 ) call dcopy ( n , F , 1 , FF , 1 ) end if end subroutine update_F","tags":"","loc":"proc/update_f.html"},{"title":"push_diff – SIESTA","text":"private subroutine push_diff(s_rres, s_res, alpha) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_rres type(Fstack_dData1D), intent(in) :: s_res real(kind=dp), intent(in), optional :: alpha Calls proc~~push_diff~~CallsGraph proc~push_diff push_diff get_pointer get_pointer proc~push_diff->get_pointer push push proc~push_diff->push n_items n_items proc~push_diff->n_items delete delete proc~push_diff->delete max_size max_size proc~push_diff->max_size get get proc~push_diff->get die die proc~push_diff->die val val proc~push_diff->val Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_diff Source Code subroutine push_diff ( s_rres , s_res , alpha ) type ( Fstack_dData1D ), intent ( inout ) :: s_rres type ( Fstack_dData1D ), intent ( in ) :: s_res real ( dp ), intent ( in ), optional :: alpha type ( dData1D ) :: dD1 type ( dData1D ), pointer :: pD1 real ( dp ), pointer :: res1 (:), res2 (:), rres (:) integer :: in , ns , i , n if ( n_items ( s_res ) < 2 ) then call die ( 'mixing: Residual residuals cannot be calculated, & &inferior residual size.' ) end if in = n_items ( s_res ) ! First get the value of in pD1 => get_pointer ( s_res , in - 1 ) res1 => val ( pD1 ) ! get the value of in pD1 => get_pointer ( s_res , in ) res2 => val ( pD1 ) in = n_items ( s_rres ) ns = max_size ( s_rres ) if ( in == ns ) then ! we have to cycle the storage call get ( s_rres , 1 , dD1 ) else call newdData1D ( dD1 , size ( res1 ), '(res)' ) end if ! Get the residual of the residual rres => val ( dD1 ) n = size ( rres ) if ( present ( alpha ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = ( res2 ( i ) - res1 ( i )) * alpha end do !$OMP end parallel do else !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = res2 ( i ) - res1 ( i ) end do !$OMP end parallel do end if ! Push the data to the stack call push ( s_rres , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_diff","tags":"","loc":"proc/push_diff.html"},{"title":"mixing – SIESTA","text":"public interface mixing Calls interface~~mixing~~CallsGraph interface~mixing mixing proc~mixing_2d mixing_2d interface~mixing->proc~mixing_2d proc~mixing_1d mixing_1d interface~mixing->proc~mixing_1d proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d->proc~mixing_ncoeff proc~mixing_calc_next mixing_calc_next proc~mixing_1d->proc~mixing_calc_next proc~mixing_init mixing_init proc~mixing_1d->proc~mixing_init proc~mixing_finalize mixing_finalize proc~mixing_1d->proc~mixing_finalize proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items proc~norm norm proc~mixing_init->proc~norm proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step proc~mixing_finalize->n_items proc~mixing_finalize->proc~current_itt reset reset proc~mixing_finalize->reset proc~mixing_step->n_items proc~mixing_step->reset get_pointer get_pointer proc~mixing_step->get_pointer push push proc~mixing_step->push Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures mixing_1d mixing_2d Module Procedures private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub","tags":"","loc":"interface/mixing.html"},{"title":"compute_max_diff_2d – SIESTA","text":"public subroutine compute_max_diff_2d(X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff Calls proc~~compute_max_diff_2d~~CallsGraph proc~compute_max_diff_2d compute_max_diff_2d die die proc~compute_max_diff_2d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_max_diff_2d~~CalledByGraph proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff compute_max_diff interface~compute_max_diff->proc~compute_max_diff_2d proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_max_diff_2d Source Code subroutine compute_max_diff_2d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:,:), X2 (:,:) real ( dp ), intent ( out ) :: max_diff integer :: n1 , n2 integer :: i1 , i2 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) n2 = size ( X1 , 2 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if if ( size ( X2 , 2 ) /= n2 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (2-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i2,i1), & !$OMP& reduction(max:max_diff), collapse(2) do i2 = 1 , n2 do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 , i2 ) - X2 ( i1 , i2 )) ) end do end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_2d","tags":"","loc":"proc/compute_max_diff_2d.html"},{"title":"compute_max_diff_1d – SIESTA","text":"public subroutine compute_max_diff_1d(X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff Calls proc~~compute_max_diff_1d~~CallsGraph proc~compute_max_diff_1d compute_max_diff_1d die die proc~compute_max_diff_1d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_max_diff_1d~~CalledByGraph proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff compute_max_diff interface~compute_max_diff->proc~compute_max_diff_1d proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_max_diff_1d Source Code subroutine compute_max_diff_1d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:), X2 (:) real ( dp ), intent ( out ) :: max_diff integer :: n1 integer :: i1 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i1), reduction(max:max_diff) do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 ) - X2 ( i1 )) ) end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_1d","tags":"","loc":"proc/compute_max_diff_1d.html"},{"title":"compute_max_diff – SIESTA","text":"public interface compute_max_diff Calls interface~~compute_max_diff~~CallsGraph interface~compute_max_diff compute_max_diff proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff->proc~compute_max_diff_2d proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff->proc~compute_max_diff_1d die die proc~compute_max_diff_2d->die proc~compute_max_diff_1d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by interface~~compute_max_diff~~CalledByGraph interface~compute_max_diff compute_max_diff proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures compute_max_diff_1d compute_max_diff_2d Module Procedures public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff","tags":"","loc":"interface/compute_max_diff.html"},{"title":"setup_H0 – SIESTA","text":"public subroutine setup_H0(g2max) Uses siesta_options sparse_matrices sparse_matrices sparse_matrices sparse_matrices siesta_geom atmfuncs atomlist metaforce molecularmechanics m_nlefsm m_kinefsm m_naefs m_dnaefs m_dhscf m_energies m_ntm m_spin spinorbit alloc class_dSpData1D class_dSpData2D m_mpi_utils proc~~setup_h0~~UsesGraph proc~setup_h0 setup_H0 m_naefs m_naefs proc~setup_h0->m_naefs atomlist atomlist proc~setup_h0->atomlist m_dhscf m_dhscf proc~setup_h0->m_dhscf sparse_matrices sparse_matrices proc~setup_h0->sparse_matrices siesta_geom siesta_geom proc~setup_h0->siesta_geom m_nlefsm m_nlefsm proc~setup_h0->m_nlefsm m_kinefsm m_kinefsm proc~setup_h0->m_kinefsm m_dnaefs m_dnaefs proc~setup_h0->m_dnaefs spinorbit spinorbit proc~setup_h0->spinorbit class_dSpData1D class_dSpData1D proc~setup_h0->class_dSpData1D m_mpi_utils m_mpi_utils proc~setup_h0->m_mpi_utils m_spin m_spin proc~setup_h0->m_spin siesta_options siesta_options proc~setup_h0->siesta_options molecularmechanics molecularmechanics proc~setup_h0->molecularmechanics class_dSpData2D class_dSpData2D proc~setup_h0->class_dSpData2D alloc alloc proc~setup_h0->alloc metaforce metaforce proc~setup_h0->metaforce m_energies m_energies proc~setup_h0->m_energies atmfuncs atmfuncs proc~setup_h0->atmfuncs m_ntm m_ntm proc~setup_h0->m_ntm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Computes non-self-consistent part of the Hamiltonian\n and initializes data structures on the grid. Self-energy of isolated ions\n@code\n@endcode Neutral-atom: energy Note In these routines, flag is defined\n  to tell them NOT to compute\n  forces and stresses in this first pass, only energies. @code\n@endcode Metadynamics energy:\n Add on force field contribution to energy:\n Now we compute matrix elements of the Kinetic and Non-local\n  parts of H Kinetic: matrix elements only\n@code\n@endcode Non-local-pseudop:  matrix elements only\n@code\n@endcode Note If in the future the spin-orbit routine is able to compute\n  forces and stresses, then \"last\" will be needed. If we are not\n  computing forces and stresses, calling it in the first iteratio      !n\n  should be enough. @code\n@endcode This will take care of possible changes to the mesh and atomic-r      !elated\nmesh structures for geometry changes:\n@code\n@endcode Arguments Type Intent Optional Attributes Name real(kind=dp), intent(inout) :: g2max Calls proc~~setup_h0~~CallsGraph proc~setup_h0 setup_H0 write_debug write_debug proc~setup_h0->write_debug uion uion proc~setup_h0->uion kinefsm kinefsm proc~setup_h0->kinefsm dhscf_init dhscf_init proc~setup_h0->dhscf_init meta meta proc~setup_h0->meta isa isa proc~setup_h0->isa twobody twobody proc~setup_h0->twobody naefs naefs proc~setup_h0->naefs timer timer proc~setup_h0->timer dnaefs dnaefs proc~setup_h0->dnaefs spinorb spinorb proc~setup_h0->spinorb nlefsm nlefsm proc~setup_h0->nlefsm val val proc~setup_h0->val Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setup_H0 Source Code subroutine setup_H0 ( G2max ) !! Computes non-self-consistent part of the Hamiltonian !! and initializes data structures on the grid. USE siesta_options , only : g2cut use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_2D use sparse_matrices , only : Dscf use sparse_matrices , only : listh , listhptr , numh , maxnh use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use m_nlefsm , only : nlefsm use m_kinefsm , only : kinefsm use m_naefs , only : naefs use m_dnaefs , only : dnaefs use m_dhscf , only : dhscf_init use m_energies , only : Eions , Ena , DEna , Emm , Emeta , Eso use m_ntm use m_spin , only : spin use spinorbit , only : spinorb use alloc , only : re_alloc , de_alloc use class_dSpData1D , only : val use class_dSpData2D , only : val #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none real ( dp ), intent ( inout ) :: g2max real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ), dummy_dm ( 1 , 1 ) real ( dp ) :: dummy_E integer :: ia , is real ( dp ), pointer :: H_val (:), H_so (:,:) #ifdef DEBUG call write_debug ( '    PRE setup_H0' ) #endif !----------------------------------------------------------------------BEGIN call timer ( 'Setup_H0' , 1 ) !!Self-energy of isolated ions !!@code Eions = 0.0_dp do ia = 1 , na_u is = isa ( ia ) Eions = Eions + uion ( is ) enddo !!@endcode !! Neutral-atom: energy !!@note !* In these routines, flag is defined !  to tell them NOT to compute !  forces and stresses in this first pass, only energies. !!@endnote !!@code call naefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , Ena , dummy_fa , dummy_stress , & forces_and_stress = . false .) call dnaefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , DEna , dummy_fa , dummy_stress , & forces_and_stress = . false .) Ena = Ena + DEna !!@endcode !! Metadynamics energy: if ( lMetaForce ) then call meta ( xa , na_u , ucell , Emeta , dummy_fa , dummy_stress , $ . false .,. false .) endif !! Add on force field contribution to energy: call twobody ( na_u , xa , isa , ucell , Emm , & ifa = 0 , fa = dummy_fa , istr = 0 , stress = dummy_stress ) !* Now we compute matrix elements of the Kinetic and Non-local !  parts of H !! Kinetic: matrix elements only !!@code H_val => val ( H_kin_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call kinefsm ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , & maxnh , maxnh , lasto , iphorb , isa , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !!@endcode !! Non-local-pseudop:  matrix elements only !!@code H_val => val ( H_vkb_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call nlefsm ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !!@endcode !!@note !* If in the future the spin-orbit routine is able to compute !  forces and stresses, then \"last\" will be needed. If we are not !  computing forces and stresses, calling it in the first iteration !  should be enough. !!@endnote !!@code if ( spin % SO ) then H_so => val ( H_so_2D ) !$OMP parallel workshare default(shared) H_so = 0._dp !$OMP end parallel workshare call spinorb ( no_u , no_l , iaorb , iphorb , isa , indxuo , & maxnh , numh , listhptr , listh , Dscf , H_so , Eso ) else Eso = 0._dp end if !!@endcode !!This will take care of possible changes to the mesh and atomic-related !!mesh structures for geometry changes: !!@code g2max = g2cut call dhscf_init ( spin % Grid , no_s , iaorb , iphorb , & no_l , no_u , na_u , na_s , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnh , numh , listhptr , listh , datm , & dummy_fa , dummy_stress ) !!@endcode call timer ( 'Setup_H0' , 2 ) #ifdef DEBUG call write_debug ( '    POS setup_H0' ) #endif !---------------------------------------------------------------------- END END subroutine setup_H0","tags":"","loc":"proc/setup_h0.html"},{"title":"state_init – SIESTA","text":"public subroutine state_init(istep) Uses Kpoint_grid m_os m_new_dm m_proximity_check siesta_options units sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices create_Sparsity_SC m_sparsity_handling m_sparsity_handling m_pivot_methods siesta_geom atomlist alloc m_hsparse m_overlap m_supercell siesta_cml zmatrix m_energies write_subs m_ioxv m_iotdxv m_steps parallel m_spin m_rmaxh m_mixing m_mixing_scf m_normalize_dm m_eo m_gamma files m_mpi_utils m_mpi_utils domain_decom ldau_specs m_sparse m_ts_kpoints m_ts_charge m_ts_options m_ts_options m_ts_options m_ts_electype m_ts_global_vars sys m_ts_io m_ts_sparse m_ts_tri_init files m_chess iodm_netcdf iodmhs_netcdf class_Sparsity class_dSpData1D class_dSpData2D class_dData2D m_test_io siesta_dicts proc~~state_init~~UsesGraph proc~state_init state_init m_ioxv m_ioxv proc~state_init->m_ioxv atomlist atomlist proc~state_init->atomlist parallel parallel proc~state_init->parallel module~m_mixing m_mixing proc~state_init->module~m_mixing m_normalize_dm m_normalize_dm proc~state_init->m_normalize_dm ldau_specs ldau_specs proc~state_init->ldau_specs module~m_mixing_scf m_mixing_scf proc~state_init->module~m_mixing_scf sys sys proc~state_init->sys class_dSpData1D class_dSpData1D proc~state_init->class_dSpData1D m_sparse m_sparse proc~state_init->m_sparse m_ts_tri_init m_ts_tri_init proc~state_init->m_ts_tri_init class_dData2D class_dData2D proc~state_init->class_dData2D Kpoint_grid Kpoint_grid proc~state_init->Kpoint_grid iodm_netcdf iodm_netcdf proc~state_init->iodm_netcdf m_os m_os proc~state_init->m_os m_eo m_eo proc~state_init->m_eo class_Sparsity class_Sparsity proc~state_init->class_Sparsity units units proc~state_init->units sparse_matrices sparse_matrices proc~state_init->sparse_matrices create_Sparsity_SC create_Sparsity_SC proc~state_init->create_Sparsity_SC files files proc~state_init->files m_iotdxv m_iotdxv proc~state_init->m_iotdxv siesta_options siesta_options proc~state_init->siesta_options m_sparsity_handling m_sparsity_handling proc~state_init->m_sparsity_handling m_ts_options m_ts_options proc~state_init->m_ts_options alloc alloc proc~state_init->alloc m_ts_electype m_ts_electype proc~state_init->m_ts_electype m_chess m_chess proc~state_init->m_chess siesta_cml siesta_cml proc~state_init->siesta_cml m_supercell m_supercell proc~state_init->m_supercell m_hsparse m_hsparse proc~state_init->m_hsparse m_rmaxh m_rmaxh proc~state_init->m_rmaxh m_ts_charge m_ts_charge proc~state_init->m_ts_charge m_new_dm m_new_dm proc~state_init->m_new_dm siesta_dicts siesta_dicts proc~state_init->siesta_dicts m_energies m_energies proc~state_init->m_energies m_test_io m_test_io proc~state_init->m_test_io m_proximity_check m_proximity_check proc~state_init->m_proximity_check m_overlap m_overlap proc~state_init->m_overlap m_gamma m_gamma proc~state_init->m_gamma m_ts_io m_ts_io proc~state_init->m_ts_io iodmhs_netcdf iodmhs_netcdf proc~state_init->iodmhs_netcdf m_steps m_steps proc~state_init->m_steps siesta_geom siesta_geom proc~state_init->siesta_geom m_mpi_utils m_mpi_utils proc~state_init->m_mpi_utils write_subs write_subs proc~state_init->write_subs m_ts_kpoints m_ts_kpoints proc~state_init->m_ts_kpoints domain_decom domain_decom proc~state_init->domain_decom m_spin m_spin proc~state_init->m_spin class_dSpData2D class_dSpData2D proc~state_init->class_dSpData2D m_pivot_methods m_pivot_methods proc~state_init->m_pivot_methods m_ts_sparse m_ts_sparse proc~state_init->m_ts_sparse zmatrix zmatrix proc~state_init->zmatrix m_ts_global_vars m_ts_global_vars proc~state_init->m_ts_global_vars precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D module~m_mixing_scf->module~m_mixing module~m_mixing_scf->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: istep Calls proc~~state_init~~CallsGraph proc~state_init state_init volcel volcel proc~state_init->volcel escf escf proc~state_init->escf setup_ordern_indexes setup_ordern_indexes proc~state_init->setup_ordern_indexes delete delete proc~state_init->delete numh numh proc~state_init->numh setup_kpoint_grid setup_kpoint_grid proc~state_init->setup_kpoint_grid xij_offset xij_offset proc~state_init->xij_offset bye bye proc~state_init->bye nscold nscold proc~state_init->nscold sp_to_spglobal sp_to_spglobal proc~state_init->sp_to_spglobal ts_write_tshs ts_write_tshs proc~state_init->ts_write_tshs overlap overlap proc~state_init->overlap proc~mixers_history_init mixers_history_init proc~state_init->proc~mixers_history_init proximity_check proximity_check proc~state_init->proximity_check file_exist file_exist proc~state_init->file_exist domaindecom domaindecom proc~state_init->domaindecom write_debug write_debug proc~state_init->write_debug ucell ucell proc~state_init->ucell dict_repopulate_md dict_repopulate_md proc~state_init->dict_repopulate_md new_dm new_dm proc~state_init->new_dm sporb_to_spatom sporb_to_spatom proc~state_init->sporb_to_spatom newdspdata2d newdspdata2d proc~state_init->newdspdata2d fname_tshs fname_tshs proc~state_init->fname_tshs ts_sparse_init ts_sparse_init proc~state_init->ts_sparse_init ioxv ioxv proc~state_init->ioxv timer timer proc~state_init->timer attach attach proc~state_init->attach get_chess_parameter get_chess_parameter proc~state_init->get_chess_parameter globalize_or globalize_or proc~state_init->globalize_or init_val init_val proc~state_init->init_val setup_ts_kpoint_grid setup_ts_kpoint_grid proc~state_init->setup_ts_kpoint_grid superc superc proc~state_init->superc chess_init chess_init proc~state_init->chess_init time_io time_io proc~state_init->time_io nsc nsc proc~state_init->nsc normalize_dm normalize_dm proc~state_init->normalize_dm madelung madelung proc~state_init->madelung newdspdata1d newdspdata1d proc~state_init->newdspdata1d re_alloc re_alloc proc~state_init->re_alloc newddata2d newddata2d proc~state_init->newddata2d iotdxv iotdxv proc~state_init->iotdxv setup_dm_netcdf_file setup_dm_netcdf_file proc~state_init->setup_dm_netcdf_file outcoor outcoor proc~state_init->outcoor siesta_write_positions siesta_write_positions proc~state_init->siesta_write_positions listhptr listhptr proc~state_init->listhptr newsparsity newsparsity proc~state_init->newsparsity crtsparsity_sc crtsparsity_sc proc~state_init->crtsparsity_sc mscell mscell proc~state_init->mscell setup_dmhs_netcdf_file setup_dmhs_netcdf_file proc~state_init->setup_dmhs_netcdf_file write_zmatrix write_zmatrix proc~state_init->write_zmatrix ts_tri_analyze ts_tri_analyze proc~state_init->ts_tri_analyze isc_off isc_off proc~state_init->isc_off val val proc~state_init->val hsparse hsparse proc~state_init->hsparse proc~mixers_history_init->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~state_init~~CalledByGraph proc~state_init state_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code state_init Source Code subroutine state_init ( istep ) use Kpoint_grid , only : setup_Kpoint_grid , nkpnt use m_os , only : file_exist use m_new_dm , only : new_dm use m_proximity_check , only : proximity_check use siesta_options use units , only : Ang use sparse_matrices , only : maxnh , numh , listh , listhptr use sparse_matrices , only : Dold , Dscf , DM_2D use sparse_matrices , only : Eold , Escf , EDM_2D use sparse_matrices , only : Hold , H , H_2D use sparse_matrices , only : xijo , xij_2D use sparse_matrices , only : S , S_1D use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D , H_so_2D use sparse_matrices , only : sparse_pattern use sparse_matrices , only : block_dist , single_dist use sparse_matrices , only : DM_history use create_Sparsity_SC , only : crtSparsity_SC use m_sparsity_handling , only : SpOrb_to_SpAtom use m_sparsity_handling , only : Sp_to_Spglobal use m_pivot_methods , only : sp2graphviz use siesta_geom use atomlist , only : iphorb , iphkb , indxua , & rmaxo , rmaxkb , rmaxv , rmaxldau , & lastkb , lasto , superc , indxuo , & no_u , no_s , no_l , iza , qtots use alloc , only : re_alloc , de_alloc , alloc_report use m_hsparse , only : hsparse use m_overlap , only : overlap use m_supercell , only : exact_sc_ag use siesta_cml , only : cml_p , cmlStartStep , mainXML use zmatrix , only : lUseZmatrix , write_zmatrix use m_energies , only : Emad use write_subs use m_ioxv , only : ioxv use m_iotdxv , only : iotdxv use m_steps use parallel , only : IOnode , node , nodes , BlockSize use m_spin , only : spin use m_rmaxh use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_normalize_dm , only : normalize_dm use m_eo use m_gamma use files , only : slabel use m_mpi_utils , only : globalize_or use m_mpi_utils , only : globalize_max use domain_decom , only : domainDecom , use_dd , use_dd_perm use ldau_specs , only : switch_ldau , ldau_init use m_sparse , only : xij_offset use m_ts_kpoints , only : setup_ts_kpoint_grid use m_ts_charge , only : TS_RHOCORR_METHOD , TS_RHOCORR_FERMI use m_ts_options , only : BTD_method use m_ts_options , only : TS_Analyze use m_ts_options , only : N_Elec , Elecs , IsVolt use m_ts_electype use m_ts_global_vars , only : TSrun , TSmode , onlyS use sys , only : bye use m_ts_io , only : fname_TSHS , ts_write_tshs use m_ts_sparse , only : ts_sparse_init use m_ts_tri_init , only : ts_tri_init , ts_tri_analyze use files , only : slabel , label_length #ifdef SIESTA__CHESS use m_chess , only : CheSS_init , get_CheSS_parameter #endif #ifdef CDF use iodm_netcdf , only : setup_dm_netcdf_file use iodmhs_netcdf , only : setup_dmhs_netcdf_file #endif use class_Sparsity use class_dSpData1D use class_dSpData2D use class_dData2D #ifdef TEST_IO use m_test_io #endif #ifdef SIESTA__FLOOK use siesta_dicts , only : dict_repopulate_MD #endif implicit none integer :: istep real ( dp ) :: veclen ! Length of a unit-cell vector real ( dp ) :: rmax logical :: cell_can_change integer :: i , ix , iadispl , ixdispl logical :: auxchanged ! Auxiliary supercell changed? logical :: folding , folding1 logical :: foundxv ! dummy for call to ioxv external :: madelung , timer real ( dp ), external :: volcel integer :: ts_kscell_file ( 3 , 3 ) = 0 real ( dp ) :: ts_kdispl_file ( 3 ) = 0.0 logical :: ts_Gamma_file = . true . character ( len = label_length + 6 ) :: fname real ( dp ) :: dummyef = 0.0 , dummyqtot = 0.0 #ifdef SIESTA__CHESS integer :: maxnh_kernel , maxnh_mult , no_l_kernel , no_l_mult integer , dimension (:), allocatable :: listh_kernel , listh_mult integer , dimension (:), allocatable :: numh_kernel , numh_mult real ( dp ) :: chess_value #endif type ( Sparsity ) :: g_Sp character ( len = 256 ) :: oname type ( dData2D ) :: tmp_2D !------------------------------------------------------------------- BEGIN call timer ( 'IterGeom' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_init' ) #endif call timer ( 'state_init' , 1 ) istp = istp + 1 if ( IOnode ) then write ( 6 , '(/,t22,a)' ) repeat ( '=' , 36 ) select case ( idyn ) case ( 0 ) if ( nmove == 0 ) then write ( 6 , '(t25,a)' ) 'Single-point calculation' if ( cml_p ) call cmlStartStep ( mainXML , type = 'Single-Point' , $ index = istp ) else if ( broyden_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin Broyden opt. move = ' , $ istep else if ( fire_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin FIRE opt. move = ' , $ istep else write ( 6 , '(t25,a,i6)' ) 'Begin CG opt. move = ' , $ istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'Geom. Optim' , $ index = istp ) endif !        Print Z-matrix coordinates if ( lUseZmatrix ) then call write_Zmatrix () endif case ( 1 , 3 ) if ( iquench > 0 ) then write ( 6 , '(t25,a,i6)' ) 'Begin MD quenched step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD-quenched' , $ index = istep ) else write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , $ index = istep ) endif case ( 2 , 4 , 5 ) write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , index = istep ) case ( 6 ) write ( 6 , '(t25,a,i6)' ) 'Begin FC step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FC' , index = istep ) if ( istep . eq . 0 ) then write ( 6 , '(t25,a)' ) 'Undisplaced coordinates' else iadispl = ( istep - mod ( istep - 1 , 6 )) / 6 + ia1 ix = mod ( istep - 1 , 6 ) + 1 ixdispl = ( ix - mod ( ix - 1 , 2 ) + 1 ) / 2 write ( 6 , '(t26,a,i0,/,t26,a,i1,a,f10.6,a)' ) 'displace atom ' , & iadispl , 'in direction ' , ixdispl , ' by' , dx / Ang , ' Ang' endif case ( 8 ) write ( 6 , '(t25,a,i6)' ) 'Begin Server step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FS' , index = istep ) case ( 9 ) if ( istep == 0 ) then write ( 6 , '(t25,a,i7)' ) 'Explicit coord. initialization' else write ( 6 , '(t25,a,i7)' ) 'Explicit coord. step =' , istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'ECS' , index = istep ) case ( 10 ) write ( 6 , '(t25,a,i7)' ) 'LUA coord. step =' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'LUA' , index = istep ) end select write ( 6 , '(t22,a)' ) repeat ( '=' , 36 ) !     Print atomic coordinates call outcoor ( ucell , xa , na_u , ' ' , writec ) !     Save structural information in crystallographic format !     (in file SystemLabel.STRUCT_OUT), !     canonical Zmatrix (if applicable), and CML record call siesta_write_positions ( moved = . false .) endif ! IONode ! Write the XV file for single-point calculations, so that ! it is there at the end for those users who rely on it call ioxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , & foundxv ) ! Write TDXV file for TDDFT restart. if ( writetdwf . or . td_elec_dyn ) then call iotdxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , foundxv ) end if !     Actualize things if variable cell auxchanged = . false . cell_can_change = ( varcel . or . & ( idyn . eq . 8 ) ! Force/stress evaluation & ) if ( change_kgrid_in_md ) then cell_can_change = cell_can_change . or . & ( idyn . eq . 3 ) . or . ! Parrinello-Rahman & ( idyn . eq . 4 ) . or . ! Nose-Parrinello-Rahman & ( idyn . eq . 5 ) ! Anneal endif if ( cell_can_change . and . & ( istep . ne . inicoor ) . and . (. not . gamma ) ) then !       Will print k-points also call setup_Kpoint_grid ( ucell ) call setup_ts_kpoint_grid ( ucell ) call re_alloc ( eo , 1 , no_u , 1 , spin % spinor , 1 , nkpnt , 'eo' , & 'state_init' ) call re_alloc ( qo , 1 , no_u , 1 , spin % spinor , 1 , nkpnt , 'qo' , & 'state_init' ) !       Find required supercell if ( gamma ) then nsc ( 1 : 3 ) = 1 else if ( fixauxcell ) then nsc ( 1 : 3 ) = nscold ( 1 : 3 ) else do i = 1 , 3 veclen = sqrt ( ucell ( 1 , i ) ** 2 + ucell ( 2 , i ) ** 2 + ucell ( 3 , i ) ** 2 ) nsc ( i ) = 1 + 2 * ceiling ( rmaxh / veclen ) end do if ( . not . naiveauxcell ) & call exact_sc_ag ( negl , ucell , na_u , isa , xa , nsc ) endif mscell = 0.0_dp do i = 1 , 3 mscell ( i , i ) = nsc ( i ) if ( nsc ( i ) /= nscold ( i )) auxchanged = . true . nscold ( i ) = nsc ( i ) enddo !       Madelung correction for charged systems if ( charnet . ne . 0.0_dp ) then call madelung ( ucell , shape , charnet , Emad ) endif endif !     End variable cell actualization !     Auxiliary supercell !     Do not move from here, as the coordinates might have changed !     even if not the unit cell call superc ( ucell , scell , nsc ) #ifdef SIESTA__FLOOK call dict_repopulate_MD () #endif !     Print unit cell and compute cell volume !     Possible BUG: !     Note that this volume is later used in write_subs and the md output !     routines, even if the cell later changes. if ( IOnode ) call outcell ( ucell ) volume_of_some_cell = volcel ( ucell ) !     Use largest possible range in program, except hsparse... !     2 * rmaxv: Vna overlap !     rmaxo + rmaxkb: Non-local KB action !     2 * (rmaxo + rmaxldau): Interaction through LDAU projector !     2.0_dp * (rmaxo+rmaxkb) : Orbital interaction through KB projectors rmax = max ( 2._dp * rmaxv , 2._dp * ( rmaxo + rmaxldau ), rmaxo + rmaxkb ) if ( . not . negl ) then rmax = max ( rmax , 2.0_dp * ( rmaxo + rmaxkb ) ) endif !     Check if any two atoms are unreasonably close call proximity_check ( rmax ) ! Clear history of mixing parameters call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) ! Ensure sparsity pattern is empty call delete ( sparse_pattern ) ! sadly deleting the sparse pattern does not necessarily ! mean that the arrays are de-associated. ! Remember that the reference counter could (in MD) ! be higher than 1, hence we need to create \"fake\" ! containers and let the new<class> delete the old ! sparsity pattern nullify ( numh , listhptr , listh ) allocate ( numh ( no_l ), listhptr ( no_l )) ! We do not need to allocate listh ! that will be allocated in hsparse #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then !         Calculate a sparsity pattern with some buffers... Only required !         for CheSS chess_value = get_chess_parameter ( 'chess_buffer_kernel' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ buffer = chess_value ) maxnh_kernel = maxnh no_l_kernel = no_l allocate ( listh_kernel ( maxnh_kernel )) allocate ( numh_kernel ( no_l_kernel )) listh_kernel = listh numh_kernel = numh chess_value = get_chess_parameter ( 'chess_buffer_mult' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ buffer = chess_value ) maxnh_mult = maxnh no_l_mult = no_l allocate ( listh_mult ( maxnh_mult )) allocate ( numh_mult ( no_l_mult )) listh_mult = listh numh_mult = numh end if #endif /* CHESS */ !     List of nonzero Hamiltonian matrix elements !     and, if applicable,  vectors between orbital centers !     Listh and xijo are allocated inside hsparse !     Note: We always generate xijo now, for COOP and other !           analyses. call delete ( xij_2D ) ! as xijo will be reallocated nullify ( xijo ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , $ set_xijo = . true ., folding = folding1 ) ! #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then call CheSS_init ( node , nodes , maxnh , maxnh_kernel , maxnh_mult , & no_u , no_l , no_l_kernel , no_l_mult , BlockSize , & spin % spinor , qtots , listh , listh_kernel , listh_mult , & numh , numh_kernel , numh_mult ) deallocate ( listh_kernel ) deallocate ( numh_kernel ) deallocate ( listh_mult ) deallocate ( numh_mult ) end if #endif /* CHESS */ call globalize_or ( folding1 , folding ) if ( folding ) then if ( IOnode ) then print * , \"Folding of H and S is implicitly performed\" endif endif ! ! If using domain decomposition, redistribute orbitals ! for this geometry, based on the hsparse info. ! The first time round, the initial distribution is a ! simple block one (given by preSetOrbitLimits). ! ! Any DM, etc, read from file will be redistributed according ! to the new pattern. ! Inherited DMs from a previous geometry cannot be used if the ! orbital distribution changes. For now, we avoid changing the ! distribution (the variable use_dd_perm is .true. if domain ! decomposition is in effect). Names should be changed... if ( use_dd . and . (. not . use_dd_perm )) then call domainDecom ( no_u , no_l , maxnh ) ! maxnh intent(in) here maxnh = sum ( numh ( 1 : no_l )) ! We still need to re-create Julian Gale's ! indexing for O(N) in parallel. print \"(a5,i3,a20,3i8)\" , $ \"Node: \" , Node , \"no_u, no_l, maxnh: \" , no_u , no_l , maxnh call setup_ordern_indexes ( no_l , no_u , Nodes ) endif ! I would like to skip this alloc/move/dealloc/attach ! by allowing sparsity to have pointer targets. ! However, this poses a problem with intel compilers, ! as it apparently errors out when de-allocating a target pointer write ( oname , \"(a,i0)\" ) \"sparsity for geom step \" , istep call newSparsity ( sparse_pattern , no_l , no_u , maxnh , & numh , listhptr , listh , name = oname ) deallocate ( numh , listhptr , listh ) call attach ( sparse_pattern , & n_col = numh , list_ptr = listhptr , list_col = listh ) ! In case the user requests to create the connectivity graph if ( write_GRAPHVIZ > 0 ) then ! first create the unit-cell sparsity pattern call crtSparsity_SC ( sparse_pattern , g_Sp , UC = . true .) ! next move to global sparsity pattern call Sp_to_Spglobal ( block_dist , g_Sp , g_Sp ) if ( IONode ) then if ( write_GRAPHVIZ /= 2 ) & call sp2graphviz ( trim ( slabel ) // '.ORB.gv' , g_Sp ) ! Convert to atomic if ( write_GRAPHVIZ /= 1 ) then call SpOrb_to_SpAtom ( single_dist , g_Sp , na_u , lasto , g_Sp ) call sp2graphviz ( trim ( slabel ) // '.ATOM.gv' , g_Sp ) end if end if call delete ( g_Sp ) end if ! Copy over xijo array (we can first do it here... :( ) call newdData2D ( tmp_2D , xijo , 'xijo' ) deallocate ( xijo ) write ( oname , \"(a,i0)\" ) \"xijo at geom step \" , istep call newdSpData2D ( sparse_pattern , tmp_2D , block_dist , xij_2D , & name = oname ) call delete ( tmp_2D ) ! decrement container... xijo => val ( xij_2D ) ! Calculate the super-cell offsets... if ( Gamma ) then ! Here we create the super-cell offsets call re_alloc ( isc_off , 1 , 3 , 1 , 1 ) isc_off (:,:) = 0 else call xij_offset ( ucell , nsc , na_u , xa , lasto , & xij_2D , isc_off , & Bcast = . true .) end if ! When the user requests to only do an analyzation, we can call ! appropriate routines and quit if ( TS_Analyze ) then ! Force the creation of the full sparsity pattern call ts_sparse_init ( slabel , IsVolt , N_Elec , Elecs , & ucell , nsc , na_u , xa , lasto , block_dist , sparse_pattern , & Gamma , isc_off ) ! create the tri-diagonal matrix call ts_tri_analyze ( block_dist , sparse_pattern , N_Elec , & Elecs , ucell , na_u , lasto , nsc , isc_off , & BTD_method ) ! Print-out timers call timer ( 'TS-rgn2tri' , 3 ) ! Bye also waits for all processors call bye ( 'transiesta analyzation performed' ) end if write ( oname , \"(a,i0)\" ) \"EDM at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % EDM , block_dist , EDM_2D , & name = oname ) !if (ionode) call print_type(EDM_2D) Escf => val ( EDM_2D ) call re_alloc ( Dold , 1 , maxnh , 1 , spin % DM , name = 'Dold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) call re_alloc ( Hold , 1 , maxnh , 1 , spin % H , name = 'Hold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) if ( converge_EDM ) then call re_alloc ( Eold , 1 , maxnh , 1 , spin % EDM , name = 'Eold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) end if !     Allocate/reallocate storage associated with Hamiltonian/Overlap matrix write ( oname , \"(a,i0)\" ) \"H at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H , block_dist , H_2D , & name = oname ) !if (ionode) call print_type(H_2D) H => val ( H_2D ) write ( oname , \"(a,i0)\" ) \"H_vkb at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_vkb_1D , name = oname ) !if (ionode) call print_type(H_vkb_1D) write ( oname , \"(a,i0)\" ) \"H_kin at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_kin_1D , name = oname ) !if (ionode) call print_type(H_kin_1D) if ( switch_ldau ) then write ( oname , \"(a,i0)\" ) \"H_ldau at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % spinor , & block_dist , H_ldau_2D , name = oname ) ! Initialize to 0, LDA+U may re-calculate !   this matrix sporadically doing the SCF. ! Hence initialization MUST be performed upon ! re-allocation. call init_val ( H_ldau_2D ) if ( inicoor /= istep ) then ! Force initialization of the LDA+U ! when changing geometry ! For the first geometry this is controlled ! by the user via an fdf-key ldau_init = . true . end if end if if ( spin % SO ) then write ( oname , \"(a,i0)\" ) \"H_so at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H - 2 , & block_dist , H_so_2D , name = oname ) end if write ( oname , \"(a,i0)\" ) \"S at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , S_1D , name = oname ) if ( ionode ) call print_type ( S_1D ) S => val ( S_1D ) !     Find overlap matrix call overlap ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , maxnh , & lasto , iphorb , isa , numh , listhptr , listh , S ) ! !     Here we could also read a Hamiltonian, either to proceed to !     the analysis section (with nscf=0) or to start a mix-H scf cycle. ! !     Initialize density matrix ! The resizing of Dscf is done inside new_dm call new_DM ( auxchanged , DM_history , DM_2D , EDM_2D ) Dscf => val ( DM_2D ) Escf => val ( EDM_2D ) ! Initialize energy-density matrix to zero for first call to overfsm ! Only part of Escf is updated in TS, so if it is put as zero here ! a continuation run gives bad forces. if ( . not . TSrun ) then call normalize_DM ( first = . true . ) !$OMP parallel workshare default(shared) Escf (:,:) = 0.0_dp !$OMP end parallel workshare end if #ifdef TEST_IO ! We test the io-performance here call time_io ( spin % H , H_2D ) #endif !     If onlyS, Save overlap matrix and exit if ( onlyS ) then fname = fname_TSHS ( slabel , onlyS = . true . ) ! We include H as S, well-knowing that we only write one of ! them, there is no need to allocate space for no reason! call ts_write_tshs ( fname , & . true ., Gamma , ts_Gamma_file , & ucell , nsc , isc_off , na_u , no_s , spin % H , & ts_kscell_file , ts_kdispl_file , & xa , lasto , & H_2D , S_1D , indxuo , & dummyEf , dummyQtot , Temp , 0 , 0 ) call bye ( 'Save overlap matrix and exit' ) ! Exit siesta endif ! In case the user is requesting a Fermi-correction ! we need to delete the TS_FERMI file after each iteration if ( TSmode . and . TS_RHOCORR_METHOD == TS_RHOCORR_FERMI & . and . IONode ) then ! Delete the TS_FERMI file (enables ! reading it in and improve on the convergence) if ( file_exist ( 'TS_FERMI' ) ) then i = 23455 ! this should just not be used any were... ! Delete the file... open ( unit = i , file = 'TS_FERMI' ) close ( i , status = 'delete' ) end if end if #ifdef CDF if ( writedm_cdf ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh ) endif if ( writedm_cdf_history ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & istep ) endif if ( writedmhs_cdf ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s ) endif if ( writedmhs_cdf_history ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s , & istep ) endif #endif call timer ( 'state_init' , 2 ) END subroutine state_init","tags":"","loc":"proc/state_init.html"},{"title":"siesta_forces – SIESTA","text":"public subroutine siesta_forces(istep) Uses units precision sys files siesta_cml m_state_init m_setup_hamiltonian m_setup_H0 m_compute_dm m_compute_max_diff m_scfconvergence_test m_post_scf_work m_mixer m_mixing_scf m_mixing_scf m_mixing_scf m_rhog siesta_options parallel m_state_analysis m_steps m_spin sparse_matrices sparse_matrices m_convergence m_convergence siesta_geom m_energies m_forces m_stress siesta_master siesta_master m_save_density_matrix m_iodm_old atomlist m_dm_charge m_pexsi_solver write_subs write_subs m_compute_energies m_mpi_utils fdf m_check_walltime m_energies m_ts_options m_ts_method m_ts_global_vars siesta_geom sparse_matrices sparse_matrices m_ts_charge m_ts_charge m_ts_charge m_ts_charge m_transiesta kpoint_scf_m m_energies m_initwf proc~~siesta_forces~~UsesGraph proc~siesta_forces siesta_forces module~m_state_init m_state_init proc~siesta_forces->module~m_state_init m_save_density_matrix m_save_density_matrix proc~siesta_forces->m_save_density_matrix module~m_compute_max_diff m_compute_max_diff proc~siesta_forces->module~m_compute_max_diff sys sys proc~siesta_forces->sys parallel parallel proc~siesta_forces->parallel module~m_mixing_scf m_mixing_scf proc~siesta_forces->module~m_mixing_scf atomlist atomlist proc~siesta_forces->atomlist m_ts_method m_ts_method proc~siesta_forces->m_ts_method module~m_compute_dm m_compute_dm proc~siesta_forces->module~m_compute_dm m_scfconvergence_test m_scfconvergence_test proc~siesta_forces->m_scfconvergence_test m_convergence m_convergence proc~siesta_forces->m_convergence m_dm_charge m_dm_charge proc~siesta_forces->m_dm_charge m_stress m_stress proc~siesta_forces->m_stress units units proc~siesta_forces->units sparse_matrices sparse_matrices proc~siesta_forces->sparse_matrices kpoint_scf_m kpoint_scf_m proc~siesta_forces->kpoint_scf_m files files proc~siesta_forces->files siesta_options siesta_options proc~siesta_forces->siesta_options m_ts_options m_ts_options proc~siesta_forces->m_ts_options siesta_cml siesta_cml proc~siesta_forces->siesta_cml fdf fdf proc~siesta_forces->fdf m_check_walltime m_check_walltime proc~siesta_forces->m_check_walltime m_forces m_forces proc~siesta_forces->m_forces precision precision proc~siesta_forces->precision m_pexsi_solver m_pexsi_solver proc~siesta_forces->m_pexsi_solver m_initwf m_initwf proc~siesta_forces->m_initwf m_ts_charge m_ts_charge proc~siesta_forces->m_ts_charge module~m_setup_h0 m_setup_H0 proc~siesta_forces->module~m_setup_h0 m_post_scf_work m_post_scf_work proc~siesta_forces->m_post_scf_work m_energies m_energies proc~siesta_forces->m_energies m_rhog m_rhog proc~siesta_forces->m_rhog m_steps m_steps proc~siesta_forces->m_steps m_mixer m_mixer proc~siesta_forces->m_mixer module~m_setup_hamiltonian m_setup_hamiltonian proc~siesta_forces->module~m_setup_hamiltonian m_iodm_old m_iodm_old proc~siesta_forces->m_iodm_old siesta_master siesta_master proc~siesta_forces->siesta_master siesta_geom siesta_geom proc~siesta_forces->siesta_geom m_compute_energies m_compute_energies proc~siesta_forces->m_compute_energies write_subs write_subs proc~siesta_forces->write_subs m_mpi_utils m_mpi_utils proc~siesta_forces->m_mpi_utils m_spin m_spin proc~siesta_forces->m_spin module~m_state_analysis m_state_analysis proc~siesta_forces->module~m_state_analysis m_transiesta m_transiesta proc~siesta_forces->m_transiesta m_ts_global_vars m_ts_global_vars proc~siesta_forces->m_ts_global_vars module~m_compute_max_diff->precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing_scf->class_Fstack_dData1D module~m_mixing m_mixing module~m_mixing_scf->module~m_mixing module~m_state_analysis->write_subs module~m_mixing->precision module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. This subroutine represents central SIESTA operation logic. Todo It might be better to split the two,\n  putting the grid initialization into state_init (link!) and moving the\n  calculation of H_0 to the body of the loop, done if first_scf=.true. This would suit analysis runs in which nscf = 0 The dHmax variable only has meaning for Hamiltonian\n  mixing, or when requiring the Hamiltonian to be converged. SCF loop The current structure of the loop tries to reproduce the\n  historical Siesta usage. It should be made more clear. Two changes: The number of scf iterations performed is exactly\n     equal to the number specified (i.e., the \"forces\"\n     phase is not counted as a final scf step) At the change to a TranSiesta GF run the variable \"first_scf\"\n     is implicitly reset to \"true\". Start of SCF cycle Conditions of exit: At the top, to catch a non-positive nscf and # of iterations At the bottom, based on convergence Note implications for TranSiesta when mixing H .\n  Now H will be recomputed instead of simply being\n  inherited, however, this is required as if\n  we have bias calculations as the electric\n  field across the junction needs to be present. end of SCF cycle Arguments Type Intent Optional Attributes Name integer, intent(inout) :: istep Calls proc~~siesta_forces~~CallsGraph proc~siesta_forces siesta_forces set_tolerance set_tolerance proc~siesta_forces->set_tolerance mix_rhog mix_rhog proc~siesta_forces->mix_rhog bye bye proc~siesta_forces->bye proc~setup_hamiltonian setup_hamiltonian proc~siesta_forces->proc~setup_hamiltonian compute_energies compute_energies proc~siesta_forces->compute_energies post_scf_work post_scf_work proc~siesta_forces->post_scf_work fdf_get fdf_get proc~siesta_forces->fdf_get scfconvergence_test scfconvergence_test proc~siesta_forces->scfconvergence_test timer timer proc~siesta_forces->timer message message proc~siesta_forces->message proc~mixers_scf_history_init mixers_scf_history_init proc~siesta_forces->proc~mixers_scf_history_init save_density_matrix save_density_matrix proc~siesta_forces->save_density_matrix initwf initwf proc~siesta_forces->initwf check_walltime check_walltime proc~siesta_forces->check_walltime write_spmatrix write_spmatrix proc~siesta_forces->write_spmatrix proc~state_analysis state_analysis proc~siesta_forces->proc~state_analysis reset reset proc~siesta_forces->reset barrier barrier proc~siesta_forces->barrier transiesta transiesta proc~siesta_forces->transiesta proc~compute_dm compute_dm proc~siesta_forces->proc~compute_dm proc~mixing_scf_converged mixing_scf_converged proc~siesta_forces->proc~mixing_scf_converged interface~compute_max_diff compute_max_diff proc~siesta_forces->interface~compute_max_diff dm_charge dm_charge proc~siesta_forces->dm_charge proc~state_init state_init proc~siesta_forces->proc~state_init mixer mixer proc~siesta_forces->mixer die die proc~siesta_forces->die compute_charge_diff compute_charge_diff proc~siesta_forces->compute_charge_diff proc~setup_hamiltonian->bye proc~setup_hamiltonian->timer proc~setup_hamiltonian->die globalize_sum globalize_sum proc~setup_hamiltonian->globalize_sum dhscf dhscf proc~setup_hamiltonian->dhscf update_e0 update_e0 proc~setup_hamiltonian->update_e0 dscf dscf proc~setup_hamiltonian->dscf h h proc~setup_hamiltonian->h hubbard_term hubbard_term proc~setup_hamiltonian->hubbard_term re_alloc re_alloc proc~setup_hamiltonian->re_alloc write_hsx write_hsx proc~setup_hamiltonian->write_hsx hold hold proc~setup_hamiltonian->hold val val proc~setup_hamiltonian->val de_alloc de_alloc proc~setup_hamiltonian->de_alloc proc~mixers_history_init mixers_history_init proc~mixers_scf_history_init->proc~mixers_history_init proc~state_analysis->timer volcel volcel proc~state_analysis->volcel slua_call slua_call proc~state_analysis->slua_call wallclock wallclock proc~state_analysis->wallclock cmlendmodule cmlendmodule proc~state_analysis->cmlendmodule siesta_write_stress_pressure siesta_write_stress_pressure proc~state_analysis->siesta_write_stress_pressure update_freee update_freee proc~state_analysis->update_freee cartesianforce_to_zmatforce cartesianforce_to_zmatforce proc~state_analysis->cartesianforce_to_zmatforce born_charge born_charge proc~state_analysis->born_charge va va proc~state_analysis->va remove_intramol_pressure remove_intramol_pressure proc~state_analysis->remove_intramol_pressure print_spin print_spin proc~state_analysis->print_spin kin_stress kin_stress proc~state_analysis->kin_stress eggbox eggbox proc~state_analysis->eggbox mulliken mulliken proc~state_analysis->mulliken amass amass proc~state_analysis->amass update_freeeharris update_freeeharris proc~state_analysis->update_freeeharris cmlstartmodule cmlstartmodule proc~state_analysis->cmlstartmodule siesta_write_forces siesta_write_forces proc~state_analysis->siesta_write_forces cmladdproperty cmladdproperty proc~state_analysis->cmladdproperty write_debug write_debug proc~state_analysis->write_debug fixed fixed proc~state_analysis->fixed moments moments proc~state_analysis->moments proc~compute_dm->bye proc~compute_dm->timer proc~compute_dm->transiesta escf escf proc~compute_dm->escf compute_ebs_shift compute_ebs_shift proc~compute_dm->compute_ebs_shift dminim dminim proc~compute_dm->dminim eold eold proc~compute_dm->eold write_dmh_netcdf write_dmh_netcdf proc~compute_dm->write_dmh_netcdf write_orb_indx write_orb_indx proc~compute_dm->write_orb_indx ordern ordern proc~compute_dm->ordern proc~compute_dm->dscf mpi_bcast mpi_bcast proc~compute_dm->mpi_bcast diagon diagon proc~compute_dm->diagon zminim zminim proc~compute_dm->zminim proc~compute_dm->mulliken proc~compute_dm->val dold dold proc~compute_dm->dold write_hs_formatted write_hs_formatted proc~compute_dm->write_hs_formatted pexsi_solver pexsi_solver proc~compute_dm->pexsi_solver normalize_dm normalize_dm proc~compute_dm->normalize_dm proc~compute_dm->moments chess_wrapper chess_wrapper proc~compute_dm->chess_wrapper proc~mixing_scf_converged->reset proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff->proc~compute_max_diff_2d proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff->proc~compute_max_diff_1d proc~state_init->bye proc~state_init->timer proc~state_init->volcel proc~state_init->escf setup_ordern_indexes setup_ordern_indexes proc~state_init->setup_ordern_indexes numh numh proc~state_init->numh xij_offset xij_offset proc~state_init->xij_offset nscold nscold proc~state_init->nscold proc~state_init->proc~mixers_history_init domaindecom domaindecom proc~state_init->domaindecom dict_repopulate_md dict_repopulate_md proc~state_init->dict_repopulate_md sporb_to_spatom sporb_to_spatom proc~state_init->sporb_to_spatom globalize_or globalize_or proc~state_init->globalize_or chess_init chess_init proc~state_init->chess_init setup_kpoint_grid setup_kpoint_grid proc~state_init->setup_kpoint_grid ts_write_tshs ts_write_tshs proc~state_init->ts_write_tshs overlap overlap proc~state_init->overlap proximity_check proximity_check proc~state_init->proximity_check ioxv ioxv proc~state_init->ioxv get_chess_parameter get_chess_parameter proc~state_init->get_chess_parameter init_val init_val proc~state_init->init_val madelung madelung proc~state_init->madelung newdspdata1d newdspdata1d proc~state_init->newdspdata1d iotdxv iotdxv proc~state_init->iotdxv outcoor outcoor proc~state_init->outcoor listhptr listhptr proc~state_init->listhptr crtsparsity_sc crtsparsity_sc proc~state_init->crtsparsity_sc mscell mscell proc~state_init->mscell setup_dmhs_netcdf_file setup_dmhs_netcdf_file proc~state_init->setup_dmhs_netcdf_file write_zmatrix write_zmatrix proc~state_init->write_zmatrix delete delete proc~state_init->delete sp_to_spglobal sp_to_spglobal proc~state_init->sp_to_spglobal file_exist file_exist proc~state_init->file_exist fname_tshs fname_tshs proc~state_init->fname_tshs newdspdata2d newdspdata2d proc~state_init->newdspdata2d ts_sparse_init ts_sparse_init proc~state_init->ts_sparse_init setup_ts_kpoint_grid setup_ts_kpoint_grid proc~state_init->setup_ts_kpoint_grid superc superc proc~state_init->superc time_io time_io proc~state_init->time_io nsc nsc proc~state_init->nsc proc~state_init->re_alloc newddata2d newddata2d proc~state_init->newddata2d setup_dm_netcdf_file setup_dm_netcdf_file proc~state_init->setup_dm_netcdf_file siesta_write_positions siesta_write_positions proc~state_init->siesta_write_positions newsparsity newsparsity proc~state_init->newsparsity isc_off isc_off proc~state_init->isc_off proc~state_init->val proc~state_init->write_debug ucell ucell proc~state_init->ucell new_dm new_dm proc~state_init->new_dm attach attach proc~state_init->attach proc~state_init->normalize_dm ts_tri_analyze ts_tri_analyze proc~state_init->ts_tri_analyze hsparse hsparse proc~state_init->hsparse proc~mixers_history_init->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new proc~compute_max_diff_2d->die proc~compute_max_diff_1d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code siesta_forces Source Code subroutine siesta_forces ( istep ) !! This subroutine represents central SIESTA operation logic. #ifdef MPI use mpi_siesta #endif use units , only : eV , Ang use precision , only : dp use sys , only : bye use files , only : slabel use siesta_cml #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call use flook_siesta , only : LUA_INIT_MD , LUA_SCF_LOOP use siesta_dicts , only : dict_variable_add use m_ts_options , only : ts_scf_mixs use variable , only : cunpack #ifndef NCDF_4 use dictionary , only : assign #endif use m_mixing , only : mixers_history_init #endif use m_state_init use m_setup_hamiltonian use m_setup_H0 use m_compute_dm use m_compute_max_diff use m_scfconvergence_test use m_post_scf_work use m_mixer , only : mixer use m_mixing_scf , only : mixing_scf_converged use m_mixing_scf , only : mixers_scf_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_rhog , only : mix_rhog , compute_charge_diff use siesta_options use parallel , only : IOnode , SIESTA_worker use m_state_analysis use m_steps use m_spin , only : spin use sparse_matrices , only : DM_2D , S_1D use sparse_matrices , only : H , Hold , Dold , Dscf , Eold , Escf , maxnh use m_convergence , only : converger_t use m_convergence , only : reset , set_tolerance use siesta_geom , only : na_u ! Number of atoms in unit cell use m_energies , only : Etot ! Total energy use m_forces , only : fa , cfa ! Forces and constrained forces use m_stress , only : cstress ! Constrained stress tensor use siesta_master , only : forcesToMaster ! Send forces to master prog use siesta_master , only : siesta_server ! Is siesta a server? use m_save_density_matrix , only : save_density_matrix use m_iodm_old , only : write_spmatrix use atomlist , only : no_u , lasto , Qtot use m_dm_charge , only : dm_charge use m_pexsi_solver , only : prevDmax use write_subs , only : siesta_write_forces use write_subs , only : siesta_write_stress_pressure #ifdef NCDF_4 use dictionary use m_ncdf_siesta , only : cdf_init_file , cdf_save_settings use m_ncdf_siesta , only : cdf_save_state , cdf_save_basis #endif use m_compute_energies , only : compute_energies use m_mpi_utils , only : broadcast , barrier use fdf #ifdef SIESTA__PEXSI use m_pexsi , only : pexsi_finalize_scfloop #endif use m_check_walltime use m_energies , only : DE_NEGF use m_ts_options , only : N_Elec use m_ts_method use m_ts_global_vars , only : TSmode , TSinit , TSrun use siesta_geom , only : nsc , xa , ucell , isc_off use sparse_matrices , only : sparse_pattern , block_dist use sparse_matrices , only : S use m_ts_charge , only : ts_get_charges use m_ts_charge , only : TS_RHOCORR_METHOD use m_ts_charge , only : TS_RHOCORR_FERMI use m_ts_charge , only : TS_RHOCORR_FERMI_TOLERANCE use m_transiesta , only : transiesta use kpoint_scf_m , only : gamma_scf use m_energies , only : Ef use m_initwf , only : initwf integer , intent ( inout ) :: istep integer :: iscf logical :: first_scf , SCFconverged real ( dp ) :: dDmax ! Max. change in DM elements real ( dp ) :: dHmax ! Max. change in H elements real ( dp ) :: dEmax ! Max. change in EDM elements real ( dp ) :: drhog ! Max. change in rho(G) (experimental) real ( dp ), target :: G2max ! actually used meshcutoff type ( converger_t ) :: conv_harris , conv_freeE ! For initwf integer :: istpp #ifdef SIESTA__FLOOK ! len=24 from m_mixing.F90 character ( len = 1 ), target :: next_mixer ( 24 ) character ( len = 24 ) :: nnext_mixer integer :: imix #endif logical :: time_is_up character ( len = 40 ) :: tmp_str real ( dp ) :: Qcur #ifdef NCDF_4 type ( dict ) :: d_sav #endif #ifdef MPI integer :: MPIerror #endif external :: die , message #ifdef DEBUG call write_debug ( '    PRE siesta_forces' ) #endif #ifdef SIESTA__PEXSI ! Broadcast relevant things for program logic ! These were set in read_options, called only by \"SIESTA_workers\". call broadcast ( nscf , comm = true_MPI_Comm_World ) #endif !  Initialization tasks for a given geometry: if ( SIESTA_worker ) then call state_init ( istep ) end if #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_init\" ) #endif if ( fdf_get ( \"Sonly\" ,. false .) ) then if ( SIESTA_worker ) then call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) end if call bye ( \"S only\" ) end if Qcur = Qtot #ifdef SIESTA__FLOOK ! Add the iscf constant to the list of variables ! that are available only in this part of the routine. call dict_variable_add ( 'SCF.iteration' , iscf ) call dict_variable_add ( 'SCF.converged' , SCFconverged ) call dict_variable_add ( 'SCF.charge' , Qcur ) call dict_variable_add ( 'SCF.dD' , dDmax ) call dict_variable_add ( 'SCF.dH' , dHmax ) call dict_variable_add ( 'SCF.dE' , dEmax ) call dict_variable_add ( 'SCF.drhoG' , drhog ) ! We have to set the meshcutoff here ! because the asked and required ones are not ! necessarily the same call dict_variable_add ( 'Mesh.Cutoff.Minimum' , G2cut ) call dict_variable_add ( 'Mesh.Cutoff.Used' , G2max ) if ( mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , wmix ) else call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) ! Just to populate the table in the dictionary call dict_variable_add ( 'SCF.Mixer.Switch' , next_mixer ) end if ! Initialize to no switch next_mixer = ' ' #endif !  This call computes the **non-scf** part of  H  and initializes the !  real-space grid structures: if ( SIESTA_worker ) call setup_H0 ( G2max ) !!@todo !* It might be better to split the two, !  putting the grid initialization into **state_init (link!)** and moving the !  calculation of  H_0  to the body of the loop, done `if first_scf=.true.` !  This would suit _analysis_ runs in which **nscf = 0** !!@endtodo #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after setup_H0\" ) #endif #ifdef SIESTA__FLOOK ! Communicate with lua, just before entering the SCF loop ! This is mainly to be able to communicate ! mesh-related quantities (g2max) call slua_call ( LUA , LUA_INIT_MD ) #endif #ifdef NCDF_4 ! Initialize the NC file if ( write_cdf ) then ! Initialize the file... call cdf_init_file ( trim ( slabel ) // '.nc' , is_MD = . false .) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif ! Save the settings call cdf_save_settings ( trim ( slabel ) // '.nc' ) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif d_sav = ( 'sp' . kv . 1 ) // ( 'S' . kv . 1 ) d_sav = d_sav // ( 'nsc' . kv . 1 ) // ( 'xij' . kv . 1 ) d_sav = d_sav // ( 'xa' . kv . 1 ) // ( 'cell' . kv . 1 ) d_sav = d_sav // ( 'isc_off' . kv . 1 ) call cdf_save_state ( trim ( slabel ) // '.nc' , d_sav ) call delete ( d_sav ) ! Save the basis set call cdf_save_basis ( trim ( slabel ) // '.nc' ) end if #endif !* The dHmax variable only has meaning for Hamiltonian !  mixing, or when requiring the Hamiltonian to be converged. dDmax = - 1._dp dHmax = - 1._dp dEmax = - 1._dp drhog = - 1._dp ! Setup convergence criteria: if ( SIESTA_worker ) then if ( converge_Eharr ) then call reset ( conv_harris ) call set_tolerance ( conv_harris , tolerance_Eharr ) end if if ( converge_FreeE ) then call reset ( conv_FreeE ) call set_tolerance ( conv_FreeE , tolerance_FreeE ) end if end if !!# SCF loop !* The current structure of the loop tries to reproduce the !  historical Siesta usage. It should be made more clear. !* Two changes: ! !  1. The number of scf iterations performed is exactly !     equal to the number specified (i.e., the \"forces\" !     phase is not counted as a final scf step) !  2. At the change to a TranSiesta GF run the variable \"first_scf\" !     is implicitly reset to \"true\". ! !!## Start of SCF cycle ! !* Conditions of exit: ! !  * At the top, to catch a non-positive nscf and # of iterations !  * At the bottom, based on convergence ! iscf = 0 do while ( iscf < nscf ) iscf = iscf + 1 !* Note implications for TranSiesta when mixing H. !  Now H will be recomputed instead of simply being !  inherited, however, this is required as if !  we have bias calculations as the electric !  field across the junction needs to be present. first_scf = ( iscf == 1 ) if ( SIESTA_worker ) then ! Check whether we are short of time to continue call check_walltime ( time_is_up ) if ( time_is_up ) then ! Save DM/H if we were not saving it... !   Do any other bookeeping not done by \"die\" call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge' // & ' before wall time exhaustion' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call barrier () ! A non-root node might get first to the 'die' call call die ( \"OUT_OF_TIME: Time is up.\" ) end if call timer ( 'IterSCF' , 1 ) if ( cml_p ) & call cmlStartStep ( xf = mainXML , type = 'SCF' , index = iscf ) if ( mixH ) then if ( first_scf ) then if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then call get_H_from_file () else call setup_hamiltonian ( iscf ) end if end if call compute_DM ( iscf ) ! Maybe set Dold to zero if reading charge or H... call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) else call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) call compute_DM ( iscf ) call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) end if ! This iteration has completed calculating the new DM call compute_energies ( iscf ) if ( mix_charge ) then call compute_charge_diff ( drhog ) end if ! Note: For DM and H convergence checks. At this point: ! If mixing the DM: !        Dscf=DM_out, Dold=DM_in(mixed), H=H_in, Hold=H_in(prev step) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H_in - H_in(prev step)) ! If mixing the Hamiltonian: !        Dscf=DM_out, Dold=DM_in, H=H_(DM_out), Hold=H_in(mixed) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H(DM_out),H_in) call scfconvergence_test ( first_scf , iscf , & dDmax , dHmax , dEmax , & conv_harris , conv_freeE , & SCFconverged ) ! ** Check this heuristic if ( mixH ) then prevDmax = dHmax else prevDmax = dDmax end if ! Calculate current charge based on the density matrix call dm_charge ( spin , DM_2D , S_1D , Qcur ) ! Check whether we should step to the next mixer call mixing_scf_converged ( SCFconverged ) if ( SCFconverged . and . iscf < min_nscf ) then SCFconverged = . false . if ( IONode ) then write ( * , \"(a,i0)\" ) & \"SCF cycle continued for minimum number of iterations: \" , & min_nscf end if end if ! In case the user has requested a Fermi-level correction ! Then we start by correcting the fermi-level if ( TSrun . and . SCFconverged . and . & TS_RHOCORR_METHOD == TS_RHOCORR_FERMI ) then if ( abs ( Qcur - Qtot ) > TS_RHOCORR_FERMI_TOLERANCE ) then ! Call transiesta with fermi-correct call transiesta ( iscf , spin % H , & block_dist , sparse_pattern , Gamma_Scf , ucell , nsc , & isc_off , no_u , na_u , lasto , xa , maxnh , H , S , & Dscf , Escf , Ef , Qtot , . true ., DE_NEGF ) ! We will not have not converged as we have just ! changed the Fermi-level SCFconverged = . false . end if end if if ( monitor_forces_in_scf ) call compute_forces () ! Mix_after_convergence preserves the old behavior of ! the program. if ( (. not . SCFconverged ) . or . mix_after_convergence ) then ! Mix for next step if ( mix_charge ) then call mix_rhog ( iscf ) else call mixer ( iscf ) end if ! Save for possible restarts if ( mixH ) then call write_spmatrix ( H , file = \"H_MIXED\" , when = writeH ) call save_density_matrix ( file = \"DM_OUT\" , when = writeDM ) else call save_density_matrix ( file = \"DM_MIXED\" , when = writeDM ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = writeH ) end if end if call timer ( 'IterSCF' , 2 ) call print_timings ( first_scf , istep == inicoor ) if ( cml_p ) call cmlEndStep ( mainXML ) #ifdef SIESTA__FLOOK ! Communicate with lua call slua_call ( LUA , LUA_SCF_LOOP ) ! Retrieve an easy character string nnext_mixer = cunpack ( next_mixer ) if ( len_trim ( nnext_mixer ) > 0 . and . . not . mix_charge ) then if ( TSrun ) then do imix = 1 , size ( ts_scf_mixs ) if ( ts_scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( ts_scf_mixs ) scf_mix => ts_scf_mixs ( imix ) exit end if end do else do imix = 1 , size ( scf_mixs ) if ( scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( imix ) exit end if end do end if ! Check that we indeed have changed the mixer if ( IONode . and . scf_mix % name /= nnext_mixer ) then write ( * , '(2a)' ) 'siesta-lua: WARNING: trying to change ' , & 'to a non-existing mixer! Not changing anything!' else if ( IONode ) then write ( * , '(2a)' ) 'siesta-lua: Switching mixer method to: ' , & trim ( nnext_mixer ) end if ! Reset for next loop next_mixer = ' ' ! Update the references call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif ! ... except that we might continue for TranSiesta if ( SCFconverged ) then call transiesta_switch () ! might reset SCFconverged and iscf end if else ! non-siesta worker call compute_DM ( iscf ) end if #ifdef SIESTA__PEXSI call broadcast ( iscf , comm = true_MPI_Comm_World ) call broadcast ( SCFconverged , comm = true_MPI_Comm_World ) #endif !  Exit if converged: if ( SCFconverged ) exit end do !! **end of SCF cycle** #ifdef SIESTA__PEXSI if ( isolve == SOLVE_PEXSI ) then call pexsi_finalize_scfloop () end if #endif if ( . not . SIESTA_worker ) return call end_of_cycle_save_operations () if ( . not . SCFconverged ) then if ( SCFMustConverge ) then call message ( 'FATAL' , 'SCF_NOT_CONV: SCF did not converge' // & ' in maximum number of steps (required).' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call barrier () call die ( 'ABNORMAL_TERMINATION' ) else if ( . not . harrisfun ) then call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge  in maximum number of steps.' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) end if end if ! To write the initial wavefunctions to be used in a ! consequent TDDFT run. if ( writetdwf ) then istpp = 0 call initwf ( istpp , totime ) end if if ( TSmode . and . TSinit . and .(. not . SCFConverged ) ) then ! Signal that the DM hasn't converged, so we cannot ! continue to the transiesta routines call die ( 'ABNORMAL_TERMINATION' ) end if ! Clean-up here to limit memory usage call mixers_scf_history_init ( ) ! End of standard SCF loop. ! Do one more pass to compute forces and stresses ! Note that this call will no longer overwrite H while computing the ! final energies, forces and stresses... if ( fdf_get ( \"compute-forces\" ,. true .) ) then call post_scf_work ( istep , iscf , SCFconverged ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after post_scf_work\" ) #endif end if ! ... so H at this point is the latest generator of the DM, except ! if mixing H beyond self-consistency or terminating the scf loop ! without convergence while mixing H call state_analysis ( istep ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_analysis\" ) #endif ! If siesta is running as a subroutine, send forces to master program if ( siesta_server ) & call forcesToMaster ( na_u , Etot , cfa , cstress ) #ifdef DEBUG call write_debug ( '    POS siesta_forces' ) #endif contains ! Read the Hamiltonian from a file subroutine get_H_from_file () use sparse_matrices , only : maxnh , numh , listh , listhptr use atomlist , only : no_l use m_spin , only : spin use m_iodm_old , only : read_spmatrix logical :: found call read_spmatrix ( maxnh , no_l , spin % H , numh , & listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) end subroutine get_H_from_file ! Computes forces and stresses with the current DM_out subroutine compute_forces () use siesta_options , only : recompute_H_after_scf use m_final_H_f_stress , only : final_H_f_stress use write_subs real ( dp ), allocatable :: fa_old (:,:), Hsave (:,:) allocate ( fa_old ( size ( fa , dim = 1 ), size ( fa , dim = 2 ))) fa_old (:,:) = fa (:,:) if ( recompute_H_after_scf ) then allocate ( Hsave ( size ( H , dim = 1 ), size ( H , dim = 2 ))) Hsave (:,:) = H (:,:) end if call final_H_f_stress ( istep , iscf , . false . ) if ( recompute_H_after_scf ) then H (:,:) = Hsave (:,:) deallocate ( Hsave ) end if if ( ionode ) then print * , \"Max diff in force (eV/Ang): \" , & maxval ( abs ( fa - fa_old )) * Ang / eV call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () endif deallocate ( fa_old ) end subroutine compute_forces ! Print out timings of the first SCF loop only subroutine print_timings ( first_scf , first_md ) use timer_options , only : use_tree_timer use m_ts_global_vars , only : TSrun logical , intent ( in ) :: first_scf , first_md character ( len = 20 ) :: routine ! If this is not the first iteration, ! we immediately return. if ( . not . first_scf ) return if ( . not . first_md ) return routine = 'IterSCF' if ( TSrun ) then ! with Green function generation ! The tree-timer requires direct ! children of the routine to be ! queried. ! This is not obeyed in the TS case... :( if ( . not . use_tree_timer ) then routine = 'TS' end if endif call timer ( routine , 3 ) end subroutine print_timings ! Depending on various conditions, save the DMin ! or the DMout, and possibly keep a copy of H ! NOTE: Only if the scf cycle converged before exit it ! is guaranteed that the DM is \"pure out\" and that ! we can recover the right H if mixing H. ! subroutine end_of_cycle_save_operations () logical :: DM_write , H_write ! Depending on the option we should overwrite the ! Hamiltonian if ( mixH . and . . not . mix_after_convergence ) then ! Make sure that we keep the H actually used ! to generate the last DM, if needed. H = Hold end if DM_write = write_DM_at_end_of_cycle . and . & . not . writeDM H_write = write_H_at_end_of_cycle . and . & . not . writeH if ( mix_after_convergence ) then ! If we have been saving them, there is no point in doing ! it one more time if ( mixH ) then call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_MIXED\" , when = H_write ) else call save_density_matrix ( file = \"DM_MIXED\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if else call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if end subroutine end_of_cycle_save_operations subroutine transiesta_switch () use precision , only : dp use parallel , only : IONode use class_dSpData2D use class_Fstack_dData1D use densematrix , only : resetDenseMatrix use siesta_options , only : fire_mix , broyden_maxit use siesta_options , only : dDtol , dHtol use sparse_matrices , only : DM_2D , EDM_2D use atomlist , only : lasto use siesta_geom , only : nsc , isc_off , na_u , xa , ucell use m_energies , only : Ef use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mix , scf_mixs use m_rhog , only : resetRhoG use m_ts_global_vars , only : TSinit , TSrun use m_ts_global_vars , only : ts_print_transiesta use m_ts_method use m_ts_options , only : N_Elec , Elecs use m_ts_options , only : DM_bulk use m_ts_options , only : val_swap use m_ts_options , only : ts_Dtol , ts_Htol use m_ts_options , only : ts_hist_keep use m_ts_options , only : ts_siesta_stop use m_ts_options , only : ts_scf_mixs use m_ts_electype integer :: iEl , na_a integer , allocatable :: allowed_a (:) real ( dp ), pointer :: DM (:,:), EDM (:,:) ! We are done with the initial diagon run ! Now we start the TRANSIESTA (Green functions) run if ( . not . TSmode ) return if ( . not . TSinit ) return ! whether we are in siesta initialization step TSinit = . false . ! whether transiesta is running TSrun = . true . ! If transiesta should stop immediately if ( ts_siesta_stop ) then if ( IONode ) then write ( * , '(a)' ) 'ts: Stopping transiesta (user option)!' end if return end if ! Reduce memory requirements call resetDenseMatrix () ! Signal to continue... ! These two variables are from the top-level ! routine (siesta_forces) SCFconverged = . false . iscf = 0 ! DANGER (when/if going back to the DIAGON run, we should ! re-instantiate the original mixing value) call val_swap ( dDtol , ts_Dtol ) call val_swap ( dHtol , ts_Htol ) ! Clean up mixing history if ( mix_charge ) then call resetRhoG (. true .) else if ( associated ( ts_scf_mixs , target = scf_mixs ) ) then do iel = 1 , size ( scf_mix % stack ) call reset ( scf_mix % stack ( iel ), - ts_hist_keep ) ! Reset iteration count as certain ! mixing schemes require this for consistency scf_mix % cur_itt = n_items ( scf_mix % stack ( iel )) end do else call mixers_history_init ( scf_mixs ) end if end if ! Transfer scf_mixing to the transiesta mixing routine scf_mix => ts_scf_mixs ( 1 ) #ifdef SIESTA__FLOOK if ( . not . mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif call ts_print_transiesta () ! In case of transiesta and DM_bulk. ! In case we ask for initialization of the DM in bulk ! we read in the DM files from the electrodes and ! initialize the bulk to those values if ( DM_bulk > 0 ) then if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Initializing bulk DM in electrodes.' end if na_a = 0 do iEl = 1 , na_u if ( . not . a_isDev ( iEl ) ) na_a = na_a + 1 end do allocate ( allowed_a ( na_a )) na_a = 0 do iEl = 1 , na_u ! We allow the buffer atoms as well (this will even out the ! potential around the back of the electrode) if ( . not . a_isDev ( iEl ) ) then na_a = na_a + 1 allowed_a ( na_a ) = iEl end if end do do iEl = 1 , N_Elec if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Reading in electrode TSDE for ' // & trim ( Elecs ( iEl )% Name ) end if ! Copy over the DM in the lead ! Notice that the EDM matrix that is copied over ! will be equivalent at Ef == 0 call copy_DM ( Elecs ( iEl ), na_u , xa , lasto , nsc , isc_off , & ucell , DM_2D , EDM_2D , na_a , allowed_a ) end do ! Clean-up deallocate ( allowed_a ) if ( IONode ) then write ( * , * ) ! new-line end if ! The electrode EDM is aligned at Ef == 0 ! We need to align the energy matrix DM => val ( DM_2D ) EDM => val ( EDM_2D ) iEl = size ( DM ) call daxpy ( iEl , Ef , DM ( 1 , 1 ), 1 , EDM ( 1 , 1 ), 1 ) end if end subroutine transiesta_switch end subroutine siesta_forces","tags":"","loc":"proc/siesta_forces.html"},{"title":"siesta_analysis – SIESTA","text":"public subroutine siesta_analysis(relaxd) Uses band writewave writewave m_ksvinit m_ksv m_projected_DOS m_local_DOS m_pexsi_local_DOS m_pexsi_dos siesta_options units sparse_matrices sparse_matrices siesta_geom m_dhscf atomlist atomlist fdf writewave siesta_cml files files zmatrix Kpoint_grid parallel parallel files m_energies m_steps m_ntm m_spin m_spin m_dipol m_eo m_forces m_gamma alloc basis_enthalpy m_partial_charges m_iodm_old m_siesta2wannier90 m_mpi_utils flook_siesta proc~~siesta_analysis~~UsesGraph proc~siesta_analysis siesta_analysis atomlist atomlist proc~siesta_analysis->atomlist parallel parallel proc~siesta_analysis->parallel m_ksvinit m_ksvinit proc~siesta_analysis->m_ksvinit Kpoint_grid Kpoint_grid proc~siesta_analysis->Kpoint_grid m_ntm m_ntm proc~siesta_analysis->m_ntm m_pexsi_dos m_pexsi_dos proc~siesta_analysis->m_pexsi_dos m_eo m_eo proc~siesta_analysis->m_eo m_dhscf m_dhscf proc~siesta_analysis->m_dhscf m_projected_DOS m_projected_DOS proc~siesta_analysis->m_projected_DOS units units proc~siesta_analysis->units sparse_matrices sparse_matrices proc~siesta_analysis->sparse_matrices files files proc~siesta_analysis->files siesta_options siesta_options proc~siesta_analysis->siesta_options alloc alloc proc~siesta_analysis->alloc siesta_cml siesta_cml proc~siesta_analysis->siesta_cml fdf fdf proc~siesta_analysis->fdf m_ksv m_ksv proc~siesta_analysis->m_ksv m_forces m_forces proc~siesta_analysis->m_forces m_partial_charges m_partial_charges proc~siesta_analysis->m_partial_charges basis_enthalpy basis_enthalpy proc~siesta_analysis->basis_enthalpy m_energies m_energies proc~siesta_analysis->m_energies m_dipol m_dipol proc~siesta_analysis->m_dipol m_gamma m_gamma proc~siesta_analysis->m_gamma m_steps m_steps proc~siesta_analysis->m_steps writewave writewave proc~siesta_analysis->writewave band band proc~siesta_analysis->band m_siesta2wannier90 m_siesta2wannier90 proc~siesta_analysis->m_siesta2wannier90 m_iodm_old m_iodm_old proc~siesta_analysis->m_iodm_old siesta_geom siesta_geom proc~siesta_analysis->siesta_geom m_mpi_utils m_mpi_utils proc~siesta_analysis->m_mpi_utils m_local_DOS m_local_DOS proc~siesta_analysis->m_local_DOS m_spin m_spin proc~siesta_analysis->m_spin flook_siesta flook_siesta proc~siesta_analysis->flook_siesta zmatrix zmatrix proc~siesta_analysis->zmatrix m_pexsi_local_DOS m_pexsi_local_DOS proc~siesta_analysis->m_pexsi_local_DOS Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Check that we are converged in geometry,\n if strictly required,\n before carrying out any analysis. @code\n@endcode Arguments Type Intent Optional Attributes Name logical :: relaxd Calls proc~~siesta_analysis~~CallsGraph proc~siesta_analysis siesta_analysis cmlstartmodule cmlstartmodule proc~siesta_analysis->cmlstartmodule local_dos local_dos proc~siesta_analysis->local_dos scell scell proc~siesta_analysis->scell siesta_write_forces siesta_write_forces proc~siesta_analysis->siesta_write_forces xa_last xa_last proc~siesta_analysis->xa_last cmladdproperty cmladdproperty proc~siesta_analysis->cmladdproperty slua_call slua_call proc~siesta_analysis->slua_call ucell ucell proc~siesta_analysis->ucell setup_wfs_list setup_wfs_list proc~siesta_analysis->setup_wfs_list wwave wwave proc~siesta_analysis->wwave fdf_get fdf_get proc~siesta_analysis->fdf_get pexsi_dos pexsi_dos proc~siesta_analysis->pexsi_dos ksv_pol ksv_pol proc~siesta_analysis->ksv_pol dipol dipol proc~siesta_analysis->dipol optical optical proc~siesta_analysis->optical pexsi_local_dos pexsi_local_dos proc~siesta_analysis->pexsi_local_dos message message proc~siesta_analysis->message eo eo proc~siesta_analysis->eo print_spin print_spin proc~siesta_analysis->print_spin re_alloc re_alloc proc~siesta_analysis->re_alloc write_basis_enthalpy write_basis_enthalpy proc~siesta_analysis->write_basis_enthalpy bk bk proc~siesta_analysis->bk siesta_write_energies siesta_write_energies proc~siesta_analysis->siesta_write_energies de_alloc de_alloc proc~siesta_analysis->de_alloc read_spmatrix read_spmatrix proc~siesta_analysis->read_spmatrix scell_last scell_last proc~siesta_analysis->scell_last fdf_boolean fdf_boolean proc~siesta_analysis->fdf_boolean outcoor outcoor proc~siesta_analysis->outcoor siesta_write_positions siesta_write_positions proc~siesta_analysis->siesta_write_positions ucell_last ucell_last proc~siesta_analysis->ucell_last projected_dos projected_dos proc~siesta_analysis->projected_dos xa xa proc~siesta_analysis->xa dhscf dhscf proc~siesta_analysis->dhscf siesta_write_stress_pressure siesta_write_stress_pressure proc~siesta_analysis->siesta_write_stress_pressure die die proc~siesta_analysis->die bands bands proc~siesta_analysis->bands Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code siesta_analysis Source Code subroutine siesta_analysis ( relaxd ) USE band , only : nbk , bk , maxbk , bands USE writewave , only : nwk , wfk , wwave USE writewave , only : setup_wfs_list , wfs_filename USE m_ksvinit , only : nkpol , kpol , wgthpol use m_ksv USE m_projected_DOS , only : projected_DOS USE m_local_DOS , only : local_DOS #ifdef SIESTA__PEXSI USE m_pexsi_local_DOS , only : pexsi_local_DOS USE m_pexsi_dos , only : pexsi_dos #endif USE siesta_options use units , only : Debye , eV use sparse_matrices , only : maxnh , listh , listhptr , numh use sparse_matrices , only : H , S , Dscf , xijo use siesta_geom use m_dhscf , only : dhscf use atomlist , only : indxuo , iaorb , lastkb , lasto , datm , no_l , & iphkb , no_u , no_s , iza , iphorb , rmaxo , indxua use atomlist , only : qtot use fdf use writewave , only : wwave use siesta_cml use files , only : slabel use files , only : filesOut_t ! derived type for output file names use zmatrix , only : lUseZmatrix , write_zmatrix use Kpoint_grid use parallel , only : IOnode use parallel , only : SIESTA_worker use files , only : label_length use m_energies use m_steps , only : final use m_ntm use m_spin , only : nspin , spinor_dim , h_spin_dim use m_spin , only : SpOrb , NonCol , SPpol , NoMagn use m_dipol use m_eo use m_forces , only : fa use m_gamma use alloc , only : re_alloc , de_alloc use basis_enthalpy , only : write_basis_enthalpy use m_partial_charges , only : want_partial_charges use m_iodm_old , only : read_spmatrix use m_siesta2wannier90 , only : siesta2wannier90 #ifdef MPI use m_mpi_utils , only : globalize_sum #endif #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_ANALYSIS #endif implicit none logical :: relaxd , getPSI , quenched_MD , found real ( dp ) :: dummy_str ( 3 , 3 ) real ( dp ) :: dummy_strl ( 3 , 3 ) real ( dp ) :: qspin ( 4 ) ! Local real ( dp ) :: polxyz ( 3 , nspin ) ! Autom., small real ( dp ) :: polR ( 3 , nspin ) ! Autom., small real ( dp ) :: qaux real ( dp ), pointer :: ebk (:,:,:) ! Band energies integer :: j , ix , ind , ik , io , ispin integer :: wfs_band_min , wfs_band_max real ( dp ) :: g2max , current_ef #ifdef MPI real ( dp ) :: qtmp ( 4 ) #endif type ( filesOut_t ) :: filesOut ! blank output file names !-----------------------------------------------------------------------BEGIN if ( SIESTA_worker ) call timer ( \"Analysis\" , 1 ) !! Check that we are converged in geometry, !! if strictly required, !! before carrying out any analysis. !!@code quenched_MD = ( ( iquench > 0 ) . and . $ (( idyn . eq . 1 ) . or . ( idyn . eq . 3 )) ) if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( GeometryMustConverge . and . (. not . relaxd )) then call message ( \"FATAL\" , $ \"GEOM_NOT_CONV: Geometry relaxation not converged\" ) call die ( \"ABNORMAL_TERMINATION\" ) endif endif !!@endcode !     All the comments below assume that this compatibility option !     is not used. !     Note also that full compatibility cannot be guaranteed if (. not . compat_pre_v4_dynamics ) then !     This is a sanity safeguard: we reset the geometry (which might !     have been moved by the relaxation or MD routines) to the one used !     in the last computation of the electronic structure. !     See the comments below for explanation !$OMP parallel workshare default(shared) xa ( 1 : 3 , 1 : na_s ) = xa_last ( 1 : 3 , 1 : na_s ) ucell ( 1 : 3 , 1 : 3 ) = ucell_last ( 1 : 3 , 1 : 3 ) scell ( 1 : 3 , 1 : 3 ) = scell_last ( 1 : 3 , 1 : 3 ) !$OMP end parallel workshare endif ! zmatrix info reset?? if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then if ( SIESTA_worker ) then call read_spmatrix ( maxnh , no_l , h_spin_dim , numh , . listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) current_ef = ef ef = fdf_get ( \"Manual-Fermi-Level\" , current_ef , \"Ry\" ) endif endif #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.DOS\" ,. false .)) then call pexsi_dos ( no_u , no_l , spinor_dim , $ maxnh , numh , listhptr , listh , H , S , qtot , ef ) endif #endif ! section done by Siesta subset of nodes if ( SIESTA_worker ) then final = . true . if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'Finalization' ) endif #ifdef SIESTA__FLOOK ! Call lua right before doing the analysis, ! possibly changing some of the variables call slua_call ( LUA , LUA_ANALYSIS ) #endif ! !     NOTE that the geometry output by the following sections !     used to be that \"predicted\" for the next MD or relaxation step. !     This is now changed ! if ( IOnode ) then ! Print atomic coordinates ! This covers CG and MD-quench (verlet, pr), instances in which ! \"relaxd\" is meaningful if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( relaxd ) then ! xa = xa_last ! The \"relaxation\" routines do not update ! the coordinates if relaxed, so this behavior is unchanged call outcoor ( ucell , xa , na_u , 'Relaxed' , . true . ) else ! Since xa = xa_last now, this will just repeat the ! last set of coordinates used, not the predicted ones. call outcoor ( ucell , xa , na_u , 'Final (unrelaxed)' , . true . ) endif endif ! This call will write xa_last to the .STRUCT_OUT file ! (again, since it has already been written by state_init), ! CML records of the latest processed structure, and ! possibly zmatrix info.  *** unmoved?? how? ! Note that the .STRUCT_NEXT_ITER file is produced ! in siesta_move for checkpointing of relaxations and MD runs. ! If all we want are the CML records (to satisfy some expectation ! of appearance in the \"finalization\" section, we might put the ! cml call explicitly and forget about the rest. if ( compat_pre_v4_dynamics ) then call siesta_write_positions ( moved = . true .) else call siesta_write_positions ( moved = . false .) endif ! ??  Clarify Zmatrix behavior **** if ( lUseZmatrix ) call write_Zmatrix ! Print unit cell (cell_last) for variable cell and server operation if ( varcel . or . ( idyn . eq . 8 )) call outcell ( ucell ) !------------------------------------------------------------------ ! It can be argued that these needed the xa_last coordinates ! all along !       Print coordinates in xmol format in a separate file if ( fdf_boolean ( 'WriteCoorXmol' ,. false .)) & call coxmol ( iza , xa , na_u ) !       Print coordinates in cerius format in a separate file if ( fdf_boolean ( 'WriteCoorCerius' ,. false .)) & call coceri ( iza , xa , ucell , na_u , sname ) !       Find interatomic distances (output in file BONDS_FINAL) call bonds ( ucell , na_u , isa , xa , & rmax_bonds , trim ( slabel ) // \".BONDS_FINAL\" ) endif ! IONode !--- end output of geometry information ! ! ! NOTE: In the following sections, wavefunction generation, computation !       of band structure, etc, are carried out using the last Hamiltonian !       generated in the SCF run for the last geometry considered. !   But, if xa /= xa_last, the computation of, say, bands, will use !      H phases which are not the same as those producing the final !      ground-state electronic structure. ! !    Also, since we have removed the replication (superx call) !      of \"moved\" coordinates !      into the auxiliary supercell from 'siesta_move' (recall that it is !      done always in state_init for every new geometry), the \"moved unit !      cell coordinates\" could coexist here with \"unmoved non-unit cell SC coords\", !      which is wrong. !      For all of the above, we should put here a sanity safeguard !        (if we have not done so already at the top of this routine) !        xa(1:3,1:na_s) = xa_last(1:3,1:na_s) !        ucell(1:3,1:3) = ucell_last(1:3,1:3) !        scell(1:3,1:3) = scell_last(1:3,1:3) !        DM and H issues ! !        Some of the routines that follow use H and S, and some use the DM. !        Those which use the DM should work with the final \"out\" DM for !        consistency. !        Those which use H,S should work with the latest diagonalized H,S pair. ! !      If mixing the DM during the scf loop we should avoid mixing it one more time !        after convergence (or restoring Dold) !        If mixing H, we should avoid mixing it one more time !        after convergence (and restoring Hold to have the exact H that generated the !        latest DM, although this is probably too much). !        See the logic in siesta_forces. !     Find and print wavefunctions at selected k-points !   This uses H,S, and xa if ( nwk . gt . 0 ) then wfs_filename = trim ( slabel ) // \".selected.WFSX\" if ( IONode ) print \"(a)\" , $ \"Writing WFSX for selected k-points in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , & nwk , & numh , listhptr , listh , H , S , Ef , xijo , indxuo , & nwk , wfk , no_u , gamma , occtol ) endif !   This uses H,S, and xa if ( write_coop ) then ! Output the wavefunctions for the kpoints in the SCF set ! Note that we allow both a band number and an energy range ! The user is responsible for using appropriate values. wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nkpnt , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . true ., $ use_energy_window = . true .) wfs_filename = trim ( slabel ) // \".fullBZ.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for COOP/COHP in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . nkpnt , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . nkpnt , kpoint , no_u , gamma , occtol ) endif !     Compute bands !   This uses H,S, and xa nullify ( ebk ) call re_alloc ( ebk , 1 , no_u , 1 , spinor_dim , 1 , maxbk , & 'ebk' , 'siesta_analysis' ) if ( nbk . gt . 0 ) then if ( IONode ) print \"(a)\" , \"Computing bands...\" getPSI = fdf_get ( 'WFS.Write.For.Bands' , . false .) if ( getPSI ) then wfs_filename = trim ( slabel ) // \".bands.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for bands in \" $ // trim ( wfs_filename ) wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nbk , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . false ., $ use_energy_window = . false .) endif call bands ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . maxbk , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . . true ., nbk , bk , ebk , occtol , getPSI ) if ( IOnode ) then if ( writbk ) then write ( 6 , '(/,a,/,a4,a12)' ) & 'siesta: Band k vectors (Bohr**-1):' , 'ik' , 'k' do ik = 1 , nbk write ( 6 , '(i4,3f12.6)' ) ik , ( bk ( ix , ik ), ix = 1 , 3 ) enddo endif if ( writb ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Band energies (eV):' , 'ik' , 'is' , 'eps' do ispin = 1 , spinor_dim do ik = 1 , nbk write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin , ( ebk ( io , ispin , ik ) / eV , io = 1 , min ( 10 , no_u )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( ebk ( io , ispin , ik ) / eV , io = 11 , no_u ) enddo enddo endif endif endif !     Print eigenvalues if ( IOnode . and . writeig ) then if (( isolve . eq . SOLVE_DIAGON . or . & (( isolve . eq . SOLVE_MINIM ) . and . minim_calc_eigenvalues )) & . and . no_l . lt . 1000 ) then if ( h_spin_dim <= 2 ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Eigenvalues (eV):' , 'ik' , 'is' , 'eps' do ik = 1 , nkpnt do ispin = 1 , spinor_dim write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin ,( eo ( io , ispin , ik ) / eV , io = 1 , min ( 10 , neigwanted )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( eo ( io , ispin , ik ) / eV , io = 11 , neigwanted ) enddo enddo else write ( 6 , '(/,a)' ) 'siesta: Eigenvalues (eV):' do ik = 1 , nkpnt write ( 6 , '(a,i6)' ) 'ik =' , ik write ( 6 , '(10f7.2)' ) & (( eo ( io , ispin , ik ) / eV , io = 1 , neigwanted ), ispin = 1 , 2 ) enddo endif write ( 6 , '(a,f15.6,a)' ) 'siesta: Fermi energy =' , ef / eV , ' eV' endif endif if ((( isolve . eq . SOLVE_DIAGON ). or . & (( isolve . eq . SOLVE_MINIM ). and . minim_calc_eigenvalues )) & . and . IOnode ) & call ioeig ( eo , ef , neigwanted , nspin , nkpnt , no_u , spinor_dim , & nkpnt , kpoint , kweight ) !   This uses H,S, and xa, as it diagonalizes them again call projected_DOS () !     Print program's energy decomposition and final forces if ( IOnode ) then call siesta_write_energies ( iscf = 0 , dDmax = 0._dp , dHmax = 0._dp ) ! final == .true. which makes the step counter irrelevant call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () call write_basis_enthalpy ( FreeE , FreeEHarris ) endif ! NOTE: Here, the spin polarization is computed using Dscf, which is !       a density matrix obtained after mixing the \"in\" and \"out\" !       DMs of the SCF run for the last geometry considered. !       This can be considered a feature or a bug. call print_spin ( qspin ) ! qspin returned for use below !     This uses the last computed dipole in dhscf during the scf cycle, !     which is in fact derived from the \"in\" DM. !     Perhaps this section should be moved after the call to dhscf below !     AND use the DM_out of the last step (but there might not be a call !     to dhscf if there are no files to output, and the computation of the !     charge density is expensive... !     Print electric dipole if ( shape . ne . 'bulk' ) then if ( IOnode ) then write ( 6 , '(/,a,3f12.6)' ) & 'siesta: Electric dipole (a.u.)  =' , dipol write ( 6 , '(a,3f12.6)' ) & 'siesta: Electric dipole (Debye) =' , & ( dipol ( ix ) / Debye , ix = 1 , 3 ) endif if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = dipol / Debye , & title = 'Electric dipole' , dictref = 'siesta:dipol' , . units = 'siestaUnits:Debye' ) endif !cml_p endif ! NOTE: The use of *_last geometries in the following sections !       guarantees that the analysis of the electronic structure !       is done for the geometry for which it was computed. !  BUT these routines need H,S, so H should not be mixed after !       convergence. !     Calculation of the bulk polarization using the Berry phase !     formulas by King-Smith and Vanderbilt if ( nkpol . gt . 0 . and . . not . bornz ) then if ( NonCol . or . SpOrb ) then if ( IOnode ) then write ( 6 , '(/a)' ) . 'siesta_analysis: bulk polarization implemented only for' write ( 6 , '(/a)' ) . 'siesta_analysis: paramagnetic or collinear spin runs' endif else call KSV_pol ( na_u , na_s , xa_last , rmaxo , scell_last , & ucell_last , no_u , no_l , no_s , nspin , qspin , & maxnh , nkpol , numh , listhptr , listh , & H , S , xijo , indxuo , isa , iphorb , & iaorb , lasto , shape , & nkpol , kpol , wgthpol , polR , polxyz ) endif endif !     Calculation of the optical conductivity call optical ( na_u , na_s , xa_last , scell_last , ucell_last , & no_u , no_l , no_s , nspin , qspin , & maxnh , numh , listhptr , listh , H , S , xijo , $ indxuo , ebk , ef , temp , & isa , iphorb , iphKB , lasto , lastkb , shape ) call de_alloc ( ebk , 'ebk' , 'siesta_analysis' ) !................................... ! !  NOTE: Dscf here might be the mixed one (see above). ! want_partial_charges = ( hirshpop . or . voropop ) . AND . $ (. not . partial_charges_at_every_geometry ) !     Save electron density and potential if ( saverho . or . savedrho . or . saverhoxc . or . & savevh . or . savevt . or . savevna . or . & savepsch . or . savetoch . or . & want_partial_charges ) then if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' g2max = g2cut dummy_str = 0.0 dummy_strl = 0.0 call dhscf ( nspin , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa_last , indxua , & ntm , 0 , 0 , 0 , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_str , fa , dummy_strl ) ! next to last argument is dummy here, ! as no forces are calculated ! todo: make all these optional endif C C Call the wannier90 interface here , as local_DOS destroys the DM ... C if ( w90_processing ) call siesta2wannier90 () C Find local density of states !  It needs H,S, and xa, as it diagonalizes them again !  NOTE: This call will obliterate Dscf !  It is better to put a explicit out argument for the partial DM computed. call local_DOS () ! In summary, it is better to: ! !   -- Avoid (or warn the user about) doing any analysis if the calculation is not converged !   -- Avoid mixing DM or H after scf convergence !   -- Set xa to xa_last at the top of this file. Write any \"next iter\" coordinate file !      in 'siesta_move' endif ! SIESTA_worker #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.LDOS\" ,. false .)) then call pexsi_local_DOS () endif #endif if ( SIESTA_worker ) call timer ( \"Analysis\" , 2 ) !------------------------------------------------------------------------- END END subroutine siesta_analysis","tags":"","loc":"proc/siesta_analysis.html"},{"title":"initMeshDistr – SIESTA","text":"public subroutine initMeshDistr(iDistr, oDistr, nm, wload) Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr integer, intent(in) :: oDistr integer, intent(in) :: nm (3) integer, intent(in), optional :: wload (*) Calls proc~~initmeshdistr~~CallsGraph proc~initmeshdistr initMeshDistr die die proc~initmeshdistr->die timer timer proc~initmeshdistr->timer re_alloc re_alloc proc~initmeshdistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code initMeshDistr Source Code subroutine initMeshDistr ( iDistr , oDistr , nm , wload ) C ================================================================== C Computes a new data distribution and the communications needed to C move data from / to the current distribution to the existing ones . C The limits of the new distributions are stored in meshDistr ( oDistr ). C ================================================================== C SUBROUTINE initMeshDistr ( iDistr , oDistr , nm , wload ) C C INPUT : C integer iDistr : Distribution index of the input vector C integer oDistr : The new data distribution index . C integer nm ( 3 ) : Number of Mesh divisions of each cell vector C integer wload : Weights of every point of the mesh using the C input distribution C C OUTPUT : C The output values are stored in the current module : C meshDistr ( oDistr ) C meshCommu ((( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 :( oDistr - 1 ) * oDistr / 2 ) C C BEHAVIOR : C If this is the first distribution , we split the mesh uniformly among C the several processes ( we only split it in dimensions Y and Z ). C C For the other data distributions we should split the vector wload . C The subroutine splitwload will return the limits of the new data C distribution . The subroutine compMeshComm will return the communications C needed to move data from / to the current distribution to / from the C previous ones . C C ================================================================== implicit none C Passed arguments integer , optional , intent ( in ) :: iDistr integer , intent ( in ) :: oDistr integer , intent ( in ) :: nm ( 3 ) integer , optional , intent ( in ) :: wload ( * ) C Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) !------------------------------------------------------------------------- BEGIN call timer ( 'INITMESH' , 1 ) C Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) C Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) C Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) C The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else C In order to compute the other data distributions , we should split C the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) C Precompute the communications needed to move data between the new data C distribution and the previous ones . jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) !--------------------------------------------------------------------------- END end subroutine initMeshDistr","tags":"","loc":"proc/initmeshdistr.html"},{"title":"allocASynBuffer – SIESTA","text":"public subroutine allocASynBuffer(ndistr) Uses mesh proc~~allocasynbuffer~~UsesGraph proc~allocasynbuffer allocASynBuffer mesh mesh proc~allocasynbuffer->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: ndistr Calls proc~~allocasynbuffer~~CallsGraph proc~allocasynbuffer allocASynBuffer proc~boxintersection boxIntersection proc~allocasynbuffer->proc~boxintersection re_alloc re_alloc proc~allocasynbuffer->re_alloc de_alloc de_alloc proc~allocasynbuffer->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocASynBuffer Source Code C SUBROUTINE allocASynBuffer ( ndistr ) C C INPUT : C integer ndistr : Total number of distributions C C OUTPUT : C The output values are stored in the current module : C real ( grid_p ) tBuff1 (:) : Buffer for distribution 1 C real ( grid_p ) tBuff2 (:) : Buffer for other distributions C C BEHAVIOR : C C C ================================================================== subroutine allocASynBuffer ( ndistr ) use mesh , only : nsm implicit none C Input variables integer :: ndistr C Local variables integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters !------------------------------------------------------------------------- BEGIN #ifdef ASYNCHRONOUS C Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) C Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo C Check the size of the intersections between the first data distributions C and the others data distributions . C Buffers don 't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr(1)%box(:,:,node+1) do ii= 2, ndistr box2 => meshDistr(ii)%box(:,:,node+1) call boxIntersection( box1, box2, Lbox, inters ) if (inters) then lsize = (Lbox(2,1)-Lbox(1,1)+1)* &            (Lbox(2,2)-Lbox(1,2)+1)* &            (Lbox(2,3)-Lbox(1,3)+1)*nsp else lsize = 0 endif imax1 = max(imax1,nsize(1)-lsize) imax2 = max(imax2,nsize(ii)-lsize) enddo C     Deallocate local memory call de_alloc( nsize, ' nsize ', moduName ) C     Allocate memory for asynchronous communications call re_alloc( tBuff1, 1, imax1, ' tBuff1 ', moduName ) call re_alloc( tBuff2, 1, imax2, ' tBuff2 ' , moduName ) #endif !--------------------------------------------------------------------------- END end subroutine allocASynBuffer","tags":"","loc":"proc/allocasynbuffer.html"},{"title":"allocExtMeshDistr – SIESTA","text":"public subroutine allocExtMeshDistr(iDistr, nep, mop) Uses mesh proc~~allocextmeshdistr~~UsesGraph proc~allocextmeshdistr allocExtMeshDistr mesh mesh proc~allocextmeshdistr->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nep integer, intent(in) :: mop Calls proc~~allocextmeshdistr~~CallsGraph proc~allocextmeshdistr allocExtMeshDistr re_alloc re_alloc proc~allocextmeshdistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocExtMeshDistr Source Code subroutine allocExtMeshDistr ( iDistr , nep , mop ) use mesh , only : indexp , idop , xdop implicit none C Input variables integer , intent ( in ) :: iDistr , nep , mop C Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % indexp , 1 , nep , 'distr%indexp' , moduName ) call re_alloc ( distr % idop , 1 , mop , 'distr%idop' , moduName ) call re_alloc ( distr % xdop , 1 , 3 , 1 , mop , 'distr%xdop' , moduName ) indexp => distr % indexp idop => distr % idop xdop => distr % xdop end subroutine allocExtMeshDistr","tags":"","loc":"proc/allocextmeshdistr.html"},{"title":"allocIpaDistr – SIESTA","text":"public subroutine allocIpaDistr(iDistr, na) Uses mesh proc~~allocipadistr~~UsesGraph proc~allocipadistr allocIpaDistr mesh mesh proc~allocipadistr->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: na Calls proc~~allocipadistr~~CallsGraph proc~allocipadistr allocIpaDistr re_alloc re_alloc proc~allocipadistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocIpaDistr Source Code subroutine allocIpaDistr ( iDistr , na ) use mesh , only : ipa implicit none C Input variables integer , intent ( in ) :: iDistr , na C Local variables type ( meshDisType ), pointer :: distr !------------------------------------------------------------------------- BEGIN distr => meshDistr ( iDistr ) call re_alloc ( distr % ipa , 1 , na , 'distr%ipa' , moduName ) ipa => meshDistr ( iDistr )% ipa !--------------------------------------------------------------------------- END end subroutine allocIpaDistr","tags":"","loc":"proc/allocipadistr.html"},{"title":"setMeshDistr – SIESTA","text":"public subroutine setMeshDistr(iDistr, nsm, nsp, nml, nmpl, ntml, ntpl) Uses mesh proc~~setmeshdistr~~UsesGraph proc~setmeshdistr setMeshDistr mesh mesh proc~setmeshdistr->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nsm integer, intent(in) :: nsp integer, intent(out) :: nml (3) integer, intent(out) :: nmpl integer, intent(out) :: ntml (3) integer, intent(out) :: ntpl Calls proc~~setmeshdistr~~CallsGraph proc~setmeshdistr setMeshDistr meshlim meshlim proc~setmeshdistr->meshlim Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setMeshDistr Source Code subroutine setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) C ================================================================== C Fixes the new data limits and dimensions of the mesh to those of C the data distribution iDistr . C ================================================================== C SUBROUTINE setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) C C INPUT : C integer iDistr : Distribution index of the input vector C integer nsm : Number of mesh sub - divisions in each direction C integer nsp : Number of sub - points of each mesh point C C OUTPUT : C integer nml ( 3 ) : Local number of Mesh divisions in each cell vector C integer nmpl : Local number of Mesh divisions C integer ntml ( 3 ) : Local number of Mesh points in each cell vector C integer ntpl : Local number of Mesh points C C BEHAVIOR : C Fixes the new data limits and dimensions of the mesh to those of C the data distribution iDistr . C C ================================================================== use mesh , only : meshLim , indexp , ipa , idop , xdop implicit none C Passed arguments integer , intent ( in ) :: iDistr , nsm , nsp integer , intent ( out ) :: nml ( 3 ), nmpl , ntml ( 3 ), ntpl C Local variables type ( meshDisType ), pointer :: distr !------------------------------------------------------------------------- BEGIN distr => meshDistr ( iDistr ) meshLim = distr % box ( 1 : 2 , 1 : 3 , node + 1 ) nml ( 1 ) = ( MeshLim ( 2 , 1 ) - MeshLim ( 1 , 1 )) + 1 nml ( 2 ) = ( MeshLim ( 2 , 2 ) - MeshLim ( 1 , 2 )) + 1 nml ( 3 ) = ( MeshLim ( 2 , 3 ) - MeshLim ( 1 , 3 )) + 1 nmpl = nml ( 1 ) * nml ( 2 ) * nml ( 3 ) ntml = nml * nsm ntpl = nmpl * nsp indexp => distr % indexp idop => distr % idop xdop => distr % xdop ipa => distr % ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr","tags":"","loc":"proc/setmeshdistr.html"},{"title":"resetMeshDistr – SIESTA","text":"public subroutine resetMeshDistr(iDistr) Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Calls proc~~resetmeshdistr~~CallsGraph proc~resetmeshdistr resetMeshDistr de_alloc de_alloc proc~resetmeshdistr->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code resetMeshDistr Source Code subroutine resetMeshDistr ( iDistr ) C ================================================================== C Reset the data of the distribution iDistr C ================================================================== C SUBROUTINE resetMeshDistr ( iDistr ) C C INPUT : C integer iDistr : Distribution index to be reset . C C OUTPUT : C Modify data of the current module . C C BEHAVIOR : C Deallocate associated arrays of the current distribution C ================================================================== implicit none C Passed arguments integer , optional , intent ( in ) :: iDistr C Local variables integer :: idis , ini , fin , icom type ( meshDisType ), pointer :: distr type ( meshCommType ), pointer :: mcomm !------------------------------------------------------------------------- BEGIN if ( present ( iDistr )) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis = ini , fin distr => meshDistr ( idis ) distr % nMesh = 0 if ( associated ( distr % box )) then call de_alloc ( distr % box , 'distr%box' , 'moreMeshSubs' ) endif if ( associated ( distr % indexp )) then call de_alloc ( distr % indexp , 'distr%indexp' , & 'moreMeshSubs' ) endif if ( associated ( distr % idop )) then call de_alloc ( distr % idop , 'distr%idop' , & 'moreMeshSubs' ) endif if ( associated ( distr % xdop )) then call de_alloc ( distr % xdop , 'distr%xdop' , & 'moreMeshSubs' ) endif if ( associated ( distr % ipa )) then call de_alloc ( distr % ipa , 'distr%ipa' , & 'moreMeshSubs' ) endif do icom = 1 , 3 mcomm => exteCommu ( idis , icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo do icom = (( idis - 2 ) * ( idis - 1 )) / 2 + 1 , (( idis - 1 ) * idis ) / 2 mcomm => meshCommu ( icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if ( associated ( tBuff1 )) then call de_alloc ( tBuff1 , 'tBuff1' , 'moreMeshSubs' ) endif if ( associated ( tBuff2 )) then call de_alloc ( tBuff2 , 'tBuff2' , 'moreMeshSubs' ) endif #endif !--------------------------------------------------------------------------- END end subroutine resetMeshDistr","tags":"","loc":"proc/resetmeshdistr.html"},{"title":"distMeshData_rea – SIESTA","text":"private subroutine distMeshData_rea(iDistr, fsrc, oDistr, fdst, itr) Uses mesh mpi_siesta proc~~distmeshdata_rea~~UsesGraph proc~distmeshdata_rea distMeshData_rea mesh mesh proc~distmeshdata_rea->mesh mpi_siesta mpi_siesta proc~distmeshdata_rea->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_rea~~CallsGraph proc~distmeshdata_rea distMeshData_rea write_debug write_debug proc~distmeshdata_rea->write_debug mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier re_alloc re_alloc proc~distmeshdata_rea->re_alloc mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg timer timer proc~distmeshdata_rea->timer de_alloc de_alloc proc~distmeshdata_rea->de_alloc die die proc~distmeshdata_rea->die reord reord proc~distmeshdata_rea->reord proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_rea Source Code subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none C Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) C Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 C The communications are stored in a triangular structure . if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then C From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C We should send data to process dst ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then C From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C We should send data to process dst ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 C From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else C We should send data to process dst ( icom ) - 1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea","tags":"","loc":"proc/distmeshdata_rea.html"},{"title":"distMeshData_rea – SIESTA","text":"private subroutine distMeshData_rea(iDistr, fsrc, oDistr, fdst, itr) Uses mesh mpi_siesta proc~~distmeshdata_rea~2~~UsesGraph proc~distmeshdata_rea~2 distMeshData_rea mesh mesh proc~distmeshdata_rea~2->mesh mpi_siesta mpi_siesta proc~distmeshdata_rea~2->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_rea~2~~CallsGraph proc~distmeshdata_rea~2 distMeshData_rea write_debug write_debug proc~distmeshdata_rea~2->write_debug mpi_barrier mpi_barrier proc~distmeshdata_rea~2->mpi_barrier re_alloc re_alloc proc~distmeshdata_rea~2->re_alloc mpitrace_event mpitrace_event proc~distmeshdata_rea~2->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea~2->nmeshg timer timer proc~distmeshdata_rea~2->timer de_alloc de_alloc proc~distmeshdata_rea~2->de_alloc die die proc~distmeshdata_rea~2->die reord reord proc~distmeshdata_rea~2->reord proc~boxintersection boxIntersection proc~distmeshdata_rea~2->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~distmeshdata_rea~2~~CalledByGraph proc~distmeshdata_rea~2 distMeshData_rea interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_rea~2 Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_rea Source Code subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none C Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) C Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 C The communications are stored in a triangular structure . if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then C From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C We should send data to process dst ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then C From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else C We should send data to process dst ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 C From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else C We should send data to process dst ( icom ) - 1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom C Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea","tags":"","loc":"proc/distmeshdata_rea~2.html"},{"title":"distMeshData_int – SIESTA","text":"private subroutine distMeshData_int(iDistr, fsrc, oDistr, fdst, itr) Uses mpi_siesta proc~~distmeshdata_int~~UsesGraph proc~distmeshdata_int distMeshData_int mpi_siesta mpi_siesta proc~distmeshdata_int->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_int~~CallsGraph proc~distmeshdata_int distMeshData_int die die proc~distmeshdata_int->die proc~boxintersection boxIntersection proc~distmeshdata_int->proc~boxintersection re_alloc re_alloc proc~distmeshdata_int->re_alloc de_alloc de_alloc proc~distmeshdata_int->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~distmeshdata_int~~CalledByGraph proc~distmeshdata_int distMeshData_int interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_int Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_int Source Code subroutine distMeshData_int ( iDistr , fsrc , oDistr , fdst , itr ) #ifdef MPI use mpi_siesta #endif implicit none C Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr integer , intent ( in ) :: fsrc ( * ) integer , intent ( out ) :: fdst ( * ) C Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & ind , ncom , icom , NSRC ( 3 ), NDST ( 3 ), & ME , MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters integer , pointer :: TBUF (:) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif !---------------------------------------------------------------------- BEGIN if ( nodes == 1 ) then call die ( \"Called _int version of distMeshData for n=1\" ) else C The communications are stored in a triangular structure . if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) ME = Node + 1 C Compute the maximum size of the buffer needed to transfer data C among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 NSRC ( 2 ) = Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 NSRC ( 3 ) = Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 Dbox => odis % box (:,:, ME ) NDST ( 1 ) = Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 NDST ( 2 ) = Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 NDST ( 3 ) = Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 if ( itr . eq . 0 ) then C From sequencial to sequencial do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then C SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else C We should send data to process dst ( icom ) - 1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else C We should receive data from process src ( icom ) - 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif endif !--------------------------------------------------------------------------- END end subroutine distMeshData_int","tags":"","loc":"proc/distmeshdata_int.html"},{"title":"boxIntersection – SIESTA","text":"private subroutine boxIntersection(ibox1, ibox2, obox, inters) Arguments Type Intent Optional Attributes Name integer, intent(in) :: ibox1 (2,3) integer, intent(in) :: ibox2 (2,3) integer, intent(out) :: obox (2,3) logical, intent(out) :: inters Called by proc~~boxintersection~~CalledByGraph proc~boxintersection boxIntersection proc~reordmeshnumbering~2 reordMeshNumbering proc~reordmeshnumbering~2->proc~boxintersection proc~splitwload splitwload proc~splitwload->proc~boxintersection proc~compmeshcomm compMeshComm proc~compmeshcomm->proc~boxintersection proc~initmeshextencil initMeshExtencil proc~initmeshextencil->proc~boxintersection proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->proc~boxintersection proc~allocasynbuffer allocASynBuffer proc~allocasynbuffer->proc~boxintersection proc~distmeshdata_int distMeshData_int proc~distmeshdata_int->proc~boxintersection proc~reordmeshnumbering reordMeshNumbering proc~reordmeshnumbering->proc~boxintersection proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->proc~boxintersection proc~gathextmeshdata gathExtMeshData proc~gathextmeshdata->proc~boxintersection proc~distextmeshdata distExtMeshData proc~distextmeshdata->proc~boxintersection interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_int interface~distmeshdata->proc~distmeshdata_rea~2 Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code boxIntersection Source Code C SUBROUTINE boxIntersection ( ibox1 , ibox2 , obox , inters ) C C INPUT : C integer ibox1 ( 2 , 3 ) : Input box 1 C integer ibox2 ( 2 , 3 ) : Input box 2 C C OUTPUT : C integer obox ( 2 , 3 ) : Intersection between ibox1 and ibox2 C logical inters : TRUE : There is intersection C FALSE : There is not intersection C C BEHAVIOR : C Checks the three axis of the input boxes to see if there is C intersection between the input boxes . C C ================================================================== subroutine boxIntersection ( ibox1 , ibox2 , obox , inters ) implicit none C Passed arguments integer , intent ( in ) :: ibox1 ( 2 , 3 ), ibox2 ( 2 , 3 ) integer , intent ( out ) :: obox ( 2 , 3 ) logical , intent ( out ) :: inters C Local variables integer :: iaxis !------------------------------------------------------------------------- BEGIN inters = . true . do iaxis = 1 , 3 obox ( 1 , iaxis ) = max ( ibox1 ( 1 , iaxis ), ibox2 ( 1 , iaxis )) obox ( 2 , iaxis ) = min ( ibox1 ( 2 , iaxis ), ibox2 ( 2 , iaxis )) if ( obox ( 2 , iaxis ). lt . obox ( 1 , iaxis )) inters = . false . enddo !--------------------------------------------------------------------------- END end subroutine boxIntersection","tags":"","loc":"proc/boxintersection.html"},{"title":"initMeshExtencil – SIESTA","text":"public subroutine initMeshExtencil(iDistr, nm) Uses scheComm proc~~initmeshextencil~~UsesGraph proc~initmeshextencil initMeshExtencil scheComm scheComm proc~initmeshextencil->scheComm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nm (3) Calls proc~~initmeshextencil~~CallsGraph proc~initmeshextencil initMeshExtencil schedulecomm schedulecomm proc~initmeshextencil->schedulecomm proc~boxintersection boxIntersection proc~initmeshextencil->proc~boxintersection re_alloc re_alloc proc~initmeshextencil->re_alloc de_alloc de_alloc proc~initmeshextencil->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code initMeshExtencil Source Code subroutine initMeshExtencil ( iDistr , nm ) C ================================================================== C Compute the needed communications in order to send / receive the C extencil ( when the data is ordered in the distribution iDistr ) C ================================================================== C SUBROUTINE initMeshExtencil ( iDistr , nm ) C C INPUT : C integer iDistr : Distribution index to be used . C integer nm ( 3 ) : Number of Mesh divisions in each cell vector C C OUTPUT : C The results are stored in the variable exteCommu ( iDistr , 1 : 3 ) of C the current module . C C BEHAVIOR : C For every dimension of the problem , search all the neightbours that C we have . Given the current data distribution we compute the limits C of our extencil and we check its intersection with all the other C processes . Once we know all our neightbours we call subroutine C scheduleComm in order to minimize the number of communications steps . C C ================================================================== use scheComm implicit none C Passed arguments integer , intent ( in ) :: iDistr , nm ( 3 ) C Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), Ibox ( 2 , 3 ), & ii , iaxis , ncom , Gcom , Lcom , P1 , P2 integer , pointer :: src (:), dst (:), Dbox (:,:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm type ( COMM_T ) :: comm logical :: inters !------------------------------------------------------------------------- BEGIN idis => meshDistr ( iDistr ) do iaxis = 1 , 3 C One communication structure for every dimension mcomm => exteCommu ( iDistr , iaxis ) C Count the number of communications needed to send / receive C the extencil ncom = 0 do P1 = 1 , Nodes C Create the extencil boxes for both sides of the current C partition Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) ncom = ncom + 1 endif enddo enddo Gcom = ncom C Create a list of communications needed to send / receive C the extencil if ( Gcom . gt . 0 ) then nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) ncom = 0 do P1 = 1 , Nodes Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif endif enddo enddo comm % np = Nodes C reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) C Count the number of communications needed by the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom C Store the ordered list of communications needed by the current C process to send / receive the extencil . if ( Lcom . gt . 0 ) then nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , & 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , & 'moreMeshSubs' ) ncom = 0 do P1 = 1 , comm % ncol ii = comm % ind ( P1 , Node + 1 ) if ( ii . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( ii ) mcomm % dst ( ncom ) = dst ( ii ) endif enddo mcomm % ncom = Lcom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) endif call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) endif enddo !--------------------------------------------------------------------------- END end subroutine initMeshExtencil","tags":"","loc":"proc/initmeshextencil.html"},{"title":"distExtMeshData – SIESTA","text":"public subroutine distExtMeshData(iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, DENS, BDENS) Uses mpi_siesta proc~~distextmeshdata~~UsesGraph proc~distextmeshdata distExtMeshData mpi_siesta mpi_siesta proc~distextmeshdata->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: iaxis integer, intent(in) :: BS integer, intent(in) :: NSM integer, intent(in) :: NN integer, intent(in) :: NSPIN integer, intent(in) :: maxp integer, intent(in) :: NMeshG (3) real(kind=gp), intent(in) :: DENS (maxp,NSPIN) real(kind=gp), intent(out) :: BDENS (BS,2*NN,NSPIN) Calls proc~~distextmeshdata~~CallsGraph proc~distextmeshdata distExtMeshData mpi_sendrecv mpi_sendrecv proc~distextmeshdata->mpi_sendrecv re_alloc re_alloc proc~distextmeshdata->re_alloc de_alloc de_alloc proc~distextmeshdata->de_alloc die die proc~distextmeshdata->die proc~boxintersection boxIntersection proc~distextmeshdata->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distExtMeshData Source Code subroutine distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , dens , BDENS ) C ================================================================== C Send / receive the extencil information from the \"dens\" matrix to the C temporal array \"BDENS\" . C ================================================================== C SUBROUTINE distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , C maxp , NMeshG , dens , BDENS ) C C INPUT : C integer iDistr : Distribution index to be used . C integer iaxis : Axe to be splitted C integer BS : Dimmension of a plane in the current axe C integer NSM : Number of mesh sub - divisions in each direction C integer NN : Size of the extencil C integer NSPIN : Number of pollarizations C integer MAXP : Total number of points C integer NMeshG ( 3 ) : Number of Mesh points in each cell vector C real DENS : electron density matrix C C OUTPUT : C real BDENS : Auxiliar arrays to store the extencil from other C partitions . C C BEHAVIOR : C Send / receive the extencil information from the \"dens\" matrix to the C temporal array \"BDENS\" . C C We have a different code for every axis . We should find if we C intersects with a neightbour node throught the upper , the lower C or both sides . C C ================================================================== use mpi_siesta implicit none C Passed arguments integer , intent ( in ) :: iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG ( 3 ) real ( gp ), intent ( in ) :: DENS ( maxp , NSPIN ) real ( gp ), intent ( out ) :: BDENS ( BS , 2 * NN , NSPIN ) C Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) !------------------------------------------------------------------------- BEGIN idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM if (. not . associated ( mcomm % dst )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif if (. not . associated ( mcomm % src )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = dimB ( 2 ) - NN + 1 , dimB ( 2 ) uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ) - NN + 1 , dimB ( 3 ) do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) !--------------------------------------------------------------------------- END end subroutine distExtMeshData","tags":"","loc":"proc/distextmeshdata.html"},{"title":"gathExtMeshData – SIESTA","text":"public subroutine gathExtMeshData(iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, BVXC, VXC) Uses mpi_siesta proc~~gathextmeshdata~~UsesGraph proc~gathextmeshdata gathExtMeshData mpi_siesta mpi_siesta proc~gathextmeshdata->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: iaxis integer, intent(in) :: BS integer, intent(in) :: NSM integer, intent(in) :: NN integer, intent(in) :: NSPIN integer, intent(in) :: maxp integer, intent(in) :: NMeshG (3) real(kind=gp), intent(in) :: BVXC (BS,2*NN,NSPIN) real(kind=gp), intent(out) :: VXC (maxp,NSPIN) Calls proc~~gathextmeshdata~~CallsGraph proc~gathextmeshdata gathExtMeshData mpi_sendrecv mpi_sendrecv proc~gathextmeshdata->mpi_sendrecv proc~boxintersection boxIntersection proc~gathextmeshdata->proc~boxintersection re_alloc re_alloc proc~gathextmeshdata->re_alloc de_alloc de_alloc proc~gathextmeshdata->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code gathExtMeshData Source Code C SUBROUTINE gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , C maxp , NMeshG , BVXC , VXC ) C C INPUT : C integer iDistr : Distribution index to be used . C integer iaxis : Axe to be splitted C integer BS : Dimmension of a plane in the current axe C integer NSM : Number of mesh sub - divisions in each direction C integer NN : Size of the extencil C integer NSPIN : Number of pollarizations C integer MAXP : Total number of points C integer NMeshG ( 3 ) : Number of Mesh points in each cell vector C real BVXC : Auxiliar array that contains the extencil of the C exch - corr potential C C OUTPUT : C real VXC : exch - corr potential C C BEHAVIOR : C Send / receive the extencil information from the \"BVXC\" temporal array C to the array \"VXC\" . C C We have a different code for every axis . We should find if we C intersects with a neightbour node throught the upper , the lower C or both sides . C C ================================================================== subroutine gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , BVXC , VXC ) use mpi_siesta implicit none C Passed arguments integer , intent ( in ) :: iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG ( 3 ) real ( gp ), intent ( in ) :: BVXC ( BS , 2 * NN , NSPIN ) real ( gp ), intent ( out ) :: VXC ( maxp , NSPIN ) C Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) !------------------------------------------------------------------------- BEGIN idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = NN + 1 , 2 * NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) !--------------------------------------------------------------------------- END end subroutine gathExtMeshData","tags":"","loc":"proc/gathextmeshdata.html"},{"title":"splitwload – SIESTA","text":"private subroutine splitwload(Nodes, Node, nm, wload, iDistr, oDistr) Uses mpi_siesta proc~~splitwload~~UsesGraph proc~splitwload splitwload mpi_siesta mpi_siesta proc~splitwload->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes integer, intent(in) :: Node integer, intent(in) :: nm (3) integer, intent(in) :: wload (*) type( meshDisType ), intent(in) :: iDistr type( meshDisType ), intent(out) :: oDistr Calls proc~~splitwload~~CallsGraph proc~splitwload splitwload mpi_recv mpi_recv proc~splitwload->mpi_recv proc~reduce3dto1d reduce3Dto1D proc~splitwload->proc~reduce3dto1d re_alloc re_alloc proc~splitwload->re_alloc mpi_send mpi_send proc~splitwload->mpi_send timer timer proc~splitwload->timer de_alloc de_alloc proc~splitwload->de_alloc mpi_bcast mpi_bcast proc~splitwload->mpi_bcast proc~boxintersection boxIntersection proc~splitwload->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code splitwload Source Code C The subroutine splitwload will return the limits of the new data C distribution . The subroutine compMeshComm will return the communications C needed to move data from / to the current distribution to / from the C previous ones . C C ================================================================== implicit none C Passed arguments integer , optional , intent ( in ) :: iDistr integer , intent ( in ) :: oDistr integer , intent ( in ) :: nm ( 3 ) integer , optional , intent ( in ) :: wload ( * ) C Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) !------------------------------------------------------------------------- BEGIN call timer ( 'INITMESH' , 1 ) C Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) C Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) C Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) C The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else C In order to compute the other data distributions , we should split C the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) C Precompute the communications needed to move data between the new data C distribution and the previous ones . jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) !--------------------------------------------------------------------------- END end subroutine initMeshDistr C ================================================================== C Allocate memory buffers for asynchronous communications . C It does nothing for synchronous communications . C ================================================================== C SUBROUTINE allocASynBuffer ( ndistr ) C C INPUT : C integer ndistr : Total number of distributions C C OUTPUT : C The output values are stored in the current module : C real ( grid_p ) tBuff1 (:) : Buffer for distribution 1 C real ( grid_p ) tBuff2 (:) : Buffer for other distributions C C BEHAVIOR : C C C ================================================================== subroutine allocASynBuffer ( ndistr ) use mesh , only : nsm implicit none C Input variables integer :: ndistr C Local variables integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters !------------------------------------------------------------------------- BEGIN #ifdef ASYNCHRONOUS C Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) C Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo C Check the size of the intersections between the first data distributions C and the others data distributions . C Buffers don 't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr(1)%box(:,:,node+1) do ii= 2, ndistr box2 => meshDistr(ii)%box(:,:,node+1) call boxIntersection( box1, box2, Lbox, inters ) if (inters) then lsize = (Lbox(2,1)-Lbox(1,1)+1)* &            (Lbox(2,2)-Lbox(1,2)+1)* &            (Lbox(2,3)-Lbox(1,3)+1)*nsp else lsize = 0 endif imax1 = max(imax1,nsize(1)-lsize) imax2 = max(imax2,nsize(ii)-lsize) enddo C     Deallocate local memory call de_alloc( nsize, ' nsize ', moduName ) C     Allocate memory for asynchronous communications call re_alloc( tBuff1, 1, imax1, ' tBuff1 ', moduName ) call re_alloc( tBuff2, 1, imax2, ' tBuff2 ', moduName ) #endif !--------------------------------------------------------------------------- END end subroutine allocASynBuffer subroutine allocExtMeshDistr( iDistr, nep, mop ) use mesh, only: indexp, idop, xdop implicit none C     Input variables integer,         intent(in) :: iDistr, nep, mop C     Local variables type(meshDisType),  pointer :: distr distr => meshDistr(iDistr) call re_alloc( distr%indexp, 1, nep, ' distr % indexp ', moduName ) call re_alloc( distr%idop, 1, mop, ' distr % idop ', moduName ) call re_alloc( distr%xdop, 1, 3, 1, mop, ' distr % xdop ', moduName ) indexp => distr%indexp idop   => distr%idop xdop   => distr%xdop end subroutine allocExtMeshDistr subroutine allocIpaDistr( iDistr, na ) use mesh, only: ipa implicit none C     Input variables integer,         intent(in) :: iDistr, na C     Local variables type(meshDisType),  pointer :: distr !------------------------------------------------------------------------- BEGIN distr => meshDistr(iDistr) call re_alloc( distr%ipa, 1, na, ' distr % ipa ', moduName ) ipa => meshDistr(iDistr)%ipa !--------------------------------------------------------------------------- END end subroutine allocIpaDistr subroutine setMeshDistr( iDistr, nsm, nsp, nml, nmpl, ntml, ntpl ) C ================================================================== C Fixes the new data limits and dimensions of the mesh to those of C the data distribution iDistr. C ================================================================== C SUBROUTINE setMeshDistr( iDistr, nsm, nsp, nml, nmpl, ntml, ntpl ) C C INPUT: C integer iDistr  : Distribution index of the input vector C integer nsm     : Number of mesh sub-divisions in each direction C integer nsp     : Number of sub-points of each mesh point C C OUTPUT: C integer nml(3)  : Local number of Mesh divisions in each cell vector C integer nmpl    : Local number of Mesh divisions C integer ntml(3) : Local number of Mesh points in each cell vector C integer ntpl    : Local number of Mesh points C C BEHAVIOR: C Fixes the new data limits and dimensions of the mesh to those of C the data distribution iDistr. C C ================================================================== use mesh, only: meshLim, indexp, ipa, idop, xdop implicit none C     Passed arguments integer,  intent(in) :: iDistr, nsm, nsp integer, intent(out) :: nml(3), nmpl, ntml(3), ntpl C     Local variables type(meshDisType),  pointer :: distr !------------------------------------------------------------------------- BEGIN distr => meshDistr(iDistr) meshLim = distr%box(1:2,1:3,node+1) nml(1) = (MeshLim(2,1)-MeshLim(1,1)) + 1 nml(2) = (MeshLim(2,2)-MeshLim(1,2)) + 1 nml(3) = (MeshLim(2,3)-MeshLim(1,3)) + 1 nmpl   = nml(1)*nml(2)*nml(3) ntml   = nml*nsm ntpl   = nmpl*nsp indexp => distr%indexp idop   => distr%idop xdop   => distr%xdop ipa    => distr%ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr subroutine resetMeshDistr( iDistr ) C ================================================================== C Reset the data of the distribution iDistr C ================================================================== C SUBROUTINE resetMeshDistr( iDistr ) C C INPUT: C integer iDistr   : Distribution index to be reset. C C OUTPUT: C Modify data of the current module. C C BEHAVIOR: C Deallocate associated arrays of the current distribution C ================================================================== implicit none C     Passed arguments integer, optional,  intent(in) :: iDistr C     Local variables integer                        :: idis, ini, fin, icom type(meshDisType),  pointer    :: distr type(meshCommType), pointer    :: mcomm !------------------------------------------------------------------------- BEGIN if (present(iDistr)) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis= ini, fin distr => meshDistr(idis) distr%nMesh = 0 if (associated(distr%box)) then call de_alloc( distr%box, ' distr % box ', ' moreMeshSubs ' ) endif if (associated(distr%indexp)) then call de_alloc( distr%indexp, ' distr % indexp ', &                   ' moreMeshSubs ' ) endif if (associated(distr%idop)) then call de_alloc( distr%idop, ' distr % idop ', &                   ' moreMeshSubs ' ) endif if (associated(distr%xdop)) then call de_alloc( distr%xdop, ' distr % xdop ', &                   ' moreMeshSubs ' ) endif if (associated(distr%ipa)) then call de_alloc( distr%ipa, ' distr % ipa ', &                   ' moreMeshSubs ' ) endif do icom=1, 3 mcomm => exteCommu(idis,icom) if (associated(mcomm%src)) then call de_alloc( mcomm%src, ' mcomm % src ', ' moreMeshSubs ' ) endif if (associated(mcomm%dst)) then call de_alloc( mcomm%dst, ' mcomm % dst ', ' moreMeshSubs ' ) endif mcomm%ncom = 0 enddo do icom= ((idis-2)*(idis-1))/2 + 1, ((idis-1)*idis)/2 mcomm => meshCommu(icom) if (associated(mcomm%src)) then call de_alloc( mcomm%src, ' mcomm % src ', ' moreMeshSubs ' ) endif if (associated(mcomm%dst)) then call de_alloc( mcomm%dst, ' mcomm % dst ', ' moreMeshSubs ' ) endif mcomm%ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if (associated(tBuff1)) then call de_alloc(tBuff1, ' tBuff1 ', ' moreMeshSubs ' ) endif if (associated(tBuff2)) then call de_alloc(tBuff2, ' tBuff2 ', ' moreMeshSubs ' ) endif #endif !--------------------------------------------------------------------------- END end subroutine resetMeshDistr C ================================================================== C Move data from vector fsrc, that uses distribution iDistr, to vector C fdst, that uses distribution oDistr. It also, re-orders a clustered C data array into a sequential one and viceversa. C If this is a sequencial execution, it only reorders the data. C C NOTE: There are two subroutines: one to deal with real data and C the other with integers. Both are called using the same interface. ! ! AG: NOTE that the integer version does NOT have the exact functionality ! of the real version. In particular, the integer version has no provision ! for a \"serial fallback\", and so this case has been trapped. C ================================================================== C SUBROUTINE distMeshData( iDistr, fsrc, oDistr, fdst, itr ) C C INPUT: C integer      iDistr : Distribution index of the input vector. C real/integer fsrc   : Input vector. C integer      oDistr : Distribution index of the output vector. C integer itr         : TRanslation-direction switch C                       ITR=+1 => From clustered to sequential C                       ITR=-1 => From sequential to clustered C                       ITR=0  => Keep the status C C OUTPUT: C real/integer fdst   : Output vector. C C BEHAVIOR: C Check the communications that this process should do to move data C from iDistr to odistr. We have 3 kind of communications (send, receive C and keep on the same node). We have 3 kind of reorderings (clustered to C sequential, sequential to clustered and keep the same ordering). C C For the sequencial code we call subroutine reord C C ================================================================== #ifdef ASYNCHRONOUS subroutine distMeshData_rea( iDistr, fsrc, oDistr, fdst, itr ) use mesh,    only : nsm, nmeshg #ifdef MPI use mpi_siesta #endif implicit none C     Passed arguments integer,         intent(in) :: iDistr, oDistr, itr real(grid_p),    intent(in) :: fsrc(*) real(grid_p),   intent(out) :: fdst(*) C     Local variables integer                     :: i, I1, I2, I3, N1, N2, N3, NN, ind, &                               J1, J2, J3, K1, K2, K3, KS, KR, &                               icom, ncom, nsp, me, nsize, lsize, &                               NSRC(3), NDST(3), Lbox(2,3), ierr, &                               nm(3), status(MPI_Status_Size), &                               Xsize, Ysize, Zsize logical                     :: inters integer,            pointer :: request(:), src(:), dst(:), &                               Sbox(:,:), Dbox(:,:), JS(:) real(grid_p),       pointer :: sBuff(:), rBuff(:) type(meshDisType),  pointer :: idis, odis #ifdef DEBUG call write_debug( ' PRE distMeshData ' ) #endif #ifdef _TRACE_ call MPI_Barrier( MPI_Comm_World, ierr ) call MPItrace_event( 1000, 6 ) #endif call timer( ' COMM_BSC ', 1 ) nm(1:3) = nmeshg(1:3) / nsm if (nodes == 1) then if (itr.gt.0) then ! Note that in reord the first argument is always ! clustered call reord( fsrc, fdst, nm, nsm, TO_SEQUENTIAL ) else if (itr .lt. 0) then call reord( fdst, fsrc, nm, nsm, TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product(nmeshg(1:3)) fdst(1:nsize) = fsrc(1:nsize) endif else  ! nodes > 1 C       The communications are stored in a triangular structure. if (iDistr.gt.oDistr) then ind  = ((iDistr-1)*(iDistr-2))/2 + oDistr ncom = meshCommu(ind)%ncom src => meshCommu(ind)%dst dst => meshCommu(ind)%src else ind = ((oDistr-1)*(oDistr-2))/2 + iDistr ncom = meshCommu(ind)%ncom src => meshCommu(ind)%src dst => meshCommu(ind)%dst endif nullify( request ) call re_alloc( request, 1, ncom, ' request ', ' distmeshdata ' ) idis => meshDistr(iDistr) odis => meshDistr(oDistr) nsp = nsm*nsm*nsm me  = node + 1 nullify( JS ) call re_alloc( JS, 1, nsp, ' JS ', ' distmeshdata ' ) if (iDistr.eq.UNIFORM) then sBuff => tBuff1(:) rBuff => tBuff2(:) else if (oDistr.eq.UNIFORM) then sBuff => tBuff2(:) rBuff => tBuff1(:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die( ' Asynchronous temporal buffer error ' ) endif endif Sbox => idis%box(:,:,ME) NSRC(1) = (Sbox(2,1) - Sbox(1,1) + 1)*nsm NSRC(2) = (Sbox(2,2) - Sbox(1,2) + 1)*nsm NSRC(3) = (Sbox(2,3) - Sbox(1,3) + 1)*nsm Dbox => odis%box(:,:,ME) NDST(1) = (Dbox(2,1) - Dbox(1,1) + 1)*nsm NDST(2) = (Dbox(2,2) - Dbox(1,2) + 1)*nsm NDST(3) = (Dbox(2,3) - Dbox(1,3) + 1)*nsm if (itr.eq.1) then C         From clustered to sequential NN = 1 I3 = 0 DO N3=0, NSM-1 I2 = 0 DO N2= 0, NSM-1 I1 = I2 + I3 DO N1= 0, NSM-1 JS(NN) = I1 NN     = NN + 1 I1     = I1 + 1 ENDDO I2 = I2 + NDST(1) ENDDO I3 = I3 + NDST(1)*NDST(2) ENDDO KS = 1 KR = 1 do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM*NSM K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSP + 1 + J2 + J3 K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1+JS(NN)) = fsrc(J1) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC(1)*NSM*NSM K2 = K2 + NDST(1)*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM K3 = K3 + NDST(1)*NDST(2)*NSM enddo else C               We should send data to process dst(icom)-1 lsize = (Lbox(2,1) - Lbox(1,1) + 1)* &                  (Lbox(2,2) - Lbox(1,2) + 1)* &                  (Lbox(2,3) - Lbox(1,3) + 1)*nsp J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K1 = KS do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSP + 1 + J2 + J3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP sBuff(K1) = fsrc(J1) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC(1)*NSM*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM enddo #ifdef MPI call mpi_isend( sBuff(KS), lsize, MPI_grid_real, &                          dst(icom)-1, 0, MPI_COMM_WORLD, &                          request(icom), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = (Lbox(2,1) - Lbox(1,1) + 1)* &                (Lbox(2,2) - Lbox(1,2) + 1)* &                (Lbox(2,3) - Lbox(1,3) + 1)*nsp #ifdef MPI call mpi_irecv( rBuff(KR), lsize, MPI_grid_real, &                        src(icom)-1, 0, MPI_COMM_WORLD, &                        request(icom), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom= 1, ncom C           Wait for received data and move it to the destination buffer if (src(icom).ne.ME) then Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) #ifdef MPI CALL MPI_WAIT( request(icom), status, ierr ) #endif K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = Lbox(1,2), Lbox(2,2) K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1+JS(NN)) = rBuff(J1) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST(1)*NSM enddo K3 = K3 + NDST(1)*NDST(2)*NSM enddo endif enddo else if (itr.eq.-1) then C         From sequencial to clustered NN = 1 I3 = 0 DO N3=0, NSM-1 I2 = 0 DO N2= 0, NSM-1 I1 = I2 + I3 DO N1= 0, NSM-1 JS(NN) = I1 NN     = NN + 1 I1     = I1 + 1 ENDDO I2 = I2 + NSRC(1) ENDDO I3 = I3 + NSRC(1)*NSRC(2) ENDDO KS = 1 KR = 1 do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 K1 = (Lbox(1,1) - Dbox(1,1))*NSP + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1) = fsrc(J1+JS(NN)) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC(1)*NSM K2 = K2 + NDST(1)*NSM*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM K3 = K3 + NDST(1)*NDST(2)*NSM enddo else C               We should send data to process dst(icom)-1 lsize = (Lbox(2,1) - Lbox(1,1) + 1)* &                  (Lbox(2,2) - Lbox(1,2) + 1)* &                  (Lbox(2,3) - Lbox(1,3) + 1)*nsp J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K1 = KS do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP sBuff(K1) = fsrc(J1+JS(NN)) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC(1)*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM enddo #ifdef MPI call mpi_isend( sBuff(KS), lsize, MPI_grid_real, &                          dst(icom)-1, 0, MPI_COMM_WORLD, &                          request(icom), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = (Lbox(2,1) - Lbox(1,1) + 1)* &                (Lbox(2,2) - Lbox(1,2) + 1)* &                (Lbox(2,3) - Lbox(1,3) + 1)*nsp #ifdef MPI call mpi_irecv( rBuff(KR), lsize, MPI_grid_real, &                        src(icom)-1, 0, MPI_COMM_WORLD, &                        request(icom), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom= 1, ncom C           Wait for received data and move it to the destination buffer if (src(icom).ne.ME) then Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) #ifdef MPI CALL MPI_WAIT( request(icom), status, ierr ) #endif K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM*NSM do I2 = Lbox(1,2), Lbox(2,2) K1 = (Lbox(1,1) - Dbox(1,1))*NSP + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1) = rBuff(J1) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST(1)*NSM*NSM enddo K3 = K3 + NDST(1)*NDST(2)*NSM enddo endif enddo else if (itr.eq.0) then KS = 1 KR = 1 C         From sequencial to sequencial or from clustered to clustered do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) Xsize = (Lbox(2,1)-Lbox(1,1)+1)*NSM Ysize = (Lbox(2,2)-Lbox(1,2)+1)*NSM Zsize = (Lbox(2,3)-Lbox(1,3)+1)*NSM if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = 1, Zsize J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = 1, Ysize J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = 1, Xsize fdst(K1) = fsrc(J1) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC(1) K2 = K2 + NDST(1) enddo J3 = J3 + NSRC(1)*NSRC(2) K3 = K3 + NDST(1)*NDST(2) enddo else C               We should send data to process dst(icom)-1 lsize = Xsize*Ysize*Zsize J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K1 = KS do I3 = 1, Zsize J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM do I2 = 1, Ysize J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 do I1 = 1, Xsize sBuff(K1) = fsrc(J1) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC(1) enddo J3 = J3 + NSRC(1)*NSRC(2) enddo #ifdef MPI call mpi_isend( sBuff(KS), lsize, MPI_grid_real, &                          dst(icom)-1, 0, MPI_COMM_WORLD, &                          request(icom), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = Xsize*Ysize*Zsize #ifdef MPI call mpi_irecv( rBuff(KR), lsize, MPI_grid_real, &                        src(icom)-1, 0, MPI_COMM_WORLD, &                        request(icom), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom= 1, ncom C           Wait for received data and move it to the destination buffer if (src(icom).ne.ME) then Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) Xsize = (Lbox(2,1)-Lbox(1,1)+1)*NSM Ysize = (Lbox(2,2)-Lbox(1,2)+1)*NSM Zsize = (Lbox(2,3)-Lbox(1,3)+1)*NSM #ifdef MPI CALL MPI_WAIT( request(icom), status, ierr ) #endif K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = 1, Zsize K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = 1, Ysize K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = 1, Xsize fdst(K1) = rBuff(J1) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST(1) enddo K3 = K3 + NDST(1)*NDST(2) enddo endif enddo else if (Node.eq.0) then write(*,*)' ERROR : Wrong parameter for function distMeshData ' endif call die() endif call de_alloc( JS,      ' JS ',      ' distmeshdata ' ) call de_alloc( request, ' request ', ' distmeshdata ' ) endif #ifdef _TRACE_ call MPI_Barrier( MPI_Comm_World, ierr ) call MPItrace_event( 1000, 0 ) #endif call timer( ' COMM_BSC ', 2 ) #ifdef DEBUG call write_debug( ' POS distMeshData ' ) #endif end subroutine distMeshData_rea #else /* SYNCHRONOUS communications */ subroutine distMeshData_rea( iDistr, fsrc, oDistr, fdst, itr ) use mesh,    only : nsm, nmeshg #ifdef MPI use mpi_siesta #endif implicit none C     Passed arguments integer,       intent(in) :: iDistr, oDistr, itr real(grid_p),  intent(in) :: fsrc(*) real(grid_p), intent(out) :: fdst(*) C     Local variables character(len=*), parameter :: myName = moduName//' distMeshData ' character(len=*), parameter :: errMsg = myName//' ERROR : ' integer                     :: I1, I2, I3, J1, J2, J3, K1, K2, K3, &                               N1, N2, N3, NN, ind, ncom, &                               icom, NSP, NSRC(3), NDST(3), ME, &                               MaxSize, Xsize, Ysize, Zsize, &                               Lbox(2,3) integer, pointer            :: src(:), dst(:), JS(:), Sbox(:,:), &                               Dbox(:,:) type(meshDisType),  pointer :: idis, odis logical                     :: inters real(grid_p),       pointer :: TBUF(:) integer                     :: nsize, nm(3) #ifdef MPI integer                     :: MPIerror, Status(MPI_Status_Size) #endif !----------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug( ' PRE distMeshData ' ) #endif #ifdef _TRACE_ call MPI_Barrier( MPI_Comm_World, MPIerror ) call MPItrace_event( 1000, 6 ) #endif call timer( ' COMM_BSC ', 1 ) if (nodes == 1) then nm(1:3) = nmeshg(1:3)/nsm if (itr.gt.0) then ! Note that in reord the first argument is always ! clustered call reord( fsrc, fdst, nm, nsm, TO_SEQUENTIAL ) else if (itr .lt. 0) then call reord( fdst, fsrc, nm, nsm, TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product(nmeshg(1:3)) fdst(1:nsize) = fsrc(1:nsize) endif else  ! nodes > 1 C       The communications are stored in a triangular structure. if (iDistr.gt.oDistr) then ind  = ((iDistr-1)*(iDistr-2))/2 + oDistr ncom = meshCommu(ind)%ncom src => meshCommu(ind)%dst dst => meshCommu(ind)%src else ind = ((oDistr-1)*(oDistr-2))/2 + iDistr ncom = meshCommu(ind)%ncom src => meshCommu(ind)%src dst => meshCommu(ind)%dst endif idis => meshDistr(iDistr) odis => meshDistr(oDistr) NSP = NSM*NSM*NSM ME  = Node + 1 nullify( JS ) call re_alloc( JS, 1, NSP, ' JS ', ' moreMeshSubs ' ) C       Compute the maximum size of the buffer needed to transfer data C       among the several processes maxSize = 0 do icom= 1, ncom if (src(icom).ne.dst(icom)) then Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 MaxSize = max(MaxSize,Xsize*Ysize*Zsize) endif enddo MaxSize = MaxSize*nsp if (MaxSize.gt.0) then nullify( TBUF ) call re_alloc( TBUF, 1, MaxSize, ' TBUF ', ' moreMeshSubs ' ) endif Sbox => idis%box(:,:,ME) NSRC(1) = (Sbox(2,1) - Sbox(1,1) + 1)*nsm NSRC(2) = (Sbox(2,2) - Sbox(1,2) + 1)*nsm NSRC(3) = (Sbox(2,3) - Sbox(1,3) + 1)*nsm Dbox => odis%box(:,:,ME) NDST(1) = (Dbox(2,1) - Dbox(1,1) + 1)*nsm NDST(2) = (Dbox(2,2) - Dbox(1,2) + 1)*nsm NDST(3) = (Dbox(2,3) - Dbox(1,3) + 1)*nsm if (itr.eq.1) then C         From clustered to sequential NN = 1 I3 = 0 DO N3=0, NSM-1 I2 = 0 DO N2= 0, NSM-1 I1 = I2 + I3 DO N1= 0, NSM-1 JS(NN) = I1 NN     = NN + 1 I1     = I1 + 1 ENDDO I2 = I2 + NDST(1) ENDDO I3 = I3 + NDST(1)*NDST(2) ENDDO do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM*NSM K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSP + 1 + J2 + J3 K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1+JS(NN)) = fsrc(J1) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC(1)*NSM*NSM K2 = K2 + NDST(1)*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM K3 = K3 + NDST(1)*NDST(2)*NSM enddo else C               We should send data to process dst(icom)-1 J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K1 = 1 do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSP + 1 + J2 + J3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP TBUF(K1) = fsrc(J1) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC(1)*NSM*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM enddo Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 #ifdef MPI call MPI_Send( TBUF, Xsize*Ysize*Zsize*nsp, &                         MPI_grid_real, dst(icom)-1, 1, &                         MPI_Comm_world, MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 #ifdef MPI call mpi_recv( TBUF, Xsize*Ysize*Zsize*nsp, &                       MPI_grid_real, src(icom)-1, 1, &                       MPI_Comm_world, Status, MPIerror ) #endif J1 = 1 K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = Lbox(1,2), Lbox(2,2) K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1+JS(NN)) = TBUF(J1) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST(1)*NSM enddo K3 = K3 + NDST(1)*NDST(2)*NSM enddo endif enddo else if (itr.eq.-1) then C         From sequencial to clustered NN = 1 I3 = 0 DO N3=0, NSM-1 I2 = 0 DO N2= 0, NSM-1 I1 = I2 + I3 DO N1= 0, NSM-1 JS(NN) = I1 NN     = NN + 1 I1     = I1 + 1 ENDDO I2 = I2 + NSRC(1) ENDDO I3 = I3 + NSRC(1)*NSRC(2) ENDDO do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 K1 = (Lbox(1,1) - Dbox(1,1))*NSP + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1) = fsrc(J1+JS(NN)) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC(1)*NSM K2 = K2 + NDST(1)*NSM*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM K3 = K3 + NDST(1)*NDST(2)*NSM enddo else C               We should send data to process dst(icom)-1 J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K1 = 1 do I3 = Lbox(1,3), Lbox(2,3) J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM do I2 = Lbox(1,2), Lbox(2,2) J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP TBUF(K1) = fsrc(J1+JS(NN)) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC(1)*NSM enddo J3 = J3 + NSRC(1)*NSRC(2)*NSM enddo Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 #ifdef MPI call MPI_Send( TBUF, Xsize*Ysize*Zsize*nsp, &                         MPI_grid_real, dst(icom)-1, 1, &                         MPI_Comm_world, MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 #ifdef MPI call mpi_recv( TBUF, Xsize*Ysize*Zsize*nsp, &                       MPI_grid_real, src(icom)-1, 1, &                       MPI_Comm_world, Status, MPIerror ) #endif J1 = 1 K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = Lbox(1,3), Lbox(2,3) K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM*NSM do I2 = Lbox(1,2), Lbox(2,2) K1 = (Lbox(1,1) - Dbox(1,1))*NSP + 1 + K2 + K3 do I1 = Lbox(1,1), Lbox(2,1) DO NN= 1, NSP fdst(K1) = TBUF(J1) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST(1)*NSM*NSM enddo K3 = K3 + NDST(1)*NDST(2)*NSM enddo endif enddo else if (itr.eq.0) then C         From sequencial to sequencial or from clustered to clustered do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) Xsize = (Lbox(2,1) - Lbox(1,1) + 1)*NSM Ysize = (Lbox(2,2) - Lbox(1,2) + 1)*NSM Zsize = (Lbox(2,3) - Lbox(1,3) + 1)*NSM if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = 1, Zsize J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = 1, Ysize J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = 1, Xsize fdst(K1) = fsrc(J1) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC(1) K2 = K2 + NDST(1) enddo J3 = J3 + NSRC(1)*NSRC(2) K3 = K3 + NDST(1)*NDST(2) enddo else C               We should send data to process dst(icom)-1 J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2)*NSM K1 = 1 do I3 = 1, Zsize J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1)*NSM do I2 = 1, Ysize J1 = (Lbox(1,1) - Sbox(1,1))*NSM + 1 + J2 + J3 do I1 = 1, Xsize TBUF(K1) = fsrc(J1) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC(1) enddo J3 = J3 + NSRC(1)*NSRC(2) enddo #ifdef MPI call MPI_Send( TBUF, Xsize*Ysize*Zsize, &                         MPI_grid_real, dst(icom)-1, 1, &                         MPI_Comm_world, MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv( TBUF, Xsize*Ysize*Zsize, &                       MPI_grid_real, src(icom)-1, 1, &                       MPI_Comm_world, Status, MPIerror ) #endif J1 = 1 K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2)*NSM do I3 = 1, Zsize K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1)*NSM do I2 = 1, Ysize K1 = (Lbox(1,1) - Dbox(1,1))*NSM + 1 + K2 + K3 do I1 = 1, Xsize fdst(K1) = TBUF(J1) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST(1) enddo K3 = K3 + NDST(1)*NDST(2) enddo endif enddo else if (Node.eq.0) then write(*,*)' ERROR : Wrong parameter for function distMeshData ' endif call die() endif if (MaxSize.gt.0) then call de_alloc( TBUF, ' TBUF ', ' moreMeshSubs ' ) endif call de_alloc( JS, ' JS ', ' moreMeshSubs ' ) endif #ifdef _TRACE_ call MPI_Barrier( MPI_Comm_World, MPIerror ) call MPItrace_event( 1000, 0 ) #endif call timer( ' COMM_BSC ', 2 ) #ifdef DEBUG call write_debug( ' POS distMeshData ' ) #endif !------------------------------------------------------------------------ END end subroutine distMeshData_rea #endif subroutine distMeshData_int( iDistr, fsrc, oDistr, fdst, itr ) #ifdef MPI use mpi_siesta #endif implicit none C     Passed arguments integer,  intent(in) :: iDistr, oDistr, itr integer,  intent(in) :: fsrc(*) integer, intent(out) :: fdst(*) C     Local variables character(len=*), parameter :: myName = moduName//' distMeshData ' character(len=*), parameter :: errMsg = myName//' ERROR : ' integer                     :: I1, I2, I3, J1, J2, J3, K1, K2, K3, &                               ind, ncom, icom, NSRC(3), NDST(3), &                               ME, MaxSize, Xsize, Ysize, Zsize, &                               Lbox(2,3) integer, pointer            :: src(:), dst(:), Sbox(:,:), &                               Dbox(:,:) type(meshDisType),  pointer :: idis, odis logical                     :: inters integer,            pointer :: TBUF(:) #ifdef MPI integer                     :: MPIerror, Status(MPI_Status_Size) #endif !---------------------------------------------------------------------- BEGIN if (nodes == 1) then call die(\"Called _int version of distMeshData for n=1\") else C       The communications are stored in a triangular structure. if (iDistr.gt.oDistr) then ind  = ((iDistr-1)*(iDistr-2))/2 + oDistr ncom = meshCommu(ind)%ncom src => meshCommu(ind)%dst dst => meshCommu(ind)%src else ind = ((oDistr-1)*(oDistr-2))/2 + iDistr ncom = meshCommu(ind)%ncom src => meshCommu(ind)%src dst => meshCommu(ind)%dst endif idis => meshDistr(iDistr) odis => meshDistr(oDistr) ME  = Node + 1 C       Compute the maximum size of the buffer needed to transfer data C       among the several processes maxSize = 0 do icom= 1, ncom if (src(icom).ne.dst(icom)) then Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 MaxSize = max(MaxSize,Xsize*Ysize*Zsize) endif enddo if (MaxSize.gt.0) then nullify( TBUF ) call re_alloc( TBUF, 1, MaxSize, ' TBUF ', ' moreMeshSubs ' ) endif Sbox => idis%box(:,:,ME) NSRC(1) = Sbox(2,1) - Sbox(1,1) + 1 NSRC(2) = Sbox(2,2) - Sbox(1,2) + 1 NSRC(3) = Sbox(2,3) - Sbox(1,3) + 1 Dbox => odis%box(:,:,ME) NDST(1) = Dbox(2,1) - Dbox(1,1) + 1 NDST(2) = Dbox(2,2) - Dbox(1,2) + 1 NDST(3) = Dbox(2,3) - Dbox(1,3) + 1 if (itr.eq.0) then C         From sequencial to sequencial do icom= 1, ncom Sbox => idis%box(:,:,src(icom)) Dbox => odis%box(:,:,dst(icom)) call boxIntersection( Sbox, Dbox, Lbox, inters ) Xsize = Lbox(2,1) - Lbox(1,1) + 1 Ysize = Lbox(2,2) - Lbox(1,2) + 1 Zsize = Lbox(2,3) - Lbox(1,3) + 1 if (src(icom).eq.ME) then if (dst(icom).eq.ME) then C               SRC and DST are the current process J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2) K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2) do I3 = 1, Zsize J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1) K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1) do I2 = 1, Ysize J1 = Lbox(1,1) - Sbox(1,1) + 1 + J2 + J3 K1 = Lbox(1,1) - Dbox(1,1) + 1 + K2 + K3 do I1 = 1, Xsize fdst(K1) = fsrc(J1) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC(1) K2 = K2 + NDST(1) enddo J3 = J3 + NSRC(1)*NSRC(2) K3 = K3 + NDST(1)*NDST(2) enddo else C               We should send data to process dst(icom)-1 J3 = (Lbox(1,3) - Sbox(1,3))*NSRC(1)*NSRC(2) K1 = 1 do I3 = 1, Zsize J2 = (Lbox(1,2) - Sbox(1,2))*NSRC(1) do I2 = 1, Ysize J1 = Lbox(1,1) - Sbox(1,1) + 1 + J2 + J3 do I1 = 1, Xsize TBUF(K1) = fsrc(J1) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC(1) enddo J3 = J3 + NSRC(1)*NSRC(2) enddo #ifdef MPI call MPI_Send( TBUF, Xsize*Ysize*Zsize, &                         MPI_Integer, dst(icom)-1, 1, &                         MPI_Comm_world, MPIerror ) #endif endif else C             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv( TBUF, Xsize*Ysize*Zsize, &                       MPI_Integer, src(icom)-1, 1, &                       MPI_Comm_world, Status, MPIerror ) #endif J1 = 1 K3 = (Lbox(1,3) - Dbox(1,3))*NDST(1)*NDST(2) do I3 = 1, Zsize K2 = (Lbox(1,2) - Dbox(1,2))*NDST(1) do I2 = 1, Ysize K1 = Lbox(1,1) - Dbox(1,1) + 1 + K2 + K3 do I1 = 1, Xsize fdst(K1) = TBUF(J1) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST(1) enddo K3 = K3 + NDST(1)*NDST(2) enddo endif enddo else if (Node.eq.0) then write(*,*)' ERROR : Wrong parameter for function distMeshData ' endif call die() endif if (MaxSize.gt.0) then call de_alloc( TBUF, ' TBUF ', ' moreMeshSubs ' ) endif endif !--------------------------------------------------------------------------- END end subroutine distMeshData_int C ================================================================== C Checks if there is an intersection between 2 boxes and, if it exist C it returns the resulting box. C ================================================================== C SUBROUTINE boxIntersection( ibox1, ibox2, obox, inters ) C C INPUT: C integer ibox1(2,3)       : Input box 1 C integer ibox2(2,3)       : Input box 2 C C OUTPUT: C integer obox(2,3)        : Intersection between ibox1 and ibox2 C logical inters           : TRUE: There is intersection C                            FALSE: There is not intersection C C BEHAVIOR: C Checks the three axis of the input boxes to see if there is C intersection between the input boxes. C C ================================================================== subroutine boxIntersection( ibox1, ibox2, obox, inters ) implicit none C     Passed arguments integer,  intent(in) :: ibox1(2,3), ibox2(2,3) integer, intent(out) :: obox(2,3) logical, intent(out) :: inters C     Local variables integer              :: iaxis !------------------------------------------------------------------------- BEGIN inters = .true. do iaxis= 1, 3 obox(1,iaxis) = max(ibox1(1,iaxis),ibox2(1,iaxis)) obox(2,iaxis) = min(ibox1(2,iaxis),ibox2(2,iaxis)) if (obox(2,iaxis).lt.obox(1,iaxis)) inters = .false. enddo !--------------------------------------------------------------------------- END end subroutine boxIntersection #ifdef MPI subroutine initMeshExtencil( iDistr, nm ) C ================================================================== C Compute the needed communications in order to send/receive the C extencil (when the data is ordered in the distribution iDistr) C ================================================================== C SUBROUTINE initMeshExtencil( iDistr, nm ) C C INPUT: C integer iDistr   : Distribution index to be used. C integer nm(3)    : Number of Mesh divisions in each cell vector C C OUTPUT: C The results are stored in the variable exteCommu(iDistr,1:3) of C the current module. C C BEHAVIOR: C For every dimension of the problem, search all the neightbours that C we have. Given the current data distribution we compute the limits C of our extencil and we check its intersection with all the other C processes. Once we know all our neightbours we call subroutine C scheduleComm in order to minimize the number of communications steps. C C ================================================================== use scheComm implicit none C     Passed arguments integer,  intent(in)        :: iDistr, nm(3) C     Local variables integer                     :: Ubox(2,3), Lbox(2,3), Ibox(2,3), &                               ii, iaxis, ncom, Gcom, Lcom, P1, P2 integer,            pointer :: src(:), dst(:), Dbox(:,:) type(meshDisType),  pointer :: idis type(meshCommType), pointer :: mcomm type(COMM_T)                :: comm logical                     :: inters !------------------------------------------------------------------------- BEGIN idis => meshDistr(iDistr) do iaxis=1, 3 C       One communication structure for every dimension mcomm => exteCommu(iDistr,iaxis) C       Count the number of communications needed to send/receive C       the extencil ncom = 0 do P1= 1, Nodes C         Create the extencil boxes for both sides of the current C         partition Ubox(1:2,1:3) = idis%box(1:2,1:3,P1) Ubox(1,iaxis) = Ubox(1,iaxis) - 1 if (Ubox(1,iaxis).lt.1) Ubox(1,iaxis) = nm(iaxis) Ubox(2,iaxis)   = Ubox(1,iaxis) Lbox(1:2,1:3) = idis%box(1:2,1:3,P1) Lbox(2,iaxis) = Lbox(2,iaxis) + 1 if (Lbox(2,iaxis).gt.nm(iaxis)) Lbox(2,iaxis) = 1 Lbox(1,iaxis) = Lbox(2,iaxis) do P2= P1+1, Nodes Dbox => idis%box(:,:,P2) call boxIntersection( Dbox, Ubox, Ibox, inters ) if (inters) then ncom = ncom + 1 else call boxIntersection( Dbox, Lbox, Ibox, inters ) if (inters) ncom = ncom + 1 endif enddo enddo Gcom = ncom C       Create a list of communications needed to send/receive C       the extencil if (Gcom.gt.0) then nullify( src, dst ) call re_alloc( src, 1, Gcom, ' src ', ' moreMeshSubs ' ) call re_alloc( dst, 1, Gcom, ' dst ', ' moreMeshSubs ' ) ncom = 0 do P1= 1, Nodes Ubox(1:2,1:3) = idis%box(1:2,1:3,P1) Ubox(1,iaxis) = Ubox(1,iaxis) - 1 if (Ubox(1,iaxis).lt.1) Ubox(1,iaxis) = nm(iaxis) Ubox(2,iaxis)   = Ubox(1,iaxis) Lbox(1:2,1:3) = idis%box(1:2,1:3,P1) Lbox(2,iaxis) = Lbox(2,iaxis) + 1 if (Lbox(2,iaxis).gt.nm(iaxis)) Lbox(2,iaxis) = 1 Lbox(1,iaxis) = Lbox(2,iaxis) do P2= P1+1, Nodes Dbox => idis%box(:,:,P2) call boxIntersection( Dbox, Ubox, Ibox, inters ) if (inters) then ncom = ncom + 1 src(ncom) = P1 dst(ncom) = P2 else call boxIntersection( Dbox, Lbox, Ibox, inters ) if (inters) then ncom = ncom + 1 src(ncom) = P1 dst(ncom) = P2 endif endif enddo enddo comm%np = Nodes C         reschedule the communications in order to minimize the time call scheduleComm( Gcom, src, dst, comm ) C         Count the number of communications needed by the current process ncom = 0 do P1= 1, comm%ncol if (comm%ind(P1,Node+1).ne.0) ncom = ncom + 1 enddo Lcom = ncom C         Store the ordered list of communications needed by the current C         process to send/receive the extencil. if (Lcom.gt.0) then nullify( mcomm%src, mcomm%dst ) call re_alloc( mcomm%src, 1, Lcom, ' mcomm % src ', &                     ' moreMeshSubs ' ) call re_alloc( mcomm%dst, 1, Lcom, ' mcomm % dst ', &                     ' moreMeshSubs ' ) ncom = 0 do P1= 1, comm%ncol ii = comm%ind(P1,Node+1) if (ii.ne.0) then ncom            = ncom + 1 mcomm%src(ncom) = src(ii) mcomm%dst(ncom) = dst(ii) endif enddo mcomm%ncom = Lcom call de_alloc( comm%ind, ' comm % ind ', ' scheComm ' ) endif call de_alloc( dst, ' dst ', ' moreMeshSubs ' ) call de_alloc( src, ' src ', ' moreMeshSubs ' ) endif enddo !--------------------------------------------------------------------------- END end subroutine initMeshExtencil subroutine distExtMeshData( iDistr, iaxis, BS, NSM, NN, NSPIN, &                            maxp, NMeshG, dens, BDENS ) C ================================================================== C Send/receive the extencil information from the \"dens\" matrix to the C temporal array \"BDENS\". C ================================================================== C SUBROUTINE distExtMeshData( iDistr, iaxis, BS, NSM, NN, NSPIN, C                             maxp, NMeshG, dens, BDENS ) C C INPUT: C integer iDistr    : Distribution index to be used. C integer iaxis     : Axe to be splitted C integer BS        : Dimmension of a plane in the current axe C integer NSM       : Number of mesh sub-divisions in each direction C integer NN        : Size of the extencil C integer NSPIN     : Number of pollarizations C integer MAXP      : Total number of points C integer NMeshG(3) : Number of Mesh points in each cell vector C real    DENS      : electron density matrix C C OUTPUT: C real    BDENS     : Auxiliar arrays to store the extencil from other C                     partitions. C C BEHAVIOR: C Send/receive the extencil information from the \"dens\" matrix to the C temporal array \"BDENS\". C C We have a different code for every axis. We should find if we C intersects with a neightbour node throught the upper, the lower C or both sides. C C ================================================================== use mpi_siesta implicit none C     Passed arguments integer,         intent(in) :: iDistr, iaxis, BS, NSM, NN, NSPIN, &                               maxp, NMeshG(3) real(gp),        intent(in) :: DENS(maxp,NSPIN) real(gp),       intent(out) :: BDENS(BS,2*NN,NSPIN) C     Local variables integer                     :: Ubox(2,3), Lbox(2,3), IUbox(2,3), &                               ILbox(2,3), nm(3), ispin, Cnode, &                               iniX, endX, iniY, endY, iniZ, endZ, &                               ix, iy, iz, tt, uu, dimB(3), ii, PP logical                     :: inter1, inter2 integer,            pointer :: Dbox(:,:) real(gp),           pointer :: SBUF(:), RBUF(:) type(meshDisType),  pointer :: idis type(meshCommType), pointer :: mcomm integer                     :: MPIerror, Status(MPI_Status_Size) !------------------------------------------------------------------------- BEGIN idis    => meshDistr(iDistr) mcomm   => exteCommu(iDistr,iaxis) nm      = NMeshG/NSM Cnode   = Node + 1 dimB(1) = (idis%box(2,1,Cnode)-idis%box(1,1,Cnode)+1)*NSM dimB(2) = (idis%box(2,2,Cnode)-idis%box(1,2,Cnode)+1)*NSM dimB(3) = (idis%box(2,3,Cnode)-idis%box(1,3,Cnode)+1)*NSM if (.not.associated(mcomm%dst)) then write(6,*) ' ERROR : Trying to communicate extencil ', &             ' with an uninitialized mesh distribution ' call die() endif if (.not.associated(mcomm%src)) then write(6,*) ' ERROR : Trying to communicate extencil ', &             ' with an uninitialized mesh distribution ' call die() endif nullify(SBUF,RBUF) call re_alloc( SBUF, 1, BS*NN*nspin, ' SBUF ', ' moreMeshSubs ' ) call re_alloc( RBUF, 1, BS*NN*nspin, ' RBUF ', ' moreMeshSubs ' ) Ubox(1:2,1:3) = idis%box(1:2,1:3,Cnode) Ubox(1,iaxis) = Ubox(1,iaxis) - 1 if (Ubox(1,iaxis).lt.1) Ubox(1,iaxis) = nm(iaxis) Ubox(2,iaxis)   = Ubox(1,iaxis) Lbox(1:2,1:3) = idis%box(1:2,1:3,Cnode) Lbox(2,iaxis) = Lbox(2,iaxis) + 1 if (Lbox(2,iaxis).gt.nm(iaxis)) Lbox(2,iaxis) = 1 Lbox(1,iaxis) = Lbox(2,iaxis) do ii= 1, mcomm%ncom if (Cnode.eq.mcomm%src(ii)) then PP = mcomm%dst(ii) else PP = mcomm%src(ii) endif Dbox => idis%box(:,:,PP) call boxIntersection( Dbox, Ubox, IUbox, inter1 ) call boxIntersection( Dbox, Lbox, ILbox, inter2 ) if (inter1) then if (iaxis.eq.1) then iniX = 1 endX = NN iniY = (IUbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (IUbox(2,2)-idis%box(1,2,Cnode)+1)*NSM iniZ = (IUbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (IUbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = dens(uu,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter2) then tt = 0 do ispin= 1, nspin do iz= iniZ, endZ uu = (iz-1)*dimB(2)+iniY do iy= iniY, endY do ix= 1, NN tt = tt + 1 BDENS(uu,NN+ix,ispin) = RBUF(tt) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= iniZ, endZ uu = (iz-1)*dimB(2)+iniY do iy= iniY, endY do ix= NN, 1, -1 tt = tt + 1 BDENS(uu,ix,ispin) = RBUF(tt) enddo uu = uu + 1 enddo enddo enddo endif else if (iaxis.eq.2) then iniX = (IUbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (IUbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniZ = (IUbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (IUbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= 1, NN uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = dens(uu,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter2) then tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= 1, NN uu = (iz-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,NN+iy,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= NN, 1, -1 uu = (iz-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,iy,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif else iniX = (IUbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (IUbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniY = (IUbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (IUbox(2,2)-idis%box(1,2,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= 1, NN do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = dens(uu,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter2) then tt = 0 do ispin= 1, nspin do iz= 1, NN do iy= iniY, endY uu = (iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,NN+iz,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= NN, 1, -1 do iy= iniY, endY uu = (iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,iz,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif endif endif if (inter2) then if (iaxis.eq.1) then iniX = dimB(1)-NN+1 endX = dimB(1) iniY = (ILbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (ILbox(2,2)-idis%box(1,2,Cnode)+1)*NSM iniZ = (ILbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (ILbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = dens(uu,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter1) then tt = 0 do ispin= 1, nspin do iz= iniZ, endZ uu = (iz-1)*dimB(2)+iniY do iy= iniY, endY do ix= NN, 1, -1 tt = tt + 1 BDENS(uu,ix,ispin) = RBUF(tt) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= iniZ, endZ uu = (iz-1)*dimB(2)+iniY do iy= iniY, endY do ix= 1, NN tt = tt + 1 BDENS(uu,NN+ix,ispin) = RBUF(tt) enddo uu = uu + 1 enddo enddo enddo endif else if (iaxis.eq.2) then iniX = (ILbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (ILbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniZ = (ILbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (ILbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= dimB(2)-NN+1, dimB(2) uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = dens(uu,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter1) then tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= NN, 1, -1 uu = (iz-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,iy,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= iniZ, endZ do iy= 1, NN uu = (iz-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,NN+iy,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif else iniX = (ILbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (ILbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniY = (ILbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (ILbox(2,2)-idis%box(1,2,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= dimB(3)-NN+1, dimB(3) do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = dens(uu,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter1) then tt = 0 do ispin= 1, nspin do iz= NN, 1, -1 do iy= iniY, endY uu = (iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,iz,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= 1, NN do iy= iniY, endY uu = (iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 BDENS(uu,NN+iz,ispin) = RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc( RBUF, ' RBUF ', ' moreMeshSubs ' ) call de_alloc( SBUF, ' SBUF ', ' moreMeshSubs ' ) !--------------------------------------------------------------------------- END end subroutine distExtMeshData C ================================================================== C Send/receive the extencil information from the \"BVXC\" temporal array C to the array \"VXC\". C ================================================================== C SUBROUTINE gathExtMeshData( iDistr, iaxis, BS, NSM, NN, NSPIN, C                             maxp, NMeshG, BVXC, VXC ) C C INPUT: C integer iDistr    : Distribution index to be used. C integer iaxis     : Axe to be splitted C integer BS        : Dimmension of a plane in the current axe C integer NSM       : Number of mesh sub-divisions in each direction C integer NN        : Size of the extencil C integer NSPIN     : Number of pollarizations C integer MAXP      : Total number of points C integer NMeshG(3) : Number of Mesh points in each cell vector C real    BVXC      : Auxiliar array that contains the extencil of the C                     exch-corr potential C C OUTPUT: C real    VXC       : exch-corr potential C C BEHAVIOR: C Send/receive the extencil information from the \"BVXC\" temporal array C to the array \"VXC\". C C We have a different code for every axis. We should find if we C intersects with a neightbour node throught the upper, the lower C or both sides. C C ================================================================== subroutine gathExtMeshData( iDistr, iaxis, BS, NSM, NN, NSPIN, &                            maxp, NMeshG, BVXC, VXC ) use mpi_siesta implicit none C     Passed arguments integer,         intent(in) :: iDistr, iaxis, BS, NSM, NN, NSPIN, &                               maxp, NMeshG(3) real(gp),        intent(in) :: BVXC(BS,2*NN,NSPIN) real(gp),       intent(out) :: VXC(maxp,NSPIN) C     Local variables integer                     :: Ubox(2,3), Lbox(2,3), IUbox(2,3), &                               ILbox(2,3), nm(3), ispin, Cnode, &                               iniX, endX, iniY, endY, iniZ, endZ, &                               ix, iy, iz, tt, uu, dimB(3), ii, PP logical                     :: inter1, inter2 integer,            pointer :: Dbox(:,:) real(gp),           pointer :: SBUF(:), RBUF(:) type(meshDisType),  pointer :: idis type(meshCommType), pointer :: mcomm integer                     :: MPIerror, Status(MPI_Status_Size) !------------------------------------------------------------------------- BEGIN idis    => meshDistr(iDistr) mcomm   => exteCommu(iDistr,iaxis) nm      = NMeshG/NSM Cnode   = Node + 1 dimB(1) = (idis%box(2,1,Cnode)-idis%box(1,1,Cnode)+1)*NSM dimB(2) = (idis%box(2,2,Cnode)-idis%box(1,2,Cnode)+1)*NSM dimB(3) = (idis%box(2,3,Cnode)-idis%box(1,3,Cnode)+1)*NSM nullify(SBUF,RBUF) call re_alloc( SBUF, 1, BS*NN*nspin, ' SBUF ', ' moreMeshSubs ' ) call re_alloc( RBUF, 1, BS*NN*nspin, ' RBUF ', ' moreMeshSubs ' ) Ubox(1:2,1:3) = idis%box(1:2,1:3,Cnode) Ubox(1,iaxis) = Ubox(1,iaxis) - 1 if (Ubox(1,iaxis).lt.1) Ubox(1,iaxis) = nm(iaxis) Ubox(2,iaxis)   = Ubox(1,iaxis) Lbox(1:2,1:3) = idis%box(1:2,1:3,Cnode) Lbox(2,iaxis) = Lbox(2,iaxis) + 1 if (Lbox(2,iaxis).gt.nm(iaxis)) Lbox(2,iaxis) = 1 Lbox(1,iaxis) = Lbox(2,iaxis) do ii= 1, mcomm%ncom if (Cnode.eq.mcomm%src(ii)) then PP = mcomm%dst(ii) else PP = mcomm%src(ii) endif Dbox => idis%box(:,:,PP) call boxIntersection( Dbox, Ubox, IUbox, inter1 ) call boxIntersection( Dbox, Lbox, ILbox, inter2 ) if (inter1) then if (iaxis.eq.1) then iniX = 1 endX = NN iniY = (IUbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (IUbox(2,2)-idis%box(1,2,Cnode)+1)*NSM iniZ = (IUbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (IUbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do ix= 1, NN do iz= iniZ, endZ uu = (iz-1)*dimB(2)+iniY do iy= iniY, endY tt = tt + 1 SBUF(tt) = BVXC(uu,ix,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter2) then tt = 0 do ispin= 1, nspin do ix= dimB(1), dimB(1)-NN+1, -1 do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iniY-1)*dimB(1)+ix do iy= iniY, endY tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + dimB(1) enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do ix=  1, NN do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iniY-1)*dimB(1)+ix do iy= iniY, endY tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + dimB(1) enddo enddo enddo enddo endif else if (iaxis.eq.2) then iniX = (IUbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (IUbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniZ = (IUbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (IUbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iy= 1, NN do iz= iniZ, endZ uu = (iz-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = BVXC(uu,iy,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter2) then tt = 0 do ispin= 1, nspin do iy= dimB(2), dimB(2)-NN+1, -1 do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iy=  1, NN do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif else iniX = (IUbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (IUbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniY = (IUbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (IUbox(2,2)-idis%box(1,2,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= 1, NN do iy= iniY, endY uu = (iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = BVXC(uu,iz,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter2) then tt = 0 do ispin= 1, nspin do iz=  dimB(3), dimB(3)-NN+1, -1 do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz=  1, NN do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif endif endif if (inter2) then if (iaxis.eq.1) then iniX = dimB(1)-NN+1 endX = dimB(1) iniY = (ILbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (ILbox(2,2)-idis%box(1,2,Cnode)+1)*NSM iniZ = (ILbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (ILbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do ix= NN+1, 2*NN do iz= iniZ, endZ uu = (iz-1)*dimB(2)+iniY do iy= iniY, endY tt = tt + 1 SBUF(tt) = BVXC(uu,ix,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter1) then tt = 0 do ispin= 1, nspin do ix=  1, NN do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iniY-1)*dimB(1)+ix do iy= iniY, endY tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + dimB(1) enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do ix= dimB(1), dimB(1)-NN+1, -1 do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iniY-1)*dimB(1)+ix do iy= iniY, endY tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + dimB(1) enddo enddo enddo enddo endif else if (iaxis.eq.2) then iniX = (ILbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (ILbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniZ = (ILbox(1,3)-idis%box(1,3,Cnode))*NSM + 1 endZ = (ILbox(2,3)-idis%box(1,3,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iy= NN+1, 2*NN do iz= iniZ, endZ uu = (iz-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = BVXC(uu,iy,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter1) then tt = 0 do ispin= 1, nspin do iy= 1, NN do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iy= dimB(2), dimB(2)-NN+1, -1 do iz= iniZ, endZ uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif else iniX = (ILbox(1,1)-idis%box(1,1,Cnode))*NSM + 1 endX = (ILbox(2,1)-idis%box(1,1,Cnode)+1)*NSM iniY = (ILbox(1,2)-idis%box(1,2,Cnode))*NSM + 1 endY = (ILbox(2,2)-idis%box(1,2,Cnode)+1)*NSM tt = 0 do ispin= 1, nspin do iz= NN+1, 2*NN do iy= iniY, endY uu = (iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 SBUF(tt) = BVXC(uu,iz,ispin) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv( SBUF, tt, MPI_grid_real, PP-1, 0, &                         RBUF, tt, MPI_grid_real, PP-1, 0, &                         MPI_Comm_world, Status, MPIerror ) if (inter1) then tt = 0 do ispin= 1, nspin do iz=  1, NN do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin= 1, nspin do iz= dimB(3), dimB(3)-NN+1, -1 do iy= iniY, endY uu = (iz-1)*dimB(1)*dimB(2)+(iy-1)*dimB(1)+iniX do ix= iniX, endX tt = tt + 1 VXC(uu,ispin) = VXC(uu,ispin) + RBUF(tt) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc( RBUF, ' RBUF ', ' moreMeshSubs ' ) call de_alloc( SBUF, ' SBUF ', ' moreMeshSubs ' ) !--------------------------------------------------------------------------- END end subroutine gathExtMeshData C ================================================================== C Compute the limits of a new distribution, trying to split the load C of the array \"wload\". We use the nested disection algorithm in C order to split the mesh in the 3 dimensions. C ================================================================== C SUBROUTINE splitwload( Nodes, Node, nm, wload, iDistr, oDistr ) C C INPUT: C integer Nodes            : Total number of nodes C integer Node             : current Process ID (from 1 to Node) C integer NM               : Number of mesh sub-divisions in each direction C integer wload            : Weights of every point of the mesh. C type(meshDisType) iDistr : Input distribution C C OUTPUT: C type(meshDisType) oDistr : Onput distribution C C BEHAVIOR: C We use the nested disection algorithm to split the load associated C to the vector wload among all the processes. The problem is that C every process have a different part of wload. Every time that we want C to split a piece of the mesh, we should find which processors have that C information. C wload is a 3D array. In every iteration of the algorithm we should C decide the direction of the cut. Then we should made a reduction of C this 3-D array to a 1-D array (according to the selected direction). C C ================================================================== subroutine splitwload( Nodes, Node, nm, wload, iDistr, oDistr ) use mpi_siesta implicit none C     Passed arguments integer,      intent(in)      :: Nodes, Node, nm(3), wload(*) type(meshDisType),intent(in)  :: iDistr type(meshDisType),intent(out) :: oDistr C     Local variables character(len=*), parameter :: myName = moduName//' splitwload ' character(len=*), parameter :: errMsg = myName//' ERROR : ' integer                     :: PP, Lbox(2,3), Ldim, &                               QQ, P1, P2, POS, ini integer(i8b),          pointer :: lwload(:), gwload(:), recvB(:) logical                     :: found, inters integer                     :: mGdim, mLdim, nAxis, nms(3) integer(i8b)                   :: h1, h2 integer,            pointer :: PROCS(:) integer                     :: MPIerror, Status(MPI_Status_Size) !------------------------------------------------------------------------- BEGIN call timer( ' SPLOAD ', 1 ) C     At the begining of the algorithm all the mesh is assigned to the C     first node:  oDistr%box(*,*,1) = nm oDistr%box(1,1,1) = 1 oDistr%box(2,1,1) = nm(1) oDistr%box(1,2,1) = 1 oDistr%box(2,2,1) = nm(2) oDistr%box(1,3,1) = 1 oDistr%box(2,3,1) = nm(3) oDistr%box(1:2,1:3,2:Nodes) = 0 nms = nm C     Array PROCS will contain the number of processes that are associated to C     every box. At the begining all the mesh is assigned to process 1, then C     PROCS(1)=Nodes, while the rest are equal to zero nullify( PROCS, lwload, gwload, recvB ) call re_alloc( PROCS, 1, Nodes, ' PROCS ', ' moreMeshSubs ' ) PROCS(1)       = Nodes PROCS(2:Nodes) = 0 found = .true. do while(found) C       Choose the direction to cut the mesh nAxis = 3 if (nms(2).gt.nms(nAxis)) nAxis = 2 if (nms(1).gt.nms(nAxis)) nAxis = 1 nms(nAxis) = (nms(nAxis)+1)/2 C       Check if we still have to keep cutting the mesh found = .false. do PP=Nodes, 1, -1 if (PROCS(PP).GT.1) then C           There are more than one processes associated to the mesh C           of process PP. We are going to split the mesh in two parts C           of p1 and p2 processors. p1 = PROCS(PP)/2 p2 = PROCS(PP) - p1 found = .true. C           Check if the current partition has intersection with the piece of C           mesh that we want to cut. call boxIntersection( oDistr%box(:,:,PP), &                            iDistr%box(:,:,Node), &                            Lbox, inters ) if (Node.eq.PP) then mGdim = oDistr%box(2,nAxis,PP)-oDistr%box(1,nAxis,PP)+1 call re_alloc( gwload, 1, mGdim, ' gwload ',' moreMeshSubs ' ) call re_alloc( recvB, 1, mGdim, ' recvB ', ' moreMeshSubs ' ) endif if (inters) then C             If there is an intersection I should reduce the intersected part C             from a 3-D array to a 1-D array. mLdim = Lbox(2,nAxis) - Lbox(1,nAxis) + 1 call re_alloc( lwload, 1, mLdim, ' lwload ', &                       ' moreMeshSubs ' ) call reduce3Dto1D( nAxis, iDistr%box(:,:,Node), Lbox, &                           wload, lwload ) endif if (Node.eq.PP) then C             If, I' m the process PP I should receive the information from other C processes gwload = 0 do QQ = 1 , Nodes call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, QQ ), & Lbox , inters ) if ( inters ) then Ldim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 ini = Lbox ( 1 , nAxis ) - oDistr % box ( 1 , nAxis , PP ) if ( PP . eq . QQ ) then gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + lwload ( 1 : Ldim ) else call mpi_recv ( recvB , Ldim , MPI_INTEGER8 , QQ - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + recvB ( 1 : Ldim ) endif endif enddo call de_alloc ( recvB , 'recvB' , 'moreMeshSubs' ) C Process PP computes where to cut the mesh call vecBisec ( mGdim , gwload ( 1 : mGdim ), & PROCS ( PP ), POS , h1 , h2 ) call de_alloc ( gwload , 'gwload' , 'moreMeshSubs' ) else if ( inters ) then C If , I 'm not the process PP I should send the information to C             the process PP call MPI_Send( lwload, mLdim, &                       MPI_INTEGER8, PP-1, 1, MPI_Comm_World, &                       MPIerror ) endif if (associated(lwload)) &        call de_alloc(lwload, ' lwload ', ' moreMeshSubs ') C           Process PP send the position of the cut to the rest of processes call MPI_Bcast( pos, 1, MPI_integer, PP-1, &                      MPI_Comm_World, MPIerror ) C           We have splitted the piece of mesh associated to process PP C           in two parts. One would be stored in position PP and the other C           would be stored in position PP+P1 QQ                     = PP + P1 oDistr%box(1:2,1:3,QQ) = oDistr%box(1:2,1:3,PP) pos                    = oDistr%box(1,naxis,QQ) + pos oDistr%box(1,naxis,QQ) = pos oDistr%box(2,naxis,PP) = pos - 1 C           We should actualize the numbers of processes associated to PP and QQ PROCS(PP)              = P1 PROCS(QQ)              = P2 endif enddo enddo call de_alloc( PROCS, ' PROCS ', ' moreMeshSubs ') call timer( ' SPLOAD ' , 2 ) !--------------------------------------------------------------------------- END end subroutine splitwload","tags":"","loc":"proc/splitwload.html"},{"title":"reduce3Dto1D – SIESTA","text":"private subroutine reduce3Dto1D(iaxis, Ibox, Lbox, wload, lwload) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iaxis integer, intent(in) :: Ibox (2,3) integer, intent(in) :: Lbox (2,3) integer, intent(in) :: wload (*) integer(kind=i8b), intent(out) :: lwload (*) Called by proc~~reduce3dto1d~~CalledByGraph proc~reduce3dto1d reduce3Dto1D proc~splitwload splitwload proc~splitwload->proc~reduce3dto1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reduce3Dto1D Source Code C SUBROUTINE reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) C C INPUT : C integer iaxis : Axe to be reduced C integer Ibox ( 2 , 3 ) : Limits of the input array C integer Lbox ( 2 , 3 ) : Limits of the intersection that we want to reduce C integer ( i8b ) wload : 3 - D array that we want to reduce to one of its C dimensions C C OUTPUT : C integer ( i8b ) lwload : 1 - D array . Reduction of the intersected part C of wload . C C BEHAVIOR : C First we compute the 3 dimensions of the input array and the C intersection . We accumulate the values of the intersection into a C 1 - D array . C C IF ( iaxis = 1 ) lwload ( II ) = SUM ( wload ( II , * , * )) C IF ( iaxis = 2 ) lwload ( II ) = SUM ( wload ( * , II , * )) C IF ( iaxis = 3 ) lwload ( II ) = SUM ( wload ( * , * , II )) C C ================================================================== subroutine reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) implicit none C Passed arguments integer , intent ( in ) :: iaxis , Ibox ( 2 , 3 ), Lbox ( 2 , 3 ), wload ( * ) integer ( i8b ), intent ( out ) :: lwload ( * ) C Local variables integer :: Idim ( 3 ), Ldim ( 3 ), ind , ind1 , ind2 , ind3 , & I1 , I2 , I3 !------------------------------------------------------------------------- BEGIN C Dimensions of the input array Idim ( 1 ) = Ibox ( 2 , 1 ) - Ibox ( 1 , 1 ) + 1 Idim ( 2 ) = Ibox ( 2 , 2 ) - Ibox ( 1 , 2 ) + 1 Idim ( 3 ) = Ibox ( 2 , 3 ) - Ibox ( 1 , 3 ) + 1 C Dimensions of the intersection . Ldim ( 1 ) = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ldim ( 2 ) = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Ldim ( 3 ) = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( iaxis . eq . 1 ) then C Reduction into the X - axis lwload ( 1 : Ldim ( 1 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I1 ) = lwload ( I1 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else if ( iaxis . eq . 2 ) then C Reduction into the Y - axis lwload ( 1 : Ldim ( 2 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I2 ) = lwload ( I2 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else C Reduction into the Z - axis lwload ( 1 : Ldim ( 3 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I3 ) = lwload ( I3 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo endif !--------------------------------------------------------------------------- END end subroutine reduce3Dto1D","tags":"","loc":"proc/reduce3dto1d.html"},{"title":"vecBisec – SIESTA","text":"private subroutine vecBisec(nval, values, nparts, pos, h1, h2) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nval integer(kind=i8b), intent(in) :: values (nval) integer, intent(in) :: nparts integer, intent(out) :: pos integer(kind=i8b), intent(out) :: h1 integer(kind=i8b), intent(out) :: h2 Contents Source Code vecBisec Source Code C SUBROUTINE vecBisec ( nval , values , nparts , pos , h1 , h2 ) C C INPUT : C integer nval : Dimension of the input array C integer ( i8b ) values : Input array C integer nparts : Numbers of partitions that we want to make from C the input array ( in this call we only make one cut ). C C OUTPUT : C integer pos : Position of the cut C integer ( i8b ) h1 : Load of the first part C integer ( i8b ) h2 : Load of the second part C C BEHAVIOR : C We want to split array \"values\" in \"nparts\" , but in this call to C vecBisec we are going to make only one cut . First , we split nparts C in two parts : p1 = nparts / 2 and p2 = nparts - p1 . Then we compute the total C load of the array \"values\" ( \"total\" ) and the desired load for the C first part : halfG = ( total * p1 ) / nparts . C C Finally , we try to find the position inside \"values\" where we are C nearer of the the desired solution . C C ================================================================== subroutine vecBisec ( nval , values , nparts , pos , h1 , h2 ) implicit none C Input variables integer , intent ( in ) :: nval , nparts integer ( i8b ), intent ( in ) :: values ( nval ) integer , intent ( out ) :: pos integer ( i8b ), intent ( out ) :: h1 , h2 C Local variables integer :: p1 , p2 , ii integer ( i8b ) :: total , halfG , halfL !------------------------------------------------------------------------- BEGIN if ( nparts . gt . 1 ) then C Split the number of parts in 2 p1 = nparts / 2 p2 = nparts - p1 C Compute the total load of the array total = 0 do ii = 1 , nval total = total + values ( ii ) enddo C Desired load of the first part halfG = ( total * p1 ) / nparts halfL = 0 pos = 0 C Loop until we reach the solution do while ( halfL . lt . halfG ) pos = pos + 1 if ( pos . eq . nval + 1 ) STOP 'ERROR in vecBisec' halfL = halfL + values ( pos ) enddo C Check if the previous position is better than the C current position if (( halfL - values ( pos ) * p2 / nparts ). gt . halfG ) then halfL = halfL - values ( pos ) pos = pos - 1 endif h1 = halfL h2 = total - halfL endif !--------------------------------------------------------------------------- END end subroutine vecBisec","tags":"","loc":"proc/vecbisec.html"},{"title":"reordMeshNumbering – SIESTA","text":"private subroutine reordMeshNumbering(distr1, distr2) Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 type( meshDisType ), intent(out) :: distr2 Calls proc~~reordmeshnumbering~~CallsGraph proc~reordmeshnumbering reordMeshNumbering proc~boxintersection boxIntersection proc~reordmeshnumbering->proc~boxintersection re_alloc re_alloc proc~reordmeshnumbering->re_alloc de_alloc de_alloc proc~reordmeshnumbering->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reordMeshNumbering Source Code C SUBROUTINE reordMeshNumbering ( distr1 , distr2 ) C Given a new distribution , distr2 , reasign each box to the proper C process . We use the following criteria : C - Minimize the number of communications . Data don 't need to C     be communicated if it belongs to the same process in C     different data distributions C C INPUT: C integer distr1  : First distribution C integer distr2  : Second distribution C C OUTPUT: C The output values are stored in the current module: C        meshDistr(distr2)%box(:,:,:) C C BEHAVIOR: C C C ================================================================== subroutine reordMeshNumbering( distr1, distr2 ) implicit none C     Passed arguments type(meshDisType),   intent(in) :: distr1 type(meshDisType),  intent(out) :: distr2 C     Local variables integer                         :: P1, P2, P3, Lbox(2,3), II, I1, &                                   PermI integer,                pointer :: Isiz(:,:), perm(:), weig(:), &                                   invp(:), box(:,:,:)=>null() logical                         :: inters C     Allocate local arrays nullify( Isiz, perm, invp, weig ) call re_alloc( Isiz, 1, Nodes, 1, Nodes, ' Isiz ', ' moreMeshSubs ' ) call re_alloc( perm, 1, Nodes, ' perm ', ' moreMeshSubs ' ) call re_alloc( invp, 1, Nodes, ' invp ', ' moreMeshSubs ' ) call re_alloc( weig, 1, Nodes, ' weig ', ' moreMeshSubs ' ) C     Check the intersections sizes between the two distributions Isiz(1:nodes,1:nodes) = 0 do P1= 1, Nodes II=-1 do P2= 1, Nodes call boxIntersection( distr1%box(:,:,P1), &                          distr2%box(:,:,P2), Lbox, inters ) if (inters) then Isiz(P2,P1) = (Lbox(2,1)-Lbox(1,1)+1)* &                    (Lbox(2,2)-Lbox(1,2)+1)* &                    (Lbox(2,3)-Lbox(1,3)+1) if (Isiz(P2,P1).gt.II) then II = Isiz(P2,P1) I1 = P2 endif endif enddo weig(P1) = II perm(P1) = I1 enddo C     Choose a proper permutation for every row invp(1:Nodes) = 0 do P1= 1, Nodes II = -1 C       Choose the node with higher weight among those not permuted before do P2= 1, Nodes if (perm(P2).gt.0) then if (weig(P2).gt.II) then II = weig(P2) I1 = P2 endif endif enddo C       Save the permutation for this node (a negative number means that C       the node has been permuted. PermI       = perm(I1) invp(PermI) = I1 perm(I1)    = -PermI C       Change the permutation of those nodes who pretend to use the C       permutation permI do P2= 1, Nodes if (perm(P2).eq.PermI) then II= -1 do P3= 1, Nodes if (invp(P3).eq.0 .and. Isiz(P3,P2).gt.II) then II = Isiz(P3,P2) I1 = P3 endif enddo weig(P2) = II perm(P2) = I1 endif enddo enddo call re_alloc( box, 1, 2, 1, 3, 1, Nodes, ' box ', ' moreMeshSubs ' ) box(1:2,1:3,1:Nodes) = distr2%box(1:2,1:3,1:Nodes) do P1= 1, Nodes II = -perm(P1) distr2%box(1:2,1:3,P1) = box(1:2,1:3,II) enddo call de_alloc( box, ' box ', ' moreMeshSubs ' ) call de_alloc( weig, ' weig ', ' moreMeshSubs ' ) call de_alloc( invp, ' invp ', ' moreMeshSubs ' ) call de_alloc( perm, ' perm ', ' moreMeshSubs ' ) call de_alloc( Isiz, ' Isiz ', ' moreMeshSubs ' ) end subroutine reordMeshNumbering","tags":"","loc":"proc/reordmeshnumbering.html"},{"title":"reordMeshNumbering – SIESTA","text":"private subroutine reordMeshNumbering(distr1, distr2) Uses fdf proc~~reordmeshnumbering~2~~UsesGraph proc~reordmeshnumbering~2 reordMeshNumbering fdf fdf proc~reordmeshnumbering~2->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 type( meshDisType ), intent(out) :: distr2 Calls proc~~reordmeshnumbering~2~~CallsGraph proc~reordmeshnumbering~2 reordMeshNumbering fdf_get fdf_get proc~reordmeshnumbering~2->fdf_get re_alloc re_alloc proc~reordmeshnumbering~2->re_alloc myqsort myqsort proc~reordmeshnumbering~2->myqsort de_alloc de_alloc proc~reordmeshnumbering~2->de_alloc proc~boxintersection boxIntersection proc~reordmeshnumbering~2->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reordMeshNumbering Source Code C SUBROUTINE reordMeshNumbering ( distr1 , distr2 ) C Given a new distribution , distr2 , reasign each box to the proper C process . We use the following criteria : C - Minimize the number of communications . Data don 't need to C     be communicated if it belongs to the same process in C     different data distributions C C INPUT: C integer distr1  : First distribution C integer distr2  : Second distribution C C OUTPUT: C The output values are stored in the current module: C        meshDistr(distr2)%box(:,:,:) C C BEHAVIOR: C C C ================================================================== subroutine reordMeshNumbering( distr1, distr2 ) implicit none C     Passed arguments type(meshDisType),   intent(in) :: distr1 type(meshDisType),  intent(out) :: distr2 C     Local variables integer                         :: P1, P2, P3, Lbox(2,3), II, I1, &                                   PermI integer,                pointer :: Isiz(:,:), perm(:), weig(:), &                                   invp(:), box(:,:,:)=>null() logical                         :: inters C     Allocate local arrays nullify( Isiz, perm, invp, weig ) call re_alloc( Isiz, 1, Nodes, 1, Nodes, ' Isiz ', ' moreMeshSubs ' ) call re_alloc( perm, 1, Nodes, ' perm ', ' moreMeshSubs ' ) call re_alloc( invp, 1, Nodes, ' invp ', ' moreMeshSubs ' ) call re_alloc( weig, 1, Nodes, ' weig ', ' moreMeshSubs ' ) C     Check the intersections sizes between the two distributions Isiz(1:nodes,1:nodes) = 0 do P1= 1, Nodes II=-1 do P2= 1, Nodes call boxIntersection( distr1%box(:,:,P1), &                          distr2%box(:,:,P2), Lbox, inters ) if (inters) then Isiz(P2,P1) = (Lbox(2,1)-Lbox(1,1)+1)* &                    (Lbox(2,2)-Lbox(1,2)+1)* &                    (Lbox(2,3)-Lbox(1,3)+1) if (Isiz(P2,P1).gt.II) then II = Isiz(P2,P1) I1 = P2 endif endif enddo weig(P1) = II perm(P1) = I1 enddo C     Choose a proper permutation for every row invp(1:Nodes) = 0 do P1= 1, Nodes II = -1 C       Choose the node with higher weight among those not permuted before do P2= 1, Nodes if (perm(P2).gt.0) then if (weig(P2).gt.II) then II = weig(P2) I1 = P2 endif endif enddo C       Save the permutation for this node (a negative number means that C       the node has been permuted. PermI       = perm(I1) invp(PermI) = I1 perm(I1)    = -PermI C       Change the permutation of those nodes who pretend to use the C       permutation permI do P2= 1, Nodes if (perm(P2).eq.PermI) then II= -1 do P3= 1, Nodes if (invp(P3).eq.0 .and. Isiz(P3,P2).gt.II) then II = Isiz(P3,P2) I1 = P3 endif enddo weig(P2) = II perm(P2) = I1 endif enddo enddo call re_alloc( box, 1, 2, 1, 3, 1, Nodes, ' box ', ' moreMeshSubs ' ) box(1:2,1:3,1:Nodes) = distr2%box(1:2,1:3,1:Nodes) do P1= 1, Nodes II = -perm(P1) distr2%box(1:2,1:3,P1) = box(1:2,1:3,II) enddo call de_alloc( box, ' box ', ' moreMeshSubs ' ) call de_alloc( weig, ' weig ', ' moreMeshSubs ' ) call de_alloc( invp, ' invp ', ' moreMeshSubs ' ) call de_alloc( perm, ' perm ', ' moreMeshSubs ' ) call de_alloc( Isiz, ' Isiz ', ' moreMeshSubs ' ) end subroutine reordMeshNumbering","tags":"","loc":"proc/reordmeshnumbering~2.html"},{"title":"compMeshComm – SIESTA","text":"private subroutine compMeshComm(distr1, distr2, mcomm) Uses scheComm proc~~compmeshcomm~~UsesGraph proc~compmeshcomm compMeshComm scheComm scheComm proc~compmeshcomm->scheComm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 type( meshDisType ), intent(in) :: distr2 type( meshCommType ), intent(out) :: mcomm Calls proc~~compmeshcomm~~CallsGraph proc~compmeshcomm compMeshComm schedulecomm schedulecomm proc~compmeshcomm->schedulecomm proc~boxintersection boxIntersection proc~compmeshcomm->proc~boxintersection re_alloc re_alloc proc~compmeshcomm->re_alloc de_alloc de_alloc proc~compmeshcomm->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compMeshComm Source Code C SUBROUTINE compMeshComm ( distr1 , distr2 , mcomm ) C C INPUT : C meshDisType distr1 : Source distribution C meshDisType distr2 : Destiny distribution C C OUTPUT : C meshCommType mcomm : Communications needed C C BEHAVIOR : C Count the number of intersections between the source distribution C and the destiny distribution . Every intersection represents a C communication . Then we call scheduleComm to optime the order of these C communications . Finally , we save the communications that belongs to C the current process in the variable \"mcomm\" C C ================================================================== subroutine compMeshComm ( distr1 , distr2 , mcomm ) use scheComm implicit none C Passed arguments type ( meshDisType ), intent ( in ) :: distr1 , distr2 type ( meshCommType ), intent ( out ) :: mcomm C Local variables integer :: P1 , P2 , ncom , Gcom , Lcom , & Lind , Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:) logical :: inters type ( COMM_T ) :: comm !------------------------------------------------------------------------- BEGIN C count the number of intersections between Source distribution and C destiny distribution . Every intersection represents a communication . ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) ncom = ncom + 1 enddo enddo Gcom = ncom C Allocate local arrays nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) C Make a list of communications ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif enddo enddo comm % np = Nodes C reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) C Count the number of communications of the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom C Allocate memory to store data of the communications of the C current process . nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , 'moreMeshSubs' ) C Save the list of communications for the current process ncom = 0 do P1 = 1 , comm % ncol Lind = comm % ind ( P1 , Node + 1 ) if ( Lind . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( Lind ) mcomm % dst ( ncom ) = dst ( Lind ) endif enddo mcomm % ncom = ncom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) !--------------------------------------------------------------------------- END end subroutine compMeshComm","tags":"","loc":"proc/compmeshcomm.html"},{"title":"distMeshData – SIESTA","text":"public interface distMeshData Calls interface~~distmeshdata~~CallsGraph interface~distmeshdata distMeshData proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea~2 distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea~2 re_alloc re_alloc proc~distmeshdata_int->re_alloc de_alloc de_alloc proc~distmeshdata_int->de_alloc die die proc~distmeshdata_int->die proc~boxintersection boxIntersection proc~distmeshdata_int->proc~boxintersection write_debug write_debug proc~distmeshdata_rea~2->write_debug mpi_barrier mpi_barrier proc~distmeshdata_rea~2->mpi_barrier proc~distmeshdata_rea~2->re_alloc mpitrace_event mpitrace_event proc~distmeshdata_rea~2->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea~2->nmeshg timer timer proc~distmeshdata_rea~2->timer proc~distmeshdata_rea~2->de_alloc proc~distmeshdata_rea~2->die reord reord proc~distmeshdata_rea~2->reord proc~distmeshdata_rea~2->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures distMeshData_rea distMeshData_int Module Procedures private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr","tags":"","loc":"interface/distmeshdata.html"},{"title":"compute_dm – SIESTA","text":"public subroutine compute_dm(iscf) Uses precision units siesta_options class_dSpData1D sparse_matrices siesta_geom atomlist sys Kpoint_grid m_energies m_energies m_rmaxh m_eo m_spin m_diagon m_gamma parallel parallel m_compute_ebs_shift m_pexsi_solver m_hsx mpi_siesta iodmhs_netcdf m_dminim m_zminim m_ordern m_steps m_normalize_dm m_chess m_energies m_ts_global_vars m_transiesta proc~~compute_dm~~UsesGraph proc~compute_dm compute_dm m_diagon m_diagon proc~compute_dm->m_diagon atomlist atomlist proc~compute_dm->atomlist parallel parallel proc~compute_dm->parallel m_normalize_dm m_normalize_dm proc~compute_dm->m_normalize_dm sys sys proc~compute_dm->sys class_dSpData1D class_dSpData1D proc~compute_dm->class_dSpData1D Kpoint_grid Kpoint_grid proc~compute_dm->Kpoint_grid m_hsx m_hsx proc~compute_dm->m_hsx mpi_siesta mpi_siesta proc~compute_dm->mpi_siesta m_eo m_eo proc~compute_dm->m_eo units units proc~compute_dm->units sparse_matrices sparse_matrices proc~compute_dm->sparse_matrices m_dminim m_dminim proc~compute_dm->m_dminim siesta_options siesta_options proc~compute_dm->siesta_options m_chess m_chess proc~compute_dm->m_chess precision precision proc~compute_dm->precision m_zminim m_zminim proc~compute_dm->m_zminim m_rmaxh m_rmaxh proc~compute_dm->m_rmaxh m_pexsi_solver m_pexsi_solver proc~compute_dm->m_pexsi_solver m_ordern m_ordern proc~compute_dm->m_ordern m_energies m_energies proc~compute_dm->m_energies iodmhs_netcdf iodmhs_netcdf proc~compute_dm->iodmhs_netcdf m_gamma m_gamma proc~compute_dm->m_gamma m_steps m_steps proc~compute_dm->m_steps siesta_geom siesta_geom proc~compute_dm->siesta_geom m_compute_ebs_shift m_compute_ebs_shift proc~compute_dm->m_compute_ebs_shift m_spin m_spin proc~compute_dm->m_spin m_transiesta m_transiesta proc~compute_dm->m_transiesta m_ts_global_vars m_ts_global_vars proc~compute_dm->m_ts_global_vars Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~compute_dm~~CallsGraph proc~compute_dm compute_dm escf escf proc~compute_dm->escf bye bye proc~compute_dm->bye dold dold proc~compute_dm->dold compute_ebs_shift compute_ebs_shift proc~compute_dm->compute_ebs_shift write_hs_formatted write_hs_formatted proc~compute_dm->write_hs_formatted pexsi_solver pexsi_solver proc~compute_dm->pexsi_solver write_orb_indx write_orb_indx proc~compute_dm->write_orb_indx timer timer proc~compute_dm->timer ordern ordern proc~compute_dm->ordern dscf dscf proc~compute_dm->dscf normalize_dm normalize_dm proc~compute_dm->normalize_dm mulliken mulliken proc~compute_dm->mulliken dminim dminim proc~compute_dm->dminim mpi_bcast mpi_bcast proc~compute_dm->mpi_bcast transiesta transiesta proc~compute_dm->transiesta moments moments proc~compute_dm->moments eold eold proc~compute_dm->eold diagon diagon proc~compute_dm->diagon chess_wrapper chess_wrapper proc~compute_dm->chess_wrapper write_dmh_netcdf write_dmh_netcdf proc~compute_dm->write_dmh_netcdf zminim zminim proc~compute_dm->zminim val val proc~compute_dm->val Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_dm~~CalledByGraph proc~compute_dm compute_dm proc~siesta_forces siesta_forces proc~siesta_forces->proc~compute_dm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_dm Source Code subroutine compute_dm ( iscf ) use precision use units , only : eV USE siesta_options use class_dSpData1D , only : val use sparse_matrices use siesta_geom use atomlist , only : qa , lasto , iphorb , iaorb , no_u , no_s , indxuo , & qtot , Qtots , no_l use sys , only : die , bye use Kpoint_grid use m_energies , only : Ebs , Ecorrec , Entropy , DE_NEGF use m_energies , only : Ef , Efs use m_rmaxh use m_eo use m_spin , only : spin use m_diagon , only : diagon use m_gamma use parallel , only : IONode use parallel , only : SIESTA_worker use m_compute_ebs_shift , only : compute_ebs_shift #ifdef SIESTA__PEXSI use m_pexsi_solver , only : pexsi_solver #endif use m_hsx , only : write_hs_formatted #ifdef MPI use mpi_siesta #endif #ifdef CDF use iodmhs_netcdf , only : write_dmh_netcdf #endif use m_dminim , only : dminim use m_zminim , only : zminim use m_ordern , only : ordern use m_steps , only : istp use m_normalize_dm , only : normalize_dm #ifdef SIESTA__CHESS use m_chess , only : CheSS_wrapper #endif use m_energies , only : DE_NEGF use m_ts_global_vars , only : TSmode , TSinit , TSrun use m_transiesta , only : transiesta implicit none !     Input variables integer , intent ( in ) :: iscf real ( dp ) :: delta_Ebs , delta_Ef logical :: CallDiagon integer :: nnz real ( dp ), pointer :: H_kin (:) ! e1>e2 to signal that we do not want DOS weights real ( dp ), parameter :: e1 = 1.0_dp , e2 = - 1.0_dp real ( dp ) :: buffer1 integer :: mpierr !       character(15)            :: filename, indexstr !       character(15), parameter :: fnameform = '(A,A,A)' !-------------------------------------------------------------------- BEGIN if ( SIESTA_worker ) call timer ( 'compute_dm' , 1 ) #ifdef MPI call MPI_Bcast ( isolve , 1 , MPI_integer , 0 , true_MPI_Comm_World , mpierr ) #endif if ( SIESTA_worker ) then ! Save present density matrix !$OMP parallel default(shared) if ( converge_EDM ) then !$OMP workshare Eold (:,:) = Escf (:,:) Dold (:,:) = Dscf (:,:) !$OMP end workshare else !$OMP workshare Dold (:,:) = Dscf (:,:) !$OMP end workshare end if !$OMP end parallel end if ! Compute shift in Tr(H*DM) for fermi-level bracketting ! Use the current H, the previous iteration H, and the ! previous iteration DM if ( SIESTA_worker ) then if ( iscf > 1 ) then call compute_Ebs_shift ( Dscf , H , Hold , delta_Ebs ) delta_Ef = delta_Ebs / qtot if ( ionode . and . isolve . eq . SOLVE_PEXSI ) then write ( 6 , \"(a,f16.5)\" ) $ \"Estimated change in band-structure energy:\" , $ delta_Ebs / eV , \"Estimated shift in E_fermi: \" , $ delta_Ef / eV endif else delta_Ebs = 0.0_dp delta_Ef = 0.0_dp endif endif #ifdef SIESTA__PEXSI if ( isolve . eq . SOLVE_PEXSI ) then ! This test done in node 0 since NonCol and SpOrb ! are not set for PEXSI-solver-only processes if ( ionode ) then if ( spin % NCol . or . spin % SO ) call die ( $ \"The PEXSI solver does not implement \" // $ \"non-coll spins or Spin-orbit yet\" ) endif call pexsi_solver ( iscf , no_u , no_l , spin % spinor , $ maxnh , numh , listhptr , listh , $ H , S , qtot , Dscf , Escf , $ ef , Entropy , temp , delta_Ef ) endif if (. not . SIESTA_worker ) RETURN #endif ! Here we decide if we want to calculate one or more SCF steps by ! diagonalization before proceeding with the OMM routine CallDiagon = . false . if ( isolve . eq . SOLVE_MINIM ) then if ( istp . eq . 1 ) then if (( iscf . le . call_diagon_first_step ) . or . & ( call_diagon_first_step < 0 )) CallDiagon = . true . else if (( iscf . le . call_diagon_default ) . or . & ( call_diagon_default < 0 )) CallDiagon = . true . endif endif if ( isolve . eq . MATRIX_WRITE ) then !             write(indexstr,'(I15)') iscf !             write(filename,fnameform) 'H_', trim(adjustl(indexstr)), !      &                                '.matrix' !             call write_global_matrix( no_s, no_l, maxnh, numh, listh, !      &           H(1:maxnh,1), filename ) ! !             write(filename,fnameform) 'S_', trim(adjustl(indexstr)), !      &                                '.matrix' !        Note: only one-shot for now call write_hs_formatted ( no_u , spin % H , $ maxnh , numh , listhptr , listh , H , S ) call bye ( \"End of run after writing H.matrix and S.matrix\" ) c$ call write_global_matrix_singlenodewrite ( c$ & no_u , no_s , maxnh , numh , listhptr , listh , c$ & H (:, 1 ), 'H.matrix' ) c$ c$ call write_global_matrix_singlenodewrite ( c$ & no_u , no_s , maxnh , numh , listhptr , listh , c$ & S , 'S.matrix' ) elseif (( isolve . eq . SOLVE_DIAGON ) . or . ( CallDiagon )) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , nkpnt , kpoint , kweight , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0.0_dp PreviousCallDiagon = . true . elseif ( isolve . eq . SOLVE_ORDERN ) then if (. not . gamma ) call die ( \"Cannot do O(N) with k-points.\" ) if ( spin % NCol . or . spin % SO ) . call die ( \"Cannot do O(N) with non-coll spins or Spin-orbit\" ) call ordern ( usesavelwf , ioptlwf , na_u , no_u , no_l , lasto , & isa , qa , rcoor , rmaxh , ucell , xa , iscf , & istp , ncgmax , etol , eta , qtot , maxnh , numh , & listhptr , listh , H , S , chebef , noeta , rcoorcp , & beta , pmax , Dscf , Escf , Ecorrec , spin % H , qtots ) Entropy = 0.0_dp elseif ( isolve . eq . SOLVE_MINIM ) then if ( spin % NCol . or . spin % SO ) & call die ( 'ERROR: Non-collinear spin calculations &                       not yet implemented with OMM!' ) H_kin => val ( H_kin_1D ) if ( gamma ) then call dminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , H , S , H_kin ) else call zminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , no_s , xijo , indxuo , nkpnt , kpoint , & kweight , H , S , H_kin ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #ifdef SIESTA__CHESS elseif ( isolve . eq . SOLVE_CHESS ) then ! FOE solver from the CheSS library if ( gamma ) then call CheSS_wrapper (. false ., PreviousCallDiagon , & iscf , istp , no_l , & spin % spinor , no_u , maxnh , numh , listhptr , listh , & qs , h , s , & Dscf , Escf , Ef ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #endif elseif ( TSmode . and . TSinit ) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , nkpnt , kpoint , kweight , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0._dp else if ( TSrun ) then call transiesta ( iscf , spin % H , block_dist , sparse_pattern , & Gamma , ucell , nsc , isc_off , no_u , na_u , lasto , xa , maxnh , & H , S , Dscf , Escf , Ef , Qtot , . false ., DE_NEGF ) Ecorrec = 0._dp Entropy = 0.0_dp else !call die('siesta: ERROR: wrong solution method') endif #ifdef CDF if ( writedmhs_cdf_history ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf ) else if ( writedmhs_cdf ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf , & overwrite = . true . ) endif #endif !     Print populations at each SCF step if requested before mixing ...... if ( muldeb ) then if ( ionode ) write ( 6 , \"(/a)\" ) & 'siesta: Mulliken populations (DM_out)' if ( spin % SO ) then call moments ( mullipop , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) else call mulliken ( mullipop , spin % DM , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) endif endif ! Write orbital indexes. JMS Dec.2009 if ( IOnode . and . iscf == 1 ) then call write_orb_indx ( na_u , na_s , no_u , no_s , isa , xa , . iaorb , iphorb , indxuo , nsc , ucell ) endif !     Normalize density matrix to exact charge !     Placed here for now to avoid disturbing EHarris if ( . not . TSrun ) then call normalize_dm ( first = . false . ) end if call timer ( 'compute_dm' , 2 ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after compute_DM\" ) #endif !-----------------------------------------------------------------------END END subroutine compute_dm","tags":"","loc":"proc/compute_dm.html"},{"title":"m_state_analysis – SIESTA","text":"Uses write_subs module~~m_state_analysis~~UsesGraph module~m_state_analysis m_state_analysis write_subs write_subs module~m_state_analysis->write_subs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_state_analysis~~UsedByGraph module~m_state_analysis m_state_analysis proc~siesta_forces siesta_forces proc~siesta_forces->module~m_state_analysis Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines state_analysis Subroutines public subroutine state_analysis (istep) Arguments Type Intent Optional Attributes Name integer :: istep","tags":"","loc":"module/m_state_analysis.html"},{"title":"m_setup_hamiltonian – SIESTA","text":"Used by module~~m_setup_hamiltonian~~UsedByGraph module~m_setup_hamiltonian m_setup_hamiltonian proc~siesta_forces siesta_forces proc~siesta_forces->module~m_setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines setup_hamiltonian Subroutines public subroutine setup_hamiltonian (iscf) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_setup_hamiltonian.html"},{"title":"m_mixing_scf – SIESTA","text":"Uses class_Fstack_dData1D m_mixing module~~m_mixing_scf~~UsesGraph module~m_mixing_scf m_mixing_scf class_Fstack_dData1D class_Fstack_dData1D module~m_mixing_scf->class_Fstack_dData1D module~m_mixing m_mixing module~m_mixing_scf->module~m_mixing module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_mixing_scf~~UsedByGraph module~m_mixing_scf m_mixing_scf proc~siesta_forces siesta_forces proc~siesta_forces->module~m_mixing_scf proc~state_init state_init proc~state_init->module~m_mixing_scf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables scf_mixs scf_mix MIX_SPIN_ALL MIX_SPIN_SPINOR MIX_SPIN_SUM MIX_SPIN_SUM_DIFF mix_spin Subroutines mixers_scf_init mixers_scf_print mixers_scf_print_block mixing_scf_converged mixers_scf_reset mixers_scf_history_init Variables Type Visibility Attributes Name Initial type( tMixer ), public, pointer :: scf_mixs (:) => null() type( tMixer ), public, pointer :: scf_mix => null() integer, public, parameter :: MIX_SPIN_ALL = 1 integer, public, parameter :: MIX_SPIN_SPINOR = 2 integer, public, parameter :: MIX_SPIN_SUM = 3 integer, public, parameter :: MIX_SPIN_SUM_DIFF = 4 integer, public :: mix_spin = MIX_SPIN_ALL Subroutines public subroutine mixers_scf_init (nspin, Comm) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in), optional :: Comm public subroutine mixers_scf_print (nspin) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin public subroutine mixers_scf_print_block () Arguments None public subroutine mixing_scf_converged (SCFconverged) Arguments Type Intent Optional Attributes Name logical, intent(inout) :: SCFconverged public subroutine mixers_scf_reset () Arguments None public subroutine mixers_scf_history_init () Arguments None","tags":"","loc":"module/m_mixing_scf.html"},{"title":"m_mixing – SIESTA","text":"Uses precision class_dData1D class_Fstack_dData1D module~~m_mixing~~UsesGraph module~m_mixing m_mixing precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_mixing~~UsedByGraph module~m_mixing m_mixing proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->module~m_mixing proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->module~m_mixing module~m_mixing_scf m_mixing_scf module~m_mixing_scf->module~m_mixing proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->module~m_mixing proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->module~m_mixing proc~state_init state_init proc~state_init->module~m_mixing proc~state_init->module~m_mixing_scf proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->module~m_mixing proc~siesta_forces siesta_forces proc~siesta_forces->module~m_mixing_scf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables MIX_LINEAR MIX_PULAY MIX_BROYDEN MIX_FIRE ACTION_MIX ACTION_RESTART ACTION_NEXT I_PREVIOUS_RES I_P_RESTART I_P_NEXT I_SVD_COND debug_mix debug_msg Interfaces mixing Derived Types tMixer Functions mix_method mix_method_variant mixing_ncoeff getstackval is_next current_itt stack_check norm Subroutines mixers_init mixer_init mixers_history_init mixers_reset mixers_print mixers_print_block mixing_init mixing_coeff mixing_calc_next mixing_finalize mixing_1d mixing_2d mixing_step inverse svd push_stack_data push_F update_F push_diff Variables Type Visibility Attributes Name Initial integer, public, parameter :: MIX_LINEAR = 1 integer, public, parameter :: MIX_PULAY = 2 integer, public, parameter :: MIX_BROYDEN = 3 integer, public, parameter :: MIX_FIRE = 4 integer, private, parameter :: ACTION_MIX = 0 integer, private, parameter :: ACTION_RESTART = 1 integer, private, parameter :: ACTION_NEXT = 2 integer, private, parameter :: I_PREVIOUS_RES = 0 integer, private, parameter :: I_P_RESTART = -1 integer, private, parameter :: I_P_NEXT = -2 integer, private, parameter :: I_SVD_COND = -3 logical, private :: debug_mix = .false. character(len=20), private :: debug_msg = 'mix:' Interfaces public interface mixing private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub Derived Types type, public :: tMixer Components Type Visibility Attributes Name Initial character(len=24), public :: name type(Fstack_dData1D), public, allocatable :: stack (:) integer, public :: m = MIX_PULAY integer, public :: v = 0 integer, public :: cur_itt = 0 integer, public :: start_itt = 0 integer, public :: n_hist = 2 integer, public :: n_itt = 0 integer, public :: restart = 0 integer, public :: restart_save = 0 integer, public :: action = ACTION_MIX type( tMixer ), public, pointer :: next => null() type( tMixer ), public, pointer :: next_conv => null() real(kind=dp), public :: w = 0._dp real(kind=dp), public, pointer :: rv (:) => null() integer, public, pointer :: iv (:) => null() Functions public function mix_method (str) result(m) Return the integer specification of the mixing type Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: str Return Value integer public function mix_method_variant (m, str) result(v) Return the variant of the mixing method Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: m character(len=*), intent(in) :: str Return Value integer private function mixing_ncoeff (mix) result(n) Function to retrieve the number of coefficients\n calculated in this iteration.\n This is so external routines can query the size\n of the arrays used. Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer private function getstackval (mix, sidx, hidx) result(d1) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix integer, intent(in) :: sidx integer, intent(in), optional :: hidx Return Value real(kind=dp),\n  pointer, (:) private function is_next (mix, method, next) result(bool) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in), target :: mix integer, intent(in) :: method type( tMixer ), optional pointer :: next Return Value logical private function current_itt (mix) result(itt) Get current iteration count Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer private function stack_check (stack, n) result(check) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: stack integer, intent(in) :: n Return Value logical private function norm (n, x1, x2) Calculate the norm of two arrays Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: x1 (n) real(kind=dp), intent(in) :: x2 (n) Return Value real(kind=dp) Subroutines public subroutine mixers_init (prefix, mixers, Comm) Initialize a set of mixers by reading in fdf information.\n @param[in] prefix the fdf-label prefixes\n @param[pointer] mixers the mixers that are to be initialized\n @param[in] Comm @opt optional MPI-communicator Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), pointer :: mixers (:) integer, intent(in), optional :: Comm public subroutine mixer_init (mix) Initialize a single mixer depending on the preset\n options. Useful for external correct setup. Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix public subroutine mixers_history_init (mixers) Initialize all history for the mixers Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout), target :: mixers (:) public subroutine mixers_reset (mixers) Reset the mixers, i.e. clean everything Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mixers (:) public subroutine mixers_print (prefix, mixers) Print (to std-out) information regarding the mixers Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) public subroutine mixers_print_block (prefix, mixers) Print (to std-out) the fdf-blocks that recreate the mixer settings Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) private subroutine mixing_init (mix, n, xin, F) Initialize the mixing algorithm Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) private subroutine mixing_coeff (mix, n, xin, F, coeff) Calculate the mixing coefficients for the\n current mixer Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: coeff (:) private subroutine mixing_calc_next (mix, n, xin, F, xnext, coeff) Calculate the guess for the next iteration Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: xnext (n) real(kind=dp), intent(in) :: coeff (:) private subroutine mixing_finalize (mix, n, xin, F, xnext) Finalize the mixing algorithm Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in) :: xnext (n) private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub private subroutine mixing_step (mix) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix private subroutine inverse (n, A, B, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) integer, intent(out) :: info private subroutine svd (n, A, B, cond, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) real(kind=dp), intent(in) :: cond integer, intent(out) :: info private subroutine push_stack_data (s_F, n) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n private subroutine push_F (s_F, n, F, fact) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in), optional :: fact private subroutine update_F (s_F, n, F) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) private subroutine push_diff (s_rres, s_res, alpha) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_rres type(Fstack_dData1D), intent(in) :: s_res real(kind=dp), intent(in), optional :: alpha","tags":"","loc":"module/m_mixing.html"},{"title":"m_compute_max_diff – SIESTA","text":"Uses precision module~~m_compute_max_diff~~UsesGraph module~m_compute_max_diff m_compute_max_diff precision precision module~m_compute_max_diff->precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_compute_max_diff~~UsedByGraph module~m_compute_max_diff m_compute_max_diff proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables dDmax_current Interfaces compute_max_diff Subroutines compute_max_diff_2d compute_max_diff_1d Variables Type Visibility Attributes Name Initial real(kind=dp), public, save :: dDmax_current Temporary for storing the old maximum change Interfaces public interface compute_max_diff public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff Subroutines public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff","tags":"","loc":"module/m_compute_max_diff.html"},{"title":"m_setup_H0 – SIESTA","text":"Used by module~~m_setup_h0~~UsedByGraph module~m_setup_h0 m_setup_H0 proc~siesta_forces siesta_forces proc~siesta_forces->module~m_setup_h0 Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines setup_H0 Subroutines public subroutine setup_H0 (g2max) Computes non-self-consistent part of the Hamiltonian\n and initializes data structures on the grid. Read more… Arguments Type Intent Optional Attributes Name real(kind=dp), intent(inout) :: g2max","tags":"","loc":"module/m_setup_h0.html"},{"title":"m_state_init – SIESTA","text":"Used by module~~m_state_init~~UsedByGraph module~m_state_init m_state_init proc~siesta_forces siesta_forces proc~siesta_forces->module~m_state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines state_init Subroutines public subroutine state_init (istep) Arguments Type Intent Optional Attributes Name integer :: istep","tags":"","loc":"module/m_state_init.html"},{"title":"m_siesta_forces – SIESTA","text":"Contents Subroutines siesta_forces Subroutines public subroutine siesta_forces (istep) This subroutine represents central SIESTA operation logic. Read more… Arguments Type Intent Optional Attributes Name integer, intent(inout) :: istep","tags":"","loc":"module/m_siesta_forces.html"},{"title":"m_siesta_analysis – SIESTA","text":"Uses write_subs module~~m_siesta_analysis~~UsesGraph module~m_siesta_analysis m_siesta_analysis write_subs write_subs module~m_siesta_analysis->write_subs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines siesta_analysis Subroutines public subroutine siesta_analysis (relaxd) Check that we are converged in geometry,\n if strictly required,\n before carrying out any analysis. Read more… Arguments Type Intent Optional Attributes Name logical :: relaxd","tags":"","loc":"module/m_siesta_analysis.html"},{"title":"moreMeshSubs – SIESTA","text":"Uses precision parallel sys alloc module~~moremeshsubs~~UsesGraph module~moremeshsubs moreMeshSubs precision precision module~moremeshsubs->precision parallel parallel module~moremeshsubs->parallel alloc alloc module~moremeshsubs->alloc sys sys module~moremeshsubs->sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables UNIFORM QUADRATIC LINEAR TO_SEQUENTIAL TO_CLUSTER KEEP moduName maxDistr gp meshDistr meshCommu exteCommu tBuff1 tBuff2 Interfaces distMeshData Derived Types meshDisType meshCommType Subroutines initMeshDistr allocASynBuffer allocExtMeshDistr allocIpaDistr setMeshDistr resetMeshDistr distMeshData_rea distMeshData_rea distMeshData_int boxIntersection initMeshExtencil distExtMeshData gathExtMeshData splitwload reduce3Dto1D vecBisec reordMeshNumbering reordMeshNumbering compMeshComm Variables Type Visibility Attributes Name Initial integer, public, parameter :: UNIFORM = 1 integer, public, parameter :: QUADRATIC = 2 integer, public, parameter :: LINEAR = 3 integer, public, parameter :: TO_SEQUENTIAL = +1 integer, public, parameter :: TO_CLUSTER = -1 integer, public, parameter :: KEEP = 0 character(len=*), private, parameter :: moduName = 'moreMeshSubs' integer, private, parameter :: maxDistr = 5 integer, private, parameter :: gp = grid_p type( meshDisType ), private, target, save :: meshDistr (maxDistr) type( meshCommType ), private, target, save :: meshCommu ((maxDistr*(maxDistr-1))/2) type( meshCommType ), private, target, save :: exteCommu (maxDistr,3) real(kind=grid_p), private, pointer :: tBuff1 (:) real(kind=grid_p), private, pointer :: tBuff2 (:) Interfaces public interface distMeshData private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr Derived Types type, private :: meshDisType Components Type Visibility Attributes Name Initial integer, public :: nMesh (3) integer, public, pointer :: box (:,:,:) integer, public, pointer :: indexp (:) integer, public, pointer :: idop (:) real(kind=dp), public, pointer :: xdop (:,:) integer, public, pointer :: ipa (:) type, private :: meshCommType Components Type Visibility Attributes Name Initial integer, public :: ncom integer, public, pointer :: src (:) integer, public, pointer :: dst (:) Subroutines public subroutine initMeshDistr (iDistr, oDistr, nm, wload) Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr integer, intent(in) :: oDistr integer, intent(in) :: nm (3) integer, intent(in), optional :: wload (*) public subroutine allocASynBuffer (ndistr) Arguments Type Intent Optional Attributes Name integer :: ndistr public subroutine allocExtMeshDistr (iDistr, nep, mop) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nep integer, intent(in) :: mop public subroutine allocIpaDistr (iDistr, na) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: na public subroutine setMeshDistr (iDistr, nsm, nsp, nml, nmpl, ntml, ntpl) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nsm integer, intent(in) :: nsp integer, intent(out) :: nml (3) integer, intent(out) :: nmpl integer, intent(out) :: ntml (3) integer, intent(out) :: ntpl public subroutine resetMeshDistr (iDistr) Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine boxIntersection (ibox1, ibox2, obox, inters) Arguments Type Intent Optional Attributes Name integer, intent(in) :: ibox1 (2,3) integer, intent(in) :: ibox2 (2,3) integer, intent(out) :: obox (2,3) logical, intent(out) :: inters public subroutine initMeshExtencil (iDistr, nm) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nm (3) public subroutine distExtMeshData (iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, DENS, BDENS) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: iaxis integer, intent(in) :: BS integer, intent(in) :: NSM integer, intent(in) :: NN integer, intent(in) :: NSPIN integer, intent(in) :: maxp integer, intent(in) :: NMeshG (3) real(kind=gp), intent(in) :: DENS (maxp,NSPIN) real(kind=gp), intent(out) :: BDENS (BS,2*NN,NSPIN) public subroutine gathExtMeshData (iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, BVXC, VXC) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: iaxis integer, intent(in) :: BS integer, intent(in) :: NSM integer, intent(in) :: NN integer, intent(in) :: NSPIN integer, intent(in) :: maxp integer, intent(in) :: NMeshG (3) real(kind=gp), intent(in) :: BVXC (BS,2*NN,NSPIN) real(kind=gp), intent(out) :: VXC (maxp,NSPIN) private subroutine splitwload (Nodes, Node, nm, wload, iDistr, oDistr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes integer, intent(in) :: Node integer, intent(in) :: nm (3) integer, intent(in) :: wload (*) type( meshDisType ), intent(in) :: iDistr type( meshDisType ), intent(out) :: oDistr private subroutine reduce3Dto1D (iaxis, Ibox, Lbox, wload, lwload) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iaxis integer, intent(in) :: Ibox (2,3) integer, intent(in) :: Lbox (2,3) integer, intent(in) :: wload (*) integer(kind=i8b), intent(out) :: lwload (*) private subroutine vecBisec (nval, values, nparts, pos, h1, h2) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nval integer(kind=i8b), intent(in) :: values (nval) integer, intent(in) :: nparts integer, intent(out) :: pos integer(kind=i8b), intent(out) :: h1 integer(kind=i8b), intent(out) :: h2 private subroutine reordMeshNumbering (distr1, distr2) Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 type( meshDisType ), intent(out) :: distr2 private subroutine reordMeshNumbering (distr1, distr2) Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 type( meshDisType ), intent(out) :: distr2 private subroutine compMeshComm (distr1, distr2, mcomm) Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 type( meshDisType ), intent(in) :: distr2 type( meshCommType ), intent(out) :: mcomm","tags":"","loc":"module/moremeshsubs.html"},{"title":"m_compute_dm – SIESTA","text":"Used by module~~m_compute_dm~~UsedByGraph module~m_compute_dm m_compute_dm proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_dm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables PreviousCallDiagon Subroutines compute_dm Variables Type Visibility Attributes Name Initial logical, public, save :: PreviousCallDiagon = .false. Subroutines public subroutine compute_dm (iscf) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_compute_dm.html"},{"title":"Program Overview – SIESTA","text":"SIESTA is both a method and its computer program implementation, to perform efficient electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SIESTA's efficiency stems from the use of strictly localized basis sets and from the implementation of linear-scaling algorithms which can be applied to suitable systems. A very important feature of the code is that its accuracy and cost can be tuned in a wide range, from quick exploratory calculations to highly accurate simulations matching the quality of other approaches, such as plane-wave and all-electron methods. This is a documentation project for SIESTA's source code. Unlike the underlying\nprinciples of SIESTA and its user options, the codebase was not well documented,\nuntil recently, and here we make an attempt to do so. This project is in active stage of development, and thus should not be considered a reliable source of information. Please, consult the manual on SIESTA's official repository .","tags":"","loc":"page//index.html"},{"title":"SIESTA Calculation Flow – SIESTA","text":"The general logic of the SIESTA's operation cycle is performed by siesta_forces . After initialization that includes setup of non-scf part of Hamiltonian setup_H0 (see subsections below) as long as initialization of\nparameters for MPI and TranSiesta, SIESTA enters the main Self-Consistent Field loop, that can be schematically represented: Within each SCF cycle normally we have setup_hamiltonian and compute_dm executed subsequently. \nOne of the two convergence criteria is checked at the beginning and\nat the end of each scf cycle. Hamiltonian setup DM computation Check for Convergence Results analysis step","tags":"","loc":"page/01_calculation_flow/index.html"},{"title":"Hamiltonian setup – SIESTA","text":"","tags":"","loc":"page/01_calculation_flow/01_hamiltonian_setup.html"},{"title":"DM computation – SIESTA","text":"Performed in the compute_dm subroutine. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/02_dm_computation.html"},{"title":"SCF Check for Convergence – SIESTA","text":"Performed in the compute_max_diff subroutine. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/03_check_convergence.html"},{"title":"SCF Results analysis – SIESTA","text":"Results analysis is performed in siesta_analysis and state_analysis routines. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/04_analyze_results.html"}]}