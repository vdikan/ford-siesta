var tipuesearch = {"pages":[{"text":"SIESTA Note This is an early stage work-in-progress build of documentation for SIESTA code. At the moment it should not by any means be considered a reliable source of information. Please, consult the manual on SIESTA's official repository . Project Dashboard Todo Ebs Density Developer Info SIESTA Group E. Artacho, J.D. Gale, A. Garcia, J. Junquera, P. Ordejon, D. Sanchez-Portal, J.M. Cela and J.M. Soler","tags":"home","loc":"index.html","title":" SIESTA "},{"text":"This file depends on sourcefile~~setup_h0.f~~EfferentGraph sourcefile~setup_h0.f setup_H0.F sourcefile~dhscf.f dhscf.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~setup_h0.f~~AfferentGraph sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_setup_H0 Source Code setup_H0.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_setup_H0 private public :: setup_H0 CONTAINS subroutine setup_H0 ( G2max ) C     Computes non-self-consistent part of the Hamiltonian C     and initializes data structures on the grid. USE siesta_options , only : g2cut use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : Dscf use m_nlefsm , only : nlefsm_SO_off use m_spin , only : spin use sparse_matrices , only : listh , listhptr , numh , maxnh use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use m_nlefsm , only : nlefsm use m_kinefsm , only : kinefsm use m_naefs , only : naefs use m_dnaefs , only : dnaefs use m_dhscf , only : dhscf_init use m_energies , only : Eions , Ena , DEna , Emm , Emeta , Eso use m_ntm use m_spin , only : spin use spinorbit , only : spinorb use alloc , only : re_alloc , de_alloc use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none real ( dp ), intent ( inout ) :: g2max real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ), dummy_dm ( 1 , 1 ) real ( dp ) :: dummy_E integer :: ia , is real ( dp ) :: dummy_Eso integer :: ispin , i , j complex ( dp ) :: Dc #ifdef MPI real ( dp ) :: buffer1 #endif real ( dp ), pointer :: H_val (:), H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) #ifdef DEBUG call write_debug ( '    PRE setup_H0' ) #endif !----------------------------------------------------------------------BEGIN call timer ( 'Setup_H0' , 1 ) C     Self-energy of isolated ions Eions = 0.0_dp do ia = 1 , na_u is = isa ( ia ) Eions = Eions + uion ( is ) enddo !     In these routines, add a flag to tell them NOT to compute !     forces and stresses in this first pass, only energies. !     Neutral-atom: energy call naefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , Ena , dummy_fa , dummy_stress , & forces_and_stress = . false .) call dnaefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , DEna , dummy_fa , dummy_stress , & forces_and_stress = . false .) Ena = Ena + DEna C     Metadynamics energy if ( lMetaForce ) then call meta ( xa , na_u , ucell , Emeta , dummy_fa , dummy_stress , $ . false .,. false .) endif C     Add on force field contribution to energy call twobody ( na_u , xa , isa , ucell , Emm , & ifa = 0 , fa = dummy_fa , istr = 0 , stress = dummy_stress ) ! !     Now we compute matrix elements of the Kinetic and Non-local !     parts of H !     Kinetic: matrix elements only H_val => val ( H_kin_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call kinefsm ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , & maxnh , maxnh , lasto , iphorb , isa , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !     Non-local-pseudop:  matrix elements only H_val => val ( H_vkb_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare Eso = 0.0d0 if ( . not . spin % SO_offsite ) then call nlefsm ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) else H_so_off => val ( H_so_off_2D ) H_so_off = dcmplx ( 0._dp , 0._dp ) call nlefsm_SO_off ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & spin % Grid , & dummy_E , dummy_Eso , dummy_fa , & dummy_stress , H_val , H_so_off , & matrix_elements_only = . true .) ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! do i = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( i , 1 ), Dscf ( i , 5 ), dp ) Eso = Eso + real ( H_so_off ( i , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( i , 2 ), Dscf ( i , 6 ), dp ) Eso = Eso + real ( H_so_off ( i , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( i , 3 ), Dscf ( i , 4 ), dp ) Eso = Eso + real ( H_so_off ( i , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( i , 7 ), - Dscf ( i , 8 ), dp ) Eso = Eso + real ( H_so_off ( i , 3 ) * Dc , dp ) enddo #ifdef MPI ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 #endif endif ! .................. ! If in the future the spin-orbit routine is able to compute ! forces and stresses, then \"last\" will be needed. If we are not ! computing forces and stresses, calling it in the first iteration ! should be enough ! if ( spin % SO_onsite ) then H_so_on => val ( H_so_on_2D ) !$OMP parallel workshare default(shared) H_so_on (:,:) = 0._dp !$OMP end parallel workshare call spinorb ( no_u , no_l , iaorb , iphorb , isa , indxuo , & maxnh , numh , listhptr , listh , Dscf , H_so_on , Eso ) end if C     This will take care of possible changes to the mesh and atomic-related C     mesh structures for geometry changes g2max = g2cut call dhscf_init ( spin % Grid , no_s , iaorb , iphorb , & no_l , no_u , na_u , na_s , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnh , numh , listhptr , listh , datm , & dummy_fa , dummy_stress ) call timer ( 'Setup_H0' , 2 ) #ifdef DEBUG call write_debug ( '    POS setup_H0' ) #endif !---------------------------------------------------------------------- END END subroutine setup_H0 END module m_setup_H0","tags":"","loc":"sourcefile/setup_h0.f.html","title":"setup_H0.F – SIESTA"},{"text":"This file contains module moreMeshSubs. It defines and handles\n  different parallel mesh distributions. Includes the following subroutines connected to the mesh : initMeshDistr - Precompute a new data distribution for the\n    grid mesh to be used in Hamiltonian construction. setMeshDistr - Select a mesh distribution and set the grid sizes. distMeshData - Move data from one data distribution to another. allocASynBuffer - Allocate buffer for asynchronous\n    communications  if necessary. Files dependent on this one sourcefile~~moremeshsubs.f~~AfferentGraph sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules moreMeshSubs Source Code moremeshsubs.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! #ifdef ASYNCHRONOUS_GRID_COMMS   /* Compile-time option */ # ifdef MPI #   define ASYNCHRONOUS           /* Internal symbol */ # endif #endif !!  author: Rogeli Grima (BSC) !!  date: December 2007 !!  license: GNU GPL !! !!  This file contains module moreMeshSubs. It defines and handles !!  different parallel mesh distributions. !! !!  Includes the following subroutines connected to the mesh : !! !!  * `[[initMeshDistr(proc)]]` - Precompute a new data distribution for the !!    grid mesh to be used in Hamiltonian construction. !!  * `[[setMeshDistr(proc)]]`- Select a mesh distribution and set the grid sizes. !!  * `[[distMeshData(proc)]]`- Move data from one data distribution to another. !!  * `[[allocASynBuffer(proc)]]` - Allocate buffer for asynchronous !!    communications  if necessary. MODULE moreMeshSubs !! author: Rogeli Grima (BSC) !! date: December 2007 use precision , only : grid_p , dp , i8b use parallel , only : node , Nodes , ProcessorY use sys , only : die use alloc , only : re_alloc , de_alloc implicit none PUBLIC :: initMeshDistr , setMeshDistr , allocExtMeshDistr PUBLIC :: allocIpaDistr , distMeshData , resetMeshDistr #ifdef MPI PUBLIC :: initMeshExtencil , distExtMeshData , gathExtMeshData #endif PUBLIC :: allocASynBuffer !     Symbolic names for parallel mesh distributions integer , parameter , public :: UNIFORM = 1 integer , parameter , public :: QUADRATIC = 2 integer , parameter , public :: LINEAR = 3 ! !     Symbolic names for \"reord\"-type operations ! integer , parameter , public :: TO_SEQUENTIAL = + 1 !! alias to translation direction for `[[reord(proc)]]` procedure integer , parameter , public :: TO_CLUSTER = - 1 !! alias to translation direction for `[[reord(proc)]]` procedure integer , parameter , public :: KEEP = 0 PRIVATE interface distMeshData !! Move data from vector `fsrc`, that uses distribution `iDistr`, to vector !! `fdst`, that uses distribution `oDistr`. It also re-orders a clustered !! data array into a sequential one and viceversa. !! If this is a sequencial execution, it only reorders the data. !!@note !! There are two subroutines: one to deal with real data and !! the other with integers. Both are called using the same interface. !!@endnote !!@note !! *AG*: Note that the integer version does NOT have the exact functionality !! of the real version. In particular, the integer version has no provision !! for a \"serial fallback\", and so this case has been trapped. !!@endnote !! Check the communications that this process should do to move data !! from `iDistr` to `oDistr`. We have 3 kind of communications (send, receive !! and keep on the same node). We have 3 kind of reorderings (clustered to !! sequential, sequential to clustered and keep the same ordering). !! For the sequencial code we call subroutine `[[reord(proc)]]` !!#### INPUT !! !! * `iDistr` : Distribution index of the input vector. !! * `fsrc`   : Input vector. !! * `oDistr` : Distribution index of the output vector. !! * `itr`    : TRanslation-direction switch: !!     `itr`=+1 => From clustered to sequential !!     `itr`=-1 => From sequential to clustered !!     `itr`=0  => Keep the status !! !! !!#### OUTPUT !! !! * `fdst`   : Output vector. ! module procedure distMeshData_rea , distMeshData_int end interface distMeshData TYPE meshDisType !! Private type to hold mesh distribution data. integer :: nMesh ( 3 ) !! Number of mesh div. in each axis. integer , pointer :: box (:,:,:) !! Mesh box bounds of each node: !! box(1,iAxis,iNode)=lower bounds !! box(2,iAxis,iNode)=upper bounds integer , pointer :: indexp (:) integer , pointer :: idop (:) real ( dp ), pointer :: xdop (:,:) integer , pointer :: ipa (:) END TYPE meshDisType TYPE meshCommType !! Private type to hold communications to move data from one !! distribution to another. integer :: ncom !! Number of needed communications integer , pointer :: src (:) !! Sources of communications integer , pointer :: dst (:) !! Destination of communications END TYPE meshCommType character ( len =* ), parameter :: moduName = 'moreMeshSubs' !! Name of the module. integer , parameter :: maxDistr = 5 !! Maximum number of data distribution that can be handled integer , parameter :: gp = grid_p !! Alias of the grid precision type ( meshDisType ), target , save :: meshDistr ( maxDistr ) !! Contains information of the several data distributions type ( meshCommType ), target , save :: & meshCommu (( maxDistr * ( maxDistr - 1 )) / 2 ) !! Contains all the communications to move among the !! several data distributions type ( meshCommType ), target , save :: exteCommu ( maxDistr , 3 ) !! Contains all the needed communications to compute !! the extencil #ifdef ASYNCHRONOUS real ( grid_p ), pointer :: tBuff1 (:) !! Memory buffer for asynchronous communications real ( grid_p ), pointer :: tBuff2 (:) !! Memory buffer for asynchronous communications #endif CONTAINS subroutine initMeshDistr ( iDistr , oDistr , nm , wload ) !! Computes a new data distribution and the communications needed to !! move data from/to the current distribution to the existing ones. !! !! The limits of the new distributions are stored in the current module !! in `[[moreMeshSubs(module):meshDistr(variable)]]`: !! !!     meshDistr(oDistr) !!     meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) !! !! If this is the first distribution, we split the mesh uniformly among !! the several processes (we only split it in dimensions Y and Z). !! !! For the other data distributions we should split the vector wload. !! The subroutine splitwload will return the limits of the new data !! distribution. The subroutine compMeshComm will return the communications !! needed to move data from/to the current distribution to/from the !! previous ones. implicit none integer , optional , intent ( in ) :: iDistr !!  Distribution index of the input vector integer , intent ( in ) :: oDistr !!  The new data distribution index integer , intent ( in ) :: nm ( 3 ) !!  Number of Mesh divisions of each cell vector integer , optional , intent ( in ) :: wload ( * ) !!  Weights of every point of the mesh using the input distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) call timer ( 'INITMESH' , 1 ) !     Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) !     Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) !     Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) !     The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else !       In order to compute the other data distributions, we should split !       the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) !       Precompute the communications needed to move data between the new data !       distribution and the previous ones. jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) end subroutine initMeshDistr subroutine allocASynBuffer ( ndistr ) !! Allocate memory buffers for asynchronous communications. !! It does nothing for synchronous communications. !! The output values are stored in the current module: !! `[[moreMeshSubs(module):tBuff1(variable)]]` : Buffer for distribution 1 !! `[[moreMeshSubs(module):tBuff2(variable)]]` : Buffer for other distributions use mesh , only : nsm implicit none integer :: ndistr !! Total number of distributions integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters #ifdef ASYNCHRONOUS !     Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) !     Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo !     Check the size of the intersections between the first data distributions !     and the others data distributions. !     Buffers don't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr ( 1 )% box (:,:, node + 1 ) do ii = 2 , ndistr box2 => meshDistr ( ii )% box (:,:, node + 1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp else lsize = 0 endif imax1 = max ( imax1 , nsize ( 1 ) - lsize ) imax2 = max ( imax2 , nsize ( ii ) - lsize ) enddo !     Deallocate local memory call de_alloc ( nsize , 'nsize' , moduName ) !     Allocate memory for asynchronous communications call re_alloc ( tBuff1 , 1 , imax1 , 'tBuff1' , moduName ) call re_alloc ( tBuff2 , 1 , imax2 , 'tBuff2' , moduName ) #endif end subroutine allocASynBuffer subroutine allocExtMeshDistr ( iDistr , nep , mop ) use mesh , only : indexp , idop , xdop implicit none !     Input variables integer , intent ( in ) :: iDistr , nep , mop !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % indexp , 1 , nep , 'distr%indexp' , moduName ) call re_alloc ( distr % idop , 1 , mop , 'distr%idop' , moduName ) call re_alloc ( distr % xdop , 1 , 3 , 1 , mop , 'distr%xdop' , moduName ) indexp => distr % indexp idop => distr % idop xdop => distr % xdop end subroutine allocExtMeshDistr subroutine allocIpaDistr ( iDistr , na ) use mesh , only : ipa implicit none !     Input variables integer , intent ( in ) :: iDistr , na !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % ipa , 1 , na , 'distr%ipa' , moduName ) ipa => meshDistr ( iDistr )% ipa end subroutine allocIpaDistr subroutine setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) !! Fixes the new data limits and dimensions of the mesh to those of !! the data distribution `iDistr`. use mesh , only : meshLim , indexp , ipa , idop , xdop implicit none integer , intent ( in ) :: iDistr !! Distribution index of the input vector integer , intent ( in ) :: nsm !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: nsp !! Number of sub-points of each mesh point integer , intent ( out ) :: nml ( 3 ) !! Local number of Mesh divisions in each cell vector integer , intent ( out ) :: nmpl !! Local number of Mesh divisions integer , intent ( out ) :: ntml ( 3 ) !! Local number of Mesh points in each cell vector integer , intent ( out ) :: ntpl !! Local number of Mesh points !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) meshLim = distr % box ( 1 : 2 , 1 : 3 , node + 1 ) nml ( 1 ) = ( MeshLim ( 2 , 1 ) - MeshLim ( 1 , 1 )) + 1 nml ( 2 ) = ( MeshLim ( 2 , 2 ) - MeshLim ( 1 , 2 )) + 1 nml ( 3 ) = ( MeshLim ( 2 , 3 ) - MeshLim ( 1 , 3 )) + 1 nmpl = nml ( 1 ) * nml ( 2 ) * nml ( 3 ) ntml = nml * nsm ntpl = nmpl * nsp indexp => distr % indexp idop => distr % idop xdop => distr % xdop ipa => distr % ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr subroutine resetMeshDistr ( iDistr ) !! Reset the data of the distribution `iDistr`. !! Deallocate associated arrays of the current distribution. !! Modifies data of the current module. implicit none integer , optional , intent ( in ) :: iDistr !! Distribution index to be reset integer :: idis , ini , fin , icom type ( meshDisType ), pointer :: distr type ( meshCommType ), pointer :: mcomm if ( present ( iDistr )) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis = ini , fin distr => meshDistr ( idis ) distr % nMesh = 0 if ( associated ( distr % box )) then call de_alloc ( distr % box , 'distr%box' , 'moreMeshSubs' ) endif if ( associated ( distr % indexp )) then call de_alloc ( distr % indexp , 'distr%indexp' , & 'moreMeshSubs' ) endif if ( associated ( distr % idop )) then call de_alloc ( distr % idop , 'distr%idop' , & 'moreMeshSubs' ) endif if ( associated ( distr % xdop )) then call de_alloc ( distr % xdop , 'distr%xdop' , & 'moreMeshSubs' ) endif if ( associated ( distr % ipa )) then call de_alloc ( distr % ipa , 'distr%ipa' , & 'moreMeshSubs' ) endif do icom = 1 , 3 mcomm => exteCommu ( idis , icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo do icom = (( idis - 2 ) * ( idis - 1 )) / 2 + 1 , (( idis - 1 ) * idis ) / 2 mcomm => meshCommu ( icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if ( associated ( tBuff1 )) then call de_alloc ( tBuff1 , 'tBuff1' , 'moreMeshSubs' ) endif if ( associated ( tBuff2 )) then call de_alloc ( tBuff2 , 'tBuff2' , 'moreMeshSubs' ) endif #endif end subroutine resetMeshDistr #ifdef ASYNCHRONOUS subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea #else /* SYNCHRONOUS communications */ subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & N1 , N2 , N3 , NN , ind , ncom , & icom , NSP , NSRC ( 3 ), NDST ( 3 ), ME , & MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), JS (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters real ( grid_p ), pointer :: TBUF (:) integer :: nsize , nm ( 3 ) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif !----------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) if ( nodes == 1 ) then nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) NSP = NSM * NSM * NSM ME = Node + 1 nullify ( JS ) call re_alloc ( JS , 1 , NSP , 'JS' , 'moreMeshSubs' ) !       Compute the maximum size of the buffer needed to transfer data !       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo MaxSize = MaxSize * nsp if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP TBUF ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = TBUF ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP TBUF ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif call de_alloc ( JS , 'JS' , 'moreMeshSubs' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea #endif subroutine distMeshData_int ( iDistr , fsrc , oDistr , fdst , itr ) #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr integer , intent ( in ) :: fsrc ( * ) integer , intent ( out ) :: fdst ( * ) !     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & ind , ncom , icom , NSRC ( 3 ), NDST ( 3 ), & ME , MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters integer , pointer :: TBUF (:) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif if ( nodes == 1 ) then call die ( \"Called _int version of distMeshData for n=1\" ) else !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) ME = Node + 1 !       Compute the maximum size of the buffer needed to transfer data !       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 NSRC ( 2 ) = Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 NSRC ( 3 ) = Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 Dbox => odis % box (:,:, ME ) NDST ( 1 ) = Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 NDST ( 2 ) = Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 NDST ( 3 ) = Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 if ( itr . eq . 0 ) then !         From sequencial to sequencial do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif endif end subroutine distMeshData_int subroutine boxIntersection ( ibox1 , ibox2 , obox , inters ) !! Checks the three axis of the input boxes to see if there is !! intersection between the input boxes. If it exists, returns !! the resulting box. implicit none !     Passed arguments integer , intent ( in ) :: ibox1 ( 2 , 3 ), ibox2 ( 2 , 3 ) !! Input box integer , intent ( out ) :: obox ( 2 , 3 ) !! Intersection between `ibox1` and `ibox2` logical , intent ( out ) :: inters !! `TRUE`, if there is an intersection. Otherwise `FALSE`. !     Local variables integer :: iaxis inters = . true . do iaxis = 1 , 3 obox ( 1 , iaxis ) = max ( ibox1 ( 1 , iaxis ), ibox2 ( 1 , iaxis )) obox ( 2 , iaxis ) = min ( ibox1 ( 2 , iaxis ), ibox2 ( 2 , iaxis )) if ( obox ( 2 , iaxis ). lt . obox ( 1 , iaxis )) inters = . false . enddo end subroutine boxIntersection #ifdef MPI subroutine initMeshExtencil ( iDistr , nm ) !! Compute the needed communications in order to send/receive the !! extencil (when the data is ordered in the distribution `iDistr`) !! The results are stored in the variable !! `[[moreMeshSubs(module):exteCommu(variable)]](iDistr,1:3)` !! of the current module. !! !! For every dimension of the problem, search all the neighbors that !! we have. Given the current data distribution we compute the limits !! of our extencil and we check its intersection with all the other !! processes. Once we know all our neighbors we call subroutine !! `[[scheduleComm(proc)]]` in order to minimize the number !! of communications steps. use scheComm implicit none !     Passed arguments integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: nm ( 3 ) !! Number of Mesh divisions in each cell vector !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), Ibox ( 2 , 3 ), & ii , iaxis , ncom , Gcom , Lcom , P1 , P2 integer , pointer :: src (:), dst (:), Dbox (:,:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm type ( COMM_T ) :: comm logical :: inters idis => meshDistr ( iDistr ) do iaxis = 1 , 3 !       One communication structure for every dimension mcomm => exteCommu ( iDistr , iaxis ) !       Count the number of communications needed to send/receive !       the extencil ncom = 0 do P1 = 1 , Nodes !         Create the extencil boxes for both sides of the current !         partition Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) ncom = ncom + 1 endif enddo enddo Gcom = ncom !       Create a list of communications needed to send/receive !       the extencil if ( Gcom . gt . 0 ) then nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) ncom = 0 do P1 = 1 , Nodes Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif endif enddo enddo comm % np = Nodes !         reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !         Count the number of communications needed by the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !         Store the ordered list of communications needed by the current !         process to send/receive the extencil. if ( Lcom . gt . 0 ) then nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , & 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , & 'moreMeshSubs' ) ncom = 0 do P1 = 1 , comm % ncol ii = comm % ind ( P1 , Node + 1 ) if ( ii . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( ii ) mcomm % dst ( ncom ) = dst ( ii ) endif enddo mcomm % ncom = Lcom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) endif call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) endif enddo end subroutine initMeshExtencil subroutine distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , dens , BDENS ) !! Send/receive the extencil information from the `DENS` matrix to the !! temporal array `BDENS`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: DENS ( maxp , NSPIN ) !! Electron density matrix real ( gp ), intent ( out ) :: BDENS ( BS , 2 * NN , NSPIN ) !! Auxiliary arrays to store the extencil from other partitions !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM if (. not . associated ( mcomm % dst )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif if (. not . associated ( mcomm % src )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = dimB ( 2 ) - NN + 1 , dimB ( 2 ) uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ) - NN + 1 , dimB ( 3 ) do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine distExtMeshData subroutine gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , BVXC , VXC ) !! Send/receive the extencil information from the `BVXC` temporal array !! to the array `VXC`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: BVXC ( BS , 2 * NN , NSPIN ) !! Auxiliar array that contains the extencil of the !! exch-corr potential real ( gp ), intent ( out ) :: VXC ( maxp , NSPIN ) !! Exch-corr potential !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = NN + 1 , 2 * NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine gathExtMeshData subroutine splitwload ( Nodes , Node , nm , wload , iDistr , oDistr ) !! Compute the limits of a new distribution, trying to split the load !! of the array `wload`. We use the nested disection algorithm in !! order to split the mesh in the 3 dimensions. !! !! We use the nested disection algorithm to split the load associated !! to the vector `wload` among all the processes. The problem is that !! every process have a different part of wload. Every time that we want !! to split a piece of the mesh, we should find which processors have that !! information. !! !! `wload` is a 3D array. In every iteration of the algorithm we should !! decide the direction of the cut. Then we should made a reduction of !! this 3-D array to a 1-D array (according to the selected direction). use mpi_siesta implicit none integer , intent ( in ) :: Nodes !! Total number of nodes integer , intent ( in ) :: Node !! Current process ID (from 1 to Node) integer , intent ( in ) :: nm ( 3 ) !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: wload ( * ) !! Weights of every point of the mesh. type ( meshDisType ), intent ( in ) :: iDistr !! Input distribution type ( meshDisType ), intent ( out ) :: oDistr !! Output distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'splitwload ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: PP , Lbox ( 2 , 3 ), Ldim , & QQ , P1 , P2 , POS , ini integer ( i8b ), pointer :: lwload (:), gwload (:), recvB (:) logical :: found , inters integer :: mGdim , mLdim , nAxis , nms ( 3 ) integer ( i8b ) :: h1 , h2 integer , pointer :: PROCS (:) integer :: MPIerror , Status ( MPI_Status_Size ) call timer ( 'SPLOAD' , 1 ) !     At the begining of the algorithm all the mesh is assigned to the !     first node:  oDistr%box(*,*,1) = nm oDistr % box ( 1 , 1 , 1 ) = 1 oDistr % box ( 2 , 1 , 1 ) = nm ( 1 ) oDistr % box ( 1 , 2 , 1 ) = 1 oDistr % box ( 2 , 2 , 1 ) = nm ( 2 ) oDistr % box ( 1 , 3 , 1 ) = 1 oDistr % box ( 2 , 3 , 1 ) = nm ( 3 ) oDistr % box ( 1 : 2 , 1 : 3 , 2 : Nodes ) = 0 nms = nm !     Array PROCS will contain the number of processes that are associated to !     every box. At the begining all the mesh is assigned to process 1, then !     PROCS(1)=Nodes, while the rest are equal to zero nullify ( PROCS , lwload , gwload , recvB ) call re_alloc ( PROCS , 1 , Nodes , 'PROCS' , 'moreMeshSubs' ) PROCS ( 1 ) = Nodes PROCS ( 2 : Nodes ) = 0 found = . true . do while ( found ) !       Choose the direction to cut the mesh nAxis = 3 if ( nms ( 2 ). gt . nms ( nAxis )) nAxis = 2 if ( nms ( 1 ). gt . nms ( nAxis )) nAxis = 1 nms ( nAxis ) = ( nms ( nAxis ) + 1 ) / 2 !       Check if we still have to keep cutting the mesh found = . false . do PP = Nodes , 1 , - 1 if ( PROCS ( PP ). GT . 1 ) then !           There are more than one processes associated to the mesh !           of process PP. We are going to split the mesh in two parts !           of p1 and p2 processors. p1 = PROCS ( PP ) / 2 p2 = PROCS ( PP ) - p1 found = . true . !           Check if the current partition has intersection with the piece of !           mesh that we want to cut. call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, Node ), & Lbox , inters ) if ( Node . eq . PP ) then mGdim = oDistr % box ( 2 , nAxis , PP ) - oDistr % box ( 1 , nAxis , PP ) + 1 call re_alloc ( gwload , 1 , mGdim , 'gwload' , 'moreMeshSubs' ) call re_alloc ( recvB , 1 , mGdim , 'recvB' , 'moreMeshSubs' ) endif if ( inters ) then !             If there is an intersection I should reduce the intersected part !             from a 3-D array to a 1-D array. mLdim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 call re_alloc ( lwload , 1 , mLdim , 'lwload' , & 'moreMeshSubs' ) call reduce3Dto1D ( nAxis , iDistr % box (:,:, Node ), Lbox , & wload , lwload ) endif if ( Node . eq . PP ) then !             If, I'm the process PP I should receive the information from other !             processes gwload = 0 do QQ = 1 , Nodes call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, QQ ), & Lbox , inters ) if ( inters ) then Ldim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 ini = Lbox ( 1 , nAxis ) - oDistr % box ( 1 , nAxis , PP ) if ( PP . eq . QQ ) then gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + lwload ( 1 : Ldim ) else call mpi_recv ( recvB , Ldim , MPI_INTEGER8 , QQ - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + recvB ( 1 : Ldim ) endif endif enddo call de_alloc ( recvB , 'recvB' , 'moreMeshSubs' ) !             Process PP computes where to cut the mesh call vecBisec ( mGdim , gwload ( 1 : mGdim ), & PROCS ( PP ), POS , h1 , h2 ) call de_alloc ( gwload , 'gwload' , 'moreMeshSubs' ) else if ( inters ) then !             If, I'm not the process PP I should send the information to !             the process PP call MPI_Send ( lwload , mLdim , & MPI_INTEGER8 , PP - 1 , 1 , MPI_Comm_World , & MPIerror ) endif if ( associated ( lwload )) & call de_alloc ( lwload , 'lwload' , 'moreMeshSubs' ) !           Process PP send the position of the cut to the rest of processes call MPI_Bcast ( pos , 1 , MPI_integer , PP - 1 , & MPI_Comm_World , MPIerror ) !           We have splitted the piece of mesh associated to process PP !           in two parts. One would be stored in position PP and the other !           would be stored in position PP+P1 QQ = PP + P1 oDistr % box ( 1 : 2 , 1 : 3 , QQ ) = oDistr % box ( 1 : 2 , 1 : 3 , PP ) pos = oDistr % box ( 1 , naxis , QQ ) + pos oDistr % box ( 1 , naxis , QQ ) = pos oDistr % box ( 2 , naxis , PP ) = pos - 1 !           We should actualize the numbers of processes associated to PP and QQ PROCS ( PP ) = P1 PROCS ( QQ ) = P2 endif enddo enddo call de_alloc ( PROCS , 'PROCS' , 'moreMeshSubs' ) call timer ( 'SPLOAD' , 2 ) end subroutine splitwload subroutine reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) !! Given a 3-D array, `wload`, we will make a reduction of its values !! to one of its dimensions (`iaxis`). `Ibox` gives the limits of the !! input array `wload` and `Lbox` gives the limits of the part that we !! want to reduce. !! !! First we compute the 3 dimensions of the input array and the !! intersection. We accumulate the values of the intersection into a !! 1-D array. !! !!     IF (iaxis=1) lwload(II) = SUM(wload(II,*,*)) !!     IF (iaxis=2) lwload(II) = SUM(wload(*,II,*)) !!     IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) implicit none integer , intent ( in ) :: iaxis !! Axe to be reduced integer , intent ( in ) :: Ibox ( 2 , 3 ) !! Limits of the input array integer , intent ( in ) :: Lbox ( 2 , 3 ) !! Limits of the intersection that we want to reduce integer , intent ( in ) :: wload ( * ) !! 3-D array that we want to reduce to one of !! its dimensions integer ( i8b ), intent ( out ) :: lwload ( * ) !! 1-D array. Reduction of the intersected part !! of wload !     Local variables integer :: Idim ( 3 ), Ldim ( 3 ), ind , ind1 , ind2 , ind3 , & I1 , I2 , I3 !     Dimensions of the input array Idim ( 1 ) = Ibox ( 2 , 1 ) - Ibox ( 1 , 1 ) + 1 Idim ( 2 ) = Ibox ( 2 , 2 ) - Ibox ( 1 , 2 ) + 1 Idim ( 3 ) = Ibox ( 2 , 3 ) - Ibox ( 1 , 3 ) + 1 !     Dimensions of the intersection. Ldim ( 1 ) = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ldim ( 2 ) = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Ldim ( 3 ) = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( iaxis . eq . 1 ) then !       Reduction into the X-axis lwload ( 1 : Ldim ( 1 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I1 ) = lwload ( I1 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else if ( iaxis . eq . 2 ) then !       Reduction into the Y-axis lwload ( 1 : Ldim ( 2 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I2 ) = lwload ( I2 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else !       Reduction into the Z-axis lwload ( 1 : Ldim ( 3 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I3 ) = lwload ( I3 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo endif end subroutine reduce3Dto1D subroutine vecBisec ( nval , values , nparts , pos , h1 , h2 ) !! Bisection of the load associated to an array. !! !! We want to split array `values` in `nparts`, but in this call to !! `vecBisec` we are going to make only one cut. First, we split `nparts` !! in two parts: `p1=nparts/2` and `p2=nparts-p1`. Then we compute the total !! load of the array `values` (`total`) and the desired load for the !! first part: !! !!     halfG = (total*p1)/nparts !! !! Finally, we try to find the position inside `values` where we are !! nearer of the the desired solution. implicit none integer , intent ( in ) :: nval !! Dimension of the input array integer ( i8b ), intent ( in ) :: values ( nval ) !! Input array integer , intent ( in ) :: nparts !! Numbers of partitions that we want to make from !! the input array (in this call we only make one cut) integer , intent ( out ) :: pos !! Position of the cut integer ( i8b ), intent ( out ) :: h1 !! Load of the first part integer ( i8b ), intent ( out ) :: h2 !! Load of the second part !     Local variables integer :: p1 , p2 , ii integer ( i8b ) :: total , halfG , halfL if ( nparts . gt . 1 ) then !       Split the number of parts in 2 p1 = nparts / 2 p2 = nparts - p1 !       Compute the total load of the array total = 0 do ii = 1 , nval total = total + values ( ii ) enddo !       Desired load of the first part halfG = ( total * p1 ) / nparts halfL = 0 pos = 0 !       Loop until we reach the solution do while ( halfL . lt . halfG ) pos = pos + 1 if ( pos . eq . nval + 1 ) STOP 'ERROR in vecBisec' halfL = halfL + values ( pos ) enddo !       Check if the previous position is better than the !       current position if (( halfL - values ( pos ) * p2 / nparts ). gt . halfG ) then halfL = halfL - values ( pos ) pos = pos - 1 endif h1 = halfL h2 = total - halfL endif end subroutine vecBisec #ifdef REORD1 subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is defined. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > Minimize the number of communications. Data don't need to !! > be communicated if it belongs to the same process in !! > different data distributions !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters !     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) !     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo !     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 !       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo !       Save the permutation for this node (a negative number means that !       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI !       Change the permutation of those nodes who pretend to use the !       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering #else subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is **NOT** defined. !! !! Here an integer parameter `PROCS_PER_NODE` is also !! used as an input. Value of `PROCS_PER_NODE` is either !! read from .fdf-file or set to 4 as default. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > * Minimize the number of communications. Data don't need to !! >   be communicated if it belongs to the same process in !! >   different data distributions !! > * Distribute memory needs among different NODES (group of processes !! >   that shares the same memory) !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` !! !!#### Behavior !! !! 1. Compute the size of all the boxes of the second distribution !! 2. Reorder the list of boxes according to its size !! 3. Create a list of buckets use fdf implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , I1 , I2 , J1 , J2 , J3 , & K1 , K2 , K3 , NN , NB , NM , SI , & SIMAX , Lbox ( 2 , 3 ) integer , pointer :: Nsiz (:), perm (:), Gprm (:), & chkb (:), box1 (:,:), box2 (:,:), & box (:,:,:) => null () integer :: PROCS_PER_NODE logical :: inters PROCS_PER_NODE = fdf_get ( 'PROCS_PER_NODE' , 4 ) !     Create groups of PROCS_PER_NODE processes NN = nodes + PROCS_PER_NODE - 1 NB = NN / PROCS_PER_NODE ! Number of buckets NM = MOD ( NN , PROCS_PER_NODE ) + 1 ! Size of the last bucket !     Allocate local arrays nullify ( Nsiz , perm , Gprm , chkb ) call re_alloc ( Nsiz , 1 , Nodes , 'Nsiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( Gprm , 1 , Nodes , 'Gprm' , 'moreMeshSubs' ) call re_alloc ( chkb , 1 , NB , 'chkb' , 'moreMeshSubs' ) call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) !     Compute the size of all the boxes of the second distribution do P1 = 1 , Nodes box2 => distr2 % box (:,:, P1 ) Nsiz ( P1 ) = ( box2 ( 2 , 1 ) - box2 ( 1 , 1 ) + 1 ) * & ( box2 ( 2 , 2 ) - box2 ( 1 , 2 ) + 1 ) * & ( box2 ( 2 , 3 ) - box2 ( 1 , 3 ) + 1 ) perm ( P1 ) = P1 enddo !     Reorder the list of boxes according to its size call myQsort ( Nodes , Nsiz , perm ) Gprm ( 1 : Nodes ) = 0 P1 = 0 !     We distribute processes in \"buckets\" of size PROCS_PER_NODE !     We have a total number of NB \"buckets\". DO I1 = 1 , PROCS_PER_NODE !       At every step of loop I1, we assign a box to every bucket if ( I1 . EQ . NM + 1 ) NB = NB - 1 !       Reset chkb. All buckets are empty. chkb ( 1 : NB ) = 0 DO I2 = 1 , NB !         At every step of loop I2, we assign a box to a different bucket P1 = P1 + 1 P2 = perm ( P1 ) ! P2 is the \"P1\"th biggest box box2 => distr2 % box (:,:, P2 ) J2 = 1 J3 = 1 SIMAX = - 1 DO J1 = 1 , Nodes !           J1=node; J2=position inside the bucket; J3=bucket index if ( chkb ( J3 ). eq . 0 . and . Gprm ( J1 ). eq . 0 ) then !             chkb(J3).eq.0 => The current bucket is not in use !             Gprm(J1).eq.0 => The node has not been permuted yet box1 => distr1 % box (:,:, J1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then SI = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) else SI = 0 endif if ( SI . gt . SIMAX ) then !               Save the information of the current node if it has the !               biggest intersection with box P2 SIMAX = SI K1 = J1 K3 = J3 endif endif !           Update information about bucket index and position J2 = J2 + 1 if ( J2 . gt . PROCS_PER_NODE ) then J2 = 1 J3 = J3 + 1 endif ENDDO !         box(P2) will be set to process K1 (that belongs to bucket K3) chkb ( K3 ) = 1 Gprm ( K1 ) = P2 ENDDO ENDDO !     Reorder boxes of the second distribution according to Gprm box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes P2 = Gprm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , P2 ) enddo !     Deallocate local arrays call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( chkb , 'chkb' , 'moreMeshSubs' ) call de_alloc ( Gprm , 'Gprm' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Nsiz , 'Nsiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering #endif subroutine compMeshComm ( distr1 , distr2 , mcomm ) !! Find the communications needed to transform one array that uses !! distribution `distr1` to distribution `distr2` !! !! Count the number of intersections between the source distribution !! and the destiny distribution. Every intersection represents a !! communication. Then we call [[scheduleComm(proc)]] !! to optimize the order of these !! communications. Finally, we save the communications that belongs to !! the current process in the variable `mcomm` use scheComm implicit none type ( meshDisType ), intent ( in ) :: distr1 !! Source distribution type ( meshDisType ), intent ( in ) :: distr2 !! Destination distribution type ( meshCommType ), intent ( out ) :: mcomm !! Communications needed !     Local variables integer :: P1 , P2 , ncom , Gcom , Lcom , & Lind , Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:) logical :: inters type ( COMM_T ) :: comm !     count the number of intersections between Source distribution and !     destiny distribution. Every intersection represents a communication. ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) ncom = ncom + 1 enddo enddo Gcom = ncom !     Allocate local arrays nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) !     Make a list of communications ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif enddo enddo comm % np = Nodes !     reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !     Count the number of communications of the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !     Allocate memory to store data of the communications of the !     current process. nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , 'moreMeshSubs' ) !     Save the list of communications for the current process ncom = 0 do P1 = 1 , comm % ncol Lind = comm % ind ( P1 , Node + 1 ) if ( Lind . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( Lind ) mcomm % dst ( ncom ) = dst ( Lind ) endif enddo mcomm % ncom = ncom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) end subroutine compMeshComm #endif END MODULE moreMeshSubs","tags":"","loc":"sourcefile/moremeshsubs.f.html","title":"moremeshsubs.F – SIESTA"},{"text":"This file depends on sourcefile~~siesta_forces.f90~~EfferentGraph sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_siesta_forces Source Code siesta_forces.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_siesta_forces implicit none private public :: siesta_forces contains subroutine siesta_forces ( istep ) !! This subroutine represents central SIESTA operation logic. #ifdef MPI use mpi_siesta #endif use units , only : eV , Ang use precision , only : dp use sys , only : bye use files , only : slabel use siesta_cml #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call use flook_siesta , only : LUA_INIT_MD , LUA_SCF_LOOP use siesta_dicts , only : dict_variable_add use m_ts_options , only : ts_scf_mixs use variable , only : cunpack #ifndef NCDF_4 use dictionary , only : assign #endif use m_mixing , only : mixers_history_init #endif use m_state_init use m_setup_hamiltonian use m_setup_H0 use m_compute_dm use m_compute_max_diff use m_scfconvergence_test use m_post_scf_work use m_mixer , only : mixer use m_mixing_scf , only : mixing_scf_converged use m_mixing_scf , only : mixers_scf_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_rhog , only : mix_rhog , compute_charge_diff use siesta_options use parallel , only : IOnode , SIESTA_worker use m_state_analysis use m_steps use m_spin , only : spin use sparse_matrices , only : DM_2D , S_1D use sparse_matrices , only : H , Hold , Dold , Dscf , Eold , Escf , maxnh use m_convergence , only : converger_t use m_convergence , only : reset , set_tolerance use siesta_geom , only : na_u ! Number of atoms in unit cell use m_energies , only : Etot ! Total energy use m_forces , only : fa , cfa ! Forces and constrained forces use m_stress , only : cstress ! Constrained stress tensor use siesta_master , only : forcesToMaster ! Send forces to master prog use siesta_master , only : siesta_server ! Is siesta a server? use m_save_density_matrix , only : save_density_matrix use m_iodm_old , only : write_spmatrix use atomlist , only : no_u , lasto , Qtot use m_dm_charge , only : dm_charge use m_pexsi_solver , only : prevDmax use write_subs , only : siesta_write_forces use write_subs , only : siesta_write_stress_pressure #ifdef NCDF_4 use dictionary use m_ncdf_siesta , only : cdf_init_file , cdf_save_settings use m_ncdf_siesta , only : cdf_save_state , cdf_save_basis #endif use m_compute_energies , only : compute_energies use m_mpi_utils , only : broadcast , barrier use fdf #ifdef SIESTA__PEXSI use m_pexsi , only : pexsi_finalize_scfloop #endif use m_check_walltime use m_energies , only : DE_NEGF use m_ts_options , only : N_Elec use m_ts_method use m_ts_global_vars , only : TSmode , TSinit , TSrun use siesta_geom , only : nsc , xa , ucell , isc_off use sparse_matrices , only : sparse_pattern , block_dist use sparse_matrices , only : S use m_ts_charge , only : ts_get_charges use m_ts_charge , only : TS_RHOCORR_METHOD use m_ts_charge , only : TS_RHOCORR_FERMI use m_ts_charge , only : TS_RHOCORR_FERMI_TOLERANCE use m_transiesta , only : transiesta use kpoint_scf_m , only : gamma_scf use m_energies , only : Ef use m_initwf , only : initwf integer , intent ( inout ) :: istep integer :: iscf logical :: first_scf , SCFconverged real ( dp ) :: dDmax ! Max. change in DM elements real ( dp ) :: dHmax ! Max. change in H elements real ( dp ) :: dEmax ! Max. change in EDM elements real ( dp ) :: drhog ! Max. change in rho(G) (experimental) real ( dp ), target :: G2max ! actually used meshcutoff type ( converger_t ) :: conv_harris , conv_freeE ! For initwf integer :: istpp #ifdef SIESTA__FLOOK ! len=24 from m_mixing.F90 character ( len = 1 ), target :: next_mixer ( 24 ) character ( len = 24 ) :: nnext_mixer integer :: imix #endif logical :: time_is_up character ( len = 40 ) :: tmp_str real ( dp ) :: Qcur #ifdef NCDF_4 type ( dict ) :: d_sav #endif #ifdef MPI integer :: MPIerror #endif external :: die , message #ifdef DEBUG call write_debug ( '    PRE siesta_forces' ) #endif #ifdef SIESTA__PEXSI ! Broadcast relevant things for program logic ! These were set in read_options, called only by \"SIESTA_workers\". call broadcast ( nscf , comm = true_MPI_Comm_World ) #endif !  Initialization tasks for a given geometry: if ( SIESTA_worker ) then call state_init ( istep ) end if #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_init\" ) #endif if ( fdf_get ( \"Sonly\" ,. false .) ) then if ( SIESTA_worker ) then call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) end if call bye ( \"S only\" ) end if Qcur = Qtot #ifdef SIESTA__FLOOK ! Add the iscf constant to the list of variables ! that are available only in this part of the routine. call dict_variable_add ( 'SCF.iteration' , iscf ) call dict_variable_add ( 'SCF.converged' , SCFconverged ) call dict_variable_add ( 'SCF.charge' , Qcur ) call dict_variable_add ( 'SCF.dD' , dDmax ) call dict_variable_add ( 'SCF.dH' , dHmax ) call dict_variable_add ( 'SCF.dE' , dEmax ) call dict_variable_add ( 'SCF.drhoG' , drhog ) ! We have to set the meshcutoff here ! because the asked and required ones are not ! necessarily the same call dict_variable_add ( 'Mesh.Cutoff.Minimum' , G2cut ) call dict_variable_add ( 'Mesh.Cutoff.Used' , G2max ) if ( mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , wmix ) else call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) ! Just to populate the table in the dictionary call dict_variable_add ( 'SCF.Mixer.Switch' , next_mixer ) end if ! Initialize to no switch next_mixer = ' ' #endif !  This call computes the **non-scf** part of  H  and initializes the !  real-space grid structures: if ( SIESTA_worker ) call setup_H0 ( G2max ) !!@todo !* It might be better to split the two, !  putting the grid initialization into **state_init (link!)** and moving the !  calculation of  H_0  to the body of the loop, done `if first_scf=.true.` !  This would suit _analysis_ runs in which **nscf = 0** !!@endtodo #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after setup_H0\" ) #endif #ifdef SIESTA__FLOOK ! Communicate with lua, just before entering the SCF loop ! This is mainly to be able to communicate ! mesh-related quantities (g2max) call slua_call ( LUA , LUA_INIT_MD ) #endif #ifdef NCDF_4 ! Initialize the NC file if ( write_cdf ) then ! Initialize the file... call cdf_init_file ( trim ( slabel ) // '.nc' , is_MD = . false .) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif ! Save the settings call cdf_save_settings ( trim ( slabel ) // '.nc' ) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif d_sav = ( 'sp' . kv . 1 ) // ( 'S' . kv . 1 ) d_sav = d_sav // ( 'nsc' . kv . 1 ) // ( 'xij' . kv . 1 ) d_sav = d_sav // ( 'xa' . kv . 1 ) // ( 'cell' . kv . 1 ) d_sav = d_sav // ( 'isc_off' . kv . 1 ) call cdf_save_state ( trim ( slabel ) // '.nc' , d_sav ) call delete ( d_sav ) ! Save the basis set call cdf_save_basis ( trim ( slabel ) // '.nc' ) end if #endif !* The dHmax variable only has meaning for Hamiltonian !  mixing, or when requiring the Hamiltonian to be converged. dDmax = - 1._dp dHmax = - 1._dp dEmax = - 1._dp drhog = - 1._dp ! Setup convergence criteria: if ( SIESTA_worker ) then if ( converge_Eharr ) then call reset ( conv_harris ) call set_tolerance ( conv_harris , tolerance_Eharr ) end if if ( converge_FreeE ) then call reset ( conv_FreeE ) call set_tolerance ( conv_FreeE , tolerance_FreeE ) end if end if !!# SCF loop !* The current structure of the loop tries to reproduce the !  historical Siesta usage. It should be made more clear. !* Two changes: ! !  1. The number of scf iterations performed is exactly !     equal to the number specified (i.e., the \"forces\" !     phase is not counted as a final scf step) !  2. At the change to a TranSiesta GF run the variable \"first_scf\" !     is implicitly reset to \"true\". ! !!## Start of SCF cycle ! !* Conditions of exit: ! !  * At the top, to catch a non-positive nscf and # of iterations !  * At the bottom, based on convergence ! iscf = 0 do while ( iscf < nscf ) iscf = iscf + 1 !* Note implications for TranSiesta when mixing H. !  Now H will be recomputed instead of simply being !  inherited, however, this is required as if !  we have bias calculations as the electric !  field across the junction needs to be present. first_scf = ( iscf == 1 ) if ( SIESTA_worker ) then ! Check whether we are short of time to continue call check_walltime ( time_is_up ) if ( time_is_up ) then ! Save DM/H if we were not saving it... !   Do any other bookeeping not done by \"die\" call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge' // & ' before wall time exhaustion' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call barrier () ! A non-root node might get first to the 'die' call call die ( \"OUT_OF_TIME: Time is up.\" ) end if call timer ( 'IterSCF' , 1 ) if ( cml_p ) & call cmlStartStep ( xf = mainXML , type = 'SCF' , index = iscf ) if ( mixH ) then if ( first_scf ) then if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then call get_H_from_file () else call setup_hamiltonian ( iscf ) end if end if call compute_DM ( iscf ) ! Maybe set Dold to zero if reading charge or H... call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) else call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) call compute_DM ( iscf ) call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) end if ! This iteration has completed calculating the new DM call compute_energies ( iscf ) if ( mix_charge ) then call compute_charge_diff ( drhog ) end if ! Note: For DM and H convergence checks. At this point: ! If mixing the DM: !        Dscf=DM_out, Dold=DM_in(mixed), H=H_in, Hold=H_in(prev step) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H_in - H_in(prev step)) ! If mixing the Hamiltonian: !        Dscf=DM_out, Dold=DM_in, H=H_(DM_out), Hold=H_in(mixed) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H(DM_out),H_in) call scfconvergence_test ( first_scf , iscf , & dDmax , dHmax , dEmax , & conv_harris , conv_freeE , & SCFconverged ) ! ** Check this heuristic if ( mixH ) then prevDmax = dHmax else prevDmax = dDmax end if ! Calculate current charge based on the density matrix call dm_charge ( spin , DM_2D , S_1D , Qcur ) ! Check whether we should step to the next mixer call mixing_scf_converged ( SCFconverged ) if ( SCFconverged . and . iscf < min_nscf ) then SCFconverged = . false . if ( IONode ) then write ( * , \"(a,i0)\" ) & \"SCF cycle continued for minimum number of iterations: \" , & min_nscf end if end if ! In case the user has requested a Fermi-level correction ! Then we start by correcting the fermi-level if ( TSrun . and . SCFconverged . and . & TS_RHOCORR_METHOD == TS_RHOCORR_FERMI ) then if ( abs ( Qcur - Qtot ) > TS_RHOCORR_FERMI_TOLERANCE ) then ! Call transiesta with fermi-correct call transiesta ( iscf , spin % H , & block_dist , sparse_pattern , Gamma_Scf , ucell , nsc , & isc_off , no_u , na_u , lasto , xa , maxnh , H , S , & Dscf , Escf , Ef , Qtot , . true ., DE_NEGF ) ! We will not have not converged as we have just ! changed the Fermi-level SCFconverged = . false . end if end if if ( monitor_forces_in_scf ) call compute_forces () ! Mix_after_convergence preserves the old behavior of ! the program. if ( (. not . SCFconverged ) . or . mix_after_convergence ) then ! Mix for next step if ( mix_charge ) then call mix_rhog ( iscf ) else call mixer ( iscf ) end if ! Save for possible restarts if ( mixH ) then call write_spmatrix ( H , file = \"H_MIXED\" , when = writeH ) call save_density_matrix ( file = \"DM_OUT\" , when = writeDM ) else call save_density_matrix ( file = \"DM_MIXED\" , when = writeDM ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = writeH ) end if end if call timer ( 'IterSCF' , 2 ) call print_timings ( first_scf , istep == inicoor ) if ( cml_p ) call cmlEndStep ( mainXML ) #ifdef SIESTA__FLOOK ! Communicate with lua call slua_call ( LUA , LUA_SCF_LOOP ) ! Retrieve an easy character string nnext_mixer = cunpack ( next_mixer ) if ( len_trim ( nnext_mixer ) > 0 . and . . not . mix_charge ) then if ( TSrun ) then do imix = 1 , size ( ts_scf_mixs ) if ( ts_scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( ts_scf_mixs ) scf_mix => ts_scf_mixs ( imix ) exit end if end do else do imix = 1 , size ( scf_mixs ) if ( scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( imix ) exit end if end do end if ! Check that we indeed have changed the mixer if ( IONode . and . scf_mix % name /= nnext_mixer ) then write ( * , '(2a)' ) 'siesta-lua: WARNING: trying to change ' , & 'to a non-existing mixer! Not changing anything!' else if ( IONode ) then write ( * , '(2a)' ) 'siesta-lua: Switching mixer method to: ' , & trim ( nnext_mixer ) end if ! Reset for next loop next_mixer = ' ' ! Update the references call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif ! ... except that we might continue for TranSiesta if ( SCFconverged ) then call transiesta_switch () ! might reset SCFconverged and iscf end if else ! non-siesta worker call compute_DM ( iscf ) end if #ifdef SIESTA__PEXSI call broadcast ( iscf , comm = true_MPI_Comm_World ) call broadcast ( SCFconverged , comm = true_MPI_Comm_World ) #endif !  Exit if converged: if ( SCFconverged ) exit end do !! **end of SCF cycle** #ifdef SIESTA__PEXSI if ( isolve == SOLVE_PEXSI ) then call pexsi_finalize_scfloop () end if #endif if ( . not . SIESTA_worker ) return call end_of_cycle_save_operations () if ( . not . SCFconverged ) then if ( SCFMustConverge ) then call message ( 'FATAL' , 'SCF_NOT_CONV: SCF did not converge' // & ' in maximum number of steps (required).' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call barrier () call die ( 'ABNORMAL_TERMINATION' ) else if ( . not . harrisfun ) then call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge  in maximum number of steps.' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) end if end if ! To write the initial wavefunctions to be used in a ! consequent TDDFT run. if ( writetdwf ) then istpp = 0 call initwf ( istpp , totime ) end if if ( TSmode . and . TSinit . and .(. not . SCFConverged ) ) then ! Signal that the DM hasn't converged, so we cannot ! continue to the transiesta routines call die ( 'ABNORMAL_TERMINATION' ) end if ! Clean-up here to limit memory usage call mixers_scf_history_init ( ) ! End of standard SCF loop. ! Do one more pass to compute forces and stresses ! Note that this call will no longer overwrite H while computing the ! final energies, forces and stresses... if ( fdf_get ( \"compute-forces\" ,. true .) ) then call post_scf_work ( istep , iscf , SCFconverged ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after post_scf_work\" ) #endif end if ! ... so H at this point is the latest generator of the DM, except ! if mixing H beyond self-consistency or terminating the scf loop ! without convergence while mixing H call state_analysis ( istep ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_analysis\" ) #endif ! If siesta is running as a subroutine, send forces to master program if ( siesta_server ) & call forcesToMaster ( na_u , Etot , cfa , cstress ) #ifdef DEBUG call write_debug ( '    POS siesta_forces' ) #endif contains ! Read the Hamiltonian from a file subroutine get_H_from_file () use sparse_matrices , only : maxnh , numh , listh , listhptr use atomlist , only : no_l use m_spin , only : spin use m_iodm_old , only : read_spmatrix logical :: found call read_spmatrix ( maxnh , no_l , spin % H , numh , & listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) end subroutine get_H_from_file ! Computes forces and stresses with the current DM_out subroutine compute_forces () use siesta_options , only : recompute_H_after_scf use m_final_H_f_stress , only : final_H_f_stress use write_subs real ( dp ), allocatable :: fa_old (:,:), Hsave (:,:) allocate ( fa_old ( size ( fa , dim = 1 ), size ( fa , dim = 2 ))) fa_old (:,:) = fa (:,:) if ( recompute_H_after_scf ) then allocate ( Hsave ( size ( H , dim = 1 ), size ( H , dim = 2 ))) Hsave (:,:) = H (:,:) end if call final_H_f_stress ( istep , iscf , . false . ) if ( recompute_H_after_scf ) then H (:,:) = Hsave (:,:) deallocate ( Hsave ) end if if ( ionode ) then print * , \"Max diff in force (eV/Ang): \" , & maxval ( abs ( fa - fa_old )) * Ang / eV call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () endif deallocate ( fa_old ) end subroutine compute_forces ! Print out timings of the first SCF loop only subroutine print_timings ( first_scf , first_md ) use timer_options , only : use_tree_timer use m_ts_global_vars , only : TSrun logical , intent ( in ) :: first_scf , first_md character ( len = 20 ) :: routine ! If this is not the first iteration, ! we immediately return. if ( . not . first_scf ) return if ( . not . first_md ) return routine = 'IterSCF' if ( TSrun ) then ! with Green function generation ! The tree-timer requires direct ! children of the routine to be ! queried. ! This is not obeyed in the TS case... :( if ( . not . use_tree_timer ) then routine = 'TS' end if endif call timer ( routine , 3 ) end subroutine print_timings ! Depending on various conditions, save the DMin ! or the DMout, and possibly keep a copy of H ! NOTE: Only if the scf cycle converged before exit it ! is guaranteed that the DM is \"pure out\" and that ! we can recover the right H if mixing H. ! subroutine end_of_cycle_save_operations () logical :: DM_write , H_write ! Depending on the option we should overwrite the ! Hamiltonian if ( mixH . and . . not . mix_after_convergence ) then ! Make sure that we keep the H actually used ! to generate the last DM, if needed. H = Hold end if DM_write = write_DM_at_end_of_cycle . and . & . not . writeDM H_write = write_H_at_end_of_cycle . and . & . not . writeH if ( mix_after_convergence ) then ! If we have been saving them, there is no point in doing ! it one more time if ( mixH ) then call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_MIXED\" , when = H_write ) else call save_density_matrix ( file = \"DM_MIXED\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if else call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if end subroutine end_of_cycle_save_operations subroutine transiesta_switch () use precision , only : dp use parallel , only : IONode use class_dSpData2D use class_Fstack_dData1D use densematrix , only : resetDenseMatrix use siesta_options , only : fire_mix , broyden_maxit use siesta_options , only : dDtol , dHtol use sparse_matrices , only : DM_2D , EDM_2D use atomlist , only : lasto use siesta_geom , only : nsc , isc_off , na_u , xa , ucell use m_energies , only : Ef use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mix , scf_mixs use m_rhog , only : resetRhoG use m_ts_global_vars , only : TSinit , TSrun use m_ts_global_vars , only : ts_print_transiesta use m_ts_method use m_ts_options , only : N_Elec , Elecs use m_ts_options , only : DM_bulk use m_ts_options , only : val_swap use m_ts_options , only : ts_Dtol , ts_Htol use m_ts_options , only : ts_hist_keep use m_ts_options , only : ts_siesta_stop use m_ts_options , only : ts_scf_mixs use m_ts_electype integer :: iEl , na_a integer , allocatable :: allowed_a (:) real ( dp ), pointer :: DM (:,:), EDM (:,:) ! We are done with the initial diagon run ! Now we start the TRANSIESTA (Green functions) run if ( . not . TSmode ) return if ( . not . TSinit ) return ! whether we are in siesta initialization step TSinit = . false . ! whether transiesta is running TSrun = . true . ! If transiesta should stop immediately if ( ts_siesta_stop ) then if ( IONode ) then write ( * , '(a)' ) 'ts: Stopping transiesta (user option)!' end if return end if ! Reduce memory requirements call resetDenseMatrix () ! Signal to continue... ! These two variables are from the top-level ! routine (siesta_forces) SCFconverged = . false . iscf = 0 ! DANGER (when/if going back to the DIAGON run, we should ! re-instantiate the original mixing value) call val_swap ( dDtol , ts_Dtol ) call val_swap ( dHtol , ts_Htol ) ! Clean up mixing history if ( mix_charge ) then call resetRhoG (. true .) else if ( associated ( ts_scf_mixs , target = scf_mixs ) ) then do iel = 1 , size ( scf_mix % stack ) call reset ( scf_mix % stack ( iel ), - ts_hist_keep ) ! Reset iteration count as certain ! mixing schemes require this for consistency scf_mix % cur_itt = n_items ( scf_mix % stack ( iel )) end do else call mixers_history_init ( scf_mixs ) end if end if ! Transfer scf_mixing to the transiesta mixing routine scf_mix => ts_scf_mixs ( 1 ) #ifdef SIESTA__FLOOK if ( . not . mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif call ts_print_transiesta () ! In case of transiesta and DM_bulk. ! In case we ask for initialization of the DM in bulk ! we read in the DM files from the electrodes and ! initialize the bulk to those values if ( DM_bulk > 0 ) then if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Initializing bulk DM in electrodes.' end if na_a = 0 do iEl = 1 , na_u if ( . not . a_isDev ( iEl ) ) na_a = na_a + 1 end do allocate ( allowed_a ( na_a )) na_a = 0 do iEl = 1 , na_u ! We allow the buffer atoms as well (this will even out the ! potential around the back of the electrode) if ( . not . a_isDev ( iEl ) ) then na_a = na_a + 1 allowed_a ( na_a ) = iEl end if end do do iEl = 1 , N_Elec if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Reading in electrode TSDE for ' // & trim ( Elecs ( iEl )% Name ) end if ! Copy over the DM in the lead ! Notice that the EDM matrix that is copied over ! will be equivalent at Ef == 0 call copy_DM ( Elecs ( iEl ), na_u , xa , lasto , nsc , isc_off , & ucell , DM_2D , EDM_2D , na_a , allowed_a ) end do ! Clean-up deallocate ( allowed_a ) if ( IONode ) then write ( * , * ) ! new-line end if ! The electrode EDM is aligned at Ef == 0 ! We need to align the energy matrix DM => val ( DM_2D ) EDM => val ( EDM_2D ) iEl = size ( DM ) call daxpy ( iEl , Ef , DM ( 1 , 1 ), 1 , EDM ( 1 , 1 ), 1 ) end if end subroutine transiesta_switch end subroutine siesta_forces end module m_siesta_forces","tags":"","loc":"sourcefile/siesta_forces.f90.html","title":"siesta_forces.F90 – SIESTA"},{"text":"Files dependent on this one sourcefile~~m_mixing.f90~~AfferentGraph sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_mixing Source Code m_mixing.F90 Source Code ! Module for all mixing methods in a standard way ! This module implements mixing of the Pulay and Broyden ! type. ! The Pulay method is implemented in the fast calculation ! setup and in the stable method. ! The stable method is executed if the inversion fails. !  - Stable: G.Kresse and J.Furthmuller, Comp. Mat. Sci. 6, 15, 1996 !  - gr (guarenteed-reduction) : http://arxiv.org/pdf/cond-mat/0005521.pdf ! All implemented methods employ a restart with variable ! history saving. module m_mixing use precision , only : dp #ifdef MPI ! MPI stuff use mpi_siesta #endif ! Intrinsic classes for retaining history use class_dData1D use class_Fstack_dData1D implicit none private save integer , parameter :: MIX_LINEAR = 1 integer , parameter :: MIX_PULAY = 2 integer , parameter :: MIX_BROYDEN = 3 integer , parameter :: MIX_FIRE = 4 ! Action tokens (binary: 0, 1, 2, 4, 8, ...!) integer , parameter :: ACTION_MIX = 0 integer , parameter :: ACTION_RESTART = 1 integer , parameter :: ACTION_NEXT = 2 type tMixer ! Name of mixer character ( len = 24 ) :: name ! The different saved variables per iteration ! and their respective stacks type ( Fstack_dData1D ), allocatable :: stack (:) ! The method of the mixer integer :: m = MIX_PULAY ! In case the mixing method has a variant ! this denote the variant ! This value is thus specific for each method integer :: v = 0 ! The currently reached iteration integer :: cur_itt = 0 , start_itt = 0 ! Different mixers may have different histories integer :: n_hist = 2 ! Number of iterations using this mixer ! There are a couple of signals here !  == 0 : !     only use this mixer until convergence !   > 0 : !     after having runned n_itt step to \"next\" integer :: n_itt = 0 ! When mod(cur_itt,restart_itt) == 0 the history will ! be _reset_ integer :: restart = 0 integer :: restart_save = 0 ! This is an action token specifying the current ! action integer :: action = ACTION_MIX ! The next mixing method following this method type ( tMixer ), pointer :: next => null () ! The next mixing method following this method ! Only used if mixing method achieved convergence ! using this method type ( tMixer ), pointer :: next_conv => null () ! ** Parameters specific for the method: ! The mixing parameter used for this mixer real ( dp ) :: w = 0._dp ! linear array of real variables used specifically ! for this mixing type real ( dp ), pointer :: rv (:) => null () integer , pointer :: iv (:) => null () #ifdef MPI ! In case we have MPI the mixing scheme ! can implement a reduction scheme. ! This can be MPI_Comm_Self to not employ any ! reductions integer :: Comm = MPI_Comm_Self #endif end type tMixer ! Indices for special constanst integer , parameter :: I_PREVIOUS_RES = 0 integer , parameter :: I_P_RESTART = - 1 integer , parameter :: I_P_NEXT = - 2 ! This index should always be the lowest index ! This is used to allocate the correct bounds for the ! additional array of information integer , parameter :: I_SVD_COND = - 3 ! Debug mixing runs logical :: debug_mix = . false . ! In case of parallel mixing this also contains the node number character ( len = 20 ) :: debug_msg = 'mix:' public :: tMixer ! Routines are divided in three sections ! 1. Routines used to construct the mixers !    Routines used to print information regarding !    the mixers public :: mixers_init , mixer_init public :: mixers_print , mixers_print_block public :: mixers_history_init public :: mixers_reset ! 2. Public functions for retrieving information !    from external routines public :: mix_method , mix_method_variant ! 3. Actual mixing methods public :: mixing public :: MIX_LINEAR , MIX_FIRE , MIX_PULAY , MIX_BROYDEN interface mixing module procedure mixing_1d , mixing_2d end interface mixing contains !> Initialize a set of mixers by reading in fdf information. !! @param[in] prefix the fdf-label prefixes !! @param[pointer] mixers the mixers that are to be initialized !! @param[in] Comm @opt optional MPI-communicator subroutine mixers_init ( prefix , mixers , Comm ) use parallel , only : IONode , Node use fdf ! FDF-prefix for searching keywords character ( len =* ), intent ( in ) :: prefix ! The array of mixers (has to be nullified upon entry) type ( tMixer ), pointer :: mixers (:) integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf type ( parsed_line ), pointer :: pline ! number of history steps saved integer :: n_hist , n_restart , n_save real ( dp ) :: w integer :: nm , im , im2 character ( len = 10 ) :: lp character ( len = 70 ) :: method , variant ! Default mixing options... if ( fdf_get ( 'Mixer.Debug' ,. false .) ) then debug_mix = IONode debug_msg = 'mix:' end if if ( fdf_get ( 'Mixer.Debug.MPI' ,. false .) ) then debug_mix = . true . write ( debug_msg , '(a,i0,a)' ) 'mix (' , Node , '):' end if lp = trim ( prefix ) // '.Mixer' ! ensure nullification call mixers_reset ( mixers ) ! Return immediately if the user hasn't defined ! an fdf-block for the mixing options... if ( . not . fdf_block ( trim ( lp ) // 's' , bfdf ) ) return ! update mixing weight and kick mixing weight w = fdf_get ( trim ( lp ) // '.Weight' , 0.1_dp ) ! Get history length n_hist = fdf_get ( trim ( lp ) // '.History' , 6 ) ! Restart after this number of iterations n_restart = fdf_get ( trim ( lp ) // '.Restart' , 0 ) n_save = fdf_get ( trim ( lp ) // '.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Read in the options regarding the mixing options nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 end do if ( nm == 0 ) then call die ( 'mixing: No mixing schemes selected. & &Please at least add one mixer.' ) end if ! Allocate all denoted mixers... allocate ( mixers ( nm )) mixers (:)% w = w mixers (:)% n_hist = n_hist mixers (:)% restart = n_restart mixers (:)% restart_save = n_save ! Rewind to grab names. call fdf_brewind ( bfdf ) nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 mixers ( nm )% name = fdf_bnames ( pline , 1 ) end do ! Now read all mixers for this segment and their options do im = 1 , nm call read_block ( mixers ( im ) ) end do ! Create history stack and associate correct ! stack pointers call mixers_history_init ( mixers ) #ifdef MPI if ( present ( Comm ) ) then mixers (:)% Comm = Comm else mixers (:)% Comm = MPI_Comm_World end if #endif contains subroutine read_block ( m ) type ( tMixer ), intent ( inout ), target :: m character ( len = 64 ) :: opt ! create block string opt = trim ( lp ) // '.' // trim ( m % name ) if ( . not . fdf_block ( opt , bfdf ) ) then call die ( 'Block: ' // trim ( opt ) // ' does not exist!' ) end if ! Default to the pulay method... ! This enables NOT writing this in the block method = 'pulay' variant = ' ' ! read method do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'method' ) ) then method = fdf_bnames ( pline , 2 ) else if ( leqi ( opt , 'variant' ) ) then variant = fdf_bnames ( pline , 2 ) end if end do ! Retrieve the method and the variant m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! Define separate defaults which are ! not part of the default input options select case ( m % m ) case ( MIX_LINEAR ) m % n_hist = 0 end select call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'iterations' ) & . or . leqi ( opt , 'itt' ) ) then m % n_itt = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'history' ) ) then m % n_hist = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'weight' ) . or . leqi ( opt , 'w' ) ) then m % w = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'restart' ) ) then m % restart = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'restart.save' ) ) then m % restart_save = fdf_bintegers ( pline , 1 ) m % restart_save = max ( 0 , m % restart_save ) end if end do ! Initialize the mixer by setting the correct ! standard options and allocate space in the mixers... call mixer_init ( m ) ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'next' ) ) then nullify ( m % next ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next ) ) then call die ( 'mixing: Could not find next mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next , target = m ) ) then call die ( 'mixing: Next *must* not be it-self. & &Please change accordingly.' ) end if else if ( leqi ( opt , 'next.conv' ) ) then nullify ( m % next_conv ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next_conv => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next_conv ) ) then call die ( 'mixing: Could not find next convergence mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next_conv , target = m ) ) then call die ( 'mixing: next.conv *must* not be it-self. & &Please change accordingly.' ) end if end if end do ! Ensure that if a next have not been specified ! it will continue indefinitely. if ( . not . associated ( m % next ) ) then m % n_itt = 0 end if ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) ! skip lines without associated content if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) ! Do options so that a pulay option may refer to ! the actual names of the constants if ( m % m == MIX_PULAY ) then ! The linear mixing weight if ( leqi ( opt , 'weight.linear' ) & . or . leqi ( opt , 'w.linear' ) ) then m % rv ( 1 ) = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'svd.cond' ) ) then ! This is only applicable to the Pulay ! mixing scheme... m % rv ( I_SVD_COND ) = fdf_bvalues ( pline , 1 ) end if end if ! Generic options for all advanced methods... if ( leqi ( opt , 'next.p' ) ) then ! Only allow stepping to the next when ! having a next associated if ( associated ( m % next ) ) then m % rv ( I_P_NEXT ) = fdf_bvalues ( pline , 1 ) end if else if ( leqi ( opt , 'restart.p' ) ) then m % rv ( I_P_RESTART ) = fdf_bvalues ( pline , 1 ) end if end do end subroutine read_block end subroutine mixers_init !> Initialize a single mixer depending on the preset !! options. Useful for external correct setup. !! !! @param[inout] mix mixer to be initialized subroutine mixer_init ( mix ) type ( tMixer ), intent ( inout ) :: mix integer :: n ! Correct amount of history in the mixing. if ( 0 < mix % restart . and . & mix % restart < mix % n_hist ) then ! This is if we restart this scheme, ! then it does not make sense to have a history ! greater than the restart count mix % n_hist = mix % restart end if if ( 0 < mix % n_itt . and . & mix % n_itt < mix % n_hist ) then ! If this only runs for n_itt itterations, ! it makes no sense to have a history greater ! than this. mix % n_hist = mix % n_itt end if select case ( mix % m ) case ( MIX_LINEAR ) allocate ( mix % rv ( I_SVD_COND : 0 )) ! Kill any history settings that do not apply to the ! linear mixer. mix % restart = 0 mix % restart_save = 0 case ( MIX_PULAY ) allocate ( mix % rv ( I_SVD_COND : 1 )) mix % rv ( 1 ) = mix % w ! We allocate the double residual (n_hist-1) mix % n_hist = max ( 2 , mix % n_hist ) if ( mix % v == 1 . or . mix % v == 3 ) then ! The GR method requires an even number ! of restart steps ! And then we ensure the history to be aligned ! with a restart (restart has precedence) mix % restart = mix % restart + mod ( mix % restart , 2 ) end if case ( MIX_BROYDEN ) ! allocate temporary array mix % n_hist = max ( 2 , mix % n_hist ) n = 1 + mix % n_hist allocate ( mix % rv ( I_SVD_COND : n )) mix % rv ( 1 : n ) = mix % w end select if ( mix % restart < 0 ) then call die ( 'mixing: restart count must be positive' ) end if mix % restart_save = min ( mix % n_hist - 1 , mix % restart_save ) mix % restart_save = max ( 0 , mix % restart_save ) ! This is the restart parameter ! I.e. if |f_k / f - 1| < rp ! only works for positive rp mix % rv ( I_PREVIOUS_RES ) = huge ( 1._dp ) mix % rv ( I_P_RESTART ) = - 1._dp mix % rv ( I_P_NEXT ) = - 1._dp mix % rv ( I_SVD_COND ) = 1.e-8_dp end subroutine mixer_init !> Initialize all history for the mixers !! !! Routine for clearing all history and setting up the !! arrays so that they may be used subsequently. !! !! @param[inout] mixers the mixers to be initialized subroutine mixers_history_init ( mixers ) type ( tMixer ), intent ( inout ), target :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns logical :: is_GR do im = 1 , size ( mixers ) m => mixers ( im ) if ( debug_mix . and . current_itt ( m ) >= 1 ) then write ( * , '(a,a)' ) trim ( debug_msg ), & ' resetting history of all mixers' exit end if end do ! Clean up all arrays and reference counted ! objects do im = 1 , size ( mixers ) m => mixers ( im ) ! reset history track m % start_itt = 0 m % cur_itt = 0 ! do not try and de-allocate something not ! allocated if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do ! clean-up deallocate ( m % stack ) end if ! Re-populate select case ( m % m ) case ( MIX_LINEAR ) ! do nothing case ( MIX_PULAY ) is_GR = ( m % v == 1 ) . or . ( m % v == 3 ) if ( . not . is_GR ) then allocate ( m % stack ( 3 )) else allocate ( m % stack ( 2 )) end if ! These arrays contains these informations !   s1 = m%stack(1) !   s2 = m%stack(2) !   s3 = m%stack(3) ! Here <> is input function, x[in], and ! <>' is the corresponding output, x[out]. ! First iteration: !   s1 = { 1' - 1 } !   s3 = { 1' } ! Second iteration !   s2 = { 2' - 2 - (1' - 1) } !   s1 = { 2 - 1 , 2' - 2 } !   s3 = { 2' } ! Third iteration !   s2 = { 2' - 2 - (1' - 1) , 3' - 3 - (2' - 2) } !   s1 = { 2 - 1 , 3 - 2, 3' - 3 } !   s3 = { 3' } ! and so on ! allocate x[i+1] - x[i] call new ( m % stack ( 1 ), m % n_hist ) ! allocate F[i+1] - F[i] call new ( m % stack ( 2 ), m % n_hist - 1 ) if ( . not . is_GR ) then call new ( m % stack ( 3 ), 1 ) end if case ( MIX_BROYDEN ) ! Same as original Pulay allocate ( m % stack ( 3 )) call new ( m % stack ( 1 ), m % n_hist ) call new ( m % stack ( 2 ), m % n_hist - 1 ) call new ( m % stack ( 3 ), 1 ) end select end do end subroutine mixers_history_init !> Reset the mixers, i.e. clean _everything_ !! !! Also deallocates (and nullifies) the input array! !! !! @param[inout] mixers array of mixers to be cleaned subroutine mixers_reset ( mixers ) type ( tMixer ), pointer :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns if ( . not . associated ( mixers ) ) return do im = 1 , size ( mixers ) m => mixers ( im ) if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do deallocate ( m % stack ) end if if ( associated ( m % rv ) ) then deallocate ( m % rv ) nullify ( m % rv ) end if if ( associated ( m % iv ) ) then deallocate ( m % iv ) nullify ( m % iv ) end if end do deallocate ( mixers ) nullify ( mixers ) end subroutine mixers_reset !> Print (to std-out) information regarding the mixers !! !! @param[in] prefix the prefix (fdf) for the mixers !! @param[in] mixers array of mixers allocated subroutine mixers_print ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m character ( len = 50 ) :: fmt logical :: bool integer :: i if ( . not . IONode ) return fmt = 'mix.' // trim ( prefix ) // ':' if ( debug_mix ) then write ( * , '(2a,t50,''= '',l)' ) trim ( fmt ), & ' Debug messages' , debug_mix end if ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Linear mixing' , trim ( m % name ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w if ( m % n_hist > 0 . and . (& associated ( m % next ) & . or . associated ( m % next_conv )) ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Carried history steps' , m % n_hist end if case ( MIX_PULAY ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Pulay mixing' , trim ( m % name ) select case ( m % v ) case ( 0 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable' case ( 1 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR' case ( 2 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable-SVD' case ( 3 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR-SVD' end select write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Linear mixing weight' , m % rv ( 1 ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w write ( * , '(2a,t50,''= '',e10.4)' ) trim ( fmt ), & '    SVD condition' , m % rv ( I_SVD_COND ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_BROYDEN ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Broyden mixing' , trim ( m % name ) !write(*,'(2a,t50,''= '',a)') trim(fmt), & !     '    Variant','original' write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Jacobian weight' , m % w write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Weight prime' , m % rv ( 1 ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_FIRE ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Fire mixing' , trim ( m % name ) end select if ( m % n_itt > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Number of mixing iterations' , m % n_itt if ( associated ( m % next ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer' , trim ( m % next % name ) else call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if end if if ( associated ( m % next_conv ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer upon convergence' , trim ( m % next_conv % name ) end if end do end subroutine mixers_print !> Print (to std-out) the fdf-blocks that recreate the mixer settings !! !! @param[in] prefix the fdf-prefix for reading the blocks !! @param[in] mixers array of mixers that should be printed !!    their fdf-blocks subroutine mixers_print_block ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m logical :: bool integer :: i if ( . not . IONode ) return ! Write block of input write ( * , '(/3a)' ) '%block ' , trim ( prefix ), '.Mixers' do i = 1 , size ( mixers ) m => mixers ( i ) write ( * , '(t3,a)' ) trim ( m % name ) end do write ( * , '(3a)' ) '%endblock ' , trim ( prefix ), '.Mixers' ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) ! Write out this block write ( * , '(/4a)' ) '%block ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) write ( * , '(t3,a)' ) '# Mixing method' ! Write out method select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'linear' case ( MIX_PULAY ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'pulay' select case ( m % v ) case ( 0 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable' case ( 1 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR' case ( 2 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable+SVD' case ( 3 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR+SVD' end select case ( MIX_BROYDEN ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'broyden' ! currently no variants exists end select ! remark write ( * , '(/,t3,a)' ) '# Mixing options' ! Weight ! For Broyden this is the inverse Jacobian write ( * , '(t3,a,f6.4)' ) 'weight ' , m % w select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) write ( * , '(t3,a,f6.4)' ) 'weight.linear ' , m % rv ( 1 ) end select if ( m % n_hist > 0 ) then write ( * , '(t3,a,i0)' ) 'history ' , m % n_hist end if bool = . false . if ( m % restart > 0 ) then write ( * , '(t3,a,i0)' ) 'restart ' , m % restart bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(t3,a,e10.5)' ) 'restart.p ' , m % rv ( I_P_RESTART ) bool = . true . end if end select if ( bool ) then write ( * , '(t3,a,i0)' ) 'restart.save ' , m % restart_save end if ! remark bool = . false . if ( m % n_itt > 0 ) then write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,i0)' ) 'iterations ' , m % n_itt bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,f6.4)' ) 'next.p ' , m % rv ( I_P_NEXT ) bool = . true . end if end select if ( bool . and . associated ( m % next ) ) then write ( * , '(t2,2(tr1,a))' ) 'next' , trim ( m % next % name ) else if ( bool ) then call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if if ( associated ( m % next_conv ) ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t2,2(tr1,a))' ) 'next.conv' , trim ( m % next_conv % name ) end if write ( * , '(4a)' ) '%endblock ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) end do write ( * , * ) ! new-line end subroutine mixers_print_block !> Return the integer specification of the mixing type !! !! @param[in] str the character representation of the mixing type !! @return the integer corresponding to the mixing type function mix_method ( str ) result ( m ) use fdf , only : leqi character ( len =* ), intent ( in ) :: str integer :: m if ( leqi ( str , 'linear' ) ) then m = MIX_LINEAR else if ( leqi ( str , 'pulay' ) . or . & leqi ( str , 'diis' ) . or . & leqi ( str , 'anderson' ) ) then m = MIX_PULAY else if ( leqi ( str , 'broyden' ) ) then m = MIX_BROYDEN else if ( leqi ( str , 'fire' ) ) then m = MIX_FIRE call die ( 'mixing: FIRE currently not supported.' ) else call die ( 'mixing: Unknown mixing variant.' ) end if end function mix_method !> Return the variant of the mixing method !! !! @param[in] m the integer type of the mixing method !! @param[in] str the character specification of the mixing method variant !! @return the variant of the mixing method function mix_method_variant ( m , str ) result ( v ) use fdf , only : leqi integer , intent ( in ) :: m character ( len =* ), intent ( in ) :: str integer :: v v = 0 select case ( m ) case ( MIX_LINEAR ) ! no variants case ( MIX_PULAY ) v = 0 ! We do not implement tho non-stable version ! There is no need to have an inferior Pulay mixer... if ( leqi ( str , 'original' ) . or . & leqi ( str , 'kresse' ) . or . leqi ( str , 'stable' ) ) then ! stable version, will nearly always succeed on inversion v = 0 else if ( leqi ( str , 'original+svd' ) . or . & leqi ( str , 'kresse+svd' ) . or . leqi ( str , 'stable+svd' ) ) then ! stable version, will nearly always succeed on inversion v = 2 else if ( leqi ( str , 'gr' ) . or . & leqi ( str , 'guarenteed-reduction' ) . or . & leqi ( str , 'bowler-gillan' ) ) then ! Guarenteed reduction version v = 1 else if ( leqi ( str , 'gr+svd' ) . or . & leqi ( str , 'guarenteed-reduction+svd' ) . or . & leqi ( str , 'bowler-gillan+svd' ) ) then ! Guarenteed reduction version v = 3 end if case ( MIX_BROYDEN ) ! Currently only one variant v = 0 case ( MIX_FIRE ) ! no variants end select end function mix_method_variant ! The basic mixing procedure is this: ! 1. Initialize the mixing algorithm !    This will typically mean that one needs to !    push input and output matrices ! 2. Calculate the mixing coefficients ! 3. Use coefficients to calculate the !    next (optimized) guess ! 4. Finalize the mixing method ! ! Having the routines split up in this manner ! allows one to skip step 2 and use coefficients ! from another set of input/output to retrieve the ! mixing coefficients. ! Say we may retrieve mixing coefficients from ! the Hamiltonian, but use them for the density-matrix !> Initialize the mixing algorithm !! !! @param[pointer] mix the mixing method !! @param[in] n size of the arrays to be used in the algorithm !! @param[in] xin array of the input variables !! @param[in] xout array of the output variables subroutine mixing_init ( mix , n , xin , F ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n ! In/out of the function real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), pointer :: res (:), rres (:) integer :: i , ns real ( dp ) :: dnorm , dtmp logical :: p_next , p_restart ! Initialize action for mixer mix % action = ACTION_MIX ! Step iterator (so first mixing has cur_itt == 1) mix % cur_itt = mix % cur_itt + 1 ! If we are going to skip to next, we signal it ! before entering if ( mix % n_itt > 0 . and . & mix % n_itt <= current_itt ( mix ) ) then mix % action = IOR ( mix % action , ACTION_NEXT ) end if ! Check whether the residual norm is below a certain ! criteria p_next = mix % rv ( I_P_NEXT ) > 0._dp p_restart = mix % rv ( I_P_RESTART ) > 0._dp ! Check whether a parameter next/restart is required if ( p_restart . or . p_next ) then ! Calculate norm: ||f_k|| dnorm = norm ( n , F , F ) #ifdef MPI dtmp = dnorm call MPI_AllReduce ( dtmp , dnorm , 1 , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) #endif ! Calculate the relative difference dtmp = abs ( dnorm / mix % rv ( I_PREVIOUS_RES ) - 1._dp ) ! We first check for next, that has precedence if ( p_next ) then if ( dtmp < mix % rv ( I_P_NEXT ) ) then ! Signal stepping mixer mix % action = IOR ( mix % action , ACTION_NEXT ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < np  :  ' , & dtmp , ' < ' , mix % rv ( I_P_NEXT ) end if if ( p_restart ) then if ( dtmp < mix % rv ( I_P_RESTART ) ) then ! Signal restart mix % action = IOR ( mix % action , ACTION_RESTART ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < rp  :  ' , & dtmp , ' < ' , mix % rv ( I_P_RESTART ) end if ! Store the new residual norm mix % rv ( I_PREVIOUS_RES ) = dnorm end if ! Push information to the stack select case ( mix % m ) case ( MIX_LINEAR ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' linear' call init_linear () case ( MIX_PULAY ) if ( debug_mix ) then select case ( mix % v ) case ( 0 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay' case ( 1 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay, GR' case ( 2 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD' case ( 3 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD, GR' end select end if call init_pulay () case ( MIX_BROYDEN ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' Broyden' call init_broyden () end select contains subroutine init_linear () ! information for this depends on the ! following method call fake_history_from_linear ( mix % next ) call fake_history_from_linear ( mix % next_conv ) end subroutine init_linear subroutine init_pulay () logical :: GR_linear select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do end if case ( 1 , 3 ) ! Whether this is the linear cycle... GR_linear = mod ( current_itt ( mix ), 2 ) == 1 ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F , mix % rv ( 1 )) ns = n_items ( mix % stack ( 1 )) if ( GR_linear . and . current_itt ( mix ) > 1 . and . & ns > 1 ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = rres ( i ) + res ( i ) end do !$OMP end parallel do else if ( ns > 1 . and . . not . GR_linear ) then ! now we can calculate RRes[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) end if end select end subroutine init_pulay subroutine init_broyden () ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do else ! Store F[x_in] (used to create the input residual) call push_stack_data ( mix % stack ( 3 ), n ) res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if end subroutine init_broyden subroutine fake_history_from_linear ( next ) type ( tMixer ), pointer :: next real ( dp ), pointer :: t1 (:), t2 (:) integer :: ns , nh , i , nhl if ( . not . associated ( next ) ) return ! Reduce to # history of linear nhl = mix % n_hist ! if the number of fake-history steps saved is ! zero we immediately return. ! Only if mix%n_hist > 0 will the below ! occur. if ( nhl == 0 ) return ! Check for the type of following method select case ( next % m ) case ( MIX_PULAY ) ! Here it depends on the variant select case ( next % v ) case ( 0 , 2 ) ! stable pulay mixing ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select case ( MIX_BROYDEN ) ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns >= 2 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select end subroutine fake_history_from_linear end subroutine mixing_init !> Function to retrieve the number of coefficients !! calculated in this iteration. !! This is so external routines can query the size !! of the arrays used. !! !! @param[in] mix the used mixer function mixing_ncoeff ( mix ) result ( n ) type ( tMixer ), intent ( in ) :: mix integer :: n n = 0 select case ( mix % m ) case ( MIX_PULAY ) n = n_items ( mix % stack ( 2 )) case ( MIX_BROYDEN ) n = n_items ( mix % stack ( 2 )) end select end function mixing_ncoeff !> Calculate the mixing coefficients for the !! current mixer !! !! @param[in] mix the current mixer !! @param[in] n the number of elements used to calculate !!           the coefficients !! @param[in] xin the input value !! @param[in] F xout - xin, (residual) !! @param[out] coeff the coefficients subroutine mixing_coeff ( mix , n , xin , F , coeff ) use parallel , only : IONode type ( tMixer ), intent ( inout ) :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), intent ( out ) :: coeff (:) integer :: ncoeff ncoeff = size ( coeff ) if ( ncoeff < mixing_ncoeff ( mix ) ) then write ( * , '(a)' ) 'mix: Error in calculating coefficients' ! Do not allow this... return end if select case ( mix % m ) case ( MIX_LINEAR ) call linear_coeff () case ( MIX_PULAY ) call pulay_coeff () case ( MIX_BROYDEN ) call broyden_coeff () end select contains subroutine linear_coeff () integer :: i do i = 1 , ncoeff coeff ( i ) = 0._dp end do end subroutine linear_coeff subroutine pulay_coeff () integer :: ns , nh , nmax integer :: i , j , info logical :: lreturn ! Calculation quantities real ( dp ) :: dnorm , G real ( dp ), pointer :: res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) lreturn = . false . ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) return ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! Allocate arrays for calculating the ! coefficients allocate ( A ( nh , nh ), Ainv ( nh , nh )) ! Calculate A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = norm(RRes[i],RRes[j]) A ( i , j ) = norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Diagonal A ( i , i ) = norm ( n , rres1 , rres1 ) end do #ifdef MPI ! Global operations, but only for the non-extended entries call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) ! copy over reduced arrays A = Ainv #endif ! Get inverse of matrix select case ( mix % v ) case ( 0 , 1 ) call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if case ( 2 , 3 ) ! We forcefully use the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end select ! NOTE, although mix%stack(1) contains ! the x[i] - x[i-1], the tip of the stack ! contains F[i]! ! res == F[i] res => getstackval ( mix , 1 ) ! Initialize the coefficients do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients on all processors do j = 1 , nh ! res  == F[i] ! rres == F[j+1] - F[j] rres => getstackval ( mix , 2 , j ) dnorm = norm ( n , rres , res ) do i = 1 , nh coeff ( i ) = coeff ( i ) - Ainv ( i , j ) * dnorm end do end do #ifdef MPI ! Reduce the coefficients call MPI_AllReduce ( coeff ( 1 ), A ( 1 , 1 ), nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh coeff ( i ) = A ( i , 1 ) end do #endif else info = 0 ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, SVD failed, > linear' end if ! Clean up memory deallocate ( A , Ainv ) end subroutine pulay_coeff subroutine broyden_coeff () integer :: ns , nh , nmax integer :: i , j , k , info ! Calculation quantities real ( dp ) :: dnorm , dtmp real ( dp ), pointer :: w (:), res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: c (:), A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) ! Easy check for initial step... if ( ns == 1 ) then ! reset coeff = 0._dp return end if ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! This is the modified Broyden algorithm... ! Retrieve the previous weights w => mix % rv ( 2 : 1 + nh ) select case ( mix % v ) case ( 2 ) ! Unity Broyden w ( nh ) = 1._dp case ( 1 ) ! RMS Broyden dnorm = norm ( n , F , F ) #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif w (:) = 1._dp / sqrt ( dnorm ) if ( debug_mix ) & write ( * , '(2(a,e10.4))' ) & trim ( debug_msg ) // ' weight = ' , w ( 1 ), & ' , norm = ' , dnorm case ( 0 ) ! Varying weight dnorm = 0._dp !$OMP parallel do default(shared), private(i), & !$OMP& reduction(max:dnorm) do i = 1 , n dnorm = max ( dnorm , abs ( F ( i )) ) end do !$OMP end parallel do #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif ! Problay 0.2 should be changed to user-defined w ( nh ) = exp ( 1._dp / ( dnorm + 0.2_dp ) ) if ( debug_mix ) & write ( * , '(2a,1000(tr1,e10.4))' ) & trim ( debug_msg ), ' weights = ' , w ( 1 : nh ) end select ! Allocate arrays used allocate ( c ( nh )) allocate ( A ( nh , nh ), Ainv ( nh , nh )) !  < RRes[i] | Res[n] > do i = 1 , nh rres => getstackval ( mix , 2 , i ) c ( i ) = norm ( n , rres , F ) end do #ifdef MPI call MPI_AllReduce ( c ( 1 ), A ( 1 , 1 ), nh , & MPI_Double_Precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh c ( i ) = A ( i , 1 ) end do #endif ! Create A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = dot_product(RRes[i],RRes[j]) A ( i , j ) = w ( i ) * w ( j ) * norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Do the diagonal term A ( i , i ) = w ( i ) * w ( i ) * norm ( n , rres1 , rres1 ) end do #ifdef MPI call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) A = Ainv #endif ! Add the diagonal term ! This should also prevent it from being ! singular (unless mix%w == 0) do i = 1 , nh A ( i , i ) = mix % rv ( 1 ) ** 2 + A ( i , i ) end do ! Calculate the inverse call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients... do i = 1 , nh do j = 1 , nh ! Ainv should be symmetric (A is) coeff ( i ) = coeff ( i ) + w ( j ) * c ( j ) * Ainv ( j , i ) end do ! Calculate correct weight... coeff ( i ) = - w ( i ) * coeff ( i ) end do else ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, SVD failed, > linear' end if deallocate ( A , Ainv ) end subroutine broyden_coeff end subroutine mixing_coeff !> Calculate the guess for the next iteration !! !! Note this gets passed the coefficients. Hence, !! they may be calculated from another set of history !! steps. !! This may be useful in certain situations. !! !! @param[in] mix the current mixer !! @param[in] n the number of elements used to calculate !!           the coefficients !! @param[in] xin the input value !! @param[in] F the xin residual !! @param[out] xnext the input for the following iteration !! @param[in] coeff the coefficients subroutine mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ) real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( out ) :: xnext ( n ) real ( dp ), intent ( in ) :: coeff (:) select case ( mix % m ) case ( MIX_LINEAR ) call mixing_linear () case ( MIX_PULAY ) call mixing_pulay () case ( MIX_BROYDEN ) call mixing_broyden () end select contains subroutine mixing_linear () integer :: i real ( dp ) :: w w = mix % w if ( debug_mix ) write ( * , '(2a,e10.4)' ) & trim ( debug_msg ), ' alpha = ' , w !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + w * F ( i ) end do !$OMP end parallel do end subroutine mixing_linear subroutine mixing_pulay () integer :: ns , nh integer :: i , j logical :: lreturn real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Pulay' xnext = 0._dp return end if ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Pulay (initial), alpha = ' , mix % rv ( 1 ) case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Direct mixing, alpha = ' , mix % rv ( 1 ) end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) then ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ),& ' G = ' , G , ', sum(alpha) = ' , sum ( coeff ), & ', alpha = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_pulay subroutine mixing_broyden () integer :: ns , nh integer :: i , j real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Broyden' xnext = 0._dp return end if if ( ns == 1 ) then if ( debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Broyden (initial), alpha = ' , mix % rv ( 1 ) ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' G = ' , G , & ', sum(coeff) = ' , sum ( coeff ), & ', coeff = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_broyden end subroutine mixing_calc_next !> Finalize the mixing algorithm !! !! @param[inout] mix mixer to be finalized !! @param[in] n size of the input arrays !! @param[in] xin the input for this iteration !! @param[in] F the residual for this iteration !! @param[in] xnext the optimized input for the next iteration subroutine mixing_finalize ( mix , n , xin , F , xnext ) use parallel , only : IONode type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ), xnext ( n ) integer :: rsave select case ( mix % m ) case ( MIX_LINEAR ) call fin_linear () case ( MIX_PULAY ) call fin_pulay () case ( MIX_BROYDEN ) call fin_broyden () end select ! Fix the action to finalize it.. if ( mix % restart > 0 . and . & mod ( current_itt ( mix ), mix % restart ) == 0 ) then mix % action = IOR ( mix % action , ACTION_RESTART ) end if ! Check the actual finalization... ! First check whether we should restart history if ( IAND ( mix % action , ACTION_RESTART ) == ACTION_RESTART ) then ! The user has requested to restart the ! mixing scheme now rsave = mix % restart_save select case ( mix % m ) case ( MIX_PULAY ) if ( IONode ) then write ( * , '(a)' ) 'mix: Pulay -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if case ( MIX_BROYDEN ) if ( IONode ) then write ( * , '(a)' ) 'mix: Broyden -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if end select if ( allocated ( mix % stack ) ) then if ( debug_mix ) & write ( * , '(a,a,i0)' ) trim ( debug_msg ), & ' saved hist = ' , n_items ( mix % stack ( 1 )) end if end if ! check whether we should change the mixer if ( IAND ( mix % action , ACTION_NEXT ) == ACTION_NEXT ) then call mixing_step ( mix ) end if contains subroutine fin_linear () ! do nothing... end subroutine fin_linear subroutine fin_pulay () integer :: ns integer :: i logical :: GR_linear real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) select case ( mix % v ) case ( 0 , 2 ) ! stable Pulay if ( n_items ( mix % stack ( 3 )) == 0 ) then call push_stack_data ( mix % stack ( 3 ), n ) end if res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do case ( 1 , 3 ) ! GR Pulay GR_linear = mod ( current_itt ( mix ), 2 ) == 1 if ( n_items ( mix % stack ( 2 )) > 0 . and . & . not . GR_linear ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  rres == F[i] - F[i-1] rres ( i ) = rres ( i ) - res ( i ) ! Output: !  rres == - F[i-1] end do !$OMP end parallel do call pop ( mix % stack ( 1 )) ! Note that this is Res[i-1] = (F&#94;i-1_out - F&#94;i-1_in) res => getstackval ( mix , 1 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - xin ( i ) + xnext ( i ) end do !$OMP end parallel do end if end select end subroutine fin_pulay subroutine fin_broyden () integer :: ns , nh integer :: i real ( dp ), pointer :: res (:), rres (:) ns = current_itt ( mix ) nh = n_items ( mix % stack ( 2 )) if ( ns >= 2 . and . n_items ( mix % stack ( 3 )) > 0 ) then ! Update the residual to reflect the input residual res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do end if ! Update weights (if necessary) if ( nh > 1 ) then do i = 2 , nh mix % rv ( i ) = mix % rv ( i + 1 ) end do end if end subroutine fin_broyden end subroutine mixing_finalize ! Perform the actual mixing... subroutine mixing_1d ( mix , n , xin , F , xnext , nsub ) ! The current mixing method type ( tMixer ), pointer :: mix ! The current step in the SCF and size of arrays integer , intent ( in ) :: n ! x1 == Input function, ! F1 == Residual from x1 real ( dp ), intent ( in ) :: xin ( n ), F ( n ) ! x2 == Next input function real ( dp ), intent ( inout ) :: xnext ( n ) ! Number of elements used for calculating the mixing ! coefficients integer , intent ( in ), optional :: nsub ! Coefficients integer :: ncoeff real ( dp ), allocatable :: coeff (:) call mixing_init ( mix , n , xin , F ) ncoeff = mixing_ncoeff ( mix ) allocate ( coeff ( ncoeff )) ! Calculate coefficients if ( present ( nsub ) ) then call mixing_coeff ( mix , nsub , xin , F , coeff ) else call mixing_coeff ( mix , n , xin , F , coeff ) end if ! Calculate the following output call mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! Coefficients are not needed anymore... deallocate ( coeff ) ! Finalize the mixer call mixing_finalize ( mix , n , xin , F , xnext ) end subroutine mixing_1d subroutine mixing_2d ( mix , n1 , n2 , xin , F , xnext , nsub ) type ( tMixer ), pointer :: mix integer , intent ( in ) :: n1 , n2 real ( dp ), intent ( in ) :: xin ( n1 , n2 ), F ( n1 , n2 ) real ( dp ), intent ( inout ) :: xnext ( n1 , n2 ) integer , intent ( in ), optional :: nsub ! Simple wrapper for 1D if ( present ( nsub ) ) then call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 ) ,& nsub = n1 * nsub ) else call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 )) end if end subroutine mixing_2d ! Step the mixing object and ensure that ! the old history is either copied over or freed subroutine mixing_step ( mix ) use parallel , only : IONode type ( tMixer ), pointer :: mix type ( tMixer ), pointer :: next => null () type ( dData1D ), pointer :: d1D integer :: i , is , n , init_itt logical :: reset_stack , copy_stack ! First try and next => mix % next if ( associated ( next ) ) then ! Whether or not the two methods are allowed ! to share history copy_stack = mix % m == next % m select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) copy_stack = . true . end select end select copy_stack = copy_stack . and . allocated ( mix % stack ) ! If the two methods are similar if ( copy_stack ) then ! They are similar, copy over the history stack do is = 1 , size ( mix % stack ) ! Get maximum size of the current stack, n = n_items ( mix % stack ( is )) ! Note that this will automatically take care of ! wrap-arounds and delete the unneccesry elements do i = 1 , n d1D => get_pointer ( mix % stack ( is ), i ) call push ( next % stack ( is ), d1D ) end do ! nullify nullify ( d1D ) end do end if end if reset_stack = . true . if ( associated ( next ) ) then if ( associated ( next % next , mix ) . and . & next % n_itt > 0 ) then ! if this is a circular mixing routine ! we should not reset the history... reset_stack = . false . end if end if if ( reset_stack ) then select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) n = size ( mix % stack ) do is = 1 , n call reset ( mix % stack ( is )) end do end select end if if ( associated ( next ) ) then init_itt = 0 ! Set-up the next mixer select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) init_itt = n_items ( next % stack ( 1 )) end select next % start_itt = init_itt next % cur_itt = init_itt if ( IONode ) then write ( * , '(3a)' ) trim ( debug_msg ), ' switching mixer --> ' , & trim ( next % name ) end if mix => mix % next end if end subroutine mixing_step ! Calculate the inverse of a matrix subroutine inverse ( n , A , B , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) integer , intent ( out ) :: info integer :: i , j ! Local arrays real ( dp ) :: pm ( n , n ), work ( n * 4 ), err ! Relative tolerance dependent on the magnitude ! For now we retain the old tolerance real ( dp ), parameter :: etol = 1.e-4_dp integer :: ipiv ( n ) ! initialize info info = 0 ! simple check and fast return if ( n == 1 ) then B ( 1 , 1 ) = 1._dp / A ( 1 , 1 ) return end if call lapack_inv () if ( info /= 0 ) call simple_inv () contains subroutine lapack_inv () B = A call dgetrf ( n , n , B , n , ipiv , info ) if ( info /= 0 ) return call dgetri ( n , B , n , ipiv , work , n * 4 , info ) if ( info /= 0 ) return ! This sets info appropriately call check_inv () end subroutine lapack_inv subroutine simple_inv () real ( dp ) :: x integer :: k ! Copy over A B = A do i = 1 , n if ( B ( i , i ) == 0._dp ) then info = - n return end if x = 1._dp / B ( i , i ) B ( i , i ) = 1._dp do j = 1 , n B ( j , i ) = B ( j , i ) * x end do do k = 1 , n if ( ( k - i ) /= 0 ) then x = B ( i , k ) B ( i , k ) = 0._dp do j = 1 , n B ( j , k ) = B ( j , k ) - B ( j , i ) * x end do end if end do end do ! This sets info appropriately call check_inv () end subroutine simple_inv subroutine check_inv () ! Check correcteness pm = matmul ( A , B ) do j = 1 , n do i = 1 , n if ( i == j ) then err = pm ( i , j ) - 1._dp else err = pm ( i , j ) end if ! This is pretty strict tolerance! if ( abs ( err ) > etol ) then ! Signal failure in inversion info = - n - 1 return end if end do end do end subroutine check_inv end subroutine inverse ! Calculate the svd of a matrix ! With   ||(Ax - b)|| << subroutine svd ( n , A , B , cond , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) real ( dp ), intent ( in ) :: cond integer , intent ( out ) :: info ! Local arrays integer :: rank , i character ( len = 50 ) :: fmt real ( dp ) :: AA ( n , n ), S ( n ), work ( n * 5 ) ! Copy A matrix AA = A ! setup pseudo inverse solution for minimizing ! constraints B = 0._dp do i = 1 , n B ( i , i ) = 1._dp end do call dgelss ( n , n , n , AA , n , B , n , S , cond , rank , work , n * 5 , info ) ! if debugging print out the different variables if ( debug_mix ) then ! also mark the rank if ( rank == n ) then ! complete rank write ( * , '(2a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' SVD singular = ' , S else ! this prints the location of the SVD rank, if not full write ( fmt , '(i0,2a)' ) rank , '(tr1,e10.4),'' >'',100(tr1,e10.4)' write ( * , '(2a,' // trim ( fmt ) // ')' ) & trim ( debug_msg ), ' SVD singular = ' , S end if end if end subroutine svd ! ************************************************* ! *                Helper routines                * ! *                    LOCAL                      * ! ************************************************* ! Returns the value array from the stack(:) ! Returns this array: !    mix%stack(sidx)(hidx) ! defaults to the last item function getstackval ( mix , sidx , hidx ) result ( d1 ) type ( tMixer ), intent ( in ) :: mix integer , intent ( in ) :: sidx integer , intent ( in ), optional :: hidx real ( dp ), pointer :: d1 (:) type ( dData1D ), pointer :: dD1 if ( present ( hidx ) ) then dD1 => get_pointer ( mix % stack ( sidx ), hidx ) else dD1 => get_pointer ( mix % stack ( sidx ), & n_items ( mix % stack ( sidx ))) end if d1 => val ( dD1 ) end function getstackval ! Returns true if the following ! \"advanced\" mixer is 'method' function is_next ( mix , method , next ) result ( bool ) type ( tMixer ), intent ( in ), target :: mix integer , intent ( in ) :: method type ( tMixer ), pointer , optional :: next logical :: bool type ( tMixer ), pointer :: m bool = . false . m => mix % next do while ( associated ( m ) ) if ( m % m == MIX_LINEAR ) then m => m % next else if ( m % m == method ) then bool = . true . exit else ! Quit if it does not do anything exit end if ! this will prevent cyclic combinations if ( associated ( m , mix ) ) exit end do if ( present ( next ) ) then next => m end if end function is_next !> Get current iteration count !! !! This is abstracted because the initial iteration !! and the current iteration may be uniquely defined. function current_itt ( mix ) result ( itt ) type ( tMixer ), intent ( in ) :: mix integer :: itt itt = mix % cur_itt - mix % start_itt end function current_itt ! Stack handling routines function stack_check ( stack , n ) result ( check ) type ( Fstack_dData1D ), intent ( inout ) :: stack integer , intent ( in ) :: n logical :: check ! Local arrays type ( dData1D ), pointer :: dD1 if ( n_items ( stack ) == 0 ) then check = . true . else ! Check that the stack stored arrays are ! of same size... dD1 => get_pointer ( stack , 1 ) check = n == size ( dD1 ) end if end function stack_check subroutine push_stack_data ( s_F , n ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n type ( dData1D ) :: dD1 if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if call newdData1D ( dD1 , n , '(F)' ) ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_stack_data subroutine push_F ( s_F , n , F , fact ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( in ), optional :: fact type ( dData1D ) :: dD1 real ( dp ), pointer :: sF (:) integer :: in , ns integer :: i if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) ns = max_size ( s_F ) if ( in == ns ) then ! we have to cycle the storage call get ( s_F , 1 , dD1 ) else call newdData1D ( dD1 , n , '(F)' ) end if sF => val ( dD1 ) if ( present ( fact ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n sF ( i ) = F ( i ) * fact end do !$OMP end parallel do else call dcopy ( n , F , 1 , sF , 1 ) end if ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_F subroutine update_F ( s_F , n , F ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) type ( dData1D ), pointer :: dD1 real ( dp ), pointer :: FF (:) integer :: in if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) if ( in == 0 ) then ! We need to add it as it does not exist call push_F ( s_F , n , F ) else ! we have an entry, update the latest dD1 => get_pointer ( s_F , in ) FF => val ( dD1 ) call dcopy ( n , F , 1 , FF , 1 ) end if end subroutine update_F subroutine push_diff ( s_rres , s_res , alpha ) type ( Fstack_dData1D ), intent ( inout ) :: s_rres type ( Fstack_dData1D ), intent ( in ) :: s_res real ( dp ), intent ( in ), optional :: alpha type ( dData1D ) :: dD1 type ( dData1D ), pointer :: pD1 real ( dp ), pointer :: res1 (:), res2 (:), rres (:) integer :: in , ns , i , n if ( n_items ( s_res ) < 2 ) then call die ( 'mixing: Residual residuals cannot be calculated, & &inferior residual size.' ) end if in = n_items ( s_res ) ! First get the value of in pD1 => get_pointer ( s_res , in - 1 ) res1 => val ( pD1 ) ! get the value of in pD1 => get_pointer ( s_res , in ) res2 => val ( pD1 ) in = n_items ( s_rres ) ns = max_size ( s_rres ) if ( in == ns ) then ! we have to cycle the storage call get ( s_rres , 1 , dD1 ) else call newdData1D ( dD1 , size ( res1 ), '(res)' ) end if ! Get the residual of the residual rres => val ( dD1 ) n = size ( rres ) if ( present ( alpha ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = ( res2 ( i ) - res1 ( i )) * alpha end do !$OMP end parallel do else !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = res2 ( i ) - res1 ( i ) end do !$OMP end parallel do end if ! Push the data to the stack call push ( s_rres , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_diff !> Calculate the norm of two arrays function norm ( n , x1 , x2 ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: x1 ( n ), x2 ( n ) real ( dp ) :: norm ! Currently we use an external routine integer :: i ! Calculate dot product norm = 0._dp !$OMP parallel do default(shared), private(i) & !$OMP& reduction(+:norm) do i = 1 , n norm = norm + x1 ( i ) * x2 ( i ) end do !$OMP end parallel do end function norm end module m_mixing","tags":"","loc":"sourcefile/m_mixing.f90.html","title":"m_mixing.F90 – SIESTA"},{"text":"Files dependent on this one sourcefile~~compute_max_diff.f90~~AfferentGraph sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_max_diff Source Code compute_max_diff.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_compute_max_diff use precision , only : dp !> Temporary for storing the old maximum change real ( dp ), public , save :: dDmax_current interface compute_max_diff module procedure compute_max_diff_1d module procedure compute_max_diff_2d end interface compute_max_diff public :: compute_max_diff contains subroutine compute_max_diff_2d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:,:), X2 (:,:) real ( dp ), intent ( out ) :: max_diff integer :: n1 , n2 integer :: i1 , i2 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) n2 = size ( X1 , 2 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if if ( size ( X2 , 2 ) /= n2 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (2-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i2,i1), & !$OMP& reduction(max:max_diff), collapse(2) do i2 = 1 , n2 do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 , i2 ) - X2 ( i1 , i2 )) ) end do end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_2d subroutine compute_max_diff_1d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:), X2 (:) real ( dp ), intent ( out ) :: max_diff integer :: n1 integer :: i1 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i1), reduction(max:max_diff) do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 ) - X2 ( i1 )) ) end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_1d end module m_compute_max_diff","tags":"","loc":"sourcefile/compute_max_diff.f90.html","title":"compute_max_diff.F90 – SIESTA"},{"text":"This file depends on sourcefile~~setup_hamiltonian.f~~EfferentGraph sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~setup_hamiltonian.f~~AfferentGraph sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_setup_hamiltonian Source Code setup_hamiltonian.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_setup_hamiltonian private public :: setup_hamiltonian CONTAINS subroutine setup_hamiltonian ( iscf ) USE siesta_options use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H , S , Hold use sparse_matrices , only : Dscf , Escf , xijo use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , . lastkb , no_s , rmaxv , indxua , iphorb , lasto , . rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use ldau_specs , only : switch_ldau ! This variable determines whether !   the subroutine to compute the !   Hubbard terms should be called !   or not use m_ldau , only : hubbard_term ! Subroutine that compute the !   Hubbard terms use m_dhscf , only : dhscf use m_stress use m_energies use parallel , only : Node use m_steps , only : istp use m_ntm use m_spin , only : spin use m_dipol use alloc , only : re_alloc , de_alloc use m_gamma use m_hsx , only : write_hsx use sys , only : die , bye use m_partial_charges , only : want_partial_charges use files , only : filesOut_t ! derived type for output file names use m_rhog , only : rhog_in , rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none integer , intent ( in ) :: iscf real ( dp ) :: stressl ( 3 , 3 ) real ( dp ), pointer :: fal (:,:) ! Local-node part of atomic F #ifdef MPI real ( dp ) :: buffer1 #endif integer :: io , is , ispin integer :: ifa ! Calc. forces?      0=>no, 1=>yes integer :: istr ! Calc. stress?      0=>no, 1=>yes integer :: ihmat ! Calc. hamiltonian? 0=>no, 1=>yes real ( dp ) :: g2max type ( filesOut_t ) :: filesOut ! blank output file names logical :: use_rhog_in real ( dp ), pointer :: H_vkb (:), H_kin (:), H_ldau (:,:) real ( dp ), pointer :: H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) complex ( dp ) :: Dc integer :: ind , i , j !------------------------------------------------------------------------- BEGIN call timer ( 'setup_H' , 1 ) ! Nullify pointers nullify ( fal ) !$OMP parallel default(shared), private(ispin,io) !     Save present H matrix !$OMP do collapse(2) do ispin = 1 , spin % H do io = 1 , maxnh Hold ( io , ispin ) = H ( io , ispin ) end do end do !$OMP end do !$OMP single H_kin => val ( H_kin_1D ) H_vkb => val ( H_vkb_1D ) if ( spin % SO_onsite ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so_on => val ( H_so_on_2D ) else if ( spin % SO_offsite ) then H_so_off => val ( H_so_off_2D ) end if !$OMP end single ! keep wait ! Initialize diagonal Hamiltonian do ispin = 1 , spin % spinor !$OMP do do io = 1 , maxnh H ( io , ispin ) = H_kin ( io ) + H_vkb ( io ) end do !$OMP end do nowait end do if ( spin % SO_onsite ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = H_so_on ( io , ispin - 2 ) end do end do !$OMP end do nowait else !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = 0._dp end do end do !$OMP end do nowait end if ! .................. ! Non-SCF part of total energy ....................................... ! Note that these will be \"impure\" for a mixed Dscf ! If mixing the charge, Dscf is the previous step's DM_out. Since ! the \"scf\" components of the energy are computed with the (mixed) ! charge, this introduces an inconsistency. In this case the energies ! coming out of this routine need to be corrected. ! !$OMP single Ekin = 0.0_dp Enl = 0.0_dp Eso = 0.0_dp !$OMP end single ! keep wait !$OMP do collapse(2), reduction(+:Ekin,Enl) do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) end do end do !$OMP end do nowait ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! if ( spin % SO_offsite ) then do io = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ), dp ) Eso = Eso + real ( H_so_off ( io , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ), dp ) Eso = Eso + real ( H_so_off ( io , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( io , 3 ), Dscf ( io , 4 ), dp ) Eso = Eso + real ( H_so_off ( io , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( io , 7 ), - Dscf ( io , 8 ), dp ) Eso = Eso + real ( H_so_off ( io , 3 ) * Dc , dp ) end do else if ( spin % SO_onsite ) then !$OMP do reduction(+:Eso) do io = 1 , maxnh Eso = Eso + H_so_on ( io , 1 ) * Dscf ( io , 7 ) + & H_so_on ( io , 2 ) * Dscf ( io , 8 ) + H_so_on ( io , 5 ) * Dscf ( io , 3 ) + & H_so_on ( io , 6 ) * Dscf ( io , 4 ) - H_so_on ( io , 3 ) * Dscf ( io , 5 ) - & H_so_on ( io , 4 ) * Dscf ( io , 6 ) end do !$OMP end do nowait end if !$OMP end parallel #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 end if #endif !     Non-SCF part of total energy call update_E0 () ! Hubbard term for LDA+U: energy, forces, stress and matrix elements .... if ( switch_ldau ) then if ( spin % NCol ) then call die ( 'LDA+U cannot be used with non-collinear spin.' ) end if if ( spin % SO ) then call die ( 'LDA+U cannot be used with spin-orbit coupling.' ) end if call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) H_ldau => val ( H_ldau_2D ) call hubbard_term ( scell , na_u , na_s , isa , xa , indxua , . maxnh , maxnh , lasto , iphorb , no_u , no_l , . numh , listhptr , listh , numh , listhptr , listh , . spin % spinor , Dscf , Eldau , DEldau , H_ldau , . fal , stressl , H , iscf , . matrix_elements_only = . true .) #ifdef MPI ! Global reduction of energy terms call globalize_sum ( Eldau , buffer1 ) Eldau = buffer1 ! DEldau should not be globalized ! as it is based on globalized occupations #endif Eldau = Eldau + DEldau call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) endif ! .................. ! Add SCF contribution to energy and matrix elements .................. g2max = g2cut call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) ifa = 0 istr = 0 ihmat = 1 if (( hirshpop . or . voropop ) $ . and . partial_charges_at_every_scf_step ) then want_partial_charges = . true . endif use_rhog_in = ( mix_charge . and . iscf > 1 ) call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa , indxua , . ntm , ifa , istr , ihmat , filesOut , . maxnh , numh , listhptr , listh , Dscf , Datm , . maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , . Exc , Dxc , dipol , stress , fal , stressl , . use_rhog_in ) if ( spin % SO_offsite ) then ! H(:, [5, 6]) are not updated in dhscf, see vmat for details. !------- H(u,u) H (:, 1 ) = H (:, 1 ) + real ( H_so_off (:, 1 ), dp ) H (:, 5 ) = dimag ( H_so_off (:, 1 )) !------- H(d,d) H (:, 2 ) = H (:, 2 ) + real ( H_so_off (:, 2 ), dp ) H (:, 6 ) = dimag ( H_so_off (:, 2 )) !------- H(u,d) H (:, 3 ) = H (:, 3 ) + real ( H_so_off (:, 3 ), dp ) H (:, 4 ) = H (:, 4 ) + dimag ( H_so_off (:, 3 )) !------- H(d,u) H (:, 7 ) = H (:, 7 ) + real ( H_so_off (:, 4 ), dp ) H (:, 8 ) = H (:, 8 ) - dimag ( H_so_off (:, 4 )) endif ! This statement will apply to iscf = 1, for example, when ! we do not use rhog_in. Rhog here is always the charge used to ! build H, that is, rhog_in. if ( mix_charge ) rhog_in = rhog want_partial_charges = . false . call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) !  It is wasteful to write over and over H and S, as there are !  no different files. ! Save Hamiltonian and overlap matrices ............................ ! Only in HSX format now.  Use Util/HSX/hsx2hs to generate an HS file if ( savehs . or . write_coop ) then call write_hsx ( gamma , no_u , no_s , spin % H , indxuo , & maxnh , numh , listhptr , listh , H , S , qtot , & temp , xijo ) endif call timer ( 'setup_H' , 2 ) #ifdef SIESTA__PEXSI if ( node == 0 ) call memory_snapshot ( \"after setup_H\" ) #endif if ( h_setup_only ) then call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call bye ( \"H-Setup-Only requested\" ) STOP endif !------------------------------------------------------------------------- END END subroutine setup_hamiltonian END module m_setup_hamiltonian","tags":"","loc":"sourcefile/setup_hamiltonian.f.html","title":"setup_hamiltonian.F – SIESTA"},{"text":"Files dependent on this one sourcefile~~compute_dm.f~~AfferentGraph sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_dm.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_dm Source Code compute_dm.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_compute_dm private public :: compute_dm logical , public , save :: PreviousCallDiagon = . false . CONTAINS subroutine compute_dm ( iscf ) use precision use units , only : eV USE siesta_options use class_dSpData1D , only : val use sparse_matrices use siesta_geom use atomlist , only : qa , lasto , iphorb , iaorb , no_u , no_s , indxuo , & qtot , Qtots , no_l use sys , only : die , bye use kpoint_scf_m , only : kpoints_scf use m_energies , only : Ebs , Ecorrec , Entropy , DE_NEGF use m_energies , only : Ef , Efs use m_rmaxh use m_eo use m_spin , only : spin use m_diagon , only : diagon use m_gamma use parallel , only : IONode use parallel , only : SIESTA_worker use m_compute_ebs_shift , only : compute_ebs_shift #ifdef SIESTA__PEXSI use m_pexsi_solver , only : pexsi_solver #endif use m_hsx , only : write_hs_formatted #ifdef MPI use mpi_siesta #endif #ifdef CDF use iodmhs_netcdf , only : write_dmh_netcdf #endif use m_dminim , only : dminim use m_zminim , only : zminim use m_ordern , only : ordern use m_steps , only : istp use m_normalize_dm , only : normalize_dm #ifdef SIESTA__CHESS use m_chess , only : CheSS_wrapper #endif use m_energies , only : DE_NEGF use m_ts_global_vars , only : TSmode , TSinit , TSrun use m_transiesta , only : transiesta implicit none !     Input variables integer , intent ( in ) :: iscf real ( dp ) :: delta_Ebs , delta_Ef logical :: CallDiagon integer :: nnz real ( dp ), pointer :: H_kin (:) ! e1>e2 to signal that we do not want DOS weights real ( dp ), parameter :: e1 = 1.0_dp , e2 = - 1.0_dp real ( dp ) :: buffer1 integer :: mpierr !       character(15)            :: filename, indexstr !       character(15), parameter :: fnameform = '(A,A,A)' !-------------------------------------------------------------------- BEGIN if ( SIESTA_worker ) call timer ( 'compute_dm' , 1 ) #ifdef MPI call MPI_Bcast ( isolve , 1 , MPI_integer , 0 , true_MPI_Comm_World , mpierr ) #endif if ( SIESTA_worker ) then ! Save present density matrix !$OMP parallel default(shared) if ( converge_EDM ) then !$OMP workshare Eold (:,:) = Escf (:,:) Dold (:,:) = Dscf (:,:) !$OMP end workshare else !$OMP workshare Dold (:,:) = Dscf (:,:) !$OMP end workshare end if !$OMP end parallel end if ! Compute shift in Tr(H*DM) for fermi-level bracketting ! Use the current H, the previous iteration H, and the ! previous iteration DM if ( SIESTA_worker ) then if ( iscf > 1 ) then call compute_Ebs_shift ( Dscf , H , Hold , delta_Ebs ) delta_Ef = delta_Ebs / qtot if ( ionode . and . isolve . eq . SOLVE_PEXSI ) then write ( 6 , \"(a,f16.5)\" ) $ \"Estimated change in band-structure energy:\" , $ delta_Ebs / eV , \"Estimated shift in E_fermi: \" , $ delta_Ef / eV endif else delta_Ebs = 0.0_dp delta_Ef = 0.0_dp endif endif #ifdef SIESTA__PEXSI if ( isolve . eq . SOLVE_PEXSI ) then ! This test done in node 0 since NonCol and SpOrb ! are not set for PEXSI-solver-only processes if ( ionode ) then if ( spin % NCol . or . spin % SO ) call die ( $ \"The PEXSI solver does not implement \" // $ \"non-coll spins or Spin-orbit yet\" ) endif call pexsi_solver ( iscf , no_u , no_l , spin % spinor , $ maxnh , numh , listhptr , listh , $ H , S , qtot , Dscf , Escf , $ ef , Entropy , temp , delta_Ef ) endif if (. not . SIESTA_worker ) RETURN #endif ! Here we decide if we want to calculate one or more SCF steps by ! diagonalization before proceeding with the OMM routine CallDiagon = . false . if ( isolve . eq . SOLVE_MINIM ) then if ( istp . eq . 1 ) then if (( iscf . le . call_diagon_first_step ) . or . & ( call_diagon_first_step < 0 )) CallDiagon = . true . else if (( iscf . le . call_diagon_default ) . or . & ( call_diagon_default < 0 )) CallDiagon = . true . endif endif if ( isolve . eq . MATRIX_WRITE ) then !             write(indexstr,'(I15)') iscf !             write(filename,fnameform) 'H_', trim(adjustl(indexstr)), !      &                                '.matrix' !             call write_global_matrix( no_s, no_l, maxnh, numh, listh, !      &           H(1:maxnh,1), filename ) ! !             write(filename,fnameform) 'S_', trim(adjustl(indexstr)), !      &                                '.matrix' !        Note: only one-shot for now call write_hs_formatted ( no_u , spin % H , $ maxnh , numh , listhptr , listh , H , S ) call bye ( \"End of run after writing H.matrix and S.matrix\" ) c$        call write_global_matrix_singlenodewrite( c$     &           no_u, no_s, maxnh, numh, listhptr, listh, c$     &           H(:,1), 'H.matrix') c$ c$        call write_global_matrix_singlenodewrite( c$     &           no_u, no_s, maxnh, numh, listhptr, listh, c$     &           S, 'S.matrix') elseif (( isolve . eq . SOLVE_DIAGON ) . or . ( CallDiagon )) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0.0_dp PreviousCallDiagon = . true . elseif ( isolve . eq . SOLVE_ORDERN ) then if (. not . gamma ) call die ( \"Cannot do O(N) with k-points.\" ) if ( spin % NCol . or . spin % SO ) . call die ( \"Cannot do O(N) with non-coll spins or Spin-orbit\" ) call ordern ( usesavelwf , ioptlwf , na_u , no_u , no_l , lasto , & isa , qa , rcoor , rmaxh , ucell , xa , iscf , & istp , ncgmax , etol , eta , qtot , maxnh , numh , & listhptr , listh , H , S , chebef , noeta , rcoorcp , & beta , pmax , Dscf , Escf , Ecorrec , spin % H , qtots ) Entropy = 0.0_dp elseif ( isolve . eq . SOLVE_MINIM ) then if ( spin % NCol . or . spin % SO ) & call die ( ' ERROR : Non - collinear spin calculations & not yet implemented with OMM !') H_kin => val ( H_kin_1D ) if ( gamma ) then call dminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , H , S , H_kin ) else call zminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , no_s , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & H , S , H_kin ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #ifdef SIESTA__CHESS elseif ( isolve . eq . SOLVE_CHESS ) then ! FOE solver from the CheSS library if ( gamma ) then call CheSS_wrapper (. false ., PreviousCallDiagon , & iscf , istp , no_l , & spin % spinor , no_u , maxnh , numh , listhptr , listh , & qs , h , s , & Dscf , Escf , Ef ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #endif elseif ( TSmode . and . TSinit ) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0._dp else if ( TSrun ) then call transiesta ( iscf , spin % H , block_dist , sparse_pattern , & Gamma , ucell , nsc , isc_off , no_u , na_u , lasto , xa , maxnh , & H , S , Dscf , Escf , Ef , Qtot , . false ., DE_NEGF ) Ecorrec = 0._dp Entropy = 0.0_dp else !call die('siesta: ERROR: wrong solution method') endif #ifdef CDF if ( writedmhs_cdf_history ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf ) else if ( writedmhs_cdf ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf , & overwrite = . true . ) endif #endif ! Write orbital indexes. JMS Dec.2009 if ( IOnode . and . iscf == 1 ) then call write_orb_indx ( na_u , na_s , no_u , no_s , isa , xa , . iaorb , iphorb , indxuo , nsc , ucell ) endif !     Normalize density matrix to exact charge !     Placed here for now to avoid disturbing EHarris if ( . not . TSrun ) then call normalize_dm ( first = . false . ) end if call timer ( 'compute_dm' , 2 ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after compute_DM\" ) #endif !-----------------------------------------------------------------------END END subroutine compute_dm END MODULE m_compute_dm","tags":"","loc":"sourcefile/compute_dm.f.html","title":"compute_dm.F – SIESTA"},{"text":"This file depends on sourcefile~~m_mixing_scf.f90~~EfferentGraph sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_mixing_scf.f90~~AfferentGraph sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_mixing_scf Source Code m_mixing_scf.F90 Source Code ! Also the mixing container module m_mixing_scf use class_Fstack_dData1D use m_mixing , only : tMixer implicit none private save type ( tMixer ), pointer :: scf_mixs (:) => null () type ( tMixer ), pointer :: scf_mix => null () ! Default mixing, no discrepancy between spin-components integer , parameter :: MIX_SPIN_ALL = 1 ! Only use spinor components for mixing integer , parameter :: MIX_SPIN_SPINOR = 2 ! Only use spin-sum for mixing (implicit on spinor) integer , parameter :: MIX_SPIN_SUM = 3 ! Use both spin-sum and spin-difference density for mixing (implicit on spinor) integer , parameter :: MIX_SPIN_SUM_DIFF = 4 ! It makes little sense to only mix difference as for spin-polarised ! calculations with no difference it will converge immediately ! How the spin mixing algorthim is chosen integer :: mix_spin = MIX_SPIN_ALL public :: scf_mixs , scf_mix public :: mix_spin public :: MIX_SPIN_ALL , MIX_SPIN_SPINOR , MIX_SPIN_SUM , MIX_SPIN_SUM_DIFF public :: mixers_scf_init public :: mixers_scf_print , mixers_scf_print_block public :: mixers_scf_history_init public :: mixers_scf_reset public :: mixing_scf_converged contains subroutine mixers_scf_init ( nspin , Comm ) use fdf use precision , only : dp #ifdef MPI use mpi_siesta , only : MPI_Comm_World #endif use m_mixing , only : mixers_reset , mixers_init use m_mixing , only : mix_method , mix_method_variant use m_mixing , only : mixer_init use m_mixing , only : mixers_history_init ! The number of spin-components integer , intent ( in ) :: nspin ! The communicator used for the mixer integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf ! Get number of history steps integer :: n_hist , n_kick , n_restart , n_save real ( dp ) :: w , w_kick integer :: n_lin_after real ( dp ) :: w_lin_after logical :: lin_after ! number of history steps saved type ( tMixer ), pointer :: m integer :: nm , im , im2 , tmp logical :: is_broyden character ( len = 70 ) :: method , variant , opt ! If the mixers are denoted by a block, then ! the entire logic *MUST* be defined in the blocks opt = fdf_get ( 'SCF.Mix.Spin' , 'all' ) if ( leqi ( opt , 'all' ) ) then mix_spin = MIX_SPIN_ALL else if ( leqi ( opt , 'spinor' ) ) then mix_spin = MIX_SPIN_SPINOR else if ( leqi ( opt , 'sum' ) ) then mix_spin = MIX_SPIN_SUM else if ( leqi ( opt , 'sum+diff' ) ) then mix_spin = MIX_SPIN_SUM_DIFF else call die ( \"Unknown option given for SCF.Mix.Spin & &all|spinor|sum|sum+diff\" ) end if ! If there is only one spinor we should mix all... if ( nspin == 1 ) mix_spin = MIX_SPIN_ALL ! Initialize to ensure debug stuff read call mixers_init ( 'SCF' , scf_mixs , Comm = Comm ) ! Check for existance of the SCF.Mix block if ( associated ( scf_mixs ) ) then if ( size ( scf_mixs ) > 0 ) then return end if ! Something has gone wrong... ! The user has supplied a block, but ! haven't added any content to the block... ! However, we fall-back to the default mechanism end if ! ensure nullification call mixers_reset ( scf_mixs ) ! >>>*** FIRST ***<<< ! Read in compatibility options ! Figure out if we are dealing with ! Broyden or Pulay n_hist = fdf_get ( 'DM.NumberPulay' , 2 ) tmp = fdf_get ( 'DM.NumberBroyden' , 0 ) is_broyden = tmp > 0 if ( is_broyden ) then n_hist = tmp end if ! Define default mixing weight (used for ! Pulay, Broyden and linear mixing) w = fdf_get ( 'DM.MixingWeight' , 0.25_dp ) ! Default kick-options n_kick = fdf_get ( 'DM.NumberKick' , 0 ) w_kick = fdf_get ( 'DM.KickMixingWeight' , 0.5_dp ) lin_after = fdf_get ( 'SCF.LinearMixingAfterPulay' , . false .) w_lin_after = fdf_get ( 'SCF.MixingWeightAfterPulay' , w ) ! >>>*** END ***<<< ! Read options in new format ! Get history length n_hist = fdf_get ( 'SCF.Mixer.History' , n_hist ) ! update mixing weight and kick mixing weight w = fdf_get ( 'SCF.Mixer.Weight' , w ) n_kick = fdf_get ( 'SCF.Mixer.Kick' , n_kick ) w_kick = fdf_get ( 'SCF.Mixer.Kick.Weight' , w_kick ) ! Restart after this number of iterations n_restart = fdf_get ( 'SCF.Mixer.Restart' , 0 ) n_save = fdf_get ( 'SCF.Mixer.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Get the variant of the mixing method if ( is_broyden ) then method = 'Broyden' else if ( n_hist > 0 ) then method = 'Pulay' else method = 'Linear' end if method = fdf_get ( 'SCF.Mixer.Method' , trim ( method )) variant = fdf_get ( 'SCF.Mixer.Variant' , 'original' ) ! Determine whether linear mixing should be ! performed after the \"advanced\" mixing n_lin_after = fdf_get ( 'SCF.Mixer.Linear.After' , - 1 ) w_lin_after = fdf_get ( 'SCF.Mixer.Linear.After.Weight' , w_lin_after ) ! Determine total number of mixers nm = 1 if ( n_lin_after >= 0 . or . lin_after ) nm = nm + 1 if ( n_kick > 0 ) nm = nm + 1 ! Initiailaze all mixers allocate ( scf_mixs ( nm )) scf_mixs (:)% w = w scf_mixs (:)% n_hist = n_hist scf_mixs (:)% restart = n_restart scf_mixs (:)% restart_save = n_save ! 1. Current mixing index im = 1 ! Store the advanced mixer index (for references to ! later mixers) im2 = im m => scf_mixs ( im ) m % name = method m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! 2. Setup the linear mixing after the actual mixing if ( n_lin_after > 0 . or . lin_after ) then im = im + 1 m => scf_mixs ( im ) ! Signal to switch to this mixer after ! convergence scf_mixs ( im2 )% next_conv => m m % name = 'Linear-After' m % m = mix_method ( 'linear' ) m % w = w_lin_after m % n_itt = n_lin_after ! jump back to previous after having run a ! few iterations m % next => scf_mixs ( im2 ) end if ! In case we have a kick, apply the kick here ! This overrides the \"linear.after\" option if ( n_kick > 0 ) then im = im + 1 m => scf_mixs ( im ) m % name = 'Linear-Kick' m % n_itt = 1 m % n_hist = 0 m % m = mix_method ( 'linear' ) m % w = w_kick m % next => scf_mixs ( im2 ) ! set the default mixer to kick scf_mixs ( im2 )% n_itt = n_kick - 1 scf_mixs ( im2 )% next => m scf_mixs ( im2 )% restart = n_kick - 1 end if ! Correct the input do im = 1 , nm call mixer_init ( scf_mixs ( im ) ) end do ! Initialize the allocation of each mixer call mixers_history_init ( scf_mixs ) #ifdef MPI if ( present ( Comm ) ) then scf_mixs (:)% Comm = Comm else scf_mixs (:)% Comm = MPI_Comm_World end if #endif end subroutine mixers_scf_init subroutine mixers_scf_print ( nspin ) use parallel , only : IONode use m_mixing , only : mixers_print integer , intent ( in ) :: nspin ! Print mixing options call mixers_print ( 'SCF' , scf_mixs ) if ( IONode . and . nspin > 1 ) then select case ( mix_spin ) case ( MIX_SPIN_ALL ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'all' case ( MIX_SPIN_SPINOR ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'spinor' if ( nspin <= 2 ) then call die ( \"SCF.Mixer.Spin spinor option only valid for & &non-collinear and spin-orbit calculations\" ) end if case ( MIX_SPIN_SUM ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum' case ( MIX_SPIN_SUM_DIFF ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum and diff' end select end if end subroutine mixers_scf_print subroutine mixers_scf_print_block ( ) use m_mixing , only : mixers_print_block ! Print mixing options call mixers_print_block ( 'SCF' , scf_mixs ) end subroutine mixers_scf_print_block subroutine mixing_scf_converged ( SCFconverged ) use parallel , only : IONode logical , intent ( inout ) :: SCFconverged integer :: i ! Return if no convergence if ( . not . SCFconverged ) return if ( associated ( scf_mix % next_conv ) ) then ! this means that we skip to the ! following algorithm scf_mix => scf_mix % next_conv SCFconverged = . false . if ( allocated ( scf_mix % stack ) ) then do i = 1 , size ( scf_mix % stack ) ! delete all but one history ! This should be fine call reset ( scf_mix % stack ( i ), - 1 ) end do end if if ( IONode ) then write ( * , '(/,2a)' ) ':!: SCF cycle continuation mixer: ' , & trim ( scf_mix % name ) end if end if end subroutine mixing_scf_converged subroutine mixers_scf_reset () use m_mixing , only : mixers_reset nullify ( scf_mix ) call mixers_reset ( scf_mixs ) end subroutine mixers_scf_reset subroutine mixers_scf_history_init ( ) use m_mixing , only : mixers_history_init call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) end subroutine mixers_scf_history_init end module m_mixing_scf","tags":"","loc":"sourcefile/m_mixing_scf.f90.html","title":"m_mixing_scf.F90 – SIESTA"},{"text":"Contents Subroutines CROSS Source Code cross.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! ! $Id: cross.f,v 1.2 1999/01/31 10:53:49 emilio Exp $ SUBROUTINE CROSS ( A , B , AXB ) !! author: J.M.Soler !! !! Finds the cross product AxB of vectors A and B IMPLICIT NONE DOUBLE PRECISION A ( 3 ), B ( 3 ), AXB ( 3 ) AXB ( 1 ) = A ( 2 ) * B ( 3 ) - A ( 3 ) * B ( 2 ) AXB ( 2 ) = A ( 3 ) * B ( 1 ) - A ( 1 ) * B ( 3 ) AXB ( 3 ) = A ( 1 ) * B ( 2 ) - A ( 2 ) * B ( 1 ) END","tags":"","loc":"sourcefile/cross.f.html","title":"cross.f – SIESTA"},{"text":"This file depends on sourcefile~~siesta_analysis.f~~EfferentGraph sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~dhscf.f dhscf.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_siesta_analysis Source Code siesta_analysis.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_siesta_analysis use write_subs private public :: siesta_analysis CONTAINS subroutine siesta_analysis ( relaxd ) USE band , only : nbk , bk , maxbk , bands USE writewave , only : nwk , wfk , wwave USE writewave , only : setup_wfs_list , wfs_filename USE m_ksvinit , only : nkpol , kpol , wgthpol use m_ksv USE m_projected_DOS , only : projected_DOS USE m_local_DOS , only : local_DOS #ifdef SIESTA__PEXSI USE m_pexsi_local_DOS , only : pexsi_local_DOS USE m_pexsi_dos , only : pexsi_dos #endif USE siesta_options use units , only : Debye , eV use sparse_matrices , only : maxnh , listh , listhptr , numh use sparse_matrices , only : H , S , Dscf , xijo use siesta_geom use m_dhscf , only : dhscf use atomlist , only : indxuo , iaorb , lastkb , lasto , datm , no_l , & iphkb , no_u , no_s , iza , iphorb , rmaxo , indxua use atomlist , only : qtot use fdf use writewave , only : wwave use siesta_cml use files , only : slabel use files , only : filesOut_t ! derived type for output file names use zmatrix , only : lUseZmatrix , write_zmatrix use kpoint_scf_m , only : kpoints_scf use parallel , only : IOnode use parallel , only : SIESTA_worker use files , only : label_length use m_energies use m_steps , only : final use m_ntm use m_spin , only : nspin , spinor_dim , h_spin_dim use m_spin , only : SpOrb , NonCol , SPpol , NoMagn use m_dipol use m_eo use m_forces , only : fa use m_gamma use alloc , only : re_alloc , de_alloc use basis_enthalpy , only : write_basis_enthalpy use m_partial_charges , only : want_partial_charges use m_iodm_old , only : read_spmatrix use m_siesta2wannier90 , only : siesta2wannier90 use m_mpi_utils , only : barrier #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_ANALYSIS #endif implicit none logical :: relaxd , getPSI , quenched_MD , found real ( dp ) :: dummy_str ( 3 , 3 ) real ( dp ) :: dummy_strl ( 3 , 3 ) real ( dp ) :: qspin ( 4 ) ! Local real ( dp ) :: polxyz ( 3 , nspin ) ! Autom., small real ( dp ) :: polR ( 3 , nspin ) ! Autom., small real ( dp ) :: qaux real ( dp ), pointer :: ebk (:,:,:) ! Band energies integer :: j , ix , ind , ik , io , ispin integer :: wfs_band_min , wfs_band_max real ( dp ) :: g2max , current_ef type ( filesOut_t ) :: filesOut ! blank output file names !-----------------------------------------------------------------------BEGIN if ( SIESTA_worker ) call timer ( \"Analysis\" , 1 ) !! Check that we are converged in geometry, !! if strictly required, !! before carrying out any analysis. !!@code quenched_MD = ( ( iquench > 0 ) . and . $ (( idyn . eq . 1 ) . or . ( idyn . eq . 3 )) ) if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( SIESTA_worker ) then ! For timing ops and associated barrier if ( GeometryMustConverge . and . (. not . relaxd )) then call message ( \"FATAL\" , $ \"GEOM_NOT_CONV: Geometry relaxation not converged\" ) call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call barrier () call die ( \"ABNORMAL_TERMINATION\" ) endif endif endif !!@endcode !     All the comments below assume that this compatibility option !     is not used. !     Note also that full compatibility cannot be guaranteed if (. not . compat_pre_v4_dynamics ) then !     This is a sanity safeguard: we reset the geometry (which might !     have been moved by the relaxation or MD routines) to the one used !     in the last computation of the electronic structure. !     See the comments below for explanation !$OMP parallel workshare default(shared) xa ( 1 : 3 , 1 : na_s ) = xa_last ( 1 : 3 , 1 : na_s ) ucell ( 1 : 3 , 1 : 3 ) = ucell_last ( 1 : 3 , 1 : 3 ) scell ( 1 : 3 , 1 : 3 ) = scell_last ( 1 : 3 , 1 : 3 ) !$OMP end parallel workshare endif ! zmatrix info reset?? if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then if ( SIESTA_worker ) then call read_spmatrix ( maxnh , no_l , h_spin_dim , numh , . listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) current_ef = ef ef = fdf_get ( \"Manual-Fermi-Level\" , current_ef , \"Ry\" ) endif endif #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.DOS\" ,. false .)) then call pexsi_dos ( no_u , no_l , spinor_dim , $ maxnh , numh , listhptr , listh , H , S , qtot , ef ) endif #endif ! section done by Siesta subset of nodes if ( SIESTA_worker ) then final = . true . if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'Finalization' ) endif #ifdef SIESTA__FLOOK ! Call lua right before doing the analysis, ! possibly changing some of the variables call slua_call ( LUA , LUA_ANALYSIS ) #endif ! !     NOTE that the geometry output by the following sections !     used to be that \"predicted\" for the next MD or relaxation step. !     This is now changed ! if ( IOnode ) then ! Print atomic coordinates ! This covers CG and MD-quench (verlet, pr), instances in which ! \"relaxd\" is meaningful if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( relaxd ) then ! xa = xa_last ! The \"relaxation\" routines do not update ! the coordinates if relaxed, so this behavior is unchanged call outcoor ( ucell , xa , na_u , 'Relaxed' , . true . ) else ! Since xa = xa_last now, this will just repeat the ! last set of coordinates used, not the predicted ones. call outcoor ( ucell , xa , na_u , 'Final (unrelaxed)' , . true . ) endif endif ! This call will write xa_last to the .STRUCT_OUT file ! (again, since it has already been written by state_init), ! CML records of the latest processed structure, and ! possibly zmatrix info.  *** unmoved?? how? ! Note that the .STRUCT_NEXT_ITER file is produced ! in siesta_move for checkpointing of relaxations and MD runs. ! If all we want are the CML records (to satisfy some expectation ! of appearance in the \"finalization\" section, we might put the ! cml call explicitly and forget about the rest. if ( compat_pre_v4_dynamics ) then call siesta_write_positions ( moved = . true .) else call siesta_write_positions ( moved = . false .) endif ! ??  Clarify Zmatrix behavior **** if ( lUseZmatrix ) call write_Zmatrix ! Print unit cell (cell_last) for variable cell and server operation if ( varcel . or . ( idyn . eq . 8 )) call outcell ( ucell ) !------------------------------------------------------------------ ! It can be argued that these needed the xa_last coordinates ! all along !       Print coordinates in xmol format in a separate file if ( fdf_boolean ( 'WriteCoorXmol' ,. false .)) & call coxmol ( iza , xa , na_u ) !       Print coordinates in cerius format in a separate file if ( fdf_boolean ( 'WriteCoorCerius' ,. false .)) & call coceri ( iza , xa , ucell , na_u , sname ) !       Find interatomic distances (output in file BONDS_FINAL) call bonds ( ucell , na_u , isa , xa , & rmax_bonds , trim ( slabel ) // \".BONDS_FINAL\" ) endif ! IONode !--- end output of geometry information ! ! ! NOTE: In the following sections, wavefunction generation, computation !       of band structure, etc, are carried out using the last Hamiltonian !       generated in the SCF run for the last geometry considered. !   But, if xa /= xa_last, the computation of, say, bands, will use !      H phases which are not the same as those producing the final !      ground-state electronic structure. ! !    Also, since we have removed the replication (superx call) !      of \"moved\" coordinates !      into the auxiliary supercell from 'siesta_move' (recall that it is !      done always in state_init for every new geometry), the \"moved unit !      cell coordinates\" could coexist here with \"unmoved non-unit cell SC coords\", !      which is wrong. !      For all of the above, we should put here a sanity safeguard !        (if we have not done so already at the top of this routine) !        xa(1:3,1:na_s) = xa_last(1:3,1:na_s) !        ucell(1:3,1:3) = ucell_last(1:3,1:3) !        scell(1:3,1:3) = scell_last(1:3,1:3) !        DM and H issues ! !        Some of the routines that follow use H and S, and some use the DM. !        Those which use the DM should work with the final \"out\" DM for !        consistency. !        Those which use H,S should work with the latest diagonalized H,S pair. ! !      If mixing the DM during the scf loop we should avoid mixing it one more time !        after convergence (or restoring Dold) !        If mixing H, we should avoid mixing it one more time !        after convergence (and restoring Hold to have the exact H that generated the !        latest DM, although this is probably too much). !        See the logic in siesta_forces. !     Find and print wavefunctions at selected k-points !   This uses H,S, and xa if ( nwk . gt . 0 ) then wfs_filename = trim ( slabel ) // \".selected.WFSX\" if ( IONode ) print \"(a)\" , $ \"Writing WFSX for selected k-points in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , & nwk , & numh , listhptr , listh , H , S , Ef , xijo , indxuo , & nwk , wfk , no_u , gamma , occtol ) endif !   This uses H,S, and xa if ( write_coop ) then ! Output the wavefunctions for the kpoints in the SCF set ! Note that we allow both a band number and an energy range ! The user is responsible for using appropriate values. wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( kpoints_scf % N , no_u , & wfs_band_min , wfs_band_max , $ use_scf_weights = . true ., $ use_energy_window = . true .) wfs_filename = trim ( slabel ) // \".fullBZ.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for COOP/COHP in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . kpoints_scf % N , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . kpoints_scf % N , kpoints_scf % k , no_u , gamma , occtol ) endif !     Compute bands !   This uses H,S, and xa nullify ( ebk ) call re_alloc ( ebk , 1 , no_u , 1 , spinor_dim , 1 , maxbk , & 'ebk' , 'siesta_analysis' ) if ( nbk . gt . 0 ) then if ( IONode ) print \"(a)\" , \"Computing bands...\" getPSI = fdf_get ( 'WFS.Write.For.Bands' , . false .) if ( getPSI ) then wfs_filename = trim ( slabel ) // \".bands.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for bands in \" $ // trim ( wfs_filename ) wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nbk , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . false ., $ use_energy_window = . false .) endif call bands ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . maxbk , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . . true ., nbk , bk , ebk , occtol , getPSI ) if ( IOnode ) then if ( writbk ) then write ( 6 , '(/,a,/,a4,a12)' ) & 'siesta: Band k vectors (Bohr**-1):' , 'ik' , 'k' do ik = 1 , nbk write ( 6 , '(i4,3f12.6)' ) ik , ( bk ( ix , ik ), ix = 1 , 3 ) enddo endif if ( writb ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Band energies (eV):' , 'ik' , 'is' , 'eps' do ispin = 1 , spinor_dim do ik = 1 , nbk write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin , ( ebk ( io , ispin , ik ) / eV , io = 1 , min ( 10 , no_u )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( ebk ( io , ispin , ik ) / eV , io = 11 , no_u ) enddo enddo endif endif endif !     Print eigenvalues if ( IOnode . and . writeig ) then if (( isolve . eq . SOLVE_DIAGON . or . & (( isolve . eq . SOLVE_MINIM ) . and . minim_calc_eigenvalues )) & . and . no_l . lt . 1000 ) then if ( h_spin_dim <= 2 ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Eigenvalues (eV):' , 'ik' , 'is' , 'eps' do ik = 1 , kpoints_scf % N do ispin = 1 , spinor_dim write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin ,( eo ( io , ispin , ik ) / eV , io = 1 , min ( 10 , neigwanted )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( eo ( io , ispin , ik ) / eV , io = 11 , neigwanted ) enddo enddo else write ( 6 , '(/,a)' ) 'siesta: Eigenvalues (eV):' do ik = 1 , kpoints_scf % N write ( 6 , '(a,i6)' ) 'ik =' , ik write ( 6 , '(10f7.2)' ) & (( eo ( io , ispin , ik ) / eV , io = 1 , neigwanted ), ispin = 1 , 2 ) enddo endif write ( 6 , '(a,f15.6,a)' ) 'siesta: Fermi energy =' , ef / eV , ' eV' endif endif if ((( isolve . eq . SOLVE_DIAGON ). or . & (( isolve . eq . SOLVE_MINIM ). and . minim_calc_eigenvalues )) & . and . IOnode ) & call ioeig ( eo , ef , neigwanted , nspin , kpoints_scf % N , & no_u , spinor_dim , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w ) !   This uses H,S, and xa, as it diagonalizes them again call projected_DOS () !     Print program's energy decomposition and final forces if ( IOnode ) then call siesta_write_energies ( iscf = 0 , dDmax = 0._dp , dHmax = 0._dp ) ! final == .true. which makes the step counter irrelevant call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () call write_basis_enthalpy ( FreeE , FreeEHarris ) endif ! NOTE: Here, the spin polarization is computed using Dscf, which is !       a density matrix obtained after mixing the \"in\" and \"out\" !       DMs of the SCF run for the last geometry considered. !       This can be considered a feature or a bug. call print_spin ( qspin ) ! qspin returned for use below !     This uses the last computed dipole in dhscf during the scf cycle, !     which is in fact derived from the \"in\" DM. !     Perhaps this section should be moved after the call to dhscf below !     AND use the DM_out of the last step (but there might not be a call !     to dhscf if there are no files to output, and the computation of the !     charge density is expensive... !     Print electric dipole if ( shape . ne . 'bulk' ) then if ( IOnode ) then write ( 6 , '(/,a,3f12.6)' ) & 'siesta: Electric dipole (a.u.)  =' , dipol write ( 6 , '(a,3f12.6)' ) & 'siesta: Electric dipole (Debye) =' , & ( dipol ( ix ) / Debye , ix = 1 , 3 ) endif if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = dipol / Debye , & title = 'Electric dipole' , dictref = 'siesta:dipol' , . units = 'siestaUnits:Debye' ) endif !cml_p endif ! NOTE: The use of *_last geometries in the following sections !       guarantees that the analysis of the electronic structure !       is done for the geometry for which it was computed. !  BUT these routines need H,S, so H should not be mixed after !       convergence. !     Calculation of the bulk polarization using the Berry phase !     formulas by King-Smith and Vanderbilt if ( nkpol . gt . 0 . and . . not . bornz ) then if ( NonCol . or . SpOrb ) then if ( IOnode ) then write ( 6 , '(/a)' ) . 'siesta_analysis: bulk polarization implemented only for' write ( 6 , '(/a)' ) . 'siesta_analysis: paramagnetic or collinear spin runs' endif else call KSV_pol ( na_u , na_s , xa_last , rmaxo , scell_last , & ucell_last , no_u , no_l , no_s , nspin , qspin , & maxnh , nkpol , numh , listhptr , listh , & H , S , xijo , indxuo , isa , iphorb , & iaorb , lasto , shape , & nkpol , kpol , wgthpol , polR , polxyz ) endif endif !     Calculation of the optical conductivity call optical ( na_u , na_s , xa_last , scell_last , ucell_last , & no_u , no_l , no_s , nspin , qspin , & maxnh , numh , listhptr , listh , H , S , xijo , $ indxuo , ebk , ef , temp , & isa , iphorb , iphKB , lasto , lastkb , shape ) call de_alloc ( ebk , 'ebk' , 'siesta_analysis' ) !................................... ! !  NOTE: Dscf here might be the mixed one (see above). ! want_partial_charges = ( hirshpop . or . voropop ) . AND . $ (. not . partial_charges_at_every_geometry ) !     Save electron density and potential if ( saverho . or . savedrho . or . saverhoxc . or . & savevh . or . savevt . or . savevna . or . & savepsch . or . savetoch . or . & save_ebs_dens . or . & want_partial_charges ) then if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( save_ebs_dens ) filesOut % ebs_dens = trim ( slabel ) // '.EBS' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' g2max = g2cut dummy_str = 0.0 dummy_strl = 0.0 call dhscf ( nspin , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa_last , indxua , & ntm , 0 , 0 , 0 , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_str , fa , dummy_strl ) ! next to last argument is dummy here, ! as no forces are calculated ! todo: make all these optional endif C C     Call the wannier90 interface here, as local_DOS destroys the DM... C if ( w90_processing ) call siesta2wannier90 () C     Find local density of states !  It needs H,S, and xa, as it diagonalizes them again !  NOTE: This call will obliterate Dscf !  It is better to put a explicit out argument for the partial DM computed. call local_DOS () ! In summary, it is better to: ! !   -- Avoid (or warn the user about) doing any analysis if the calculation is not converged !   -- Avoid mixing DM or H after scf convergence !   -- Set xa to xa_last at the top of this file. Write any \"next iter\" coordinate file !      in 'siesta_move' endif ! SIESTA_worker #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.LDOS\" ,. false .)) then call pexsi_local_DOS () endif #endif if ( SIESTA_worker ) call timer ( \"Analysis\" , 2 ) !------------------------------------------------------------------------- END END subroutine siesta_analysis END module m_siesta_analysis","tags":"","loc":"sourcefile/siesta_analysis.f.html","title":"siesta_analysis.F – SIESTA"},{"text":"This file depends on sourcefile~~state_init.f~~EfferentGraph sourcefile~state_init.f state_init.F sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~state_init.f~~AfferentGraph sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~state_init.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_state_init Source Code state_init.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_state_init private public :: state_init CONTAINS subroutine state_init ( istep ) use kpoint_scf_m , only : setup_kpoint_scf , kpoints_scf use kpoint_t_m , only : kpoint_delete , kpoint_nullify use m_os , only : file_exist use m_new_dm , only : new_dm use m_proximity_check , only : proximity_check use siesta_options use units , only : Ang use sparse_matrices , only : maxnh , numh , listh , listhptr use sparse_matrices , only : Dold , Dscf , DM_2D use sparse_matrices , only : Eold , Escf , EDM_2D use sparse_matrices , only : Hold , H , H_2D use sparse_matrices , only : xijo , xij_2D use sparse_matrices , only : S , S_1D use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : sparse_pattern use sparse_matrices , only : block_dist , single_dist use sparse_matrices , only : DM_history use create_Sparsity_SC , only : crtSparsity_SC use m_sparsity_handling , only : SpOrb_to_SpAtom use m_sparsity_handling , only : Sp_to_Spglobal use m_pivot_methods , only : sp2graphviz use siesta_geom use atomlist , only : iphorb , iphkb , indxua , & rmaxo , rmaxkb , rmaxv , rmaxldau , & lastkb , lasto , superc , indxuo , & no_u , no_s , no_l , iza , qtots use alloc , only : re_alloc , de_alloc , alloc_report use m_hsparse , only : hsparse use m_overlap , only : overlap use m_supercell , only : exact_sc_ag use siesta_cml , only : cml_p , cmlStartStep , mainXML use siesta_cml , only : cmlStartPropertyList use siesta_cml , only : cmlEndPropertyList use siesta_cml , only : cmlAddProperty use zmatrix , only : lUseZmatrix , write_zmatrix use m_energies , only : Emad use write_subs use m_ioxv , only : ioxv use m_iotdxv , only : iotdxv use m_steps use parallel , only : IOnode , node , nodes , BlockSize use m_spin , only : spin use m_rmaxh use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_normalize_dm , only : normalize_dm use m_eo use m_gamma use files , only : slabel use m_mpi_utils , only : globalize_or use m_mpi_utils , only : globalize_sum use domain_decom , only : domainDecom , use_dd , use_dd_perm use ldau_specs , only : switch_ldau , ldau_init use fdf , only : fdf_get use sys , only : message , die use m_sparse , only : xij_offset use ts_kpoint_scf_m , only : setup_ts_kpoint_scf , ts_kpoints_scf use m_ts_charge , only : TS_RHOCORR_METHOD , TS_RHOCORR_FERMI use m_ts_options , only : BTD_method use m_ts_options , only : TS_Analyze use m_ts_options , only : N_Elec , Elecs , IsVolt use m_ts_electype use m_ts_global_vars , only : TSrun , TSmode , onlyS use sys , only : bye use m_ts_io , only : fname_TSHS , ts_write_tshs use m_ts_sparse , only : ts_sparse_init use m_ts_tri_init , only : ts_tri_init , ts_tri_analyze use files , only : slabel , label_length #ifdef SIESTA__CHESS use m_chess , only : CheSS_init , get_CheSS_parameter #endif #ifdef CDF use iodm_netcdf , only : setup_dm_netcdf_file use iodmhs_netcdf , only : setup_dmhs_netcdf_file #endif use class_Sparsity use class_dSpData1D use class_dSpData2D use class_zSpData2D use class_dData2D #ifdef TEST_IO use m_test_io #endif #ifdef SIESTA__FLOOK use siesta_dicts , only : dict_repopulate_MD #endif implicit none integer :: istep , nnz real ( dp ) :: veclen ! Length of a unit-cell vector real ( dp ) :: rmax logical :: cell_can_change integer :: i , ix , iadispl , ixdispl logical :: auxchanged ! Auxiliary supercell changed? logical :: folding , folding1 logical :: diag_folding , diag_folding1 logical :: foundxv ! dummy for call to ioxv external :: madelung , timer real ( dp ), external :: volcel integer :: ts_kscell_file ( 3 , 3 ) = 0 real ( dp ) :: ts_kdispl_file ( 3 ) = 0.0 logical :: ts_Gamma_file = . true . character ( len = label_length + 6 ) :: fname real ( dp ) :: dummyef = 0.0 , dummyqtot = 0.0 #ifdef SIESTA__CHESS integer :: maxnh_kernel , maxnh_mult , no_l_kernel , no_l_mult integer , dimension (:), allocatable :: listh_kernel , listh_mult integer , dimension (:), allocatable :: numh_kernel , numh_mult real ( dp ) :: chess_value #endif type ( Sparsity ) :: g_Sp character ( len = 256 ) :: oname type ( dData2D ) :: tmp_2D real ( dp ) :: dummy_qspin ( 8 ) !------------------------------------------------------------------- BEGIN call timer ( 'IterGeom' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_init' ) #endif call timer ( 'state_init' , 1 ) istp = istp + 1 if ( IOnode ) then write ( 6 , '(/,t22,a)' ) repeat ( '=' , 36 ) select case ( idyn ) case ( 0 ) if ( nmove == 0 ) then write ( 6 , '(t25,a)' ) 'Single-point calculation' if ( cml_p ) call cmlStartStep ( mainXML , type = 'Single-Point' , $ index = istp ) else if ( broyden_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin Broyden opt. move = ' , $ istep else if ( fire_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin FIRE opt. move = ' , $ istep else write ( 6 , '(t25,a,i6)' ) 'Begin CG opt. move = ' , $ istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'Geom. Optim' , $ index = istp ) endif !        Print Z-matrix coordinates if ( lUseZmatrix ) then call write_Zmatrix () endif case ( 1 , 3 ) if ( iquench > 0 ) then write ( 6 , '(t25,a,i6)' ) 'Begin MD quenched step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD-quenched' , $ index = istep ) else write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , $ index = istep ) endif case ( 2 , 4 , 5 ) write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , index = istep ) case ( 6 ) write ( 6 , '(t25,a,i6)' ) 'Begin FC step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FC' , index = istep ) if ( istep . eq . 0 ) then write ( 6 , '(t25,a)' ) 'Undisplaced coordinates' else iadispl = ( istep - mod ( istep - 1 , 6 )) / 6 + ia1 ix = mod ( istep - 1 , 6 ) + 1 ixdispl = ( ix - mod ( ix - 1 , 2 ) + 1 ) / 2 write ( 6 , '(t26,a,i0,/,t26,a,i1,a,f10.6,a)' ) 'displace atom ' , & iadispl , 'in direction ' , ixdispl , ' by' , dx / Ang , ' Ang' endif case ( 8 ) write ( 6 , '(t25,a,i6)' ) 'Begin Server step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FS' , index = istep ) case ( 9 ) if ( istep == 0 ) then write ( 6 , '(t25,a,i7)' ) 'Explicit coord. initialization' else write ( 6 , '(t25,a,i7)' ) 'Explicit coord. step =' , istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'ECS' , index = istep ) case ( 10 ) write ( 6 , '(t25,a,i7)' ) 'LUA coord. step =' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'LUA' , index = istep ) end select write ( 6 , '(t22,a)' ) repeat ( '=' , 36 ) !     Print atomic coordinates call outcoor ( ucell , xa , na_u , ' ' , writec ) !     Save structural information in crystallographic format !     (in file SystemLabel.STRUCT_OUT), !     canonical Zmatrix (if applicable), and CML record call siesta_write_positions ( moved = . false .) endif ! IONode ! Write the XV file for single-point calculations, so that ! it is there at the end for those users who rely on it call ioxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , & foundxv ) ! Write TDXV file for TDDFT restart. if ( writetdwf . or . td_elec_dyn ) then call iotdxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , foundxv ) end if !     Actualize things if variable cell auxchanged = . false . cell_can_change = ( varcel . or . & ( idyn . eq . 8 ) ! Force/stress evaluation & ) if ( change_kgrid_in_md ) then cell_can_change = cell_can_change . or . & ( idyn . eq . 3 ) . or . ! Parrinello-Rahman & ( idyn . eq . 4 ) . or . ! Nose-Parrinello-Rahman & ( idyn . eq . 5 ) ! Anneal endif if ( cell_can_change . and . & ( istep . ne . inicoor ) . and . (. not . gamma ) ) then !       Will print k-points also call kpoint_delete ( kpoints_scf ) call setup_kpoint_scf ( ucell ) if ( TSmode ) then call kpoint_delete ( ts_kpoints_scf ) else call kpoint_nullify ( ts_kpoints_scf ) end if call setup_ts_kpoint_scf ( ucell , kpoints_scf ) call re_alloc ( eo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'eo' , 'state_init' ) call re_alloc ( qo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'qo' , 'state_init' ) !       Find required supercell if ( gamma ) then nsc ( 1 : 3 ) = 1 else do i = 1 , 3 veclen = sqrt ( ucell ( 1 , i ) ** 2 + ucell ( 2 , i ) ** 2 + ucell ( 3 , i ) ** 2 ) nsc ( i ) = 1 + 2 * ceiling ( rmaxh / veclen ) end do ! The above is kept for historical reasons, ! but a tight supercell can be found from the atom-graph info: call exact_sc_ag ( negl , ucell , na_u , isa , xa , nsc ) endif mscell = 0.0_dp do i = 1 , 3 mscell ( i , i ) = nsc ( i ) if ( nsc ( i ) /= nscold ( i )) auxchanged = . true . nscold ( i ) = nsc ( i ) enddo !       Madelung correction for charged systems if ( charnet . ne . 0.0_dp ) then call madelung ( ucell , shape , charnet , Emad ) endif endif !     End variable cell actualization !     Auxiliary supercell !     Do not move from here, as the coordinates might have changed !     even if not the unit cell call superc ( ucell , scell , nsc ) #ifdef SIESTA__FLOOK call dict_repopulate_MD () #endif !     Print unit cell and compute cell volume !     Possible BUG: !     Note that this volume is later used in write_subs and the md output !     routines, even if the cell later changes. if ( IOnode ) call outcell ( ucell ) volume_of_some_cell = volcel ( ucell ) !     Use largest possible range in program, except hsparse... !     2 * rmaxv: Vna overlap !     rmaxo + rmaxkb: Non-local KB action !     2 * (rmaxo + rmaxldau): Interaction through LDAU projector !     2.0_dp * (rmaxo+rmaxkb) : Orbital interaction through KB projectors rmax = max ( 2._dp * rmaxv , 2._dp * ( rmaxo + rmaxldau ), rmaxo + rmaxkb ) if ( . not . negl ) then rmax = max ( rmax , 2.0_dp * ( rmaxo + rmaxkb ) ) endif !     Check if any two atoms are unreasonably close call proximity_check ( rmax ) ! Clear history of mixing parameters call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) ! Ensure sparsity pattern is empty call delete ( sparse_pattern ) ! sadly deleting the sparse pattern does not necessarily ! mean that the arrays are de-associated. ! Remember that the reference counter could (in MD) ! be higher than 1, hence we need to create \"fake\" ! containers and let the new<class> delete the old ! sparsity pattern nullify ( numh , listhptr , listh ) allocate ( numh ( no_l ), listhptr ( no_l )) ! We do not need to allocate listh ! that will be allocated in hsparse #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then !         Calculate a sparsity pattern with some buffers... Only required !         for CheSS chess_value = get_chess_parameter ( 'chess_buffer_kernel' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_kernel = maxnh no_l_kernel = no_l allocate ( listh_kernel ( maxnh_kernel )) allocate ( numh_kernel ( no_l_kernel )) listh_kernel = listh numh_kernel = numh chess_value = get_chess_parameter ( 'chess_buffer_mult' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_mult = maxnh no_l_mult = no_l allocate ( listh_mult ( maxnh_mult )) allocate ( numh_mult ( no_l_mult )) listh_mult = listh numh_mult = numh end if #endif /* CHESS */ !     List of nonzero Hamiltonian matrix elements !     and, if applicable,  vectors between orbital centers !     Listh and xijo are allocated inside hsparse !     Note: We always generate xijo now, for COOP and other !           analyses. call delete ( xij_2D ) ! as xijo will be reallocated nullify ( xijo ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , $ set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ debug_folding = fdf_get ( 'debug-folding' ,. false .)) ! call globalize_or ( diag_folding1 , diag_folding ) call globalize_or ( folding1 , folding ) if ( diag_folding . and . gamma ) then call message ( \"WARNING\" , \"Gamma-point calculation \" // $ \"with interaction between periodic images\" ) call message ( \"WARNING\" , $ \"Some features might not work optimally:\" ) call message ( \"WARNING\" , $ \"e.g. DM initialization from atomic data\" ) if ( harrisfun ) call die ( \"Harris functional run needs \" // $ \"'force-aux-cell T'\" ) else if ( folding ) then if ( gamma ) then call message ( \"INFO\" , \"Gamma-point calculation \" // $ \"with multiply-connected orbital pairs\" ) call message ( \"INFO\" , $ \"Folding of H and S implicitly performed\" ) call check_cohp () else write ( 6 , \"(a,/,a)\" ) \"Non Gamma-point calculation \" // $ \"with multiply-connected orbital pairs \" // $ \"in auxiliary supercell.\" , $ \"Possible internal error. \" // $ \"Use 'debug-folding T' to debug.\" call die ( \"Inadequate auxiliary supercell\" ) endif endif ! call globalize_sum ( maxnh , nnz ) if ( cml_p ) then call cmlStartPropertyList ( mainXML , title = 'Orbital info' ) call cmlAddProperty ( xf = mainXML , value = no_u , $ title = 'Number of orbitals in unit cell' , $ dictref = 'siesta:no_u' , units = \"cmlUnits:countable\" ) call cmlAddProperty ( xf = mainXML , value = nnz , $ title = 'Number of non-zeros' , $ dictref = 'siesta:nnz' , units = \"cmlUnits:countable\" ) call cmlEndPropertyList ( mainXML ) endif ! #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then call CheSS_init ( node , nodes , maxnh , maxnh_kernel , maxnh_mult , & no_u , no_l , no_l_kernel , no_l_mult , BlockSize , & spin % spinor , qtots , listh , listh_kernel , listh_mult , & numh , numh_kernel , numh_mult ) deallocate ( listh_kernel ) deallocate ( numh_kernel ) deallocate ( listh_mult ) deallocate ( numh_mult ) end if #endif /* CHESS */ ! ! If using domain decomposition, redistribute orbitals ! for this geometry, based on the hsparse info. ! The first time round, the initial distribution is a ! simple block one (given by preSetOrbitLimits). ! ! Any DM, etc, read from file will be redistributed according ! to the new pattern. ! Inherited DMs from a previous geometry cannot be used if the ! orbital distribution changes. For now, we avoid changing the ! distribution (the variable use_dd_perm is .true. if domain ! decomposition is in effect). Names should be changed... if ( use_dd . and . (. not . use_dd_perm )) then call domainDecom ( no_u , no_l , maxnh ) ! maxnh intent(in) here maxnh = sum ( numh ( 1 : no_l )) ! We still need to re-create Julian Gale's ! indexing for O(N) in parallel. print \"(a5,i3,a20,3i8)\" , $ \"Node: \" , Node , \"no_u, no_l, maxnh: \" , no_u , no_l , maxnh call setup_ordern_indexes ( no_l , no_u , Nodes ) endif ! I would like to skip this alloc/move/dealloc/attach ! by allowing sparsity to have pointer targets. ! However, this poses a problem with intel compilers, ! as it apparently errors out when de-allocating a target pointer write ( oname , \"(a,i0)\" ) \"sparsity for geom step \" , istep call newSparsity ( sparse_pattern , no_l , no_u , maxnh , & numh , listhptr , listh , name = oname ) deallocate ( numh , listhptr , listh ) call attach ( sparse_pattern , & n_col = numh , list_ptr = listhptr , list_col = listh ) ! In case the user requests to create the connectivity graph if ( write_GRAPHVIZ > 0 ) then ! first create the unit-cell sparsity pattern call crtSparsity_SC ( sparse_pattern , g_Sp , UC = . true .) ! next move to global sparsity pattern call Sp_to_Spglobal ( block_dist , g_Sp , g_Sp ) if ( IONode ) then if ( write_GRAPHVIZ /= 2 ) & call sp2graphviz ( trim ( slabel ) // '.ORB.gv' , g_Sp ) ! Convert to atomic if ( write_GRAPHVIZ /= 1 ) then call SpOrb_to_SpAtom ( single_dist , g_Sp , na_u , lasto , g_Sp ) call sp2graphviz ( trim ( slabel ) // '.ATOM.gv' , g_Sp ) end if end if call delete ( g_Sp ) end if ! Copy over xijo array (we can first do it here... :( ) call newdData2D ( tmp_2D , xijo , 'xijo' ) deallocate ( xijo ) write ( oname , \"(a,i0)\" ) \"xijo at geom step \" , istep call newdSpData2D ( sparse_pattern , tmp_2D , block_dist , xij_2D , & name = oname ) call delete ( tmp_2D ) ! decrement container... xijo => val ( xij_2D ) ! Calculate the super-cell offsets... if ( Gamma ) then ! Here we create the super-cell offsets call re_alloc ( isc_off , 1 , 3 , 1 , 1 ) isc_off (:,:) = 0 else call xij_offset ( ucell , nsc , na_u , xa , lasto , & xij_2D , isc_off , & Bcast = . true .) end if ! When the user requests to only do an analyzation, we can call ! appropriate routines and quit if ( TS_Analyze ) then ! Force the creation of the full sparsity pattern call ts_sparse_init ( slabel , IsVolt , N_Elec , Elecs , & ucell , nsc , na_u , xa , lasto , block_dist , sparse_pattern , & Gamma , isc_off ) ! create the tri-diagonal matrix call ts_tri_analyze ( block_dist , sparse_pattern , N_Elec , & Elecs , ucell , na_u , lasto , nsc , isc_off , & BTD_method ) ! Print-out timers call timer ( 'TS-rgn2tri' , 3 ) ! Bye also waits for all processors call bye ( 'transiesta analyzation performed' ) end if write ( oname , \"(a,i0)\" ) \"EDM at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % EDM , block_dist , EDM_2D , & name = oname ) !if (ionode) call print_type(EDM_2D) Escf => val ( EDM_2D ) call re_alloc ( Dold , 1 , maxnh , 1 , spin % DM , name = 'Dold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) call re_alloc ( Hold , 1 , maxnh , 1 , spin % H , name = 'Hold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) if ( converge_EDM ) then call re_alloc ( Eold , 1 , maxnh , 1 , spin % EDM , name = 'Eold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) end if !     Allocate/reallocate storage associated with Hamiltonian/Overlap matrix write ( oname , \"(a,i0)\" ) \"H at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H , block_dist , H_2D , & name = oname ) !if (ionode) call print_type(H_2D) H => val ( H_2D ) write ( oname , \"(a,i0)\" ) \"H_vkb at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_vkb_1D , name = oname ) !if (ionode) call print_type(H_vkb_1D) write ( oname , \"(a,i0)\" ) \"H_kin at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_kin_1D , name = oname ) !if (ionode) call print_type(H_kin_1D) if ( switch_ldau ) then write ( oname , \"(a,i0)\" ) \"H_ldau at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % spinor , & block_dist , H_ldau_2D , name = oname ) ! Initialize to 0, LDA+U may re-calculate !   this matrix sporadically doing the SCF. ! Hence initialization MUST be performed upon ! re-allocation. call init_val ( H_ldau_2D ) if ( inicoor /= istep ) then ! Force initialization of the LDA+U ! when changing geometry ! For the first geometry this is controlled ! by the user via an fdf-key ldau_init = . true . end if end if if ( spin % SO_onsite ) then write ( oname , \"(a,i0)\" ) \"H_so (onsite) at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H - 2 , & block_dist , H_so_on_2D , name = oname ) else if ( spin % SO_offsite ) then write ( oname , \"(a,i0)\" ) \"H_so (offsite) at geom step \" , istep call newzSpData2D ( sparse_pattern , 4 , & block_dist , H_so_off_2D , name = oname ) endif write ( oname , \"(a,i0)\" ) \"S at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , S_1D , name = oname ) if ( ionode ) call print_type ( S_1D ) S => val ( S_1D ) !     Find overlap matrix call overlap ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , maxnh , & lasto , iphorb , isa , numh , listhptr , listh , S ) ! !     Here we could also read a Hamiltonian, either to proceed to !     the analysis section (with nscf=0) or to start a mix-H scf cycle. ! !     Initialize density matrix ! The resizing of Dscf is done inside new_dm call new_DM ( auxchanged , DM_history , DM_2D , EDM_2D ) Dscf => val ( DM_2D ) Escf => val ( EDM_2D ) if ( spin % H > 1 ) call print_spin ( dummy_qspin ) ! Initialize energy-density matrix to zero for first call to overfsm ! Only part of Escf is updated in TS, so if it is put as zero here ! a continuation run gives bad forces. if ( . not . TSrun ) then call normalize_DM ( first = . true . ) !$OMP parallel workshare default(shared) Escf (:,:) = 0.0_dp !$OMP end parallel workshare end if #ifdef TEST_IO ! We test the io-performance here call time_io ( spin % H , H_2D ) #endif !     If onlyS, Save overlap matrix and exit if ( onlyS ) then fname = fname_TSHS ( slabel , onlyS = . true . ) ! We include H as S, well-knowing that we only write one of ! them, there is no need to allocate space for no reason! call ts_write_tshs ( fname , & . true ., Gamma , ts_Gamma_file , & ucell , nsc , isc_off , na_u , no_s , spin % H , & ts_kscell_file , ts_kdispl_file , & xa , lasto , & H_2D , S_1D , indxuo , & dummyEf , dummyQtot , Temp , 0 , 0 ) call bye ( 'Save overlap matrix and exit' ) ! Exit siesta endif ! In case the user is requesting a Fermi-correction ! we need to delete the TS_FERMI file after each iteration if ( TSmode . and . TS_RHOCORR_METHOD == TS_RHOCORR_FERMI & . and . IONode ) then ! Delete the TS_FERMI file (enables ! reading it in and improve on the convergence) if ( file_exist ( 'TS_FERMI' ) ) then i = 23455 ! this should just not be used any were... ! Delete the file... open ( unit = i , file = 'TS_FERMI' ) close ( i , status = 'delete' ) end if end if #ifdef CDF if ( writedm_cdf ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh ) endif if ( writedm_cdf_history ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & istep ) endif if ( writedmhs_cdf ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s ) endif if ( writedmhs_cdf_history ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s , & istep ) endif #endif call timer ( 'state_init' , 2 ) END subroutine state_init subroutine check_cohp () use siesta_options , only : write_coop use sys , only : message if ( write_coop ) then call message ( \"WARNING\" , \"There are multiply-connected \" // $ \"orbitals.\" ) call message ( \"WARNING\" , \"Your COOP/COHP analysis might \" // $ \"be affected by folding.\" ) call message ( \"WARNING\" , 'Use \"force-aux-cell T \"' // $ 'or k-point sampling' ) endif end subroutine check_cohp END module m_state_init","tags":"","loc":"sourcefile/state_init.f.html","title":"state_init.F – SIESTA"},{"text":"Files dependent on this one sourcefile~~state_analysis.f~~AfferentGraph sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~state_analysis.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_state_analysis Source Code state_analysis.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_state_analysis use write_subs private public :: state_analysis CONTAINS subroutine state_analysis ( istep ) use siesta_cml use m_born_charge , only : born_charge use parallel , only : IOnode use m_wallclock , only : wallclock use zmatrix , only : lUseZmatrix , iofaZmat , & CartesianForce_to_ZmatForce use atomlist , only : iaorb , iphorb , amass , no_u , lasto use atomlist , only : indxuo use m_spin , only : spin use m_fixed , only : fixed use sparse_matrices use siesta_geom USE siesta_options use units , only : amu , eV use m_stress use m_energies , only : Etot , FreeE , Eharrs , FreeEHarris , Entropy use m_energies , only : Ebs , Ef use m_ntm use m_forces use m_energies , only : update_FreeE , update_FreeEHarris use m_intramol_pressure , only : remove_intramol_pressure #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_FORCES #endif implicit none integer :: istep integer :: ia , jx , ix real ( dp ) :: volume logical :: eggbox_block = . true . ! Read eggbox info from data file? real ( dp ) :: qspin external :: eggbox , mulliken , moments real ( dp ), external :: volcel !------------------------------------------------------------------------- BEGIN call timer ( 'state_analysis' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_analysis' ) #endif if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'SCF Finalization' ) endif !     Write final Kohn-Sham and Free Energy FreeE = Etot - Temp * Entropy FreeEHarris = Eharrs - Temp * Entropy if ( cml_p ) call cmlStartPropertyList ( mainXML , & title = 'Energies and spin' ) if ( IOnode ) then if ( . not . harrisfun ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS(eV) =        ' , Etot / eV if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = FreeE / eV , & dictref = 'siesta:FreeE' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ebs / eV , & dictref = 'siesta:Ebs' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ef / eV , & dictref = 'siesta:E_Fermi' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif endif !     Substract egg box effect from energy if ( eggbox_block ) then call eggbox ( 'energy' , ucell , na_u , isa , ntm , xa , fa , Etot , & eggbox_block ) FreeE = Etot - Temp * Entropy if ( IOnode ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS - E_eggbox = ' , Etot / eV if ( cml_p ) call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS_egg' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif call update_FreeE ( Temp ) call update_FreeEHarris ( Temp ) call print_spin ( qspin ) if ( cml_p ) call cmlEndPropertyList ( mainXML ) !     Substract egg box effect from the forces if ( eggbox_block ) then call eggbox ( 'forces' , ucell , na_u , isa , ntm , xa , fa , Etot , eggbox_block ) endif if ( IOnode ) call write_raw_efs ( stress , na_u , fa , FreeE ) !     Compute stress without internal molecular pressure call remove_intramol_pressure ( ucell , stress , na_u , xa , fa , mstress ) !     Impose constraints to atomic movements by changing forces ........... if ( RemoveIntraMolecularPressure ) then !        Consider intramolecular pressure-removal as another !        kind of constraint call fixed ( ucell , mstress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) else call fixed ( ucell , stress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) endif #ifdef SIESTA__FLOOK ! We call it right after using the ! geometry constraints. ! In that way we can use both methods on top ! of each other! ! The easy, already implemented methods in fixed, ! and custom ones in Lua :) call slua_call ( LUA , LUA_FORCES ) #endif !     Calculate and output Zmatrix forces if ( lUseZmatrix . and . ( idyn . eq . 0 )) then call CartesianForce_to_ZmatForce ( na_u , xa , fa ) if ( IOnode ) call iofaZmat () endif !     Compute kinetic contribution to stress kin_stress ( 1 : 3 , 1 : 3 ) = 0.0_dp volume = volcel ( ucell ) do ia = 1 , na_u do jx = 1 , 3 do ix = 1 , 3 kin_stress ( ix , jx ) = kin_stress ( ix , jx ) - & amu * amass ( ia ) * va ( ix , ia ) * va ( jx , ia ) / volume enddo enddo enddo !     Add kinetic term to stress tensor tstress = stress + kin_stress !     Force output if ( IOnode ) then call siesta_write_forces ( istep ) call siesta_write_stress_pressure () call wallclock ( '--- end of geometry step' ) endif !     Population and moment analysis if ( spin % SO . and . orbmoms ) then call moments ( 1 , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) endif ! Call this unconditionally call mulliken ( mullipop , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) ! !     Call the born effective charge routine only in those steps (even) !     in which the dx  is positive. if ( bornz . and . ( mod ( istep , 2 ) . eq . 0 )) then call born_charge () endif !     End the xml module corresponding to the analysis if ( cml_p ) then call cmlEndModule ( mainXML ) endif call timer ( 'state_analysis' , 2 ) !--------------------------------------------------------------------------- END END subroutine state_analysis END MODULE m_state_analysis","tags":"","loc":"sourcefile/state_analysis.f.html","title":"state_analysis.F – SIESTA"},{"text":"The old dhscf has been split in two parts: an initialization routine dhscf_init , which is called after\n every geometry change but before\n the main scf loop, and a dhscf proper,\n which is called at every step\n of the scf loop Note The mesh initialization part is now done unconditionally in dhscf_init , i.e, after every geometry change, even if the\n change does not involve a cell change. The reason is to avoid\n complexity, since now the mesh parallel distributions will depend on\n the detailed atomic positions even if the cell does not change. Besides, the relative cost of a \"mesh only\" initialization is negligible.\n The only real observable effect would be a printout of \"initmesh\" data\n at every geometry iteration. This file depends on sourcefile~~dhscf.f~~EfferentGraph sourcefile~dhscf.f dhscf.F sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~dhscf.f~~AfferentGraph sourcefile~dhscf.f dhscf.F sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_dhscf Source Code dhscf.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! !! The old dhscf has been split in two parts: an initialization routine !! `[[dhscf_init(proc)]]`, which is called after !! every geometry change but before !! the main scf loop, and a `[[dhscf(proc)]]` proper, !! which is called at every step !! of the scf loop !!@note !! The mesh initialization part is now done *unconditionally* !! in `dhscf_init`, i.e, after *every* geometry change, even if the !! change does not involve a cell change. The reason is to avoid !! complexity, since now the mesh parallel distributions will depend on !! the detailed atomic positions even if the cell does not change. !!@endnote !! Besides, the relative cost of a \"mesh only\" initialization is negligible. !! The only real observable effect would be a printout of \"initmesh\" data !! at every geometry iteration. module m_dhscf !! To facilitate the communication among !! `[[dhscf_init(proc)]]` and `[[dhscf(proc)]]`, !! some arrays that hold data which do not change during the SCF loop !! have been made into module variables !! !! Some others are scratch, such as `nmpl`, `ntpl`, etc use precision , only : dp , grid_p use m_dfscf , only : dfscf implicit none real ( grid_p ), pointer :: rhopcc (:), rhoatm (:), Vna (:) real ( dp ) :: Uharrs !! Harris energy logical :: IsDiag , spiral character ( len = 10 ) :: shape integer :: nml ( 3 ), ntml ( 3 ), npcc , & nmpl , ntpl real ( dp ) :: bcell ( 3 , 3 ), cell ( 3 , 3 ), & dvol , field ( 3 ), rmax , scell ( 3 , 3 ) real ( dp ) :: G2mesh = 0.0_dp logical :: debug_dhscf = . false . character ( len =* ), parameter :: debug_fmt = & '('' -- dhscf : Node '',i3,tr1,a,4(tr1,e20.7))' public :: dhscf_init , dhscf CONTAINS subroutine dhscf_init ( nspin , norb , iaorb , iphorb , & nuo , nuotot , nua , na , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnd , numd , listdptr , listd , datm , & Fal , stressl ) use precision , only : dp , grid_p use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use fdf use sys , only : die use mesh , only : xdsp , nsm , nsp , meshLim use parsing #ifndef BSC_CELLXC use siestaXC , only : getXC ! Returns the XC functional used #else /* BSC_CELLXC */ use bsc_xcmod , only : nXCfunc , XCauth #endif /* BSC_CELLXC */ use alloc , only : re_alloc , de_alloc use siesta_options , only : harrisfun use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use meshsubs , only : PhiOnMesh use meshsubs , only : InitMesh use meshsubs , only : InitAtomMesh use meshsubs , only : setupExtMesh use meshsubs , only : distriPhiOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use meshdscf , only : createLocalDscfPointers use iogrid_netcdf , only : set_box_limits #ifdef NCDF_4 use m_ncdf_io , only : cdf_init_mesh #endif #ifdef BSC_CELLXC use cellxc_mod , only : setGGA #endif /* BSC_CELLXC */ use m_efield , only : initialize_efield , acting_efield use m_efield , only : get_field_from_dipole use m_efield , only : dipole_correction use m_efield , only : user_specified_field use m_doping_uniform , only : initialize_doping_uniform use m_doping_uniform , only : compute_doping_structs_uniform , $ doping_active use m_rhog , only : rhog , rhog_in use m_rhog , only : order_rhog use siesta_options , only : mix_charge #ifdef MPI use mpi_siesta #endif use m_mesh_node , only : init_mesh_node use m_charge_add , only : init_charge_add use m_hartree_add , only : init_hartree_add use m_ts_global_vars , only : TSmode use m_ts_options , only : IsVolt , N_Elec , Elecs use m_ts_voltage , only : ts_init_voltage use m_ts_hartree , only : ts_init_hartree_fix implicit none integer , intent ( in ) :: nspin , norb , iaorb ( norb ), iphorb ( norb ), & nuo , nuotot , nua , na , isa ( na ), & indxua ( na ), mscell ( 3 , 3 ), maxnd , & numd ( nuo ), listdptr ( nuo ), listd ( maxnd ) real ( dp ), intent ( in ) :: xa ( 3 , na ), ucell ( 3 , 3 ), datm ( norb ) real ( dp ), intent ( inout ) :: g2max integer , intent ( inout ) :: ntm ( 3 ) real ( dp ), intent ( inout ) :: Fal ( 3 , nua ), stressl ( 3 , 3 ) real ( dp ), parameter :: tiny = 1.e-12_dp integer :: io , ia , iphi , is , n , i , j integer :: nsc ( 3 ), nbcell , nsd real ( dp ) :: DStres ( 3 , 3 ), volume real ( dp ), external :: volcel , ddot real ( grid_p ) :: dummy_Drho ( 1 , 1 ), dummy_Vaux ( 1 ), & dummy_Vscf ( 1 ) logical , save :: frstme = . true . ! Keeps state real ( grid_p ), pointer :: Vscf (:,:), rhoatm_par (:) integer , pointer :: numphi (:), numphi_par (:) integer :: nm ( 3 ) ! For call to initMesh #ifndef BSC_CELLXC integer :: nXCfunc character ( len = 20 ) :: XCauth ( 10 ), XCfunc ( 10 ) #endif /* ! BSC_CELLXC */ ! Transport direction (unit-cell aligned) integer :: iE real ( dp ) :: ortho , field ( 3 ), field2 ( 3 ) !--------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE dhscf_init' ) #endif ! ---------------------------------------------------------------------- !     General initialisation ! ---------------------------------------------------------------------- !     Start time counter call timer ( 'DHSCF_Init' , 1 ) nsd = min ( nspin , 2 ) nullify ( Vscf , rhoatm_par ) if ( frstme ) then debug_dhscf = fdf_get ( 'Debug.DHSCF' , . false .) nullify ( xdsp , rhopcc , Vna , rhoatm ) !       nsm lives in module m_dhscf now    !! AG** nsm = fdf_integer ( 'MeshSubDivisions' , 2 ) nsm = max ( nsm , 1 ) !       Set mesh sub-division variables & perform one off allocation nsp = nsm * nsm * nsm call re_alloc ( xdsp , 1 , 3 , 1 , nsp , 'xdsp' , 'dhscf_init' ) !       Check spin-spiral wavevector (if defined) if ( spiral . and . nspin . lt . 4 ) & call die ( 'dhscf: ERROR: spiral defined but nspin < 4' ) endif ! First time #ifndef BSC_CELLXC ! Get functional(s) being used call getXC ( nXCfunc , XCfunc , XCauth ) #endif /* ! BSC_CELLXC */ if ( harrisfun ) then do n = 1 , nXCfunc if (. not .( leqi ( XCauth ( n ), 'PZ' ). or . leqi ( XCauth ( n ), 'CA' ))) then call die ( \"** Harris forces not implemented for non-LDA XC\" ) endif enddo endif ! ---------------------------------------------------------------------- !     Orbital initialisation : part 1 ! ---------------------------------------------------------------------- !     Find the maximum orbital radius rmax = 0.0_dp do io = 1 , norb ia = iaorb ( io ) ! Atomic index of each orbital iphi = iphorb ( io ) ! Orbital index of each  orbital in its atom is = isa ( ia ) ! Species index of each atom rmax = max ( rmax , rcut ( is , iphi ) ) enddo !     Start time counter for mesh initialization call timer ( 'DHSCF1' , 1 ) ! ---------------------------------------------------------------------- !     Unit cell handling ! ---------------------------------------------------------------------- !     Find diagonal unit cell and supercell call digcel ( ucell , mscell , cell , scell , nsc , IsDiag ) if (. not . IsDiag ) then if ( Node . eq . 0 ) then write ( 6 , '(/,a,3(/,a,3f12.6,a,i6))' ) & 'DHSCF: WARNING: New shape of unit cell and supercell:' , & ( 'DHSCF:' ,( cell ( i , j ), i = 1 , 3 ), '   x' , nsc ( j ), j = 1 , 3 ) endif endif !     Find the system shape call shaper ( cell , nua , isa , xa , shape , nbcell , bcell ) !     Find system volume volume = volcel ( cell ) ! ---------------------------------------------------------------------- !     Mesh initialization ! ---------------------------------------------------------------------- call InitMesh ( na , cell , norb , iaorb , iphorb , isa , rmax , & G2max , G2mesh , nsc , nmpl , nm , & nml , ntm , ntml , ntpl , dvol ) !     Setup box descriptors for each processor, !     held in module iogrid_netcdf call set_box_limits ( ntm , nsm ) ! Initialize information on local mesh for each node call init_mesh_node ( cell , ntm , meshLim , nsm ) ! Setup charge additions in the mesh call init_charge_add ( cell , ntm ) ! Setup Hartree additions in the mesh call init_hartree_add ( cell , ntm ) #ifdef NCDF_4 ! Initialize the box for each node... call cdf_init_mesh ( ntm , nsm ) #endif !     Stop time counter for mesh initialization call timer ( 'DHSCF1' , 2 ) ! ---------------------------------------------------------------------- !     End of mesh initialization ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     Initialize atomic orbitals, density and potential ! ---------------------------------------------------------------------- !     Start time counter for atomic initializations call timer ( 'DHSCF2' , 1 ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Initialise quantities relating to the atom-mesh positioning call InitAtomMesh ( UNIFORM , na , xa ) #ifdef BSC_CELLXC !     Check if we need extencils in cellxc call setGGA ( ) #endif /* BSC_CELLXC */ !     Compute the number of orbitals on the mesh and recompute the !     partions for every processor in order to have a similar load !     in each of them. nullify ( numphi ) call re_alloc ( numphi , 1 , nmpl , 'numphi' , 'dhscf_init' ) !$OMP parallel do default(shared), private(i) do i = 1 , nmpl numphi ( i ) = 0 enddo !$OMP end parallel do call distriPhiOnMesh ( nm , nmpl , norb , iaorb , iphorb , & isa , numphi ) !     Find if there are partial-core-corrections for any atom npcc = 0 do ia = 1 , na if ( rcore ( isa ( ia )) . gt . tiny ) npcc = 1 enddo !     Find partial-core-correction energy density !     Vscf and Vaux are not used here call re_alloc ( rhopcc , 1 , ntpl * npcc + 1 , 'rhopcc' , 'dhscf_init' ) if ( npcc . eq . 1 ) then call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , & nsd , dvol , volume , dummy_Vscf , dummy_Vaux , Fal , stressl , & . false ., . false . ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhopcc' , sqrt ( sum ( rhopcc ** 2 )) end if endif !     Find neutral-atom potential !     Drho is not used here call re_alloc ( Vna , 1 , ntpl , 'Vna' , 'dhscf_init' ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , dummy_DRho , Fal , stressl , & . false ., . false . ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Vna' , sqrt ( sum ( Vna ** 2 )) end if if ( nodes . gt . 1 ) then if ( node . eq . 0 ) then write ( 6 , \"(a)\" ) \"Setting up quadratic distribution...\" endif call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) !       Create extended mesh arrays for the second data distribution call setupExtMesh ( QUADRATIC , rmax ) !       Compute atom positions for the second data distribution call InitAtomMesh ( QUADRATIC , na , xa ) endif !     Calculate orbital values on mesh !     numphi has already been computed in distriPhiOnMesh !     in the UNIFORM distribution if ( nodes . eq . 1 ) then numphi_par => numphi else nullify ( numphi_par ) call re_alloc ( numphi_par , 1 , nmpl , 'numphi_par' , & 'dhscf_init' ) call distMeshData ( UNIFORM , numphi , QUADRATIC , & numphi_par , KEEP ) endif call PhiOnMesh ( nmpl , norb , iaorb , iphorb , isa , numphi_par ) if ( nodes . gt . 1 ) then call de_alloc ( numphi_par , 'numphi_par' , 'dhscf_init' ) endif call de_alloc ( numphi , 'numphi' , 'dhscf_init' ) ! ---------------------------------------------------------------------- !       Create sparse indexing for Dscf as needed for local mesh !       Note that this is done in the QUADRATIC distribution !       since 'endpht' (computed finally in PhiOnMesh and stored in !       meshphi module) is in that distribution. ! ---------------------------------------------------------------------- if ( Nodes . gt . 1 ) then call CreateLocalDscfPointers ( nmpl , nuotot , numd , listdptr , & listd ) endif ! ---------------------------------------------------------------------- !     Calculate terms relating to the neutral atoms on the mesh ! ---------------------------------------------------------------------- !     Find Harris (sum of atomic) electron density call re_alloc ( rhoatm_par , 1 , ntpl , 'rhoatm_par' , 'dhscf_init' ) call rhooda ( norb , nmpl , datm , rhoatm_par , iaorb , iphorb , isa ) !     rhoatm_par comes out of here in clustered form in QUADRATIC dist !     Routine Poison should use the uniform data distribution if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Create Rhoatm using UNIFORM distr, in sequential form call re_alloc ( rhoatm , 1 , ntpl , 'rhoatm' , 'dhscf_init' ) call distMeshData ( QUADRATIC , rhoatm_par , & UNIFORM , rhoatm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhoatm' , sqrt ( sum ( rhoatm ** 2 )) end if ! !  AG: The initialization of doping structs could be done here now, !      in the uniform distribution, and with a simple loop over !      rhoatm. if ( frstme ) call initialize_doping_uniform () if ( doping_active ) then call compute_doping_structs_uniform ( ntpl , rhoatm , nsd ) ! Will get the global number of hit points ! Then, the doping density to be added can be simply computed endif !     Allocate Temporal array call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf_init' ) !     Vscf is filled here but not used later !     Uharrs is computed (and saved) !     DStres is computed but not used later call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , rhoatm , & Uharrs , Vscf , DStres , nsm ) call de_alloc ( Vscf , 'Vscf' , 'dhscf_init' ) !     Always deallocate rhoatm_par, as it was used even if nodes=1 call de_alloc ( rhoatm_par , 'rhoatm_par' , 'dhscf_init' ) if ( mix_charge ) then call re_alloc ( rhog , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog' , 'dhscf_init' ) call re_alloc ( rhog_in , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog_in' , 'dhscf_init' ) call order_rhog ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nsm ) endif !     Stop time counter for atomic initializations call timer ( 'DHSCF2' , 2 ) ! ---------------------------------------------------------------------- !     At the end of initializations: !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution ! ---------------------------------------------------------------------- if ( frstme ) then call initialize_efield () end if ! Check if we need to add the potential ! corresponding to the voltage-drop. if ( TSmode ) then ! These routines are important if there are cell-changes call ts_init_hartree_fix ( cell , nua , xa , ntm , ntml ) if ( IsVolt ) then call ts_init_voltage ( cell , nua , xa , ntm ) end if if ( acting_efield ) then ! We do not allow the electric field for ! transiesta runs with V = 0, either. ! It does not make sense, only for fields perpendicular ! to the applied bias. ! We need to check that the e-field is perpendicular ! to the transport direction, and that the system is ! either a chain, or a slab. ! However, due to the allowance of a dipole correction ! along the transport direction for buffer calculations ! we have to allow all shapes. (atom is not transiesta ! compatible anyway) ! check that we do not violate the periodicity if ( Node . eq . 0 ) then write ( * , '(/,2(2a,/))' ) 'ts-WARNING: ' , & 'E-field/dipole-correction! ' , & 'ts-WARNING: ' , & 'I hope you know what you are doing!' end if ! This is either dipole or user, or both field (:) = user_specified_field (:) do iE = 1 , N_Elec field2 = Elecs ( iE )% cell (:, Elecs ( iE )% t_dir ) ortho = ddot ( 3 , field2 , 1 , field , 1 ) if ( abs ( ortho ) > 1.e-9_dp ) then call die ( ' User defined E - field must be & perpendicular to semi - infinite directions ' ) end if end do end if ! acting_efield ! We know that we currently allow people to do more than ! they probably should be allowed. However, there are many ! corner cases that may require dipole corrections, or ! electric fields to \"correct\" an intrinsic dipole. ! For instance, what should we do with a dipole in a transiesta ! calculation? ! Should we apply a field to counter act it in a device ! calculation? end if frstme = . false . call timer ( 'DHSCF_Init' , 2 ) #ifdef DEBUG call write_debug ( '    POS dhscf_init' ) #endif !------------------------------------------------------------------------- END end subroutine dhscf_init subroutine dhscf ( nspin , norb , iaorb , iphorb , nuo , & nuotot , nua , na , isa , xa , indxua , & ntm , ifa , istr , iHmat , & filesOut , maxnd , numd , & listdptr , listd , Dscf , datm , maxnh , Hmat , & Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , stress , Fal , stressl , & use_rhog_in , charge_density_only ) !! author: J.M. Soler !! date: August 1996 !! !! Calculates the self-consistent field contributions to Hamiltonian !! matrix elements, total energy and atomic forces. !! !! Coded by J.M. Soler, August 1996. July 1997. !! Modified by J.D. Gale, February 2000. !! !! !!### Units !! Energies in Rydbergs. !! Distances in Bohr. !! !!### Routines called internally !! * cellxc(...)    : Finds total exch-corr energy and potential !! * [[cross(proc)]]   : Finds the cross product of two vectors !! * dfscf(...)     : Finds SCF contribution to atomic forces !! * dipole(...)    : Finds electric dipole moment !! * doping(...)    : Adds a background charge for doped systems !! * write_rho(...)     : Saves electron density on a file !! * poison(...)    : Solves Poisson equation !! * reord(...)     : Reorders electron density and potential arrays !! * rhooda(...)    : Finds Harris electron density in the mesh !! * rhoofd(...)    : Finds SCF electron density in the mesh !! * rhoofdsp(...)  : Finds SCF electron density in the mesh for !! *                  spiral arrangement of spins !! * timer(...)     : Finds CPU times !! * vmat(...)      : Finds matrix elements of SCF potential !! * vmatsp(...)    : Finds matrix elements of SCF potential for !!                    spiral arrangement of spins !! * delk(...)      : Finds matrix elements of  exp(i \\vec{k} \\cdot \\vec{r})  !! * real*8 volcel( cell ) : Returns volume of unit cell !! !!### Internal variables and arrays !! * `real*8  bcell(3,3)`    : Bulk lattice vectors !! * `real*8  cell(3,3)`     : Auxiliary lattice vectors (same as ucell) !! * `real*8  const`         : Auxiliary variable (constant within a loop) !! * `real*8  DEc`           : Auxiliary variable to call cellxc !! * `real*8  DEx`           : Auxiliary variable to call cellxc !! * `real*8  dvol`          : Mesh-cell volume !! * `real*8  Ec`            : Correlation energy !! * `real*8  Ex`            : Exchange energy !! * `real*8  field(3)`      : External electric field !! * `integer i`             : General-purpose index !! * `integer ia`            : Atom index !! * `integer io`            : Orbital index !! * `integer ip`            : Point index !! * `integer is`            : Species index !! * `logical IsDiag`        : Is supercell diagonal? !! * `integer ispin`         : Spin index !! * `integer j`             : General-purpose index #ifndef BSC_CELLXC !! * `integer JDGdistr`      : J.D.Gale's parallel distribution of mesh points !! * `integer myBox(2,3)`    : My processor's mesh box #endif /* ! BSC_CELLXC */ !! * `integer nbcell`        : Number of independent bulk lattice vectors !! * `integer npcc`          : Partial core corrections? (0=no, 1=yes) !! * `integer nsd`           : Number of diagonal spin values (1 or 2) !! * `integer ntpl`          : Number of mesh Total Points in unit cell !!                           (including subpoints) locally !! * `real*4  rhoatm(ntpl)`  : Harris electron density !! * `real*4  rhopcc(ntpl)`  : Partial-core-correction density for xc !! * `real*4  DRho(ntpl)`    : Selfconsistent electron density difference !! * `real*8  rhotot`        : Total density at one point !! * `real*8  rmax`          : Maximum orbital radius !! * `real*8  scell(3,3)`    : Supercell vectors !! * `character shape*10`    : Name of system shape !! * `real*4  Vaux(ntpl)`    : Auxiliary potential array !! * `real*4  Vna(ntpl)`     : Sum of neutral-atom potentials !! * `real*8  volume`        : Unit cell volume !! * `real*4  Vscf(ntpl)`    : Hartree potential of selfconsistent density !! * `real*8  x0(3)`         : Center of molecule !! * `logical harrisfun`     : Harris functional or Kohn-Sham? use precision , only : dp , grid_p #ifndef BSC_CELLXC use parallel , only : ProcessorY #endif /* ! BSC_CELLXC */ !     Number of Mesh divisions of each cell vector (global) !     The status of this variable is confusing use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use units , only : Debye , eV , Ang use fdf use sys , only : die , bye use mesh , only : nsm , nsp use parsing use m_iorho , only : write_rho use m_forhar , only : forhar use alloc , only : re_alloc , de_alloc use files , only : slabel use files , only : filesOut_t ! derived type for output file names use siesta_options , only : harrisfun , save_initial_charge_density use siesta_options , only : analyze_charge_density_only use meshsubs , only : LocalChargeOnMesh use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use m_partial_charges , only : compute_partial_charges use m_partial_charges , only : want_partial_charges #ifndef BSC_CELLXC use siestaXC , only : cellXC ! Finds xc energy and potential use siestaXC , only : myMeshBox ! Returns my processor mesh box use siestaXC , only : jms_setMeshDistr => setMeshDistr ! Sets a distribution of mesh ! points over parallel processors #endif /* BSC_CELLXC */ use m_vmat , only : vmat use m_rhoofd , only : rhoofd #ifdef MPI use mpi_siesta #endif use iogrid_netcdf , only : write_grid_netcdf use iogrid_netcdf , only : read_grid_netcdf use siesta_options , only : read_charge_cdf use siesta_options , only : savebader use siesta_options , only : read_deformation_charge_cdf use siesta_options , only : mix_charge use m_efield , only : get_field_from_dipole , dipole_correction use m_efield , only : add_potential_from_field use m_efield , only : user_specified_field , acting_efield use m_doping_uniform , only : doping_active , doping_uniform use m_charge_add , only : charge_add use m_hartree_add , only : hartree_add #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif use m_rhofft , only : rhofft , FORWARD , BACKWARD use m_rhog , only : rhog_in , rhog use m_spin , only : spin use m_spin , only : Spiral , qSpiral use m_iotddft , only : write_tdrho use m_ts_global_vars , only : TSmode , TSrun use m_ts_options , only : IsVolt , Elecs , N_elec use m_ts_voltage , only : ts_voltage use m_ts_hartree , only : ts_hartree_fix implicit none integer , intent ( in ) :: nspin !! Number of different spin polarisations: !! nspin=1 => Unpolarized, nspin=2 => polarized !! nspin=4 => Noncollinear spin or spin-orbit. integer , intent ( in ) :: norb !! Total number of basis orbitals in supercell integer , intent ( in ) :: iaorb ( norb ) !! Atom to which each orbital belongs integer , intent ( in ) :: iphorb ( norb ) !! Orbital index (within atom) of each orbital integer , intent ( in ) :: nuo !! Number of orbitals in a unit cell in this node integer , intent ( in ) :: nuotot !! Number of orbitals in a unit cell integer , intent ( in ) :: nua !! Number of atoms in unit cell integer , intent ( in ) :: na !! Number of atoms in supercell integer , intent ( in ) :: isa ( na ) !! Species index of all atoms in supercell integer , intent ( in ) :: indxua ( na ) !! Index of equivalent atom in unit cell integer , intent ( in ) :: ifa !! Switch which fixes whether the SCF contrib: !! to atomic forces is calculated and added to fa. integer , intent ( in ) :: istr !! Switch which fixes whether the SCF contrib: !! to stress is calculated and added to stress. integer , intent ( in ) :: iHmat !! Switch which fixes whether the Hmat matrix !! elements are calculated or not. integer , intent ( in ) :: maxnd !! First dimension of listd and Dscf integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero density-matrix !! elements for each matrix row integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of rows of density-matrix integer , intent ( in ) :: listd ( * ) !! `listd(maxnd)`: Nonzero-density-matrix-element column !! indexes for each matrix row integer , intent ( in ) :: maxnh !! First dimension of listh and Hmat real ( dp ), intent ( in ) :: xa ( 3 , na ) !! Atomic positions of all atoms in supercell real ( dp ), intent ( in ) :: Dscf (:,:) !! `Dscf(maxnd,h_spin_dim)`: !! SCF density-matrix elements real ( dp ), intent ( in ) :: datm ( norb ) !! Harris density-matrix diagonal elements !! (atomic occupation charges of orbitals) real ( dp ), intent ( in ) :: Hmat (:,:) !! `Hmat(maxnh,h_spin_dim)`: !! Hamiltonian matrix in sparse form, !! to which are added the matrix elements !! `<ORB_I | DeltaV | ORB_J>`, where !! `DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris)` type ( filesOut_t ), intent ( inout ) :: filesOut !! Output file names (If blank => not saved) integer , intent ( inout ) :: ntm ( 3 ) !! Number of mesh divisions of each cell !! vector, including subgrid. real ( dp ), intent ( inout ) :: Fal ( 3 , nua ) !! Atomic forces, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative !! of `(Enascf - Enaatm + DUscf + Exc)` with !! respect to atomic positions, in Ry/Bohr. !! Contributions local to this node. real ( dp ), intent ( inout ) :: stressl ( 3 , 3 ) !! Stress tensor, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative of !! `(Enascf - Enaatm + DUscf + Exc) / volume` !! with respect to the strain tensor, in Ry. !! Contributions local to this node. real ( dp ) :: stress ( 3 , 3 ) real ( dp ), intent ( out ) :: Enaatm !! Integral of `Vna * rhoatm` real ( dp ), intent ( out ) :: Enascf !! Integral of `Vna * rhoscf` real ( dp ), intent ( out ) :: Uatm !! Harris hartree electron-interaction energy real ( dp ), intent ( out ) :: Uscf !! SCF hartree electron-interaction energy real ( dp ), intent ( out ) :: DUscf !! Electrostatic (Hartree) energy of !! `(rhoscf - rhoatm)` density real ( dp ), intent ( out ) :: DUext !! Interaction energy with external electric field real ( dp ), intent ( out ) :: Exc !! SCF exchange-correlation energy real ( dp ), intent ( out ) :: Dxc !! SCF double-counting correction to Exc !! `Dxc = integral of ( (epsxc - Vxc) * Rho )` !! All energies in Rydbergs real ( dp ), intent ( out ) :: dipol ( 3 ) !! Electric dipole (in a.u.) !! only when the system is a molecule logical , intent ( in ), optional :: use_rhog_in logical , intent ( in ), optional :: charge_density_only !     Local variables integer :: i , ia , ip , ispin , nsd , np_vac #ifndef BSC_CELLXC !     Interface to JMS's SiestaXC integer :: myBox ( 2 , 3 ) integer , save :: JDGdistr =- 1 real ( dp ) :: stressXC ( 3 , 3 ) #endif /* ! BSC_CELLXC */ real ( dp ) :: b1Xb2 ( 3 ), const , DEc , DEx , DStres ( 3 , 3 ), & Ec , Ex , rhotot , x0 ( 3 ), volume , Vmax_vac , Vmean_vac #ifdef BSC_CELLXC !     Dummy arrays for cellxc call real ( grid_p ) :: aux3 ( 3 , 1 ) real ( grid_p ) :: dummy_DVxcdn ( 1 , 1 , 1 ) #endif /* BSC_CELLXC */ logical :: use_rhog real ( dp ), external :: volcel , ddot external & cross , & dipole , & poison , & reord , rhooda , rhoofdsp , & timer , vmatsp , & readsp #ifdef BSC_CELLXC external bsc_cellxc #endif /* BSC_CELLXC */ !     Work arrays real ( grid_p ), pointer :: Vscf (:,:), Vscf_par (:,:), & DRho (:,:), DRho_par (:,:), & Vaux (:), Vaux_par (:), Chlocal (:), & Totchar (:), fsrc (:), fdst (:), & rhoatm_quad (:) => null (), & DRho_quad (:,:) => null () ! Temporary reciprocal spin quantity real ( grid_p ) :: rnsd #ifdef BSC_CELLXC real ( grid_p ), pointer :: Vscf_gga (:,:), DRho_gga (:,:) #endif /* BSC_CELLXC */ #ifdef MPI integer :: MPIerror real ( dp ) :: sbuffer ( 7 ), rbuffer ( 7 ) #endif #ifdef DEBUG call write_debug ( '    PRE DHSCF' ) #endif if ( spin % H /= size ( Dscf , dim = 2 ) ) then call die ( 'Spin components is not equal to options.' ) end if if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DM' , & ( sqrt ( sum ( Dscf (:, ispin ) ** 2 )), ispin = 1 , spin % H ) write ( * , debug_fmt ) Node , 'H' , & ( sqrt ( sum ( Hmat (:, ispin ) ** 2 )), ispin = 1 , spin % H ) end if !-------------------------------------------------------------------- BEGIN ! ---------------------------------------------------------------------- ! Start of SCF iteration part ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     At the end of DHSCF_INIT, and also at the end of any previous !     call to dhscf, we were in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna were in UNIFORM dist, sequential form !     The index array endpht was in the QUADRATIC distribution ! ---------------------------------------------------------------------- #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_restart ( ) #endif call timer ( 'DHSCF' , 1 ) call timer ( 'DHSCF3' , 1 ) nullify ( Vscf , Vscf_par , DRho , DRho_par , & Vaux , Vaux_par , Chlocal , Totchar ) #ifdef BSC_CELLXC nullify ( Vscf_gga , DRho_gga ) #endif /* BSC_CELLXC */ volume = volcel ( cell ) !------------------------------------------------------------------------- if ( analyze_charge_density_only ) then !! Use the functionality in the first block !! of the routine to get charge files and partial charges call setup_analysis_options () endif if ( filesOut % vna . ne . ' ' ) then ! Uniform dist, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vna' , 1 , ntml , Vna ) else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) end if #else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) #endif endif !     Allocate memory for DRho using the UNIFORM data distribution call re_alloc ( DRho , 1 , ntpl , 1 , nspin , 'DRho' , 'dhscf' ) ! Find number of diagonal spin values nsd = min ( nspin , 2 ) if ( nsd == 1 ) then rnsd = 1._grid_p else rnsd = 1._grid_p / nsd end if ! ---------------------------------------------------------------------- ! Find SCF electron density at mesh points. Store it in array DRho ! ---------------------------------------------------------------------- ! !     The reading routine works in the uniform distribution, in !     sequential form ! if ( present ( use_rhog_in )) then use_rhog = use_rhog_in else use_rhog = . false . endif if ( use_rhog ) then ! fourier transform back into drho call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog_in , BACKWARD ) else if ( read_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"Rho\" ) read_charge_cdf = . false . else if ( read_deformation_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"DeltaRho\" ) ! Add to diagonal components only do ispin = 1 , nsd do ip = 1 , ntpl !             rhoatm and Drho are in sequential mode DRho ( ip , ispin ) = DRho ( ip , ispin ) + rhoatm ( ip ) * rnsd enddo enddo read_deformation_charge_cdf = . false . else ! Set the QUADRATIC distribution and allocate memory for DRho_par ! since the construction of the density from the DM and orbital ! data needs that distribution if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_par , 1 , ntpl , 1 , nspin , & 'DRho_par' , 'dhscf' ) if ( Spiral ) then call rhoofdsp ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , nuo , nuotot , iaorb , & iphorb , isa , qspiral ) else call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , & nuo , nuotot , iaorb , iphorb , isa ) endif ! DRHO_par is here in QUADRATIC, clustered form !       Set the UNIFORM distribution again and copy DRho to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => DRho_par (:, ispin ) fdst => DRho (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( DRho_par , 'DRho_par' , 'dhscf' ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DRho' , & ( sqrt ( sum ( DRho (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if if ( save_initial_charge_density ) then ! This section is to be deprecated in favor ! of \"analyze_charge_density_only\" ! (except for the special name for the .nc file) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoInit' , nspin , & ntml , DRho ) else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) end if #else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) #endif call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after producing RHO_INIT from input DM\" ) endif endif if ( mix_charge ) then ! Save fourier transform of charge density call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog , FORWARD ) endif ! !     Proper place to integrate Hirshfeld and Voronoi code, !     since we have just computed rhoatm and Rho. if ( want_partial_charges ) then ! The endpht array is in the quadratic distribution, so ! we need to use it for this... if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_quad , 1 , ntpl , 1 , nspin , & 'DRho_quad' , 'dhscf' ) call re_alloc ( rhoatm_quad , 1 , ntpl , & 'rhoatm_quad' , 'dhscf' ) ! Redistribute grid-density do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_quad (:, ispin ) ! if nodes==1, this call will just reorder call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo call distMeshData ( UNIFORM , rhoatm , & QUADRATIC , rhoatm_quad , TO_CLUSTER ) call compute_partial_charges ( DRho_quad , rhoatm_quad , . nspin , iaorb , iphorb , . isa , nmpl , dvol ) call de_alloc ( rhoatm_quad , 'rhoatm_quad' , 'dhscf' ) call de_alloc ( Drho_quad , 'DRho_quad' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif ! ---------------------------------------------------------------------- ! Save electron density ! ---------------------------------------------------------------------- if ( filesOut % rho . ne . ' ' ) then !  DRho is already using a uniform, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Rho' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) end if #else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) #endif endif !----------------------------------------------------------------------- ! Save TD-electron density after every given number of steps- Rafi, Jan 2016 !----------------------------------------------------------------------- call write_tdrho ( filesOut ) if ( filesOut % tdrho . ne . ' ' ) then !  DRho is already using a uniform, sequential form call write_rho ( filesOut % tdrho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"TDRho\" ) endif ! ---------------------------------------------------------------------- ! Save the diffuse ionic charge and/or the total (ionic+electronic) charge ! ---------------------------------------------------------------------- if ( filesOut % psch . ne . ' ' . or . filesOut % toch . ne . ' ' ) then !       Find diffuse ionic charge on mesh ! Note that the *OnMesh routines, except PhiOnMesh, ! work with any distribution, thanks to the fact that ! the ipa, idop, and indexp arrays are distro-specific call re_alloc ( Chlocal , 1 , ntpl , 'Chlocal' , 'dhscf' ) call LocalChargeOnMesh ( na , isa , ntpl , Chlocal , indxua ) ! Chlocal comes out in clustered form, so we convert it call reord ( Chlocal , Chlocal , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Chlocal' , sqrt ( sum ( Chlocal ** 2 )) end if !       Save diffuse ionic charge if ( filesOut % psch . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Chlocal' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & 'Chlocal' ) end if #else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , 'Chlocal' ) #endif endif !       Save total (ionic+electronic) charge if ( filesOut % toch . ne . ' ' ) then ! ***************** ! **  IMPORTANT  ** ! The Chlocal array is re-used to minimize memory ! usage. In the this small snippet the Chlocal ! array will contain the total charge, and ! if the logic should change, (i.e. should Chlocal ! be retained) is the Totchar needed to be re-instantiated. ! ***************** !$OMP parallel default(shared), private(ispin,ip) do ispin = 1 , nsd !$OMP do do ip = 1 , ntpl Chlocal ( ip ) = Chlocal ( ip ) + DRho ( ip , ispin ) end do !$OMP end do end do !$OMP end parallel ! See note above #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoTot' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & \"TotalCharge\" ) end if #else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , \"TotalCharge\" ) #endif end if call de_alloc ( Chlocal , 'Chlocal' , 'dhscf' ) endif ! ---------------------------------------------------------------------- ! Save the total charge (model core + valence) for Bader analysis ! ---------------------------------------------------------------------- ! The test for toch guarantees that we are in \"analysis mode\" if ( filesOut % toch . ne . ' ' . and . savebader ) then call save_bader_charge () endif ! Find difference between selfconsistent and atomic densities !Both DRho and rhoatm are using a UNIFORM, sequential form !$OMP parallel do default(shared), private(ispin,ip), !$OMP&collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do ! ---------------------------------------------------------------------- ! Save electron density difference ! ---------------------------------------------------------------------- if ( filesOut % drho . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoDelta' , nspin , ntml , & DRho ) else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"DeltaRho\" ) end if #else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & DRho , \"DeltaRho\" ) #endif endif if ( present ( charge_density_only )) then if ( charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) RETURN endif endif ! End of analysis section ! Can exit now, if requested if ( analyze_charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after analyzing charge from input DM\" ) endif !------------------------------------------------------------- !     Transform spin density into sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif ! Add a background charge to neutralize the net charge, to ! model doped systems. It only adds the charge at points ! where there are atoms (i.e., not in vacuum). ! First, call with 'task=0' to add background charge if ( doping_active ) call doping_uniform ( cell , ntpl , 0 , $ DRho (:, 1 ), rhoatm ) ! Add doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '+' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Calculate the dipole moment ! ---------------------------------------------------------------------- dipol ( 1 : 3 ) = 0.0_dp if ( shape . ne . 'bulk' ) then ! Find center of system x0 ( 1 : 3 ) = 0.0_dp do ia = 1 , nua x0 ( 1 : 3 ) = x0 ( 1 : 3 ) + xa ( 1 : 3 , ia ) / nua enddo ! Find dipole ! This routine is distribution-blind ! and will reduce over all processors. call dipole ( cell , ntm , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), nsm , & DRho , x0 , dipol ) ! Orthogonalize dipole to bulk directions if ( shape . eq . 'chain' ) then const = ddot ( 3 , dipol , 1 , bcell , 1 ) / ddot ( 3 , bcell , 1 , bcell , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * bcell ( 1 : 3 , 1 ) else if ( shape . eq . 'slab' ) then call cross ( bcell ( 1 , 1 ), bcell ( 1 , 2 ), b1Xb2 ) const = ddot ( 3 , dipol , 1 , b1Xb2 , 1 ) / ddot ( 3 , b1Xb2 , 1 , b1Xb2 , 1 ) dipol ( 1 : 3 ) = const * b1Xb2 ( 1 : 3 ) end if if ( TSmode ) then if ( N_elec > 1 ) then ! Orthogonalize dipole to electrode transport directions do ia = 1 , N_Elec x0 = Elecs ( ia )% cell (:, Elecs ( ia )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * x0 end do else if ( ( shape == 'molecule' ) . or . ( shape == 'chain' ) ) then ! Only allow dipole correction for chains and molecules ! along the semi-infinite direciton. ! Note this is *only* for 1-electrode setups ! Note that since the above removes the periodic directions ! this should not do anything for 'chain' with the same semi-infinite ! direction x0 = Elecs ( 1 )% cell (:, Elecs ( 1 )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = const * x0 end if end if endif ! ---------------------------------------------------------------------- !     Find Hartree potential of DRho = rhoscf-rhoatm. Store it in Vaux ! ---------------------------------------------------------------------- !     Solve Poisson's equation call re_alloc ( Vaux , 1 , ntpl , 'Vaux' , 'dhscf' ) call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , DRho , & DUscf , Vaux , DStres , nsm ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Poisson' , sqrt ( sum ( Vaux (:) ** 2 )) end if ! Vscf is in the UNIFORM, sequential form, and only using ! the first spin index ! We require that even the SIESTA potential is \"fixed\" ! NOTE, this will only do something if !   TS.Hartree.Fix is set call ts_hartree_fix ( ntm , ntml , Vaux ) ! Add contribution to stress from electrostatic energy of rhoscf-rhoatm if ( istr . eq . 1 ) then stressl ( 1 : 3 , 1 : 3 ) = stressl ( 1 : 3 , 1 : 3 ) + DStres ( 1 : 3 , 1 : 3 ) endif ! ---------------------------------------------------------------------- !     Find electrostatic (Hartree) energy of full SCF electron density !     using the original data distribution ! ---------------------------------------------------------------------- Uatm = Uharrs Uscf = 0._dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Uscf) do ip = 1 , ntpl Uscf = Uscf + Vaux ( ip ) * rhoatm ( ip ) enddo !$OMP end parallel do Uscf = Uscf * dVol + Uatm + DUscf ! Call doping with 'task=1' to remove background charge added previously ! The extra charge thus only affects the Hartree energy and potential, ! but not the contribution to Enascf ( = \\Int_{Vna*\\rho}) if ( doping_active ) call doping_uniform ( cell , ntpl , 1 , $ DRho (:, 1 ), rhoatm ) ! Remove doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '-' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Add neutral-atom potential to Vaux ! ---------------------------------------------------------------------- Enaatm = 0.0_dp Enascf = 0.0_dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Enaatm,Enascf) do ip = 1 , ntpl Enaatm = Enaatm + Vna ( ip ) * rhoatm ( ip ) Enascf = Enascf + Vna ( ip ) * DRho ( ip , 1 ) Vaux ( ip ) = Vaux ( ip ) + Vna ( ip ) enddo !$OMP end parallel do Enaatm = Enaatm * dVol Enascf = Enaatm + Enascf * dVol ! ---------------------------------------------------------------------- ! Add potential from external electric field (if present) ! ---------------------------------------------------------------------- if ( acting_efield ) then if ( dipole_correction ) then field = get_field_from_dipole ( dipol , cell ) if ( Node == 0 ) then write ( 6 , '(a,3f12.4,a)' ) $ 'Dipole moment in unit cell   =' , dipol / Debye , ' D' write ( 6 , '(a,3f12.6,a)' ) $ 'Electric field for dipole correction =' , $ field / eV * Ang , ' eV/Ang/e' end if ! The dipole correction energy has an extra factor ! of one half because the field involved is internal. ! See the paper by Bengtsson DOI:10.1103/PhysRevB.59.12301 ! Hence we compute this part separately DUext = - 0.5_dp * ddot ( 3 , field , 1 , dipol , 1 ) else field = 0._dp DUext = 0._dp end if ! Add the external electric field field = field + user_specified_field ! This routine expects a sequential array, ! but it is distribution-blind call add_potential_from_field ( field , cell , nua , isa , xa , & ntm , nsm , Vaux ) ! Add energy of external electric field DUext = DUext - ddot ( 3 , user_specified_field , 1 , dipol , 1 ) endif ! --------------------------------------------------------------------- !     Transiesta: !     add the potential corresponding to the (possible) voltage-drop. !     note that ts_voltage is not sharing the reord wih efield since !     we should not encounter both at the same time. ! --------------------------------------------------------------------- if ( TSmode . and . IsVolt . and . TSrun ) then ! This routine expects a sequential array, ! in whatever distribution #ifdef TRANSIESTA_VOLTAGE_DEBUG !$OMP parallel workshare default(shared) Vaux (:) = 0._dp !$OMP end parallel workshare #endif call ts_voltage ( cell , ntm , ntml , Vaux ) #ifdef TRANSIESTA_VOLTAGE_DEBUG call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"TransiestaHartreePotential\" ) call timer ( 'ts_volt' , 3 ) call bye ( 'transiesta debug for Hartree potential' ) #endif endif ! ---------------------------------------------------------------------- ! Add potential from user defined geometries (if present) ! ---------------------------------------------------------------------- call hartree_add ( cell , ntpl , Vaux ) ! ---------------------------------------------------------------------- !     Save electrostatic potential ! ---------------------------------------------------------------------- if ( filesOut % vh . ne . ' ' ) then ! Note that only the first spin component is used #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vh' , 1 , ntml , & Vaux ) else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"ElectrostaticPotential\" ) end if #else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Vaux , \"ElectrostaticPotential\" ) #endif endif !     Get back spin density from sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(ip,rhotot) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) DRho ( ip , 1 ) = 0.5_dp * ( rhotot - DRho ( ip , 2 )) DRho ( ip , 2 ) = 0.5_dp * ( rhotot + DRho ( ip , 2 )) enddo !$OMP end parallel do endif ! ---------------------------------------------------------------------- #ifndef BSC_CELLXC ! Set uniform distribution of mesh points and find my processor mesh box ! This is the interface to JM Soler's own cellxc routine, which sets ! up the right distribution internally. ! ---------------------------------------------------------------------- call jms_setMeshDistr ( distrID = JDGdistr , nMesh = ntm , . nNodesX = 1 , nNodesY = ProcessorY , nBlock = nsm ) call myMeshBox ( ntm , JDGdistr , myBox ) ! ---------------------------------------------------------------------- #endif /* ! BSC_CELLXC */ ! Exchange-correlation energy ! ---------------------------------------------------------------------- call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf' ) if ( npcc . eq . 1 ) then !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & ( rhopcc ( ip ) + rhoatm ( ip )) * rnsd enddo enddo !$OMP end parallel do else !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do end if ! Write the electron density used by cellxc if ( filesOut % rhoxc . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoXC' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) end if #else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) #endif endif !     Everything now is in UNIFORM, sequential form call timer ( \"CellXC\" , 1 ) #ifdef BSC_CELLXC if ( nodes . gt . 1 ) then call setMeshDistr ( LINEAR , nsm , nsp , nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_gga , 1 , ntpl , 1 , nspin , 'Vscf_gga' , 'dhscf' ) call re_alloc ( DRho_gga , 1 , ntpl , 1 , nspin , 'DRho_gga' , 'dhscf' ) ! Redistribute all spin densities do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_gga (:, ispin ) call distMeshData ( UNIFORM , fsrc , LINEAR , fdst , KEEP ) enddo call bsc_cellxc ( 0 , 0 , cell , ntml , ntml , ntpl , 0 , aux3 , nspin , & DRho_gga , Ex , Ec , DEx , DEc , Vscf_gga , & dummy_DVxcdn , stressl ) #endif /* BSC_CELLXC */ #ifndef BSC_CELLXC call cellXC ( 0 , cell , ntm , myBox ( 1 , 1 ), myBox ( 2 , 1 ), . myBox ( 1 , 2 ), myBox ( 2 , 2 ), . myBox ( 1 , 3 ), myBox ( 2 , 3 ), nspin , . DRho , Ex , Ec , DEx , DEc , stressXC , Vscf ) #else /* BSC_CELLXC */ if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif ! Redistribute to the Vxc array do ispin = 1 , nspin fsrc => Vscf_gga (:, ispin ) fdst => Vscf (:, ispin ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) enddo #endif /* BSC_CELLXC */ if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'XC' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if #ifndef BSC_CELLXC !     Vscf is still sequential after the call to JMS's cellxc #else /* BSC_CELLXC */ call de_alloc ( DRho_gga , 'DRho_gga' , 'dhscf' ) call de_alloc ( Vscf_gga , 'Vscf_gga' , 'dhscf' ) #endif /* BSC_CELLXC */ Exc = Ex + Ec Dxc = DEx + DEc call timer ( \"CellXC\" , 2 ) !     Vscf contains only Vxc, and is UNIFORM and sequential !     Now we add up the other contributions to it, at !     the same time that we get DRho back to true DeltaRho form !$OMP parallel default(shared), private(ip,ispin) ! Hartree potential only has diagonal components do ispin = 1 , nsd if ( npcc . eq . 1 ) then !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - & ( rhoatm ( ip ) + rhopcc ( ip )) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do else !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do endif enddo !$OMP end parallel #ifndef BSC_CELLXC stress = stress + stressXC #endif /* ! BSC_CELLXC */ ! ---------------------------------------------------------------------- !     Save total potential ! ---------------------------------------------------------------------- if ( filesOut % vt . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vt' , nspin , ntml , & Vscf ) else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , & Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Vscf , & \"TotalPotential\" ) end if #else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Vscf , \"TotalPotential\" ) #endif endif ! ---------------------------------------------------------------------- ! Print vacuum level ! ---------------------------------------------------------------------- if ( filesOut % vt /= ' ' . or . filesOut % vh /= ' ' ) then forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) + rhoatm (:) * rnsd call vacuum_level ( ntpl , nspin , DRho , Vscf , . np_vac , Vmax_vac , Vmean_vac ) forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) - rhoatm (:) * rnsd if ( np_vac > 0 . and . Node == 0 ) print '(/,a,2f12.6,a)' , . 'dhscf: Vacuum level (max, mean) =' , . Vmax_vac / eV , Vmean_vac / eV , ' eV' endif if ( filesOut % ebs_dens /= '' ) then call save_ebs_density () endif ! ---------------------------------------------------------------------- !     Find SCF contribution to hamiltonian matrix elements ! ---------------------------------------------------------------------- if ( iHmat . eq . 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !        This is a work array, to which we copy Vscf call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo if ( Spiral ) then call vmatsp ( norb , nmpl , dvol , nspin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa , qspiral ) else call vmat ( norb , nmpl , dvol , spin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa ) endif call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then !          Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif #ifdef MPI !     Global reduction of Uscf/DUscf/Uatm/Enaatm/Enascf #ifndef BSC_CELLXC !     Note that Exc and Dxc are already reduced in the new cellxc #endif /* ! BSC_CELLXC */ sbuffer ( 1 ) = Uscf sbuffer ( 2 ) = DUscf sbuffer ( 3 ) = Uatm sbuffer ( 4 ) = Enaatm sbuffer ( 5 ) = Enascf #ifdef BSC_CELLXC sbuffer ( 6 ) = Exc sbuffer ( 7 ) = Dxc #else sbuffer ( 6 : 7 ) = 0._dp #endif /* BSC_CELLXC */ call MPI_AllReduce ( sbuffer , rbuffer , 7 , MPI_double_precision , & MPI_Sum , MPI_Comm_World , MPIerror ) Uscf = rbuffer ( 1 ) DUscf = rbuffer ( 2 ) Uatm = rbuffer ( 3 ) Enaatm = rbuffer ( 4 ) Enascf = rbuffer ( 5 ) #ifdef BSC_CELLXC Exc = rbuffer ( 6 ) Dxc = rbuffer ( 7 ) #endif /* BSC_CELLXC */ #endif /* MPI */ !     Add contribution to stress from the derivative of the Jacobian of --- !     r->r' (strained r) in the integral of Vna*(rhoscf-rhoatm) if ( istr . eq . 1 ) then do i = 1 , 3 stress ( i , i ) = stress ( i , i ) + ( Enascf - Enaatm ) / volume enddo endif !     Stop time counter for SCF iteration part call timer ( 'DHSCF3' , 2 ) ! ---------------------------------------------------------------------- !     End of SCF iteration part ! ---------------------------------------------------------------------- if ( ifa . eq . 1 . or . istr . eq . 1 ) then ! ---------------------------------------------------------------------- ! Forces and stress : SCF contribution ! ---------------------------------------------------------------------- !       Start time counter for force calculation part call timer ( 'DHSCF4' , 1 ) !       Find contribution of partial-core-correction if ( npcc . eq . 1 ) then call reord ( rhopcc , rhopcc , nml , nsm , TO_CLUSTER ) call reord ( Vaux , Vaux , nml , nsm , TO_CLUSTER ) ! The partial core calculation only acts on ! the diagonal spin-components (no need to ! redistribute un-used elements) do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_CLUSTER ) enddo call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , nsd , & dvol , volume , Vscf , Vaux , Fal , & stressl , ifa . ne . 0 , istr . ne . 0 ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) call reord ( Vaux , Vaux , nml , nsm , TO_SEQUENTIAL ) ! ** see above do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_SEQUENTIAL ) enddo if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'PartialCore' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nsd ) end if endif if ( harrisfun ) then !         Forhar deals internally with its own needs !         for distribution changes #ifndef BSC_CELLXC call forhar ( ntpl , nspin , nml , ntml , ntm , npcc , cell , #else /* BSC_CELLXC */ call forhar ( ntpl , nspin , nml , ntml , npcc , cell , #endif /* BSC_CELLXC */ & rhoatm , rhopcc , Vna , DRho , Vscf , Vaux ) !         Upon return, everything is UNIFORM, sequential form endif !     Transform spin density into sum and difference ! TODO NC/SO ! Should we perform local diagonalization? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif !       Find contribution of neutral-atom potential call reord ( Vna , Vna , nml , nsm , TO_CLUSTER ) call reord ( DRho , DRho , nml , nsm , TO_CLUSTER ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , DRho , Fal , stressl , & ifa . ne . 0 , istr . ne . 0 ) call reord ( DRho , DRho , nml , nsm , TO_SEQUENTIAL ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo !       Remember that Vaux contains everything except Vxc call re_alloc ( Vaux_par , 1 , ntpl , 'Vaux_par' , 'dhscf' ) call distMeshData ( UNIFORM , Vaux , & QUADRATIC , Vaux_par , TO_CLUSTER ) call dfscf ( ifa , istr , na , norb , nuo , nuotot , nmpl , nspin , & indxua , isa , iaorb , iphorb , & maxnd , numd , listdptr , listd , Dscf , datm , & Vscf_par , Vaux_par , dvol , volume , Fal , stressl ) call de_alloc ( Vaux_par , 'Vaux_par' , 'dhscf' ) call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif !       Stop time counter for force calculation part call timer ( 'DHSCF4' , 2 ) ! ---------------------------------------------------------------------- !       End of force and stress calculation ! ---------------------------------------------------------------------- endif !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution !     Stop time counter call timer ( 'DHSCF' , 2 ) ! ---------------------------------------------------------------------- !     Free locally allocated memory ! ---------------------------------------------------------------------- call de_alloc ( Vaux , 'Vaux' , 'dhscf' ) call de_alloc ( Vscf , 'Vscf' , 'dhscf' ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) #ifdef DEBUG call write_debug ( '    POS DHSCF' ) #endif !------------------------------------------------------------------------ END CONTAINS subroutine save_bader_charge () use meshsubs , only : ModelCoreChargeOnMesh #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif ! Auxiliary routine to output the Bader Charge ! real ( grid_p ), pointer :: BaderCharge (:) => null () call re_alloc ( BaderCharge , 1 , ntpl , name = 'BaderCharge' , & routine = 'dhscf' ) ! Find a model core charge by re-scaling the local charge call ModelCoreChargeOnMesh ( na , isa , ntpl , BaderCharge , indxua ) ! It comes out in clustered form, so we convert it call reord ( BaderCharge , BaderCharge , nml , nsm , TO_SEQUENTIAL ) do ispin = 1 , nsd BaderCharge ( 1 : ntpl ) = BaderCharge ( 1 : ntpl ) + DRho ( 1 : ntpl , ispin ) enddo #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoBader' , 1 , ntml , & BaderCharge ) else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) end if #else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) #endif call de_alloc ( BaderCharge , name = 'BaderCharge' ) end subroutine save_bader_charge subroutine setup_analysis_options () !! For the analyze-charge-density-only case, !! avoiding any diagonalization use siesta_options , only : hirshpop , voropop use siesta_options , only : saverho , savedrho , saverhoxc use siesta_options , only : savevh , savevt , savevna use siesta_options , only : savepsch , savetoch want_partial_charges = ( hirshpop . or . voropop ) if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' end subroutine setup_analysis_options subroutine save_ebs_density () !! Optional output of the \"band-structure energy density\", which !! is just the charge density weighted by the eigenvalues, i.e., !! using EDM instead of DM in rhoofd use sparse_matrices , only : Escf real ( grid_p ), pointer :: Ebs_dens (:,:) => null (), & Ebs_dens_quad (:,:) => null () !     Allocate memory for Ebs_dens using the UNIFORM data distribution call re_alloc ( Ebs_dens , 1 , ntpl , 1 , nspin , 'Ebs_dens' , 'dhscf' ) !     Switch to quadratic distribution for call to rhoofd if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( ebs_dens_quad , 1 , ntpl , 1 , nspin , & 'Ebs_dens_quad' , 'dhscf' ) call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Escf , Ebs_dens_quad , & nuo , nuotot , iaorb , iphorb , isa ) !     Ebs_dens_par is here in QUADRATIC, clustered form !     Set the UNIFORM distribution again and copy Ebs_dens to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => Ebs_dens_quad (:, ispin ) fdst => Ebs_dens (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( Ebs_dens_quad , 'Ebs_dens_quad' , 'dhscf' ) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Ebs_density' , $ nspin , ntml , Ebs_dens ) else call write_rho ( filesOut % ebs_dens , $ cell , ntm , nsm , ntpl , nspin , Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Ebs_dens , & \"Ebs_density\" ) end if #else call write_rho ( filesOut % ebs_dens , cell , ntm , nsm , ntpl , nspin , $ Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Ebs_dens , \"Ebs_density\" ) #endif call de_alloc ( Ebs_dens , 'Ebs_dens' , 'dhscf' ) end subroutine save_ebs_density end subroutine dhscf subroutine delk_wrapper ( isigneikr , norb , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) !! This is a wrapper to call delk, using some of the module !! variables of m_dhscf, but from outside dhscf itself. !! !! The dhscf module variables used are: !! !! * nmpl !! * dvol !! * nml !! * nmpl !! * ntml !! * ntpl !! !! Some of them might be put somewhere else (mesh?) to allow some !! of the kitchen-sink functionality of dhscf to be made more modular. !! For example, this wrapper might live independently if enough mesh !! information is made available to it. use m_delk , only : delk ! The real workhorse, similar to vmat use moreMeshSubs , only : setMeshDistr use moreMeshSubs , only : UNIFORM , QUADRATIC use parallel , only : Nodes use mesh , only : nsm , nsp integer :: isigneikr , & norb , nuo , nuotot , maxnd , & iaorb ( * ), iphorb ( * ), isa ( * ), & numd ( nuo ), & listdptr ( nuo ), listd ( maxnd ) ! ---------------------------------------------------------------------- ! Calculate matrix elements of exp(i \\vec{k} \\cdot \\vec{r}) ! ---------------------------------------------------------------------- if ( isigneikr . eq . 1 . or . isigneikr . eq . - 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call delk ( isigneikr , norb , nmpl , dvol , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) if ( nodes . gt . 1 ) then !           Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif end subroutine delk_wrapper end module m_dhscf","tags":"","loc":"sourcefile/dhscf.f.html","title":"dhscf.F – SIESTA"},{"text":"Contents Subroutines reord Source Code reord.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! subroutine reord ( fclust , fseq , nm , nsm , itr ) !! author: J.M.Soler !! date: May 1995 !! Re-orders a clustered data array into a sequential one and viceversa use precision , only : grid_p use alloc , only : re_alloc , de_alloc IMPLICIT NONE INTEGER , intent ( in ) :: NM ( 3 ) !! Number of Mesh divisions in each cell vector INTEGER , intent ( in ) :: NSM !! Number of Sub-divisions in each Mesh division INTEGER , intent ( in ) :: ITR !! TRanslation-direction switch !! ITR=+1 => From clustered to sequential !! ITR=-1 => From sequential to clustered REAL ( grid_p ), intent ( inout ) :: FCLUST ( * ) !! CLUSTered data: !!    REAL*4 FCLUST(NSM,NSM,NSM,NM1,NM2,NM3) REAL ( grid_p ), intent ( inout ) :: FSEQ ( * ) !! SEQuential data: !! !!    REAL*4 FSEQ(NSM*NM1,NSM*NM2,NSM*NM3) INTEGER . I , I0 , I1 , I2 , I3 , IS , IS1 , IS2 , IS3 , . J , J0 , NSM3 , NTM ( 3 ), NAUX real ( grid_p ), dimension (:), pointer :: AUX integer , dimension (:), pointer :: JS CALL TIMER ( 'REORD' , 1 ) NTM ( 1 ) = NM ( 1 ) * NSM NTM ( 2 ) = NM ( 2 ) * NSM NTM ( 3 ) = NM ( 3 ) * NSM NSM3 = NSM ** 3 NAUX = NM ( 1 ) * NM ( 2 ) * NSM3 ! !  Allocate local memory ! nullify ( AUX ) call re_alloc ( AUX , 1 , NAUX , 'AUX' , 'reord' ) nullify ( JS ) call re_alloc ( JS , 1 , NSM3 , 'JS' , 'reord' ) IS = 0 DO IS3 = 0 , NSM - 1 DO IS2 = 0 , NSM - 1 DO IS1 = 0 , NSM - 1 IS = IS + 1 JS ( IS ) = 1 + IS1 + NTM ( 1 ) * IS2 + NTM ( 1 ) * NTM ( 2 ) * IS3 ENDDO ENDDO ENDDO IF ( ITR . GT . 0 ) THEN DO I3 = 0 , NM ( 3 ) - 1 DO I2 = 0 , NM ( 2 ) - 1 I0 = NSM3 * ( NM ( 1 ) * I2 + NM ( 1 ) * NM ( 2 ) * I3 ) J0 = NTM ( 1 ) * NSM * I2 DO IS = 1 , NSM3 I = I0 + IS J = J0 + JS ( IS ) DO I1 = 1 , NM ( 1 ) AUX ( J ) = FCLUST ( I ) I = I + NSM3 J = J + NSM ENDDO ENDDO ENDDO I = NM ( 1 ) * NM ( 2 ) * NSM3 * I3 DO J = 1 , NM ( 1 ) * NM ( 2 ) * NSM3 FSEQ ( I + J ) = AUX ( J ) ENDDO ENDDO ELSE DO I3 = 0 , NM ( 3 ) - 1 I = NM ( 1 ) * NM ( 2 ) * NSM3 * I3 DO J = 1 , NM ( 1 ) * NM ( 2 ) * NSM3 AUX ( J ) = FSEQ ( I + J ) ENDDO DO I2 = 0 , NM ( 2 ) - 1 I0 = NSM3 * ( NM ( 1 ) * I2 + NM ( 1 ) * NM ( 2 ) * I3 ) J0 = NTM ( 1 ) * NSM * I2 DO IS = 1 , NSM3 I = I0 + IS J = J0 + JS ( IS ) DO I1 = 1 , NM ( 1 ) FCLUST ( I ) = AUX ( J ) I = I + NSM3 J = J + NSM ENDDO ENDDO ENDDO ENDDO ENDIF ! !  Free local memory ! call de_alloc ( JS , 'JS' , 'reord' ) call de_alloc ( AUX , 'AUX' , 'reord' ) CALL TIMER ( 'REORD' , 2 ) END","tags":"","loc":"sourcefile/reord.f.html","title":"reord.f – SIESTA"},{"text":"type, private :: meshDisType Private type to hold mesh distribution data. Contents Variables nMesh box indexp idop xdop ipa Source Code meshDisType Components Type Visibility Attributes Name Initial integer, public :: nMesh (3) Number of mesh div. in each axis. integer, public, pointer :: box (:,:,:) Mesh box bounds of each node: box(1,iAxis,iNode)=lower bounds box(2,iAxis,iNode)=upper bounds integer, public, pointer :: indexp (:) integer, public, pointer :: idop (:) real(kind=dp), public, pointer :: xdop (:,:) integer, public, pointer :: ipa (:) Source Code TYPE meshDisType !! Private type to hold mesh distribution data. integer :: nMesh ( 3 ) !! Number of mesh div. in each axis. integer , pointer :: box (:,:,:) !! Mesh box bounds of each node: !! box(1,iAxis,iNode)=lower bounds !! box(2,iAxis,iNode)=upper bounds integer , pointer :: indexp (:) integer , pointer :: idop (:) real ( dp ), pointer :: xdop (:,:) integer , pointer :: ipa (:) END TYPE meshDisType","tags":"","loc":"type/meshdistype.html","title":"meshDisType – SIESTA "},{"text":"type, private :: meshCommType Private type to hold communications to move data from one\n distribution to another. Contents Variables ncom src dst Source Code meshCommType Components Type Visibility Attributes Name Initial integer, public :: ncom Number of needed communications integer, public, pointer :: src (:) Sources of communications integer, public, pointer :: dst (:) Destination of communications Source Code TYPE meshCommType !! Private type to hold communications to move data from one !! distribution to another. integer :: ncom !! Number of needed communications integer , pointer :: src (:) !! Sources of communications integer , pointer :: dst (:) !! Destination of communications END TYPE meshCommType","tags":"","loc":"type/meshcommtype.html","title":"meshCommType – SIESTA "},{"text":"type, public :: tMixer Inherits type~~tmixer~~InheritsGraph type~tmixer tMixer type~tmixer->type~tmixer next, next_conv Fstack_dData1D Fstack_dData1D type~tmixer->Fstack_dData1D stack Help × Graph Key Nodes of different colours represent the following: Graph Key Type Type This Page's Entity This Page's Entity Solid arrows point from a derived type to the parent type which it\n    extends. Dashed arrows point from a derived type to the other\n    types it contains as a components, with a label listing the name(s) of\n    said component(s). Contents Variables name stack m v cur_itt start_itt n_hist n_itt restart restart_save action next next_conv w rv iv Source Code tMixer Components Type Visibility Attributes Name Initial character(len=24), public :: name type(Fstack_dData1D), public, allocatable :: stack (:) integer, public :: m = MIX_PULAY integer, public :: v = 0 integer, public :: cur_itt = 0 integer, public :: start_itt = 0 integer, public :: n_hist = 2 integer, public :: n_itt = 0 integer, public :: restart = 0 integer, public :: restart_save = 0 integer, public :: action = ACTION_MIX type( tMixer ), public, pointer :: next => null() type( tMixer ), public, pointer :: next_conv => null() real(kind=dp), public :: w = 0._dp real(kind=dp), public, pointer :: rv (:) => null() integer, public, pointer :: iv (:) => null() Source Code type tMixer ! Name of mixer character ( len = 24 ) :: name ! The different saved variables per iteration ! and their respective stacks type ( Fstack_dData1D ), allocatable :: stack (:) ! The method of the mixer integer :: m = MIX_PULAY ! In case the mixing method has a variant ! this denote the variant ! This value is thus specific for each method integer :: v = 0 ! The currently reached iteration integer :: cur_itt = 0 , start_itt = 0 ! Different mixers may have different histories integer :: n_hist = 2 ! Number of iterations using this mixer ! There are a couple of signals here !  == 0 : !     only use this mixer until convergence !   > 0 : !     after having runned n_itt step to \"next\" integer :: n_itt = 0 ! When mod(cur_itt,restart_itt) == 0 the history will ! be _reset_ integer :: restart = 0 integer :: restart_save = 0 ! This is an action token specifying the current ! action integer :: action = ACTION_MIX ! The next mixing method following this method type ( tMixer ), pointer :: next => null () ! The next mixing method following this method ! Only used if mixing method achieved convergence ! using this method type ( tMixer ), pointer :: next_conv => null () ! ** Parameters specific for the method: ! The mixing parameter used for this mixer real ( dp ) :: w = 0._dp ! linear array of real variables used specifically ! for this mixing type real ( dp ), pointer :: rv (:) => null () integer , pointer :: iv (:) => null () #ifdef MPI ! In case we have MPI the mixing scheme ! can implement a reduction scheme. ! This can be MPI_Comm_Self to not employ any ! reductions integer :: Comm = MPI_Comm_Self #endif end type tMixer","tags":"","loc":"type/tmixer.html","title":"tMixer – SIESTA "},{"text":"subroutine CROSS(A, B, AXB) Finds the cross product AxB of vectors A and B Arguments Type Intent Optional Attributes Name double precision :: A (3) double precision :: B (3) double precision :: AXB (3) Contents None","tags":"","loc":"proc/cross.html","title":"CROSS – SIESTA"},{"text":"subroutine reord(FCLUST, FSEQ, NM, NSM, ITR) Uses precision alloc proc~~reord~~UsesGraph proc~reord reord alloc alloc proc~reord->alloc precision precision proc~reord->precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Re-orders a clustered data array into a sequential one and viceversa Arguments Type Intent Optional Attributes Name real(kind=grid_p), intent(inout) :: FCLUST (*) CLUSTered data: REAL*4 FCLUST(NSM,NSM,NSM,NM1,NM2,NM3) real(kind=grid_p), intent(inout) :: FSEQ (*) SEQuential data: REAL*4 FSEQ(NSM*NM1,NSM*NM2,NSM*NM3) integer, intent(in) :: NM (3) Number of Mesh divisions in each cell vector integer, intent(in) :: NSM Number of Sub-divisions in each Mesh division integer, intent(in) :: ITR TRanslation-direction switch ITR=+1 => From clustered to sequential ITR=-1 => From sequential to clustered Calls proc~~reord~~CallsGraph proc~reord reord re_alloc re_alloc proc~reord->re_alloc de_alloc de_alloc proc~reord->de_alloc timer timer proc~reord->timer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~reord~~CalledByGraph proc~reord reord proc~dhscf_init dhscf_init proc~dhscf_init->proc~reord interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->proc~reord proc~dhscf->interface~distmeshdata proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->proc~reord proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf interface~distmeshdata->proc~distmeshdata_rea proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian var panprocreordCalledByGraph = svgPanZoom('#procreordCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/reord.html","title":"reord – SIESTA"},{"text":"public subroutine setup_H0(g2max) Uses siesta_options sparse_matrices sparse_matrices sparse_matrices m_nlefsm m_spin sparse_matrices siesta_geom atmfuncs atomlist metaforce molecularmechanics m_nlefsm m_kinefsm m_naefs m_dnaefs m_dhscf m_energies m_ntm m_spin spinorbit alloc class_dSpData1D class_dSpData2D class_zSpData2D m_mpi_utils proc~~setup_h0~~UsesGraph proc~setup_h0 setup_H0 siesta_options siesta_options proc~setup_h0->siesta_options class_dSpData1D class_dSpData1D proc~setup_h0->class_dSpData1D m_naefs m_naefs proc~setup_h0->m_naefs m_nlefsm m_nlefsm proc~setup_h0->m_nlefsm atmfuncs atmfuncs proc~setup_h0->atmfuncs siesta_geom siesta_geom proc~setup_h0->siesta_geom sparse_matrices sparse_matrices proc~setup_h0->sparse_matrices m_kinefsm m_kinefsm proc~setup_h0->m_kinefsm atomlist atomlist proc~setup_h0->atomlist module~m_dhscf m_dhscf proc~setup_h0->module~m_dhscf m_dnaefs m_dnaefs proc~setup_h0->m_dnaefs m_energies m_energies proc~setup_h0->m_energies molecularmechanics molecularmechanics proc~setup_h0->molecularmechanics m_spin m_spin proc~setup_h0->m_spin spinorbit spinorbit proc~setup_h0->spinorbit m_ntm m_ntm proc~setup_h0->m_ntm class_zSpData2D class_zSpData2D proc~setup_h0->class_zSpData2D alloc alloc proc~setup_h0->alloc m_mpi_utils m_mpi_utils proc~setup_h0->m_mpi_utils metaforce metaforce proc~setup_h0->metaforce class_dSpData2D class_dSpData2D proc~setup_h0->class_dSpData2D precision precision module~m_dhscf->precision m_dfscf m_dfscf module~m_dhscf->m_dfscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name real(kind=dp), intent(inout) :: g2max Calls proc~~setup_h0~~CallsGraph proc~setup_h0 setup_H0 proc~dhscf_init dhscf_init proc~setup_h0->proc~dhscf_init globalize_sum globalize_sum proc~setup_h0->globalize_sum nlefsm nlefsm proc~setup_h0->nlefsm val val proc~setup_h0->val naefs naefs proc~setup_h0->naefs uion uion proc~setup_h0->uion timer timer proc~setup_h0->timer nlefsm_so_off nlefsm_so_off proc~setup_h0->nlefsm_so_off spinorb spinorb proc~setup_h0->spinorb meta meta proc~setup_h0->meta kinefsm kinefsm proc~setup_h0->kinefsm write_debug write_debug proc~setup_h0->write_debug dnaefs dnaefs proc~setup_h0->dnaefs dscf dscf proc~setup_h0->dscf isa isa proc~setup_h0->isa twobody twobody proc~setup_h0->twobody proc~dhscf_init->timer proc~dhscf_init->write_debug volcel volcel proc~dhscf_init->volcel ts_init_voltage ts_init_voltage proc~dhscf_init->ts_init_voltage re_alloc re_alloc proc~dhscf_init->re_alloc rcut rcut proc~dhscf_init->rcut de_alloc de_alloc proc~dhscf_init->de_alloc ddot ddot proc~dhscf_init->ddot setupextmesh setupextmesh proc~dhscf_init->setupextmesh distriphionmesh distriphionmesh proc~dhscf_init->distriphionmesh initmesh initmesh proc~dhscf_init->initmesh leqi leqi proc~dhscf_init->leqi proc~setmeshdistr setMeshDistr proc~dhscf_init->proc~setmeshdistr initatommesh initatommesh proc~dhscf_init->initatommesh fdf_integer fdf_integer proc~dhscf_init->fdf_integer init_hartree_add init_hartree_add proc~dhscf_init->init_hartree_add ts_init_hartree_fix ts_init_hartree_fix proc~dhscf_init->ts_init_hartree_fix partialcoreonmesh partialcoreonmesh proc~dhscf_init->partialcoreonmesh rcore rcore proc~dhscf_init->rcore proc~reord reord proc~dhscf_init->proc~reord init_mesh_node init_mesh_node proc~dhscf_init->init_mesh_node setgga setgga proc~dhscf_init->setgga phionmesh phionmesh proc~dhscf_init->phionmesh initialize_efield initialize_efield proc~dhscf_init->initialize_efield getxc getxc proc~dhscf_init->getxc createlocaldscfpointers createlocaldscfpointers proc~dhscf_init->createlocaldscfpointers init_charge_add init_charge_add proc~dhscf_init->init_charge_add neutralatomonmesh neutralatomonmesh proc~dhscf_init->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata die die proc~dhscf_init->die elecs elecs proc~dhscf_init->elecs fdf_get fdf_get proc~dhscf_init->fdf_get shaper shaper proc~dhscf_init->shaper user_specified_field user_specified_field proc~dhscf_init->user_specified_field digcel digcel proc~dhscf_init->digcel cdf_init_mesh cdf_init_mesh proc~dhscf_init->cdf_init_mesh set_box_limits set_box_limits proc~dhscf_init->set_box_limits rhooda rhooda proc~dhscf_init->rhooda compute_doping_structs_uniform compute_doping_structs_uniform proc~dhscf_init->compute_doping_structs_uniform meshlim meshlim proc~setmeshdistr->meshlim proc~reord->timer proc~reord->re_alloc proc~reord->de_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->timer proc~distmeshdata_rea->write_debug proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->die mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->die proc~distmeshdata_int->proc~boxintersection var panprocsetup_h0CallsGraph = svgPanZoom('#procsetup_h0CallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setup_H0 Source Code subroutine setup_H0 ( G2max ) C Computes non - self - consistent part of the Hamiltonian C and initializes data structures on the grid . USE siesta_options , only : g2cut use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : Dscf use m_nlefsm , only : nlefsm_SO_off use m_spin , only : spin use sparse_matrices , only : listh , listhptr , numh , maxnh use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use m_nlefsm , only : nlefsm use m_kinefsm , only : kinefsm use m_naefs , only : naefs use m_dnaefs , only : dnaefs use m_dhscf , only : dhscf_init use m_energies , only : Eions , Ena , DEna , Emm , Emeta , Eso use m_ntm use m_spin , only : spin use spinorbit , only : spinorb use alloc , only : re_alloc , de_alloc use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none real ( dp ), intent ( inout ) :: g2max real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ), dummy_dm ( 1 , 1 ) real ( dp ) :: dummy_E integer :: ia , is real ( dp ) :: dummy_Eso integer :: ispin , i , j complex ( dp ) :: Dc #ifdef MPI real ( dp ) :: buffer1 #endif real ( dp ), pointer :: H_val (:), H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) #ifdef DEBUG call write_debug ( '    PRE setup_H0' ) #endif !----------------------------------------------------------------------BEGIN call timer ( 'Setup_H0' , 1 ) C Self - energy of isolated ions Eions = 0.0_dp do ia = 1 , na_u is = isa ( ia ) Eions = Eions + uion ( is ) enddo !     In these routines, add a flag to tell them NOT to compute !     forces and stresses in this first pass, only energies. !     Neutral-atom: energy call naefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , Ena , dummy_fa , dummy_stress , & forces_and_stress = . false .) call dnaefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , DEna , dummy_fa , dummy_stress , & forces_and_stress = . false .) Ena = Ena + DEna C Metadynamics energy if ( lMetaForce ) then call meta ( xa , na_u , ucell , Emeta , dummy_fa , dummy_stress , $ . false .,. false .) endif C Add on force field contribution to energy call twobody ( na_u , xa , isa , ucell , Emm , & ifa = 0 , fa = dummy_fa , istr = 0 , stress = dummy_stress ) ! !     Now we compute matrix elements of the Kinetic and Non-local !     parts of H !     Kinetic: matrix elements only H_val => val ( H_kin_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call kinefsm ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , & maxnh , maxnh , lasto , iphorb , isa , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !     Non-local-pseudop:  matrix elements only H_val => val ( H_vkb_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare Eso = 0.0d0 if ( . not . spin % SO_offsite ) then call nlefsm ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) else H_so_off => val ( H_so_off_2D ) H_so_off = dcmplx ( 0._dp , 0._dp ) call nlefsm_SO_off ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & spin % Grid , & dummy_E , dummy_Eso , dummy_fa , & dummy_stress , H_val , H_so_off , & matrix_elements_only = . true .) ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! do i = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( i , 1 ), Dscf ( i , 5 ), dp ) Eso = Eso + real ( H_so_off ( i , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( i , 2 ), Dscf ( i , 6 ), dp ) Eso = Eso + real ( H_so_off ( i , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( i , 3 ), Dscf ( i , 4 ), dp ) Eso = Eso + real ( H_so_off ( i , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( i , 7 ), - Dscf ( i , 8 ), dp ) Eso = Eso + real ( H_so_off ( i , 3 ) * Dc , dp ) enddo #ifdef MPI ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 #endif endif ! .................. ! If in the future the spin-orbit routine is able to compute ! forces and stresses, then \"last\" will be needed. If we are not ! computing forces and stresses, calling it in the first iteration ! should be enough ! if ( spin % SO_onsite ) then H_so_on => val ( H_so_on_2D ) !$OMP parallel workshare default(shared) H_so_on (:,:) = 0._dp !$OMP end parallel workshare call spinorb ( no_u , no_l , iaorb , iphorb , isa , indxuo , & maxnh , numh , listhptr , listh , Dscf , H_so_on , Eso ) end if C This will take care of possible changes to the mesh and atomic - related C mesh structures for geometry changes g2max = g2cut call dhscf_init ( spin % Grid , no_s , iaorb , iphorb , & no_l , no_u , na_u , na_s , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnh , numh , listhptr , listh , datm , & dummy_fa , dummy_stress ) call timer ( 'Setup_H0' , 2 ) #ifdef DEBUG call write_debug ( '    POS setup_H0' ) #endif !---------------------------------------------------------------------- END END subroutine setup_H0","tags":"","loc":"proc/setup_h0.html","title":"setup_H0 – SIESTA"},{"text":"public subroutine initMeshDistr(iDistr, oDistr, nm, wload) Computes a new data distribution and the communications needed to\n move data from/to the current distribution to the existing ones. The limits of the new distributions are stored in the current module\n in meshDistr : meshDistr(oDistr)\n meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) If this is the first distribution, we split the mesh uniformly among\n the several processes (we only split it in dimensions Y and Z). For the other data distributions we should split the vector wload.\n The subroutine splitwload will return the limits of the new data\n distribution. The subroutine compMeshComm will return the communications\n needed to move data from/to the current distribution to/from the\n previous ones. Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index of the input vector integer, intent(in) :: oDistr The new data distribution index integer, intent(in) :: nm (3) Number of Mesh divisions of each cell vector integer, intent(in), optional :: wload (*) Weights of every point of the mesh using the input distribut      !ion Calls proc~~initmeshdistr~~CallsGraph proc~initmeshdistr initMeshDistr re_alloc re_alloc proc~initmeshdistr->re_alloc timer timer proc~initmeshdistr->timer die die proc~initmeshdistr->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code initMeshDistr Source Code subroutine initMeshDistr ( iDistr , oDistr , nm , wload ) !! Computes a new data distribution and the communications needed to !! move data from/to the current distribution to the existing ones. !! !! The limits of the new distributions are stored in the current module !! in `[[moreMeshSubs(module):meshDistr(variable)]]`: !! !!     meshDistr(oDistr) !!     meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) !! !! If this is the first distribution, we split the mesh uniformly among !! the several processes (we only split it in dimensions Y and Z). !! !! For the other data distributions we should split the vector wload. !! The subroutine splitwload will return the limits of the new data !! distribution. The subroutine compMeshComm will return the communications !! needed to move data from/to the current distribution to/from the !! previous ones. implicit none integer , optional , intent ( in ) :: iDistr !!  Distribution index of the input vector integer , intent ( in ) :: oDistr !!  The new data distribution index integer , intent ( in ) :: nm ( 3 ) !!  Number of Mesh divisions of each cell vector integer , optional , intent ( in ) :: wload ( * ) !!  Weights of every point of the mesh using the input distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) call timer ( 'INITMESH' , 1 ) !     Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) !     Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) !     Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) !     The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else !       In order to compute the other data distributions, we should split !       the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) !       Precompute the communications needed to move data between the new data !       distribution and the previous ones. jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) end subroutine initMeshDistr","tags":"","loc":"proc/initmeshdistr.html","title":"initMeshDistr – SIESTA"},{"text":"public subroutine allocASynBuffer(ndistr) Uses mesh proc~~allocasynbuffer~~UsesGraph proc~allocasynbuffer allocASynBuffer mesh mesh proc~allocasynbuffer->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Allocate memory buffers for asynchronous communications.\n It does nothing for synchronous communications. The output values are stored in the current module: tBuff1 : Buffer for distribution 1 tBuff2 : Buffer for other distributions Arguments Type Intent Optional Attributes Name integer :: ndistr Total number of distributions Calls proc~~allocasynbuffer~~CallsGraph proc~allocasynbuffer allocASynBuffer re_alloc re_alloc proc~allocasynbuffer->re_alloc proc~boxintersection boxIntersection proc~allocasynbuffer->proc~boxintersection de_alloc de_alloc proc~allocasynbuffer->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocASynBuffer Source Code subroutine allocASynBuffer ( ndistr ) !! Allocate memory buffers for asynchronous communications. !! It does nothing for synchronous communications. !! The output values are stored in the current module: !! `[[moreMeshSubs(module):tBuff1(variable)]]` : Buffer for distribution 1 !! `[[moreMeshSubs(module):tBuff2(variable)]]` : Buffer for other distributions use mesh , only : nsm implicit none integer :: ndistr !! Total number of distributions integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters #ifdef ASYNCHRONOUS !     Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) !     Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo !     Check the size of the intersections between the first data distributions !     and the others data distributions. !     Buffers don't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr ( 1 )% box (:,:, node + 1 ) do ii = 2 , ndistr box2 => meshDistr ( ii )% box (:,:, node + 1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp else lsize = 0 endif imax1 = max ( imax1 , nsize ( 1 ) - lsize ) imax2 = max ( imax2 , nsize ( ii ) - lsize ) enddo !     Deallocate local memory call de_alloc ( nsize , 'nsize' , moduName ) !     Allocate memory for asynchronous communications call re_alloc ( tBuff1 , 1 , imax1 , 'tBuff1' , moduName ) call re_alloc ( tBuff2 , 1 , imax2 , 'tBuff2' , moduName ) #endif end subroutine allocASynBuffer","tags":"","loc":"proc/allocasynbuffer.html","title":"allocASynBuffer – SIESTA"},{"text":"public subroutine allocExtMeshDistr(iDistr, nep, mop) Uses mesh proc~~allocextmeshdistr~~UsesGraph proc~allocextmeshdistr allocExtMeshDistr mesh mesh proc~allocextmeshdistr->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nep integer, intent(in) :: mop Calls proc~~allocextmeshdistr~~CallsGraph proc~allocextmeshdistr allocExtMeshDistr re_alloc re_alloc proc~allocextmeshdistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocExtMeshDistr Source Code subroutine allocExtMeshDistr ( iDistr , nep , mop ) use mesh , only : indexp , idop , xdop implicit none !     Input variables integer , intent ( in ) :: iDistr , nep , mop !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % indexp , 1 , nep , 'distr%indexp' , moduName ) call re_alloc ( distr % idop , 1 , mop , 'distr%idop' , moduName ) call re_alloc ( distr % xdop , 1 , 3 , 1 , mop , 'distr%xdop' , moduName ) indexp => distr % indexp idop => distr % idop xdop => distr % xdop end subroutine allocExtMeshDistr","tags":"","loc":"proc/allocextmeshdistr.html","title":"allocExtMeshDistr – SIESTA"},{"text":"public subroutine allocIpaDistr(iDistr, na) Uses mesh proc~~allocipadistr~~UsesGraph proc~allocipadistr allocIpaDistr mesh mesh proc~allocipadistr->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: na Calls proc~~allocipadistr~~CallsGraph proc~allocipadistr allocIpaDistr re_alloc re_alloc proc~allocipadistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocIpaDistr Source Code subroutine allocIpaDistr ( iDistr , na ) use mesh , only : ipa implicit none !     Input variables integer , intent ( in ) :: iDistr , na !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % ipa , 1 , na , 'distr%ipa' , moduName ) ipa => meshDistr ( iDistr )% ipa end subroutine allocIpaDistr","tags":"","loc":"proc/allocipadistr.html","title":"allocIpaDistr – SIESTA"},{"text":"public subroutine setMeshDistr(iDistr, nsm, nsp, nml, nmpl, ntml, ntpl) Uses mesh proc~~setmeshdistr~~UsesGraph proc~setmeshdistr setMeshDistr mesh mesh proc~setmeshdistr->mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Fixes the new data limits and dimensions of the mesh to those of\n the data distribution iDistr . Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index of the input vector integer, intent(in) :: nsm Number of mesh sub-divisions in each direction integer, intent(in) :: nsp Number of sub-points of each mesh point integer, intent(out) :: nml (3) Local number of Mesh divisions in each cell vector integer, intent(out) :: nmpl Local number of Mesh divisions integer, intent(out) :: ntml (3) Local number of Mesh points in each cell vector integer, intent(out) :: ntpl Local number of Mesh points Calls proc~~setmeshdistr~~CallsGraph proc~setmeshdistr setMeshDistr meshlim meshlim proc~setmeshdistr->meshlim Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~setmeshdistr~~CalledByGraph proc~setmeshdistr setMeshDistr proc~dhscf_init dhscf_init proc~dhscf_init->proc~setmeshdistr proc~dhscf dhscf proc~dhscf->proc~setmeshdistr proc~delk_wrapper delk_wrapper proc~delk_wrapper->proc~setmeshdistr proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setMeshDistr Source Code subroutine setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) !! Fixes the new data limits and dimensions of the mesh to those of !! the data distribution `iDistr`. use mesh , only : meshLim , indexp , ipa , idop , xdop implicit none integer , intent ( in ) :: iDistr !! Distribution index of the input vector integer , intent ( in ) :: nsm !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: nsp !! Number of sub-points of each mesh point integer , intent ( out ) :: nml ( 3 ) !! Local number of Mesh divisions in each cell vector integer , intent ( out ) :: nmpl !! Local number of Mesh divisions integer , intent ( out ) :: ntml ( 3 ) !! Local number of Mesh points in each cell vector integer , intent ( out ) :: ntpl !! Local number of Mesh points !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) meshLim = distr % box ( 1 : 2 , 1 : 3 , node + 1 ) nml ( 1 ) = ( MeshLim ( 2 , 1 ) - MeshLim ( 1 , 1 )) + 1 nml ( 2 ) = ( MeshLim ( 2 , 2 ) - MeshLim ( 1 , 2 )) + 1 nml ( 3 ) = ( MeshLim ( 2 , 3 ) - MeshLim ( 1 , 3 )) + 1 nmpl = nml ( 1 ) * nml ( 2 ) * nml ( 3 ) ntml = nml * nsm ntpl = nmpl * nsp indexp => distr % indexp idop => distr % idop xdop => distr % xdop ipa => distr % ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr","tags":"","loc":"proc/setmeshdistr.html","title":"setMeshDistr – SIESTA"},{"text":"public subroutine resetMeshDistr(iDistr) Reset the data of the distribution iDistr .\n Deallocate associated arrays of the current distribution. Modifies data of the current module. Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index to be reset Calls proc~~resetmeshdistr~~CallsGraph proc~resetmeshdistr resetMeshDistr de_alloc de_alloc proc~resetmeshdistr->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code resetMeshDistr Source Code subroutine resetMeshDistr ( iDistr ) !! Reset the data of the distribution `iDistr`. !! Deallocate associated arrays of the current distribution. !! Modifies data of the current module. implicit none integer , optional , intent ( in ) :: iDistr !! Distribution index to be reset integer :: idis , ini , fin , icom type ( meshDisType ), pointer :: distr type ( meshCommType ), pointer :: mcomm if ( present ( iDistr )) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis = ini , fin distr => meshDistr ( idis ) distr % nMesh = 0 if ( associated ( distr % box )) then call de_alloc ( distr % box , 'distr%box' , 'moreMeshSubs' ) endif if ( associated ( distr % indexp )) then call de_alloc ( distr % indexp , 'distr%indexp' , & 'moreMeshSubs' ) endif if ( associated ( distr % idop )) then call de_alloc ( distr % idop , 'distr%idop' , & 'moreMeshSubs' ) endif if ( associated ( distr % xdop )) then call de_alloc ( distr % xdop , 'distr%xdop' , & 'moreMeshSubs' ) endif if ( associated ( distr % ipa )) then call de_alloc ( distr % ipa , 'distr%ipa' , & 'moreMeshSubs' ) endif do icom = 1 , 3 mcomm => exteCommu ( idis , icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo do icom = (( idis - 2 ) * ( idis - 1 )) / 2 + 1 , (( idis - 1 ) * idis ) / 2 mcomm => meshCommu ( icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if ( associated ( tBuff1 )) then call de_alloc ( tBuff1 , 'tBuff1' , 'moreMeshSubs' ) endif if ( associated ( tBuff2 )) then call de_alloc ( tBuff2 , 'tBuff2' , 'moreMeshSubs' ) endif #endif end subroutine resetMeshDistr","tags":"","loc":"proc/resetmeshdistr.html","title":"resetMeshDistr – SIESTA"},{"text":"private subroutine distMeshData_rea(iDistr, fsrc, oDistr, fdst, itr) Uses mesh mpi_siesta proc~~distmeshdata_rea~2~~UsesGraph proc~distmeshdata_rea~2 distMeshData_rea mesh mesh proc~distmeshdata_rea~2->mesh mpi_siesta mpi_siesta proc~distmeshdata_rea~2->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_rea~2~~CallsGraph proc~distmeshdata_rea~2 distMeshData_rea mpitrace_event mpitrace_event proc~distmeshdata_rea~2->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea~2->nmeshg proc~reord reord proc~distmeshdata_rea~2->proc~reord re_alloc re_alloc proc~distmeshdata_rea~2->re_alloc proc~boxintersection boxIntersection proc~distmeshdata_rea~2->proc~boxintersection timer timer proc~distmeshdata_rea~2->timer mpi_barrier mpi_barrier proc~distmeshdata_rea~2->mpi_barrier de_alloc de_alloc proc~distmeshdata_rea~2->de_alloc write_debug write_debug proc~distmeshdata_rea~2->write_debug die die proc~distmeshdata_rea~2->die proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_rea Source Code subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea","tags":"","loc":"proc/distmeshdata_rea~2.html","title":"distMeshData_rea – SIESTA"},{"text":"private subroutine distMeshData_rea(iDistr, fsrc, oDistr, fdst, itr) Uses mesh mpi_siesta proc~~distmeshdata_rea~~UsesGraph proc~distmeshdata_rea distMeshData_rea mesh mesh proc~distmeshdata_rea->mesh mpi_siesta mpi_siesta proc~distmeshdata_rea->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_rea~~CallsGraph proc~distmeshdata_rea distMeshData_rea mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~reord reord proc~distmeshdata_rea->proc~reord re_alloc re_alloc proc~distmeshdata_rea->re_alloc proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection timer timer proc~distmeshdata_rea->timer mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier de_alloc de_alloc proc~distmeshdata_rea->de_alloc write_debug write_debug proc~distmeshdata_rea->write_debug die die proc~distmeshdata_rea->die proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~distmeshdata_rea~~CalledByGraph proc~distmeshdata_rea distMeshData_rea interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_rea proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_rea Source Code subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea","tags":"","loc":"proc/distmeshdata_rea.html","title":"distMeshData_rea – SIESTA"},{"text":"private subroutine distMeshData_int(iDistr, fsrc, oDistr, fdst, itr) Uses mpi_siesta proc~~distmeshdata_int~~UsesGraph proc~distmeshdata_int distMeshData_int mpi_siesta mpi_siesta proc~distmeshdata_int->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_int~~CallsGraph proc~distmeshdata_int distMeshData_int die die proc~distmeshdata_int->die proc~boxintersection boxIntersection proc~distmeshdata_int->proc~boxintersection de_alloc de_alloc proc~distmeshdata_int->de_alloc re_alloc re_alloc proc~distmeshdata_int->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~distmeshdata_int~~CalledByGraph proc~distmeshdata_int distMeshData_int interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_int proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_int Source Code subroutine distMeshData_int ( iDistr , fsrc , oDistr , fdst , itr ) #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr integer , intent ( in ) :: fsrc ( * ) integer , intent ( out ) :: fdst ( * ) !     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & ind , ncom , icom , NSRC ( 3 ), NDST ( 3 ), & ME , MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters integer , pointer :: TBUF (:) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif if ( nodes == 1 ) then call die ( \"Called _int version of distMeshData for n=1\" ) else !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) ME = Node + 1 !       Compute the maximum size of the buffer needed to transfer data !       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 NSRC ( 2 ) = Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 NSRC ( 3 ) = Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 Dbox => odis % box (:,:, ME ) NDST ( 1 ) = Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 NDST ( 2 ) = Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 NDST ( 3 ) = Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 if ( itr . eq . 0 ) then !         From sequencial to sequencial do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif endif end subroutine distMeshData_int","tags":"","loc":"proc/distmeshdata_int.html","title":"distMeshData_int – SIESTA"},{"text":"private subroutine boxIntersection(ibox1, ibox2, obox, inters) Checks the three axis of the input boxes to see if there is\n intersection between the input boxes. If it exists, returns\n the resulting box. Arguments Type Intent Optional Attributes Name integer, intent(in) :: ibox1 (2,3) Input box integer, intent(in) :: ibox2 (2,3) Input box integer, intent(out) :: obox (2,3) Intersection between ibox1 and ibox2 logical, intent(out) :: inters TRUE , if there is an intersection. Otherwise FALSE . Called by proc~~boxintersection~~CalledByGraph proc~boxintersection boxIntersection proc~reordmeshnumbering reordMeshNumbering proc~reordmeshnumbering->proc~boxintersection proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->proc~boxintersection proc~distextmeshdata distExtMeshData proc~distextmeshdata->proc~boxintersection proc~compmeshcomm compMeshComm proc~compmeshcomm->proc~boxintersection proc~allocasynbuffer allocASynBuffer proc~allocasynbuffer->proc~boxintersection proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->proc~boxintersection proc~gathextmeshdata gathExtMeshData proc~gathextmeshdata->proc~boxintersection proc~splitwload splitwload proc~splitwload->proc~boxintersection proc~distmeshdata_int distMeshData_int proc~distmeshdata_int->proc~boxintersection proc~initmeshextencil initMeshExtencil proc~initmeshextencil->proc~boxintersection proc~reordmeshnumbering~2 reordMeshNumbering proc~reordmeshnumbering~2->proc~boxintersection interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_rea interface~distmeshdata->proc~distmeshdata_int proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian var panprocboxintersectionCalledByGraph = svgPanZoom('#procboxintersectionCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code boxIntersection Source Code subroutine boxIntersection ( ibox1 , ibox2 , obox , inters ) !! Checks the three axis of the input boxes to see if there is !! intersection between the input boxes. If it exists, returns !! the resulting box. implicit none !     Passed arguments integer , intent ( in ) :: ibox1 ( 2 , 3 ), ibox2 ( 2 , 3 ) !! Input box integer , intent ( out ) :: obox ( 2 , 3 ) !! Intersection between `ibox1` and `ibox2` logical , intent ( out ) :: inters !! `TRUE`, if there is an intersection. Otherwise `FALSE`. !     Local variables integer :: iaxis inters = . true . do iaxis = 1 , 3 obox ( 1 , iaxis ) = max ( ibox1 ( 1 , iaxis ), ibox2 ( 1 , iaxis )) obox ( 2 , iaxis ) = min ( ibox1 ( 2 , iaxis ), ibox2 ( 2 , iaxis )) if ( obox ( 2 , iaxis ). lt . obox ( 1 , iaxis )) inters = . false . enddo end subroutine boxIntersection","tags":"","loc":"proc/boxintersection.html","title":"boxIntersection – SIESTA"},{"text":"public subroutine initMeshExtencil(iDistr, nm) Uses scheComm proc~~initmeshextencil~~UsesGraph proc~initmeshextencil initMeshExtencil scheComm scheComm proc~initmeshextencil->scheComm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Compute the needed communications in order to send/receive the\n extencil (when the data is ordered in the distribution iDistr )\n The results are stored in the variable exteCommu (iDistr,1:3) of the current module. For every dimension of the problem, search all the neighbors that\n we have. Given the current data distribution we compute the limits\n of our extencil and we check its intersection with all the other\n processes. Once we know all our neighbors we call subroutine scheduleComm in order to minimize the number\n of communications steps. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: nm (3) Number of Mesh divisions in each cell vector Calls proc~~initmeshextencil~~CallsGraph proc~initmeshextencil initMeshExtencil re_alloc re_alloc proc~initmeshextencil->re_alloc proc~boxintersection boxIntersection proc~initmeshextencil->proc~boxintersection de_alloc de_alloc proc~initmeshextencil->de_alloc schedulecomm schedulecomm proc~initmeshextencil->schedulecomm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code initMeshExtencil Source Code subroutine initMeshExtencil ( iDistr , nm ) !! Compute the needed communications in order to send/receive the !! extencil (when the data is ordered in the distribution `iDistr`) !! The results are stored in the variable !! `[[moreMeshSubs(module):exteCommu(variable)]](iDistr,1:3)` !! of the current module. !! !! For every dimension of the problem, search all the neighbors that !! we have. Given the current data distribution we compute the limits !! of our extencil and we check its intersection with all the other !! processes. Once we know all our neighbors we call subroutine !! `[[scheduleComm(proc)]]` in order to minimize the number !! of communications steps. use scheComm implicit none !     Passed arguments integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: nm ( 3 ) !! Number of Mesh divisions in each cell vector !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), Ibox ( 2 , 3 ), & ii , iaxis , ncom , Gcom , Lcom , P1 , P2 integer , pointer :: src (:), dst (:), Dbox (:,:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm type ( COMM_T ) :: comm logical :: inters idis => meshDistr ( iDistr ) do iaxis = 1 , 3 !       One communication structure for every dimension mcomm => exteCommu ( iDistr , iaxis ) !       Count the number of communications needed to send/receive !       the extencil ncom = 0 do P1 = 1 , Nodes !         Create the extencil boxes for both sides of the current !         partition Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) ncom = ncom + 1 endif enddo enddo Gcom = ncom !       Create a list of communications needed to send/receive !       the extencil if ( Gcom . gt . 0 ) then nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) ncom = 0 do P1 = 1 , Nodes Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif endif enddo enddo comm % np = Nodes !         reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !         Count the number of communications needed by the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !         Store the ordered list of communications needed by the current !         process to send/receive the extencil. if ( Lcom . gt . 0 ) then nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , & 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , & 'moreMeshSubs' ) ncom = 0 do P1 = 1 , comm % ncol ii = comm % ind ( P1 , Node + 1 ) if ( ii . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( ii ) mcomm % dst ( ncom ) = dst ( ii ) endif enddo mcomm % ncom = Lcom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) endif call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) endif enddo end subroutine initMeshExtencil","tags":"","loc":"proc/initmeshextencil.html","title":"initMeshExtencil – SIESTA"},{"text":"public subroutine distExtMeshData(iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, DENS, BDENS) Uses mpi_siesta proc~~distextmeshdata~~UsesGraph proc~distextmeshdata distExtMeshData mpi_siesta mpi_siesta proc~distextmeshdata->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Send/receive the extencil information from the DENS matrix to the\n temporal array BDENS . We have a different code for every axis. We should find if we\n intersects with a neightbour node throught the upper, the lower\n or both sides. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: DENS (maxp,NSPIN) Electron density matrix real(kind=gp), intent(out) :: BDENS (BS,2*NN,NSPIN) Auxiliary arrays to store the extencil from other partitions Calls proc~~distextmeshdata~~CallsGraph proc~distextmeshdata distExtMeshData die die proc~distextmeshdata->die proc~boxintersection boxIntersection proc~distextmeshdata->proc~boxintersection de_alloc de_alloc proc~distextmeshdata->de_alloc re_alloc re_alloc proc~distextmeshdata->re_alloc mpi_sendrecv mpi_sendrecv proc~distextmeshdata->mpi_sendrecv Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distExtMeshData Source Code subroutine distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , dens , BDENS ) !! Send/receive the extencil information from the `DENS` matrix to the !! temporal array `BDENS`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: DENS ( maxp , NSPIN ) !! Electron density matrix real ( gp ), intent ( out ) :: BDENS ( BS , 2 * NN , NSPIN ) !! Auxiliary arrays to store the extencil from other partitions !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM if (. not . associated ( mcomm % dst )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif if (. not . associated ( mcomm % src )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = dimB ( 2 ) - NN + 1 , dimB ( 2 ) uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ) - NN + 1 , dimB ( 3 ) do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine distExtMeshData","tags":"","loc":"proc/distextmeshdata.html","title":"distExtMeshData – SIESTA"},{"text":"public subroutine gathExtMeshData(iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, BVXC, VXC) Uses mpi_siesta proc~~gathextmeshdata~~UsesGraph proc~gathextmeshdata gathExtMeshData mpi_siesta mpi_siesta proc~gathextmeshdata->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Send/receive the extencil information from the BVXC temporal array\n to the array VXC . We have a different code for every axis. We should find if we\n intersects with a neightbour node throught the upper, the lower\n or both sides. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: BVXC (BS,2*NN,NSPIN) Auxiliar array that contains the extencil of the\n exch-corr potential real(kind=gp), intent(out) :: VXC (maxp,NSPIN) Exch-corr potential Calls proc~~gathextmeshdata~~CallsGraph proc~gathextmeshdata gathExtMeshData re_alloc re_alloc proc~gathextmeshdata->re_alloc proc~boxintersection boxIntersection proc~gathextmeshdata->proc~boxintersection de_alloc de_alloc proc~gathextmeshdata->de_alloc mpi_sendrecv mpi_sendrecv proc~gathextmeshdata->mpi_sendrecv Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code gathExtMeshData Source Code subroutine gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , BVXC , VXC ) !! Send/receive the extencil information from the `BVXC` temporal array !! to the array `VXC`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: BVXC ( BS , 2 * NN , NSPIN ) !! Auxiliar array that contains the extencil of the !! exch-corr potential real ( gp ), intent ( out ) :: VXC ( maxp , NSPIN ) !! Exch-corr potential !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = NN + 1 , 2 * NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine gathExtMeshData","tags":"","loc":"proc/gathextmeshdata.html","title":"gathExtMeshData – SIESTA"},{"text":"private subroutine splitwload(Nodes, Node, nm, wload, iDistr, oDistr) Uses mpi_siesta proc~~splitwload~~UsesGraph proc~splitwload splitwload mpi_siesta mpi_siesta proc~splitwload->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Compute the limits of a new distribution, trying to split the load\n of the array wload . We use the nested disection algorithm in\n order to split the mesh in the 3 dimensions. We use the nested disection algorithm to split the load associated\n to the vector wload among all the processes. The problem is that\n every process have a different part of wload. Every time that we want\n to split a piece of the mesh, we should find which processors have that\n information. wload is a 3D array. In every iteration of the algorithm we should\n decide the direction of the cut. Then we should made a reduction of\n this 3-D array to a 1-D array (according to the selected direction). Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes Total number of nodes integer, intent(in) :: Node Current process ID (from 1 to Node) integer, intent(in) :: nm (3) Number of mesh sub-divisions in each direction integer, intent(in) :: wload (*) Weights of every point of the mesh. type( meshDisType ), intent(in) :: iDistr Input distribution type( meshDisType ), intent(out) :: oDistr Output distribution Calls proc~~splitwload~~CallsGraph proc~splitwload splitwload proc~boxintersection boxIntersection proc~splitwload->proc~boxintersection mpi_bcast mpi_bcast proc~splitwload->mpi_bcast re_alloc re_alloc proc~splitwload->re_alloc mpi_recv mpi_recv proc~splitwload->mpi_recv timer timer proc~splitwload->timer de_alloc de_alloc proc~splitwload->de_alloc proc~reduce3dto1d reduce3Dto1D proc~splitwload->proc~reduce3dto1d mpi_send mpi_send proc~splitwload->mpi_send Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code splitwload Source Code subroutine splitwload ( Nodes , Node , nm , wload , iDistr , oDistr ) !! Compute the limits of a new distribution, trying to split the load !! of the array `wload`. We use the nested disection algorithm in !! order to split the mesh in the 3 dimensions. !! !! We use the nested disection algorithm to split the load associated !! to the vector `wload` among all the processes. The problem is that !! every process have a different part of wload. Every time that we want !! to split a piece of the mesh, we should find which processors have that !! information. !! !! `wload` is a 3D array. In every iteration of the algorithm we should !! decide the direction of the cut. Then we should made a reduction of !! this 3-D array to a 1-D array (according to the selected direction). use mpi_siesta implicit none integer , intent ( in ) :: Nodes !! Total number of nodes integer , intent ( in ) :: Node !! Current process ID (from 1 to Node) integer , intent ( in ) :: nm ( 3 ) !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: wload ( * ) !! Weights of every point of the mesh. type ( meshDisType ), intent ( in ) :: iDistr !! Input distribution type ( meshDisType ), intent ( out ) :: oDistr !! Output distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'splitwload ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: PP , Lbox ( 2 , 3 ), Ldim , & QQ , P1 , P2 , POS , ini integer ( i8b ), pointer :: lwload (:), gwload (:), recvB (:) logical :: found , inters integer :: mGdim , mLdim , nAxis , nms ( 3 ) integer ( i8b ) :: h1 , h2 integer , pointer :: PROCS (:) integer :: MPIerror , Status ( MPI_Status_Size ) call timer ( 'SPLOAD' , 1 ) !     At the begining of the algorithm all the mesh is assigned to the !     first node:  oDistr%box(*,*,1) = nm oDistr % box ( 1 , 1 , 1 ) = 1 oDistr % box ( 2 , 1 , 1 ) = nm ( 1 ) oDistr % box ( 1 , 2 , 1 ) = 1 oDistr % box ( 2 , 2 , 1 ) = nm ( 2 ) oDistr % box ( 1 , 3 , 1 ) = 1 oDistr % box ( 2 , 3 , 1 ) = nm ( 3 ) oDistr % box ( 1 : 2 , 1 : 3 , 2 : Nodes ) = 0 nms = nm !     Array PROCS will contain the number of processes that are associated to !     every box. At the begining all the mesh is assigned to process 1, then !     PROCS(1)=Nodes, while the rest are equal to zero nullify ( PROCS , lwload , gwload , recvB ) call re_alloc ( PROCS , 1 , Nodes , 'PROCS' , 'moreMeshSubs' ) PROCS ( 1 ) = Nodes PROCS ( 2 : Nodes ) = 0 found = . true . do while ( found ) !       Choose the direction to cut the mesh nAxis = 3 if ( nms ( 2 ). gt . nms ( nAxis )) nAxis = 2 if ( nms ( 1 ). gt . nms ( nAxis )) nAxis = 1 nms ( nAxis ) = ( nms ( nAxis ) + 1 ) / 2 !       Check if we still have to keep cutting the mesh found = . false . do PP = Nodes , 1 , - 1 if ( PROCS ( PP ). GT . 1 ) then !           There are more than one processes associated to the mesh !           of process PP. We are going to split the mesh in two parts !           of p1 and p2 processors. p1 = PROCS ( PP ) / 2 p2 = PROCS ( PP ) - p1 found = . true . !           Check if the current partition has intersection with the piece of !           mesh that we want to cut. call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, Node ), & Lbox , inters ) if ( Node . eq . PP ) then mGdim = oDistr % box ( 2 , nAxis , PP ) - oDistr % box ( 1 , nAxis , PP ) + 1 call re_alloc ( gwload , 1 , mGdim , 'gwload' , 'moreMeshSubs' ) call re_alloc ( recvB , 1 , mGdim , 'recvB' , 'moreMeshSubs' ) endif if ( inters ) then !             If there is an intersection I should reduce the intersected part !             from a 3-D array to a 1-D array. mLdim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 call re_alloc ( lwload , 1 , mLdim , 'lwload' , & 'moreMeshSubs' ) call reduce3Dto1D ( nAxis , iDistr % box (:,:, Node ), Lbox , & wload , lwload ) endif if ( Node . eq . PP ) then !             If, I'm the process PP I should receive the information from other !             processes gwload = 0 do QQ = 1 , Nodes call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, QQ ), & Lbox , inters ) if ( inters ) then Ldim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 ini = Lbox ( 1 , nAxis ) - oDistr % box ( 1 , nAxis , PP ) if ( PP . eq . QQ ) then gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + lwload ( 1 : Ldim ) else call mpi_recv ( recvB , Ldim , MPI_INTEGER8 , QQ - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + recvB ( 1 : Ldim ) endif endif enddo call de_alloc ( recvB , 'recvB' , 'moreMeshSubs' ) !             Process PP computes where to cut the mesh call vecBisec ( mGdim , gwload ( 1 : mGdim ), & PROCS ( PP ), POS , h1 , h2 ) call de_alloc ( gwload , 'gwload' , 'moreMeshSubs' ) else if ( inters ) then !             If, I'm not the process PP I should send the information to !             the process PP call MPI_Send ( lwload , mLdim , & MPI_INTEGER8 , PP - 1 , 1 , MPI_Comm_World , & MPIerror ) endif if ( associated ( lwload )) & call de_alloc ( lwload , 'lwload' , 'moreMeshSubs' ) !           Process PP send the position of the cut to the rest of processes call MPI_Bcast ( pos , 1 , MPI_integer , PP - 1 , & MPI_Comm_World , MPIerror ) !           We have splitted the piece of mesh associated to process PP !           in two parts. One would be stored in position PP and the other !           would be stored in position PP+P1 QQ = PP + P1 oDistr % box ( 1 : 2 , 1 : 3 , QQ ) = oDistr % box ( 1 : 2 , 1 : 3 , PP ) pos = oDistr % box ( 1 , naxis , QQ ) + pos oDistr % box ( 1 , naxis , QQ ) = pos oDistr % box ( 2 , naxis , PP ) = pos - 1 !           We should actualize the numbers of processes associated to PP and QQ PROCS ( PP ) = P1 PROCS ( QQ ) = P2 endif enddo enddo call de_alloc ( PROCS , 'PROCS' , 'moreMeshSubs' ) call timer ( 'SPLOAD' , 2 ) end subroutine splitwload","tags":"","loc":"proc/splitwload.html","title":"splitwload – SIESTA"},{"text":"private subroutine reduce3Dto1D(iaxis, Ibox, Lbox, wload, lwload) Given a 3-D array, wload , we will make a reduction of its values\n to one of its dimensions ( iaxis ). Ibox gives the limits of the\n input array wload and Lbox gives the limits of the part that we\n want to reduce. First we compute the 3 dimensions of the input array and the\n intersection. We accumulate the values of the intersection into a\n 1-D array. IF (iaxis=1) lwload(II) = SUM(wload(II,*,*))\n IF (iaxis=2) lwload(II) = SUM(wload(*,II,*))\n IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iaxis Axe to be reduced integer, intent(in) :: Ibox (2,3) Limits of the input array integer, intent(in) :: Lbox (2,3) Limits of the intersection that we want to reduce integer, intent(in) :: wload (*) 3-D array that we want to reduce to one of\n its dimensions integer(kind=i8b), intent(out) :: lwload (*) 1-D array. Reduction of the intersected part\n of wload Called by proc~~reduce3dto1d~~CalledByGraph proc~reduce3dto1d reduce3Dto1D proc~splitwload splitwload proc~splitwload->proc~reduce3dto1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reduce3Dto1D Source Code subroutine reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) !! Given a 3-D array, `wload`, we will make a reduction of its values !! to one of its dimensions (`iaxis`). `Ibox` gives the limits of the !! input array `wload` and `Lbox` gives the limits of the part that we !! want to reduce. !! !! First we compute the 3 dimensions of the input array and the !! intersection. We accumulate the values of the intersection into a !! 1-D array. !! !!     IF (iaxis=1) lwload(II) = SUM(wload(II,*,*)) !!     IF (iaxis=2) lwload(II) = SUM(wload(*,II,*)) !!     IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) implicit none integer , intent ( in ) :: iaxis !! Axe to be reduced integer , intent ( in ) :: Ibox ( 2 , 3 ) !! Limits of the input array integer , intent ( in ) :: Lbox ( 2 , 3 ) !! Limits of the intersection that we want to reduce integer , intent ( in ) :: wload ( * ) !! 3-D array that we want to reduce to one of !! its dimensions integer ( i8b ), intent ( out ) :: lwload ( * ) !! 1-D array. Reduction of the intersected part !! of wload !     Local variables integer :: Idim ( 3 ), Ldim ( 3 ), ind , ind1 , ind2 , ind3 , & I1 , I2 , I3 !     Dimensions of the input array Idim ( 1 ) = Ibox ( 2 , 1 ) - Ibox ( 1 , 1 ) + 1 Idim ( 2 ) = Ibox ( 2 , 2 ) - Ibox ( 1 , 2 ) + 1 Idim ( 3 ) = Ibox ( 2 , 3 ) - Ibox ( 1 , 3 ) + 1 !     Dimensions of the intersection. Ldim ( 1 ) = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ldim ( 2 ) = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Ldim ( 3 ) = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( iaxis . eq . 1 ) then !       Reduction into the X-axis lwload ( 1 : Ldim ( 1 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I1 ) = lwload ( I1 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else if ( iaxis . eq . 2 ) then !       Reduction into the Y-axis lwload ( 1 : Ldim ( 2 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I2 ) = lwload ( I2 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else !       Reduction into the Z-axis lwload ( 1 : Ldim ( 3 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I3 ) = lwload ( I3 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo endif end subroutine reduce3Dto1D","tags":"","loc":"proc/reduce3dto1d.html","title":"reduce3Dto1D – SIESTA"},{"text":"private subroutine vecBisec(nval, values, nparts, pos, h1, h2) Bisection of the load associated to an array. We want to split array values in nparts , but in this call to vecBisec we are going to make only one cut. First, we split nparts in two parts: p1=nparts/2 and p2=nparts-p1 . Then we compute the total\n load of the array values ( total ) and the desired load for the\n first part: halfG = (total*p1)/nparts Finally, we try to find the position inside values where we are\n nearer of the the desired solution. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nval Dimension of the input array integer(kind=i8b), intent(in) :: values (nval) Input array integer, intent(in) :: nparts Numbers of partitions that we want to make from\n the input array (in this call we only make one cut) integer, intent(out) :: pos Position of the cut integer(kind=i8b), intent(out) :: h1 Load of the first part integer(kind=i8b), intent(out) :: h2 Load of the second part Contents Source Code vecBisec Source Code subroutine vecBisec ( nval , values , nparts , pos , h1 , h2 ) !! Bisection of the load associated to an array. !! !! We want to split array `values` in `nparts`, but in this call to !! `vecBisec` we are going to make only one cut. First, we split `nparts` !! in two parts: `p1=nparts/2` and `p2=nparts-p1`. Then we compute the total !! load of the array `values` (`total`) and the desired load for the !! first part: !! !!     halfG = (total*p1)/nparts !! !! Finally, we try to find the position inside `values` where we are !! nearer of the the desired solution. implicit none integer , intent ( in ) :: nval !! Dimension of the input array integer ( i8b ), intent ( in ) :: values ( nval ) !! Input array integer , intent ( in ) :: nparts !! Numbers of partitions that we want to make from !! the input array (in this call we only make one cut) integer , intent ( out ) :: pos !! Position of the cut integer ( i8b ), intent ( out ) :: h1 !! Load of the first part integer ( i8b ), intent ( out ) :: h2 !! Load of the second part !     Local variables integer :: p1 , p2 , ii integer ( i8b ) :: total , halfG , halfL if ( nparts . gt . 1 ) then !       Split the number of parts in 2 p1 = nparts / 2 p2 = nparts - p1 !       Compute the total load of the array total = 0 do ii = 1 , nval total = total + values ( ii ) enddo !       Desired load of the first part halfG = ( total * p1 ) / nparts halfL = 0 pos = 0 !       Loop until we reach the solution do while ( halfL . lt . halfG ) pos = pos + 1 if ( pos . eq . nval + 1 ) STOP 'ERROR in vecBisec' halfL = halfL + values ( pos ) enddo !       Check if the previous position is better than the !       current position if (( halfL - values ( pos ) * p2 / nparts ). gt . halfG ) then halfL = halfL - values ( pos ) pos = pos - 1 endif h1 = halfL h2 = total - halfL endif end subroutine vecBisec","tags":"","loc":"proc/vecbisec.html","title":"vecBisec – SIESTA"},{"text":"private subroutine reordMeshNumbering(distr1, distr2) Warning This version of subroutine is called if\n the directive REORD1 is defined. Given a new distribution, distr2 , reasign each box to the proper\n process. We use the following criteria: Minimize the number of communications. Data don't need to\nbe communicated if it belongs to the same process in\ndifferent data distributions The output values are stored in the current module: meshDistr (distr2)%box(:,:,:) Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution Calls proc~~reordmeshnumbering~~CallsGraph proc~reordmeshnumbering reordMeshNumbering re_alloc re_alloc proc~reordmeshnumbering->re_alloc proc~boxintersection boxIntersection proc~reordmeshnumbering->proc~boxintersection de_alloc de_alloc proc~reordmeshnumbering->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reordMeshNumbering Source Code subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is defined. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > Minimize the number of communications. Data don't need to !! > be communicated if it belongs to the same process in !! > different data distributions !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters !     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) !     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo !     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 !       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo !       Save the permutation for this node (a negative number means that !       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI !       Change the permutation of those nodes who pretend to use the !       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering","tags":"","loc":"proc/reordmeshnumbering.html","title":"reordMeshNumbering – SIESTA"},{"text":"private subroutine reordMeshNumbering(distr1, distr2) Uses fdf proc~~reordmeshnumbering~2~~UsesGraph proc~reordmeshnumbering~2 reordMeshNumbering fdf fdf proc~reordmeshnumbering~2->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Warning This version of subroutine is called if\n the directive REORD1 is NOT defined. Here an integer parameter PROCS_PER_NODE is also\n used as an input. Value of PROCS_PER_NODE is either\n read from .fdf-file or set to 4 as default. Given a new distribution, distr2 , reasign each box to the proper\n process. We use the following criteria: Minimize the number of communications. Data don't need to\n  be communicated if it belongs to the same process in\n  different data distributions Distribute memory needs among different NODES (group of processes\n  that shares the same memory) The output values are stored in the current module: meshDistr (distr2)%box(:,:,:) Behavior Compute the size of all the boxes of the second distribution Reorder the list of boxes according to its size Create a list of buckets Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution Calls proc~~reordmeshnumbering~2~~CallsGraph proc~reordmeshnumbering~2 reordMeshNumbering myqsort myqsort proc~reordmeshnumbering~2->myqsort re_alloc re_alloc proc~reordmeshnumbering~2->re_alloc proc~boxintersection boxIntersection proc~reordmeshnumbering~2->proc~boxintersection de_alloc de_alloc proc~reordmeshnumbering~2->de_alloc fdf_get fdf_get proc~reordmeshnumbering~2->fdf_get Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reordMeshNumbering Source Code subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is defined. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > Minimize the number of communications. Data don't need to !! > be communicated if it belongs to the same process in !! > different data distributions !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters !     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) !     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo !     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 !       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo !       Save the permutation for this node (a negative number means that !       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI !       Change the permutation of those nodes who pretend to use the !       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering","tags":"","loc":"proc/reordmeshnumbering~2.html","title":"reordMeshNumbering – SIESTA"},{"text":"private subroutine compMeshComm(distr1, distr2, mcomm) Uses scheComm proc~~compmeshcomm~~UsesGraph proc~compmeshcomm compMeshComm scheComm scheComm proc~compmeshcomm->scheComm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Find the communications needed to transform one array that uses\n distribution distr1 to distribution distr2 Count the number of intersections between the source distribution\n and the destiny distribution. Every intersection represents a\n communication. Then we call scheduleComm to optimize the order of these\n communications. Finally, we save the communications that belongs to\n the current process in the variable mcomm Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 Source distribution type( meshDisType ), intent(in) :: distr2 Destination distribution type( meshCommType ), intent(out) :: mcomm Communications needed Calls proc~~compmeshcomm~~CallsGraph proc~compmeshcomm compMeshComm re_alloc re_alloc proc~compmeshcomm->re_alloc proc~boxintersection boxIntersection proc~compmeshcomm->proc~boxintersection de_alloc de_alloc proc~compmeshcomm->de_alloc schedulecomm schedulecomm proc~compmeshcomm->schedulecomm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compMeshComm Source Code subroutine compMeshComm ( distr1 , distr2 , mcomm ) !! Find the communications needed to transform one array that uses !! distribution `distr1` to distribution `distr2` !! !! Count the number of intersections between the source distribution !! and the destiny distribution. Every intersection represents a !! communication. Then we call [[scheduleComm(proc)]] !! to optimize the order of these !! communications. Finally, we save the communications that belongs to !! the current process in the variable `mcomm` use scheComm implicit none type ( meshDisType ), intent ( in ) :: distr1 !! Source distribution type ( meshDisType ), intent ( in ) :: distr2 !! Destination distribution type ( meshCommType ), intent ( out ) :: mcomm !! Communications needed !     Local variables integer :: P1 , P2 , ncom , Gcom , Lcom , & Lind , Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:) logical :: inters type ( COMM_T ) :: comm !     count the number of intersections between Source distribution and !     destiny distribution. Every intersection represents a communication. ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) ncom = ncom + 1 enddo enddo Gcom = ncom !     Allocate local arrays nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) !     Make a list of communications ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif enddo enddo comm % np = Nodes !     reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !     Count the number of communications of the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !     Allocate memory to store data of the communications of the !     current process. nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , 'moreMeshSubs' ) !     Save the list of communications for the current process ncom = 0 do P1 = 1 , comm % ncol Lind = comm % ind ( P1 , Node + 1 ) if ( Lind . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( Lind ) mcomm % dst ( ncom ) = dst ( Lind ) endif enddo mcomm % ncom = ncom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) end subroutine compMeshComm","tags":"","loc":"proc/compmeshcomm.html","title":"compMeshComm – SIESTA"},{"text":"public interface distMeshData Move data from vector fsrc , that uses distribution iDistr , to vector fdst , that uses distribution oDistr . It also re-orders a clustered\n data array into a sequential one and viceversa.\n If this is a sequencial execution, it only reorders the data. Note There are two subroutines: one to deal with real data and\n the other with integers. Both are called using the same interface. Note AG : Note that the integer version does NOT have the exact functionality\n of the real version. In particular, the integer version has no provision\n for a \"serial fallback\", and so this case has been trapped. Check the communications that this process should do to move data\n from iDistr to oDistr . We have 3 kind of communications (send, receive\n and keep on the same node). We have 3 kind of reorderings (clustered to\n sequential, sequential to clustered and keep the same ordering). For the sequencial code we call subroutine reord INPUT iDistr : Distribution index of the input vector. fsrc : Input vector. oDistr : Distribution index of the output vector. itr : TRanslation-direction switch: itr =+1 => From clustered to sequential itr =-1 => From sequential to clustered itr =0  => Keep the status OUTPUT fdst : Output vector. Calls interface~~distmeshdata~~CallsGraph interface~distmeshdata distMeshData proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~reord reord proc~distmeshdata_rea->proc~reord re_alloc re_alloc proc~distmeshdata_rea->re_alloc proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection timer timer proc~distmeshdata_rea->timer mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier de_alloc de_alloc proc~distmeshdata_rea->de_alloc write_debug write_debug proc~distmeshdata_rea->write_debug die die proc~distmeshdata_rea->die proc~distmeshdata_int->re_alloc proc~distmeshdata_int->proc~boxintersection proc~distmeshdata_int->de_alloc proc~distmeshdata_int->die proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by interface~~distmeshdata~~CalledByGraph interface~distmeshdata distMeshData proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures distMeshData_rea distMeshData_int Module Procedures private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr","tags":"","loc":"interface/distmeshdata.html","title":"distMeshData – SIESTA"},{"text":"public subroutine siesta_forces(istep) Uses units precision sys files siesta_cml m_state_init m_setup_hamiltonian m_setup_H0 m_compute_dm m_compute_max_diff m_scfconvergence_test m_post_scf_work m_mixer m_mixing_scf m_mixing_scf m_mixing_scf m_rhog siesta_options parallel m_state_analysis m_steps m_spin sparse_matrices sparse_matrices m_convergence m_convergence siesta_geom m_energies m_forces m_stress siesta_master siesta_master m_save_density_matrix m_iodm_old atomlist m_dm_charge m_pexsi_solver write_subs write_subs m_compute_energies m_mpi_utils fdf m_check_walltime m_energies m_ts_options m_ts_method m_ts_global_vars siesta_geom sparse_matrices sparse_matrices m_ts_charge m_ts_charge m_ts_charge m_ts_charge m_transiesta kpoint_scf_m m_energies m_initwf proc~~siesta_forces~~UsesGraph proc~siesta_forces siesta_forces files files proc~siesta_forces->files m_check_walltime m_check_walltime proc~siesta_forces->m_check_walltime m_spin m_spin proc~siesta_forces->m_spin write_subs write_subs proc~siesta_forces->write_subs m_ts_options m_ts_options proc~siesta_forces->m_ts_options siesta_cml siesta_cml proc~siesta_forces->siesta_cml m_energies m_energies proc~siesta_forces->m_energies module~m_setup_h0 m_setup_H0 proc~siesta_forces->module~m_setup_h0 kpoint_scf_m kpoint_scf_m proc~siesta_forces->kpoint_scf_m module~m_state_analysis m_state_analysis proc~siesta_forces->module~m_state_analysis m_steps m_steps proc~siesta_forces->m_steps m_mixer m_mixer proc~siesta_forces->m_mixer module~m_compute_max_diff m_compute_max_diff proc~siesta_forces->module~m_compute_max_diff siesta_geom siesta_geom proc~siesta_forces->siesta_geom module~m_state_init m_state_init proc~siesta_forces->module~m_state_init m_ts_method m_ts_method proc~siesta_forces->m_ts_method m_ts_charge m_ts_charge proc~siesta_forces->m_ts_charge m_save_density_matrix m_save_density_matrix proc~siesta_forces->m_save_density_matrix m_stress m_stress proc~siesta_forces->m_stress units units proc~siesta_forces->units m_mpi_utils m_mpi_utils proc~siesta_forces->m_mpi_utils siesta_options siesta_options proc~siesta_forces->siesta_options m_pexsi_solver m_pexsi_solver proc~siesta_forces->m_pexsi_solver m_dm_charge m_dm_charge proc~siesta_forces->m_dm_charge m_convergence m_convergence proc~siesta_forces->m_convergence m_ts_global_vars m_ts_global_vars proc~siesta_forces->m_ts_global_vars m_post_scf_work m_post_scf_work proc~siesta_forces->m_post_scf_work precision precision proc~siesta_forces->precision sys sys proc~siesta_forces->sys m_scfconvergence_test m_scfconvergence_test proc~siesta_forces->m_scfconvergence_test m_compute_energies m_compute_energies proc~siesta_forces->m_compute_energies m_rhog m_rhog proc~siesta_forces->m_rhog module~m_setup_hamiltonian m_setup_hamiltonian proc~siesta_forces->module~m_setup_hamiltonian siesta_master siesta_master proc~siesta_forces->siesta_master m_transiesta m_transiesta proc~siesta_forces->m_transiesta parallel parallel proc~siesta_forces->parallel m_iodm_old m_iodm_old proc~siesta_forces->m_iodm_old m_forces m_forces proc~siesta_forces->m_forces module~m_compute_dm m_compute_dm proc~siesta_forces->module~m_compute_dm fdf fdf proc~siesta_forces->fdf m_initwf m_initwf proc~siesta_forces->m_initwf sparse_matrices sparse_matrices proc~siesta_forces->sparse_matrices atomlist atomlist proc~siesta_forces->atomlist module~m_mixing_scf m_mixing_scf proc~siesta_forces->module~m_mixing_scf module~m_state_analysis->write_subs module~m_compute_max_diff->precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing_scf->class_Fstack_dData1D module~m_mixing m_mixing module~m_mixing_scf->module~m_mixing module~m_mixing->precision module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. This subroutine represents central SIESTA operation logic. Todo It might be better to split the two,\n  putting the grid initialization into state_init (link!) and moving the\n  calculation of H_0 to the body of the loop, done if first_scf=.true. This would suit analysis runs in which nscf = 0 The dHmax variable only has meaning for Hamiltonian\n  mixing, or when requiring the Hamiltonian to be converged. SCF loop The current structure of the loop tries to reproduce the\n  historical Siesta usage. It should be made more clear. Two changes: The number of scf iterations performed is exactly\n     equal to the number specified (i.e., the \"forces\"\n     phase is not counted as a final scf step) At the change to a TranSiesta GF run the variable \"first_scf\"\n     is implicitly reset to \"true\". Start of SCF cycle Conditions of exit: At the top, to catch a non-positive nscf and # of iterations At the bottom, based on convergence Note implications for TranSiesta when mixing H .\n  Now H will be recomputed instead of simply being\n  inherited, however, this is required as if\n  we have bias calculations as the electric\n  field across the junction needs to be present. end of SCF cycle Arguments Type Intent Optional Attributes Name integer, intent(inout) :: istep Calls proc~~siesta_forces~~CallsGraph proc~siesta_forces siesta_forces proc~state_init state_init proc~siesta_forces->proc~state_init scfconvergence_test scfconvergence_test proc~siesta_forces->scfconvergence_test compute_energies compute_energies proc~siesta_forces->compute_energies mix_rhog mix_rhog proc~siesta_forces->mix_rhog fdf_get fdf_get proc~siesta_forces->fdf_get message message proc~siesta_forces->message proc~mixers_scf_history_init mixers_scf_history_init proc~siesta_forces->proc~mixers_scf_history_init interface~compute_max_diff compute_max_diff proc~siesta_forces->interface~compute_max_diff write_spmatrix write_spmatrix proc~siesta_forces->write_spmatrix set_tolerance set_tolerance proc~siesta_forces->set_tolerance proc~mixing_scf_converged mixing_scf_converged proc~siesta_forces->proc~mixing_scf_converged save_density_matrix save_density_matrix proc~siesta_forces->save_density_matrix proc~setup_hamiltonian setup_hamiltonian proc~siesta_forces->proc~setup_hamiltonian bye bye proc~siesta_forces->bye reset reset proc~siesta_forces->reset check_walltime check_walltime proc~siesta_forces->check_walltime barrier barrier proc~siesta_forces->barrier dm_charge dm_charge proc~siesta_forces->dm_charge proc~compute_dm compute_dm proc~siesta_forces->proc~compute_dm compute_charge_diff compute_charge_diff proc~siesta_forces->compute_charge_diff proc~state_analysis state_analysis proc~siesta_forces->proc~state_analysis initwf initwf proc~siesta_forces->initwf post_scf_work post_scf_work proc~siesta_forces->post_scf_work mixer mixer proc~siesta_forces->mixer transiesta transiesta proc~siesta_forces->transiesta die die proc~siesta_forces->die timer timer proc~siesta_forces->timer proc~state_init->fdf_get proc~state_init->message proc~state_init->bye proc~state_init->die proc~state_init->timer volcel volcel proc~state_init->volcel mscell mscell proc~state_init->mscell re_alloc re_alloc proc~state_init->re_alloc newddata2d newddata2d proc~state_init->newddata2d superc superc proc~state_init->superc ioxv ioxv proc~state_init->ioxv setup_ts_kpoint_scf setup_ts_kpoint_scf proc~state_init->setup_ts_kpoint_scf setup_ordern_indexes setup_ordern_indexes proc~state_init->setup_ordern_indexes setup_dmhs_netcdf_file setup_dmhs_netcdf_file proc~state_init->setup_dmhs_netcdf_file new_dm new_dm proc~state_init->new_dm exact_sc_ag exact_sc_ag proc~state_init->exact_sc_ag cmladdproperty cmladdproperty proc~state_init->cmladdproperty sporb_to_spatom sporb_to_spatom proc~state_init->sporb_to_spatom newzspdata2d newzspdata2d proc~state_init->newzspdata2d siesta_write_positions siesta_write_positions proc~state_init->siesta_write_positions write_debug write_debug proc~state_init->write_debug domaindecom domaindecom proc~state_init->domaindecom globalize_or globalize_or proc~state_init->globalize_or crtsparsity_sc crtsparsity_sc proc~state_init->crtsparsity_sc proximity_check proximity_check proc~state_init->proximity_check overlap overlap proc~state_init->overlap cmlendpropertylist cmlendpropertylist proc~state_init->cmlendpropertylist newdspdata1d newdspdata1d proc~state_init->newdspdata1d newsparsity newsparsity proc~state_init->newsparsity kpoint_nullify kpoint_nullify proc~state_init->kpoint_nullify hsparse hsparse proc~state_init->hsparse outcoor outcoor proc~state_init->outcoor cmlstartpropertylist cmlstartpropertylist proc~state_init->cmlstartpropertylist isc_off isc_off proc~state_init->isc_off setup_dm_netcdf_file setup_dm_netcdf_file proc~state_init->setup_dm_netcdf_file nsc nsc proc~state_init->nsc proc~mixers_history_init mixers_history_init proc~state_init->proc~mixers_history_init chess_init chess_init proc~state_init->chess_init iotdxv iotdxv proc~state_init->iotdxv file_exist file_exist proc~state_init->file_exist val val proc~state_init->val attach attach proc~state_init->attach sp_to_spglobal sp_to_spglobal proc~state_init->sp_to_spglobal init_val init_val proc~state_init->init_val dict_repopulate_md dict_repopulate_md proc~state_init->dict_repopulate_md kpoint_delete kpoint_delete proc~state_init->kpoint_delete get_chess_parameter get_chess_parameter proc~state_init->get_chess_parameter ts_tri_analyze ts_tri_analyze proc~state_init->ts_tri_analyze ucell ucell proc~state_init->ucell nscold nscold proc~state_init->nscold delete delete proc~state_init->delete escf escf proc~state_init->escf time_io time_io proc~state_init->time_io proc~check_cohp check_cohp proc~state_init->proc~check_cohp listhptr listhptr proc~state_init->listhptr xij_offset xij_offset proc~state_init->xij_offset setup_kpoint_scf setup_kpoint_scf proc~state_init->setup_kpoint_scf ts_sparse_init ts_sparse_init proc~state_init->ts_sparse_init normalize_dm normalize_dm proc~state_init->normalize_dm globalize_sum globalize_sum proc~state_init->globalize_sum ts_write_tshs ts_write_tshs proc~state_init->ts_write_tshs newdspdata2d newdspdata2d proc~state_init->newdspdata2d madelung madelung proc~state_init->madelung numh numh proc~state_init->numh write_zmatrix write_zmatrix proc~state_init->write_zmatrix fname_tshs fname_tshs proc~state_init->fname_tshs proc~mixers_scf_history_init->proc~mixers_history_init proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff->proc~compute_max_diff_2d proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff->proc~compute_max_diff_1d proc~mixing_scf_converged->reset proc~setup_hamiltonian->bye proc~setup_hamiltonian->die proc~setup_hamiltonian->timer proc~setup_hamiltonian->re_alloc h h proc~setup_hamiltonian->h proc~dhscf dhscf proc~setup_hamiltonian->proc~dhscf write_hsx write_hsx proc~setup_hamiltonian->write_hsx hubbard_term hubbard_term proc~setup_hamiltonian->hubbard_term proc~setup_hamiltonian->val hold hold proc~setup_hamiltonian->hold de_alloc de_alloc proc~setup_hamiltonian->de_alloc dimag dimag proc~setup_hamiltonian->dimag update_e0 update_e0 proc~setup_hamiltonian->update_e0 proc~setup_hamiltonian->globalize_sum dscf dscf proc~setup_hamiltonian->dscf proc~compute_dm->bye proc~compute_dm->transiesta proc~compute_dm->timer zminim zminim proc~compute_dm->zminim dold dold proc~compute_dm->dold write_orb_indx write_orb_indx proc~compute_dm->write_orb_indx eold eold proc~compute_dm->eold dminim dminim proc~compute_dm->dminim diagon diagon proc~compute_dm->diagon proc~compute_dm->val write_dmh_netcdf write_dmh_netcdf proc~compute_dm->write_dmh_netcdf pexsi_solver pexsi_solver proc~compute_dm->pexsi_solver ordern ordern proc~compute_dm->ordern proc~compute_dm->escf proc~compute_dm->normalize_dm compute_ebs_shift compute_ebs_shift proc~compute_dm->compute_ebs_shift mpi_bcast mpi_bcast proc~compute_dm->mpi_bcast proc~compute_dm->dscf write_hs_formatted write_hs_formatted proc~compute_dm->write_hs_formatted chess_wrapper chess_wrapper proc~compute_dm->chess_wrapper proc~state_analysis->timer proc~state_analysis->volcel moments moments proc~state_analysis->moments cartesianforce_to_zmatforce cartesianforce_to_zmatforce proc~state_analysis->cartesianforce_to_zmatforce print_spin print_spin proc~state_analysis->print_spin siesta_write_stress_pressure siesta_write_stress_pressure proc~state_analysis->siesta_write_stress_pressure slua_call slua_call proc~state_analysis->slua_call update_freeeharris update_freeeharris proc~state_analysis->update_freeeharris update_freee update_freee proc~state_analysis->update_freee born_charge born_charge proc~state_analysis->born_charge proc~state_analysis->cmladdproperty mulliken mulliken proc~state_analysis->mulliken eggbox eggbox proc~state_analysis->eggbox cmlendmodule cmlendmodule proc~state_analysis->cmlendmodule proc~state_analysis->write_debug fixed fixed proc~state_analysis->fixed cmlstartmodule cmlstartmodule proc~state_analysis->cmlstartmodule amass amass proc~state_analysis->amass kin_stress kin_stress proc~state_analysis->kin_stress va va proc~state_analysis->va siesta_write_forces siesta_write_forces proc~state_analysis->siesta_write_forces wallclock wallclock proc~state_analysis->wallclock remove_intramol_pressure remove_intramol_pressure proc~state_analysis->remove_intramol_pressure proc~compute_max_diff_2d->die proc~dhscf->bye proc~dhscf->die proc~dhscf->timer proc~dhscf->volcel proc~dhscf->re_alloc proc~dhscf->write_debug proc~dhscf->de_alloc elecs elecs proc~dhscf->elecs ts_voltage ts_voltage proc~dhscf->ts_voltage dfscf dfscf proc~dhscf->dfscf ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofdsp rhoofdsp proc~dhscf->rhoofdsp get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp rhoofd rhoofd proc~dhscf->rhoofd hartree_add hartree_add proc~dhscf->hartree_add add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field proc~reord reord proc~dhscf->proc~reord compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho mpi_barrier mpi_barrier proc~dhscf->mpi_barrier write_rho write_rho proc~dhscf->write_rho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce forhar forhar proc~dhscf->forhar vacuum_level vacuum_level proc~dhscf->vacuum_level localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh vmat vmat proc~dhscf->vmat proc~mixers_history_init->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new proc~compute_max_diff_1d->die proc~check_cohp->message meshlim meshlim proc~setmeshdistr->meshlim proc~reord->timer proc~reord->re_alloc proc~reord->de_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->die proc~distmeshdata_rea->timer proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->write_debug proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->mpi_barrier mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->die proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~boxintersection var panprocsiesta_forcesCallsGraph = svgPanZoom('#procsiesta_forcesCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code siesta_forces Source Code subroutine siesta_forces ( istep ) !! This subroutine represents central SIESTA operation logic. #ifdef MPI use mpi_siesta #endif use units , only : eV , Ang use precision , only : dp use sys , only : bye use files , only : slabel use siesta_cml #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call use flook_siesta , only : LUA_INIT_MD , LUA_SCF_LOOP use siesta_dicts , only : dict_variable_add use m_ts_options , only : ts_scf_mixs use variable , only : cunpack #ifndef NCDF_4 use dictionary , only : assign #endif use m_mixing , only : mixers_history_init #endif use m_state_init use m_setup_hamiltonian use m_setup_H0 use m_compute_dm use m_compute_max_diff use m_scfconvergence_test use m_post_scf_work use m_mixer , only : mixer use m_mixing_scf , only : mixing_scf_converged use m_mixing_scf , only : mixers_scf_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_rhog , only : mix_rhog , compute_charge_diff use siesta_options use parallel , only : IOnode , SIESTA_worker use m_state_analysis use m_steps use m_spin , only : spin use sparse_matrices , only : DM_2D , S_1D use sparse_matrices , only : H , Hold , Dold , Dscf , Eold , Escf , maxnh use m_convergence , only : converger_t use m_convergence , only : reset , set_tolerance use siesta_geom , only : na_u ! Number of atoms in unit cell use m_energies , only : Etot ! Total energy use m_forces , only : fa , cfa ! Forces and constrained forces use m_stress , only : cstress ! Constrained stress tensor use siesta_master , only : forcesToMaster ! Send forces to master prog use siesta_master , only : siesta_server ! Is siesta a server? use m_save_density_matrix , only : save_density_matrix use m_iodm_old , only : write_spmatrix use atomlist , only : no_u , lasto , Qtot use m_dm_charge , only : dm_charge use m_pexsi_solver , only : prevDmax use write_subs , only : siesta_write_forces use write_subs , only : siesta_write_stress_pressure #ifdef NCDF_4 use dictionary use m_ncdf_siesta , only : cdf_init_file , cdf_save_settings use m_ncdf_siesta , only : cdf_save_state , cdf_save_basis #endif use m_compute_energies , only : compute_energies use m_mpi_utils , only : broadcast , barrier use fdf #ifdef SIESTA__PEXSI use m_pexsi , only : pexsi_finalize_scfloop #endif use m_check_walltime use m_energies , only : DE_NEGF use m_ts_options , only : N_Elec use m_ts_method use m_ts_global_vars , only : TSmode , TSinit , TSrun use siesta_geom , only : nsc , xa , ucell , isc_off use sparse_matrices , only : sparse_pattern , block_dist use sparse_matrices , only : S use m_ts_charge , only : ts_get_charges use m_ts_charge , only : TS_RHOCORR_METHOD use m_ts_charge , only : TS_RHOCORR_FERMI use m_ts_charge , only : TS_RHOCORR_FERMI_TOLERANCE use m_transiesta , only : transiesta use kpoint_scf_m , only : gamma_scf use m_energies , only : Ef use m_initwf , only : initwf integer , intent ( inout ) :: istep integer :: iscf logical :: first_scf , SCFconverged real ( dp ) :: dDmax ! Max. change in DM elements real ( dp ) :: dHmax ! Max. change in H elements real ( dp ) :: dEmax ! Max. change in EDM elements real ( dp ) :: drhog ! Max. change in rho(G) (experimental) real ( dp ), target :: G2max ! actually used meshcutoff type ( converger_t ) :: conv_harris , conv_freeE ! For initwf integer :: istpp #ifdef SIESTA__FLOOK ! len=24 from m_mixing.F90 character ( len = 1 ), target :: next_mixer ( 24 ) character ( len = 24 ) :: nnext_mixer integer :: imix #endif logical :: time_is_up character ( len = 40 ) :: tmp_str real ( dp ) :: Qcur #ifdef NCDF_4 type ( dict ) :: d_sav #endif #ifdef MPI integer :: MPIerror #endif external :: die , message #ifdef DEBUG call write_debug ( '    PRE siesta_forces' ) #endif #ifdef SIESTA__PEXSI ! Broadcast relevant things for program logic ! These were set in read_options, called only by \"SIESTA_workers\". call broadcast ( nscf , comm = true_MPI_Comm_World ) #endif !  Initialization tasks for a given geometry: if ( SIESTA_worker ) then call state_init ( istep ) end if #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_init\" ) #endif if ( fdf_get ( \"Sonly\" ,. false .) ) then if ( SIESTA_worker ) then call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) end if call bye ( \"S only\" ) end if Qcur = Qtot #ifdef SIESTA__FLOOK ! Add the iscf constant to the list of variables ! that are available only in this part of the routine. call dict_variable_add ( 'SCF.iteration' , iscf ) call dict_variable_add ( 'SCF.converged' , SCFconverged ) call dict_variable_add ( 'SCF.charge' , Qcur ) call dict_variable_add ( 'SCF.dD' , dDmax ) call dict_variable_add ( 'SCF.dH' , dHmax ) call dict_variable_add ( 'SCF.dE' , dEmax ) call dict_variable_add ( 'SCF.drhoG' , drhog ) ! We have to set the meshcutoff here ! because the asked and required ones are not ! necessarily the same call dict_variable_add ( 'Mesh.Cutoff.Minimum' , G2cut ) call dict_variable_add ( 'Mesh.Cutoff.Used' , G2max ) if ( mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , wmix ) else call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) ! Just to populate the table in the dictionary call dict_variable_add ( 'SCF.Mixer.Switch' , next_mixer ) end if ! Initialize to no switch next_mixer = ' ' #endif !  This call computes the **non-scf** part of  H  and initializes the !  real-space grid structures: if ( SIESTA_worker ) call setup_H0 ( G2max ) !!@todo !* It might be better to split the two, !  putting the grid initialization into **state_init (link!)** and moving the !  calculation of  H_0  to the body of the loop, done `if first_scf=.true.` !  This would suit _analysis_ runs in which **nscf = 0** !!@endtodo #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after setup_H0\" ) #endif #ifdef SIESTA__FLOOK ! Communicate with lua, just before entering the SCF loop ! This is mainly to be able to communicate ! mesh-related quantities (g2max) call slua_call ( LUA , LUA_INIT_MD ) #endif #ifdef NCDF_4 ! Initialize the NC file if ( write_cdf ) then ! Initialize the file... call cdf_init_file ( trim ( slabel ) // '.nc' , is_MD = . false .) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif ! Save the settings call cdf_save_settings ( trim ( slabel ) // '.nc' ) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif d_sav = ( 'sp' . kv . 1 ) // ( 'S' . kv . 1 ) d_sav = d_sav // ( 'nsc' . kv . 1 ) // ( 'xij' . kv . 1 ) d_sav = d_sav // ( 'xa' . kv . 1 ) // ( 'cell' . kv . 1 ) d_sav = d_sav // ( 'isc_off' . kv . 1 ) call cdf_save_state ( trim ( slabel ) // '.nc' , d_sav ) call delete ( d_sav ) ! Save the basis set call cdf_save_basis ( trim ( slabel ) // '.nc' ) end if #endif !* The dHmax variable only has meaning for Hamiltonian !  mixing, or when requiring the Hamiltonian to be converged. dDmax = - 1._dp dHmax = - 1._dp dEmax = - 1._dp drhog = - 1._dp ! Setup convergence criteria: if ( SIESTA_worker ) then if ( converge_Eharr ) then call reset ( conv_harris ) call set_tolerance ( conv_harris , tolerance_Eharr ) end if if ( converge_FreeE ) then call reset ( conv_FreeE ) call set_tolerance ( conv_FreeE , tolerance_FreeE ) end if end if !!# SCF loop !* The current structure of the loop tries to reproduce the !  historical Siesta usage. It should be made more clear. !* Two changes: ! !  1. The number of scf iterations performed is exactly !     equal to the number specified (i.e., the \"forces\" !     phase is not counted as a final scf step) !  2. At the change to a TranSiesta GF run the variable \"first_scf\" !     is implicitly reset to \"true\". ! !!## Start of SCF cycle ! !* Conditions of exit: ! !  * At the top, to catch a non-positive nscf and # of iterations !  * At the bottom, based on convergence ! iscf = 0 do while ( iscf < nscf ) iscf = iscf + 1 !* Note implications for TranSiesta when mixing H. !  Now H will be recomputed instead of simply being !  inherited, however, this is required as if !  we have bias calculations as the electric !  field across the junction needs to be present. first_scf = ( iscf == 1 ) if ( SIESTA_worker ) then ! Check whether we are short of time to continue call check_walltime ( time_is_up ) if ( time_is_up ) then ! Save DM/H if we were not saving it... !   Do any other bookeeping not done by \"die\" call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge' // & ' before wall time exhaustion' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call barrier () ! A non-root node might get first to the 'die' call call die ( \"OUT_OF_TIME: Time is up.\" ) end if call timer ( 'IterSCF' , 1 ) if ( cml_p ) & call cmlStartStep ( xf = mainXML , type = 'SCF' , index = iscf ) if ( mixH ) then if ( first_scf ) then if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then call get_H_from_file () else call setup_hamiltonian ( iscf ) end if end if call compute_DM ( iscf ) ! Maybe set Dold to zero if reading charge or H... call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) else call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) call compute_DM ( iscf ) call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) end if ! This iteration has completed calculating the new DM call compute_energies ( iscf ) if ( mix_charge ) then call compute_charge_diff ( drhog ) end if ! Note: For DM and H convergence checks. At this point: ! If mixing the DM: !        Dscf=DM_out, Dold=DM_in(mixed), H=H_in, Hold=H_in(prev step) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H_in - H_in(prev step)) ! If mixing the Hamiltonian: !        Dscf=DM_out, Dold=DM_in, H=H_(DM_out), Hold=H_in(mixed) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H(DM_out),H_in) call scfconvergence_test ( first_scf , iscf , & dDmax , dHmax , dEmax , & conv_harris , conv_freeE , & SCFconverged ) ! ** Check this heuristic if ( mixH ) then prevDmax = dHmax else prevDmax = dDmax end if ! Calculate current charge based on the density matrix call dm_charge ( spin , DM_2D , S_1D , Qcur ) ! Check whether we should step to the next mixer call mixing_scf_converged ( SCFconverged ) if ( SCFconverged . and . iscf < min_nscf ) then SCFconverged = . false . if ( IONode ) then write ( * , \"(a,i0)\" ) & \"SCF cycle continued for minimum number of iterations: \" , & min_nscf end if end if ! In case the user has requested a Fermi-level correction ! Then we start by correcting the fermi-level if ( TSrun . and . SCFconverged . and . & TS_RHOCORR_METHOD == TS_RHOCORR_FERMI ) then if ( abs ( Qcur - Qtot ) > TS_RHOCORR_FERMI_TOLERANCE ) then ! Call transiesta with fermi-correct call transiesta ( iscf , spin % H , & block_dist , sparse_pattern , Gamma_Scf , ucell , nsc , & isc_off , no_u , na_u , lasto , xa , maxnh , H , S , & Dscf , Escf , Ef , Qtot , . true ., DE_NEGF ) ! We will not have not converged as we have just ! changed the Fermi-level SCFconverged = . false . end if end if if ( monitor_forces_in_scf ) call compute_forces () ! Mix_after_convergence preserves the old behavior of ! the program. if ( (. not . SCFconverged ) . or . mix_after_convergence ) then ! Mix for next step if ( mix_charge ) then call mix_rhog ( iscf ) else call mixer ( iscf ) end if ! Save for possible restarts if ( mixH ) then call write_spmatrix ( H , file = \"H_MIXED\" , when = writeH ) call save_density_matrix ( file = \"DM_OUT\" , when = writeDM ) else call save_density_matrix ( file = \"DM_MIXED\" , when = writeDM ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = writeH ) end if end if call timer ( 'IterSCF' , 2 ) call print_timings ( first_scf , istep == inicoor ) if ( cml_p ) call cmlEndStep ( mainXML ) #ifdef SIESTA__FLOOK ! Communicate with lua call slua_call ( LUA , LUA_SCF_LOOP ) ! Retrieve an easy character string nnext_mixer = cunpack ( next_mixer ) if ( len_trim ( nnext_mixer ) > 0 . and . . not . mix_charge ) then if ( TSrun ) then do imix = 1 , size ( ts_scf_mixs ) if ( ts_scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( ts_scf_mixs ) scf_mix => ts_scf_mixs ( imix ) exit end if end do else do imix = 1 , size ( scf_mixs ) if ( scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( imix ) exit end if end do end if ! Check that we indeed have changed the mixer if ( IONode . and . scf_mix % name /= nnext_mixer ) then write ( * , '(2a)' ) 'siesta-lua: WARNING: trying to change ' , & 'to a non-existing mixer! Not changing anything!' else if ( IONode ) then write ( * , '(2a)' ) 'siesta-lua: Switching mixer method to: ' , & trim ( nnext_mixer ) end if ! Reset for next loop next_mixer = ' ' ! Update the references call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif ! ... except that we might continue for TranSiesta if ( SCFconverged ) then call transiesta_switch () ! might reset SCFconverged and iscf end if else ! non-siesta worker call compute_DM ( iscf ) end if #ifdef SIESTA__PEXSI call broadcast ( iscf , comm = true_MPI_Comm_World ) call broadcast ( SCFconverged , comm = true_MPI_Comm_World ) #endif !  Exit if converged: if ( SCFconverged ) exit end do !! **end of SCF cycle** #ifdef SIESTA__PEXSI if ( isolve == SOLVE_PEXSI ) then call pexsi_finalize_scfloop () end if #endif if ( . not . SIESTA_worker ) return call end_of_cycle_save_operations () if ( . not . SCFconverged ) then if ( SCFMustConverge ) then call message ( 'FATAL' , 'SCF_NOT_CONV: SCF did not converge' // & ' in maximum number of steps (required).' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call barrier () call die ( 'ABNORMAL_TERMINATION' ) else if ( . not . harrisfun ) then call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge  in maximum number of steps.' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) end if end if ! To write the initial wavefunctions to be used in a ! consequent TDDFT run. if ( writetdwf ) then istpp = 0 call initwf ( istpp , totime ) end if if ( TSmode . and . TSinit . and .(. not . SCFConverged ) ) then ! Signal that the DM hasn't converged, so we cannot ! continue to the transiesta routines call die ( 'ABNORMAL_TERMINATION' ) end if ! Clean-up here to limit memory usage call mixers_scf_history_init ( ) ! End of standard SCF loop. ! Do one more pass to compute forces and stresses ! Note that this call will no longer overwrite H while computing the ! final energies, forces and stresses... if ( fdf_get ( \"compute-forces\" ,. true .) ) then call post_scf_work ( istep , iscf , SCFconverged ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after post_scf_work\" ) #endif end if ! ... so H at this point is the latest generator of the DM, except ! if mixing H beyond self-consistency or terminating the scf loop ! without convergence while mixing H call state_analysis ( istep ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_analysis\" ) #endif ! If siesta is running as a subroutine, send forces to master program if ( siesta_server ) & call forcesToMaster ( na_u , Etot , cfa , cstress ) #ifdef DEBUG call write_debug ( '    POS siesta_forces' ) #endif contains ! Read the Hamiltonian from a file subroutine get_H_from_file () use sparse_matrices , only : maxnh , numh , listh , listhptr use atomlist , only : no_l use m_spin , only : spin use m_iodm_old , only : read_spmatrix logical :: found call read_spmatrix ( maxnh , no_l , spin % H , numh , & listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) end subroutine get_H_from_file ! Computes forces and stresses with the current DM_out subroutine compute_forces () use siesta_options , only : recompute_H_after_scf use m_final_H_f_stress , only : final_H_f_stress use write_subs real ( dp ), allocatable :: fa_old (:,:), Hsave (:,:) allocate ( fa_old ( size ( fa , dim = 1 ), size ( fa , dim = 2 ))) fa_old (:,:) = fa (:,:) if ( recompute_H_after_scf ) then allocate ( Hsave ( size ( H , dim = 1 ), size ( H , dim = 2 ))) Hsave (:,:) = H (:,:) end if call final_H_f_stress ( istep , iscf , . false . ) if ( recompute_H_after_scf ) then H (:,:) = Hsave (:,:) deallocate ( Hsave ) end if if ( ionode ) then print * , \"Max diff in force (eV/Ang): \" , & maxval ( abs ( fa - fa_old )) * Ang / eV call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () endif deallocate ( fa_old ) end subroutine compute_forces ! Print out timings of the first SCF loop only subroutine print_timings ( first_scf , first_md ) use timer_options , only : use_tree_timer use m_ts_global_vars , only : TSrun logical , intent ( in ) :: first_scf , first_md character ( len = 20 ) :: routine ! If this is not the first iteration, ! we immediately return. if ( . not . first_scf ) return if ( . not . first_md ) return routine = 'IterSCF' if ( TSrun ) then ! with Green function generation ! The tree-timer requires direct ! children of the routine to be ! queried. ! This is not obeyed in the TS case... :( if ( . not . use_tree_timer ) then routine = 'TS' end if endif call timer ( routine , 3 ) end subroutine print_timings ! Depending on various conditions, save the DMin ! or the DMout, and possibly keep a copy of H ! NOTE: Only if the scf cycle converged before exit it ! is guaranteed that the DM is \"pure out\" and that ! we can recover the right H if mixing H. ! subroutine end_of_cycle_save_operations () logical :: DM_write , H_write ! Depending on the option we should overwrite the ! Hamiltonian if ( mixH . and . . not . mix_after_convergence ) then ! Make sure that we keep the H actually used ! to generate the last DM, if needed. H = Hold end if DM_write = write_DM_at_end_of_cycle . and . & . not . writeDM H_write = write_H_at_end_of_cycle . and . & . not . writeH if ( mix_after_convergence ) then ! If we have been saving them, there is no point in doing ! it one more time if ( mixH ) then call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_MIXED\" , when = H_write ) else call save_density_matrix ( file = \"DM_MIXED\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if else call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if end subroutine end_of_cycle_save_operations subroutine transiesta_switch () use precision , only : dp use parallel , only : IONode use class_dSpData2D use class_Fstack_dData1D use densematrix , only : resetDenseMatrix use siesta_options , only : fire_mix , broyden_maxit use siesta_options , only : dDtol , dHtol use sparse_matrices , only : DM_2D , EDM_2D use atomlist , only : lasto use siesta_geom , only : nsc , isc_off , na_u , xa , ucell use m_energies , only : Ef use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mix , scf_mixs use m_rhog , only : resetRhoG use m_ts_global_vars , only : TSinit , TSrun use m_ts_global_vars , only : ts_print_transiesta use m_ts_method use m_ts_options , only : N_Elec , Elecs use m_ts_options , only : DM_bulk use m_ts_options , only : val_swap use m_ts_options , only : ts_Dtol , ts_Htol use m_ts_options , only : ts_hist_keep use m_ts_options , only : ts_siesta_stop use m_ts_options , only : ts_scf_mixs use m_ts_electype integer :: iEl , na_a integer , allocatable :: allowed_a (:) real ( dp ), pointer :: DM (:,:), EDM (:,:) ! We are done with the initial diagon run ! Now we start the TRANSIESTA (Green functions) run if ( . not . TSmode ) return if ( . not . TSinit ) return ! whether we are in siesta initialization step TSinit = . false . ! whether transiesta is running TSrun = . true . ! If transiesta should stop immediately if ( ts_siesta_stop ) then if ( IONode ) then write ( * , '(a)' ) 'ts: Stopping transiesta (user option)!' end if return end if ! Reduce memory requirements call resetDenseMatrix () ! Signal to continue... ! These two variables are from the top-level ! routine (siesta_forces) SCFconverged = . false . iscf = 0 ! DANGER (when/if going back to the DIAGON run, we should ! re-instantiate the original mixing value) call val_swap ( dDtol , ts_Dtol ) call val_swap ( dHtol , ts_Htol ) ! Clean up mixing history if ( mix_charge ) then call resetRhoG (. true .) else if ( associated ( ts_scf_mixs , target = scf_mixs ) ) then do iel = 1 , size ( scf_mix % stack ) call reset ( scf_mix % stack ( iel ), - ts_hist_keep ) ! Reset iteration count as certain ! mixing schemes require this for consistency scf_mix % cur_itt = n_items ( scf_mix % stack ( iel )) end do else call mixers_history_init ( scf_mixs ) end if end if ! Transfer scf_mixing to the transiesta mixing routine scf_mix => ts_scf_mixs ( 1 ) #ifdef SIESTA__FLOOK if ( . not . mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif call ts_print_transiesta () ! In case of transiesta and DM_bulk. ! In case we ask for initialization of the DM in bulk ! we read in the DM files from the electrodes and ! initialize the bulk to those values if ( DM_bulk > 0 ) then if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Initializing bulk DM in electrodes.' end if na_a = 0 do iEl = 1 , na_u if ( . not . a_isDev ( iEl ) ) na_a = na_a + 1 end do allocate ( allowed_a ( na_a )) na_a = 0 do iEl = 1 , na_u ! We allow the buffer atoms as well (this will even out the ! potential around the back of the electrode) if ( . not . a_isDev ( iEl ) ) then na_a = na_a + 1 allowed_a ( na_a ) = iEl end if end do do iEl = 1 , N_Elec if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Reading in electrode TSDE for ' // & trim ( Elecs ( iEl )% Name ) end if ! Copy over the DM in the lead ! Notice that the EDM matrix that is copied over ! will be equivalent at Ef == 0 call copy_DM ( Elecs ( iEl ), na_u , xa , lasto , nsc , isc_off , & ucell , DM_2D , EDM_2D , na_a , allowed_a ) end do ! Clean-up deallocate ( allowed_a ) if ( IONode ) then write ( * , * ) ! new-line end if ! The electrode EDM is aligned at Ef == 0 ! We need to align the energy matrix DM => val ( DM_2D ) EDM => val ( EDM_2D ) iEl = size ( DM ) call daxpy ( iEl , Ef , DM ( 1 , 1 ), 1 , EDM ( 1 , 1 ), 1 ) end if end subroutine transiesta_switch end subroutine siesta_forces","tags":"","loc":"proc/siesta_forces.html","title":"siesta_forces – SIESTA"},{"text":"public function mix_method(str) result(m) Uses fdf proc~~mix_method~~UsesGraph proc~mix_method mix_method fdf fdf proc~mix_method->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Return the integer specification of the mixing type @param[in] str the character representation of the mixing type\n @return the integer corresponding to the mixing type Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: str Return Value integer Calls proc~~mix_method~~CallsGraph proc~mix_method mix_method die die proc~mix_method->die leqi leqi proc~mix_method->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_method~~CalledByGraph proc~mix_method mix_method proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mix_method Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_method Source Code function mix_method ( str ) result ( m ) use fdf , only : leqi character ( len =* ), intent ( in ) :: str integer :: m if ( leqi ( str , 'linear' ) ) then m = MIX_LINEAR else if ( leqi ( str , 'pulay' ) . or . & leqi ( str , 'diis' ) . or . & leqi ( str , 'anderson' ) ) then m = MIX_PULAY else if ( leqi ( str , 'broyden' ) ) then m = MIX_BROYDEN else if ( leqi ( str , 'fire' ) ) then m = MIX_FIRE call die ( 'mixing: FIRE currently not supported.' ) else call die ( 'mixing: Unknown mixing variant.' ) end if end function mix_method","tags":"","loc":"proc/mix_method.html","title":"mix_method – SIESTA"},{"text":"public function mix_method_variant(m, str) result(v) Uses fdf proc~~mix_method_variant~~UsesGraph proc~mix_method_variant mix_method_variant fdf fdf proc~mix_method_variant->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Return the variant of the mixing method @param[in] m the integer type of the mixing method\n @param[in] str the character specification of the mixing method variant\n @return the variant of the mixing method Arguments Type Intent Optional Attributes Name integer, intent(in) :: m character(len=*), intent(in) :: str Return Value integer Calls proc~~mix_method_variant~~CallsGraph proc~mix_method_variant mix_method_variant leqi leqi proc~mix_method_variant->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_method_variant~~CalledByGraph proc~mix_method_variant mix_method_variant proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mix_method_variant Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_method_variant Source Code function mix_method_variant ( m , str ) result ( v ) use fdf , only : leqi integer , intent ( in ) :: m character ( len =* ), intent ( in ) :: str integer :: v v = 0 select case ( m ) case ( MIX_LINEAR ) ! no variants case ( MIX_PULAY ) v = 0 ! We do not implement tho non-stable version ! There is no need to have an inferior Pulay mixer... if ( leqi ( str , 'original' ) . or . & leqi ( str , 'kresse' ) . or . leqi ( str , 'stable' ) ) then ! stable version, will nearly always succeed on inversion v = 0 else if ( leqi ( str , 'original+svd' ) . or . & leqi ( str , 'kresse+svd' ) . or . leqi ( str , 'stable+svd' ) ) then ! stable version, will nearly always succeed on inversion v = 2 else if ( leqi ( str , 'gr' ) . or . & leqi ( str , 'guarenteed-reduction' ) . or . & leqi ( str , 'bowler-gillan' ) ) then ! Guarenteed reduction version v = 1 else if ( leqi ( str , 'gr+svd' ) . or . & leqi ( str , 'guarenteed-reduction+svd' ) . or . & leqi ( str , 'bowler-gillan+svd' ) ) then ! Guarenteed reduction version v = 3 end if case ( MIX_BROYDEN ) ! Currently only one variant v = 0 case ( MIX_FIRE ) ! no variants end select end function mix_method_variant","tags":"","loc":"proc/mix_method_variant.html","title":"mix_method_variant – SIESTA"},{"text":"private function mixing_ncoeff(mix) result(n) Function to retrieve the number of coefficients\n calculated in this iteration.\n This is so external routines can query the size\n of the arrays used. @param[in] mix the used mixer Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer Calls proc~~mixing_ncoeff~~CallsGraph proc~mixing_ncoeff mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_ncoeff~~CalledByGraph proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_ncoeff proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_coeff->proc~mixing_ncoeff interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_ncoeff Source Code function mixing_ncoeff ( mix ) result ( n ) type ( tMixer ), intent ( in ) :: mix integer :: n n = 0 select case ( mix % m ) case ( MIX_PULAY ) n = n_items ( mix % stack ( 2 )) case ( MIX_BROYDEN ) n = n_items ( mix % stack ( 2 )) end select end function mixing_ncoeff","tags":"","loc":"proc/mixing_ncoeff.html","title":"mixing_ncoeff – SIESTA"},{"text":"private function getstackval(mix, sidx, hidx) result(d1) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix integer, intent(in) :: sidx integer, intent(in), optional :: hidx Return Value real(kind=dp),\n  pointer,(:) Calls proc~~getstackval~~CallsGraph proc~getstackval getstackval get_pointer get_pointer proc~getstackval->get_pointer n_items n_items proc~getstackval->n_items val val proc~getstackval->val Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code getstackval Source Code function getstackval ( mix , sidx , hidx ) result ( d1 ) type ( tMixer ), intent ( in ) :: mix integer , intent ( in ) :: sidx integer , intent ( in ), optional :: hidx real ( dp ), pointer :: d1 (:) type ( dData1D ), pointer :: dD1 if ( present ( hidx ) ) then dD1 => get_pointer ( mix % stack ( sidx ), hidx ) else dD1 => get_pointer ( mix % stack ( sidx ), & n_items ( mix % stack ( sidx ))) end if d1 => val ( dD1 ) end function getstackval","tags":"","loc":"proc/getstackval.html","title":"getstackval – SIESTA"},{"text":"private function is_next(mix, method, next) result(bool) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in), target :: mix integer, intent(in) :: method type( tMixer ), optional pointer :: next Return Value logical Contents Source Code is_next Source Code function is_next ( mix , method , next ) result ( bool ) type ( tMixer ), intent ( in ), target :: mix integer , intent ( in ) :: method type ( tMixer ), pointer , optional :: next logical :: bool type ( tMixer ), pointer :: m bool = . false . m => mix % next do while ( associated ( m ) ) if ( m % m == MIX_LINEAR ) then m => m % next else if ( m % m == method ) then bool = . true . exit else ! Quit if it does not do anything exit end if ! this will prevent cyclic combinations if ( associated ( m , mix ) ) exit end do if ( present ( next ) ) then next => m end if end function is_next","tags":"","loc":"proc/is_next.html","title":"is_next – SIESTA"},{"text":"private function current_itt(mix) result(itt) Get current iteration count This is abstracted because the initial iteration\n and the current iteration may be uniquely defined. Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer Called by proc~~current_itt~~CalledByGraph proc~current_itt current_itt proc~mixing_init mixing_init proc~mixing_init->proc~current_itt proc~mixers_history_init mixers_history_init proc~mixers_history_init->proc~current_itt proc~mixing_finalize mixing_finalize proc~mixing_finalize->proc~current_itt proc~mixers_init mixers_init proc~mixers_init->proc~mixers_history_init proc~state_init state_init proc~state_init->proc~mixers_history_init proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init proc~mixing_1d->proc~mixing_finalize proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_scf_init->proc~mixers_init proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->proc~mixers_history_init interface~mixing mixing interface~mixing->proc~mixing_1d proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code current_itt Source Code function current_itt ( mix ) result ( itt ) type ( tMixer ), intent ( in ) :: mix integer :: itt itt = mix % cur_itt - mix % start_itt end function current_itt","tags":"","loc":"proc/current_itt.html","title":"current_itt – SIESTA"},{"text":"private function stack_check(stack, n) result(check) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: stack integer, intent(in) :: n Return Value logical Calls proc~~stack_check~~CallsGraph proc~stack_check stack_check get_pointer get_pointer proc~stack_check->get_pointer n_items n_items proc~stack_check->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~stack_check~~CalledByGraph proc~stack_check stack_check proc~push_stack_data push_stack_data proc~push_stack_data->proc~stack_check proc~push_f push_F proc~push_f->proc~stack_check proc~update_f update_F proc~update_f->proc~stack_check proc~update_f->proc~push_f Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code stack_check Source Code function stack_check ( stack , n ) result ( check ) type ( Fstack_dData1D ), intent ( inout ) :: stack integer , intent ( in ) :: n logical :: check ! Local arrays type ( dData1D ), pointer :: dD1 if ( n_items ( stack ) == 0 ) then check = . true . else ! Check that the stack stored arrays are ! of same size... dD1 => get_pointer ( stack , 1 ) check = n == size ( dD1 ) end if end function stack_check","tags":"","loc":"proc/stack_check.html","title":"stack_check – SIESTA"},{"text":"private function norm(n, x1, x2) Calculate the norm of two arrays Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: x1 (n) real(kind=dp), intent(in) :: x2 (n) Return Value real(kind=dp) Called by proc~~norm~~CalledByGraph proc~norm norm proc~mixing_init mixing_init proc~mixing_init->proc~norm proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code norm Source Code function norm ( n , x1 , x2 ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: x1 ( n ), x2 ( n ) real ( dp ) :: norm ! Currently we use an external routine integer :: i ! Calculate dot product norm = 0._dp !$OMP parallel do default(shared), private(i) & !$OMP& reduction(+:norm) do i = 1 , n norm = norm + x1 ( i ) * x2 ( i ) end do !$OMP end parallel do end function norm","tags":"","loc":"proc/norm.html","title":"norm – SIESTA"},{"text":"public subroutine mixers_init(prefix, mixers, Comm) Uses parallel fdf proc~~mixers_init~~UsesGraph proc~mixers_init mixers_init parallel parallel proc~mixers_init->parallel fdf fdf proc~mixers_init->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Initialize a set of mixers by reading in fdf information.\n @param[in] prefix the fdf-label prefixes\n @param[pointer] mixers the mixers that are to be initialized\n @param[in] Comm @opt optional MPI-communicator Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), pointer :: mixers (:) integer, intent(in), optional :: Comm Calls proc~~mixers_init~~CallsGraph proc~mixers_init mixers_init fdf_block fdf_block proc~mixers_init->fdf_block fdf_bnnames fdf_bnnames proc~mixers_init->fdf_bnnames die die proc~mixers_init->die proc~mixers_history_init mixers_history_init proc~mixers_init->proc~mixers_history_init fdf_bline fdf_bline proc~mixers_init->fdf_bline proc~mixers_reset mixers_reset proc~mixers_init->proc~mixers_reset fdf_bnames fdf_bnames proc~mixers_init->fdf_bnames fdf_get fdf_get proc~mixers_init->fdf_get fdf_brewind fdf_brewind proc~mixers_init->fdf_brewind new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt delete delete proc~mixers_history_init->delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_init~~CalledByGraph proc~mixers_init mixers_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_init Source Code subroutine mixers_init ( prefix , mixers , Comm ) use parallel , only : IONode , Node use fdf ! FDF-prefix for searching keywords character ( len =* ), intent ( in ) :: prefix ! The array of mixers (has to be nullified upon entry) type ( tMixer ), pointer :: mixers (:) integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf type ( parsed_line ), pointer :: pline ! number of history steps saved integer :: n_hist , n_restart , n_save real ( dp ) :: w integer :: nm , im , im2 character ( len = 10 ) :: lp character ( len = 70 ) :: method , variant ! Default mixing options... if ( fdf_get ( 'Mixer.Debug' ,. false .) ) then debug_mix = IONode debug_msg = 'mix:' end if if ( fdf_get ( 'Mixer.Debug.MPI' ,. false .) ) then debug_mix = . true . write ( debug_msg , '(a,i0,a)' ) 'mix (' , Node , '):' end if lp = trim ( prefix ) // '.Mixer' ! ensure nullification call mixers_reset ( mixers ) ! Return immediately if the user hasn't defined ! an fdf-block for the mixing options... if ( . not . fdf_block ( trim ( lp ) // 's' , bfdf ) ) return ! update mixing weight and kick mixing weight w = fdf_get ( trim ( lp ) // '.Weight' , 0.1_dp ) ! Get history length n_hist = fdf_get ( trim ( lp ) // '.History' , 6 ) ! Restart after this number of iterations n_restart = fdf_get ( trim ( lp ) // '.Restart' , 0 ) n_save = fdf_get ( trim ( lp ) // '.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Read in the options regarding the mixing options nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 end do if ( nm == 0 ) then call die ( 'mixing: No mixing schemes selected. & &Please at least add one mixer.' ) end if ! Allocate all denoted mixers... allocate ( mixers ( nm )) mixers (:)% w = w mixers (:)% n_hist = n_hist mixers (:)% restart = n_restart mixers (:)% restart_save = n_save ! Rewind to grab names. call fdf_brewind ( bfdf ) nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 mixers ( nm )% name = fdf_bnames ( pline , 1 ) end do ! Now read all mixers for this segment and their options do im = 1 , nm call read_block ( mixers ( im ) ) end do ! Create history stack and associate correct ! stack pointers call mixers_history_init ( mixers ) #ifdef MPI if ( present ( Comm ) ) then mixers (:)% Comm = Comm else mixers (:)% Comm = MPI_Comm_World end if #endif contains subroutine read_block ( m ) type ( tMixer ), intent ( inout ), target :: m character ( len = 64 ) :: opt ! create block string opt = trim ( lp ) // '.' // trim ( m % name ) if ( . not . fdf_block ( opt , bfdf ) ) then call die ( 'Block: ' // trim ( opt ) // ' does not exist!' ) end if ! Default to the pulay method... ! This enables NOT writing this in the block method = 'pulay' variant = ' ' ! read method do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'method' ) ) then method = fdf_bnames ( pline , 2 ) else if ( leqi ( opt , 'variant' ) ) then variant = fdf_bnames ( pline , 2 ) end if end do ! Retrieve the method and the variant m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! Define separate defaults which are ! not part of the default input options select case ( m % m ) case ( MIX_LINEAR ) m % n_hist = 0 end select call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'iterations' ) & . or . leqi ( opt , 'itt' ) ) then m % n_itt = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'history' ) ) then m % n_hist = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'weight' ) . or . leqi ( opt , 'w' ) ) then m % w = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'restart' ) ) then m % restart = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'restart.save' ) ) then m % restart_save = fdf_bintegers ( pline , 1 ) m % restart_save = max ( 0 , m % restart_save ) end if end do ! Initialize the mixer by setting the correct ! standard options and allocate space in the mixers... call mixer_init ( m ) ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'next' ) ) then nullify ( m % next ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next ) ) then call die ( 'mixing: Could not find next mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next , target = m ) ) then call die ( 'mixing: Next *must* not be it-self. & &Please change accordingly.' ) end if else if ( leqi ( opt , 'next.conv' ) ) then nullify ( m % next_conv ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next_conv => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next_conv ) ) then call die ( 'mixing: Could not find next convergence mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next_conv , target = m ) ) then call die ( 'mixing: next.conv *must* not be it-self. & &Please change accordingly.' ) end if end if end do ! Ensure that if a next have not been specified ! it will continue indefinitely. if ( . not . associated ( m % next ) ) then m % n_itt = 0 end if ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) ! skip lines without associated content if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) ! Do options so that a pulay option may refer to ! the actual names of the constants if ( m % m == MIX_PULAY ) then ! The linear mixing weight if ( leqi ( opt , 'weight.linear' ) & . or . leqi ( opt , 'w.linear' ) ) then m % rv ( 1 ) = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'svd.cond' ) ) then ! This is only applicable to the Pulay ! mixing scheme... m % rv ( I_SVD_COND ) = fdf_bvalues ( pline , 1 ) end if end if ! Generic options for all advanced methods... if ( leqi ( opt , 'next.p' ) ) then ! Only allow stepping to the next when ! having a next associated if ( associated ( m % next ) ) then m % rv ( I_P_NEXT ) = fdf_bvalues ( pline , 1 ) end if else if ( leqi ( opt , 'restart.p' ) ) then m % rv ( I_P_RESTART ) = fdf_bvalues ( pline , 1 ) end if end do end subroutine read_block end subroutine mixers_init","tags":"","loc":"proc/mixers_init.html","title":"mixers_init – SIESTA"},{"text":"public subroutine mixer_init(mix) Initialize a single mixer depending on the preset\n options. Useful for external correct setup. @param[inout] mix mixer to be initialized Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix Calls proc~~mixer_init~~CallsGraph proc~mixer_init mixer_init die die proc~mixer_init->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixer_init Source Code subroutine mixer_init ( mix ) type ( tMixer ), intent ( inout ) :: mix integer :: n ! Correct amount of history in the mixing. if ( 0 < mix % restart . and . & mix % restart < mix % n_hist ) then ! This is if we restart this scheme, ! then it does not make sense to have a history ! greater than the restart count mix % n_hist = mix % restart end if if ( 0 < mix % n_itt . and . & mix % n_itt < mix % n_hist ) then ! If this only runs for n_itt itterations, ! it makes no sense to have a history greater ! than this. mix % n_hist = mix % n_itt end if select case ( mix % m ) case ( MIX_LINEAR ) allocate ( mix % rv ( I_SVD_COND : 0 )) ! Kill any history settings that do not apply to the ! linear mixer. mix % restart = 0 mix % restart_save = 0 case ( MIX_PULAY ) allocate ( mix % rv ( I_SVD_COND : 1 )) mix % rv ( 1 ) = mix % w ! We allocate the double residual (n_hist-1) mix % n_hist = max ( 2 , mix % n_hist ) if ( mix % v == 1 . or . mix % v == 3 ) then ! The GR method requires an even number ! of restart steps ! And then we ensure the history to be aligned ! with a restart (restart has precedence) mix % restart = mix % restart + mod ( mix % restart , 2 ) end if case ( MIX_BROYDEN ) ! allocate temporary array mix % n_hist = max ( 2 , mix % n_hist ) n = 1 + mix % n_hist allocate ( mix % rv ( I_SVD_COND : n )) mix % rv ( 1 : n ) = mix % w end select if ( mix % restart < 0 ) then call die ( 'mixing: restart count must be positive' ) end if mix % restart_save = min ( mix % n_hist - 1 , mix % restart_save ) mix % restart_save = max ( 0 , mix % restart_save ) ! This is the restart parameter ! I.e. if |f_k / f - 1| < rp ! only works for positive rp mix % rv ( I_PREVIOUS_RES ) = huge ( 1._dp ) mix % rv ( I_P_RESTART ) = - 1._dp mix % rv ( I_P_NEXT ) = - 1._dp mix % rv ( I_SVD_COND ) = 1.e-8_dp end subroutine mixer_init","tags":"","loc":"proc/mixer_init.html","title":"mixer_init – SIESTA"},{"text":"public subroutine mixers_history_init(mixers) Initialize all history for the mixers Routine for clearing all history and setting up the\n arrays so that they may be used subsequently. @param[inout] mixers the mixers to be initialized Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout), target :: mixers (:) Calls proc~~mixers_history_init~~CallsGraph proc~mixers_history_init mixers_history_init new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt delete delete proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_history_init~~CalledByGraph proc~mixers_history_init mixers_history_init proc~state_init state_init proc~state_init->proc~mixers_history_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_init mixers_init proc~mixers_scf_init->proc~mixers_init proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->proc~mixers_history_init proc~mixers_init->proc~mixers_history_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_history_init Source Code subroutine mixers_history_init ( mixers ) type ( tMixer ), intent ( inout ), target :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns logical :: is_GR do im = 1 , size ( mixers ) m => mixers ( im ) if ( debug_mix . and . current_itt ( m ) >= 1 ) then write ( * , '(a,a)' ) trim ( debug_msg ), & ' resetting history of all mixers' exit end if end do ! Clean up all arrays and reference counted ! objects do im = 1 , size ( mixers ) m => mixers ( im ) ! reset history track m % start_itt = 0 m % cur_itt = 0 ! do not try and de-allocate something not ! allocated if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do ! clean-up deallocate ( m % stack ) end if ! Re-populate select case ( m % m ) case ( MIX_LINEAR ) ! do nothing case ( MIX_PULAY ) is_GR = ( m % v == 1 ) . or . ( m % v == 3 ) if ( . not . is_GR ) then allocate ( m % stack ( 3 )) else allocate ( m % stack ( 2 )) end if ! These arrays contains these informations !   s1 = m%stack(1) !   s2 = m%stack(2) !   s3 = m%stack(3) ! Here <> is input function, x[in], and ! <>' is the corresponding output, x[out]. ! First iteration: !   s1 = { 1' - 1 } !   s3 = { 1' } ! Second iteration !   s2 = { 2' - 2 - (1' - 1) } !   s1 = { 2 - 1 , 2' - 2 } !   s3 = { 2' } ! Third iteration !   s2 = { 2' - 2 - (1' - 1) , 3' - 3 - (2' - 2) } !   s1 = { 2 - 1 , 3 - 2, 3' - 3 } !   s3 = { 3' } ! and so on ! allocate x[i+1] - x[i] call new ( m % stack ( 1 ), m % n_hist ) ! allocate F[i+1] - F[i] call new ( m % stack ( 2 ), m % n_hist - 1 ) if ( . not . is_GR ) then call new ( m % stack ( 3 ), 1 ) end if case ( MIX_BROYDEN ) ! Same as original Pulay allocate ( m % stack ( 3 )) call new ( m % stack ( 1 ), m % n_hist ) call new ( m % stack ( 2 ), m % n_hist - 1 ) call new ( m % stack ( 3 ), 1 ) end select end do end subroutine mixers_history_init","tags":"","loc":"proc/mixers_history_init.html","title":"mixers_history_init – SIESTA"},{"text":"public subroutine mixers_reset(mixers) Reset the mixers, i.e. clean everything Also deallocates (and nullifies) the input array! @param[inout] mixers array of mixers to be cleaned Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mixers (:) Calls proc~~mixers_reset~~CallsGraph proc~mixers_reset mixers_reset delete delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_reset~~CalledByGraph proc~mixers_reset mixers_reset proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->proc~mixers_reset proc~mixers_init mixers_init proc~mixers_init->proc~mixers_reset proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_reset proc~mixers_scf_init->proc~mixers_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_reset Source Code subroutine mixers_reset ( mixers ) type ( tMixer ), pointer :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns if ( . not . associated ( mixers ) ) return do im = 1 , size ( mixers ) m => mixers ( im ) if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do deallocate ( m % stack ) end if if ( associated ( m % rv ) ) then deallocate ( m % rv ) nullify ( m % rv ) end if if ( associated ( m % iv ) ) then deallocate ( m % iv ) nullify ( m % iv ) end if end do deallocate ( mixers ) nullify ( mixers ) end subroutine mixers_reset","tags":"","loc":"proc/mixers_reset.html","title":"mixers_reset – SIESTA"},{"text":"public subroutine mixers_print(prefix, mixers) Uses parallel proc~~mixers_print~~UsesGraph proc~mixers_print mixers_print parallel parallel proc~mixers_print->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Print (to std-out) information regarding the mixers @param[in] prefix the prefix (fdf) for the mixers\n @param[in] mixers array of mixers allocated Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) Calls proc~~mixers_print~~CallsGraph proc~mixers_print mixers_print die die proc~mixers_print->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_print~~CalledByGraph proc~mixers_print mixers_print proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->proc~mixers_print Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_print Source Code subroutine mixers_print ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m character ( len = 50 ) :: fmt logical :: bool integer :: i if ( . not . IONode ) return fmt = 'mix.' // trim ( prefix ) // ':' if ( debug_mix ) then write ( * , '(2a,t50,''= '',l)' ) trim ( fmt ), & ' Debug messages' , debug_mix end if ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Linear mixing' , trim ( m % name ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w if ( m % n_hist > 0 . and . (& associated ( m % next ) & . or . associated ( m % next_conv )) ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Carried history steps' , m % n_hist end if case ( MIX_PULAY ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Pulay mixing' , trim ( m % name ) select case ( m % v ) case ( 0 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable' case ( 1 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR' case ( 2 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable-SVD' case ( 3 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR-SVD' end select write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Linear mixing weight' , m % rv ( 1 ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w write ( * , '(2a,t50,''= '',e10.4)' ) trim ( fmt ), & '    SVD condition' , m % rv ( I_SVD_COND ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_BROYDEN ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Broyden mixing' , trim ( m % name ) !write(*,'(2a,t50,''= '',a)') trim(fmt), & !     '    Variant','original' write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Jacobian weight' , m % w write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Weight prime' , m % rv ( 1 ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_FIRE ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Fire mixing' , trim ( m % name ) end select if ( m % n_itt > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Number of mixing iterations' , m % n_itt if ( associated ( m % next ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer' , trim ( m % next % name ) else call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if end if if ( associated ( m % next_conv ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer upon convergence' , trim ( m % next_conv % name ) end if end do end subroutine mixers_print","tags":"","loc":"proc/mixers_print.html","title":"mixers_print – SIESTA"},{"text":"public subroutine mixers_print_block(prefix, mixers) Uses parallel proc~~mixers_print_block~~UsesGraph proc~mixers_print_block mixers_print_block parallel parallel proc~mixers_print_block->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Print (to std-out) the fdf-blocks that recreate the mixer settings @param[in] prefix the fdf-prefix for reading the blocks\n @param[in] mixers array of mixers that should be printed\n    their fdf-blocks Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) Calls proc~~mixers_print_block~~CallsGraph proc~mixers_print_block mixers_print_block die die proc~mixers_print_block->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_print_block~~CalledByGraph proc~mixers_print_block mixers_print_block proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->proc~mixers_print_block Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_print_block Source Code subroutine mixers_print_block ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m logical :: bool integer :: i if ( . not . IONode ) return ! Write block of input write ( * , '(/3a)' ) '%block ' , trim ( prefix ), '.Mixers' do i = 1 , size ( mixers ) m => mixers ( i ) write ( * , '(t3,a)' ) trim ( m % name ) end do write ( * , '(3a)' ) '%endblock ' , trim ( prefix ), '.Mixers' ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) ! Write out this block write ( * , '(/4a)' ) '%block ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) write ( * , '(t3,a)' ) '# Mixing method' ! Write out method select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'linear' case ( MIX_PULAY ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'pulay' select case ( m % v ) case ( 0 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable' case ( 1 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR' case ( 2 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable+SVD' case ( 3 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR+SVD' end select case ( MIX_BROYDEN ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'broyden' ! currently no variants exists end select ! remark write ( * , '(/,t3,a)' ) '# Mixing options' ! Weight ! For Broyden this is the inverse Jacobian write ( * , '(t3,a,f6.4)' ) 'weight ' , m % w select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) write ( * , '(t3,a,f6.4)' ) 'weight.linear ' , m % rv ( 1 ) end select if ( m % n_hist > 0 ) then write ( * , '(t3,a,i0)' ) 'history ' , m % n_hist end if bool = . false . if ( m % restart > 0 ) then write ( * , '(t3,a,i0)' ) 'restart ' , m % restart bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(t3,a,e10.5)' ) 'restart.p ' , m % rv ( I_P_RESTART ) bool = . true . end if end select if ( bool ) then write ( * , '(t3,a,i0)' ) 'restart.save ' , m % restart_save end if ! remark bool = . false . if ( m % n_itt > 0 ) then write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,i0)' ) 'iterations ' , m % n_itt bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,f6.4)' ) 'next.p ' , m % rv ( I_P_NEXT ) bool = . true . end if end select if ( bool . and . associated ( m % next ) ) then write ( * , '(t2,2(tr1,a))' ) 'next' , trim ( m % next % name ) else if ( bool ) then call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if if ( associated ( m % next_conv ) ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t2,2(tr1,a))' ) 'next.conv' , trim ( m % next_conv % name ) end if write ( * , '(4a)' ) '%endblock ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) end do write ( * , * ) ! new-line end subroutine mixers_print_block","tags":"","loc":"proc/mixers_print_block.html","title":"mixers_print_block – SIESTA"},{"text":"private subroutine mixing_init(mix, n, xin, F) Initialize the mixing algorithm @param[pointer] mix the mixing method\n @param[in] n size of the arrays to be used in the algorithm\n @param[in] xin array of the input variables\n @param[in] xout array of the output variables Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) Calls proc~~mixing_init~~CallsGraph proc~mixing_init mixing_init proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_init~~CalledByGraph proc~mixing_init mixing_init proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_init Source Code subroutine mixing_init ( mix , n , xin , F ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n ! In/out of the function real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), pointer :: res (:), rres (:) integer :: i , ns real ( dp ) :: dnorm , dtmp logical :: p_next , p_restart ! Initialize action for mixer mix % action = ACTION_MIX ! Step iterator (so first mixing has cur_itt == 1) mix % cur_itt = mix % cur_itt + 1 ! If we are going to skip to next, we signal it ! before entering if ( mix % n_itt > 0 . and . & mix % n_itt <= current_itt ( mix ) ) then mix % action = IOR ( mix % action , ACTION_NEXT ) end if ! Check whether the residual norm is below a certain ! criteria p_next = mix % rv ( I_P_NEXT ) > 0._dp p_restart = mix % rv ( I_P_RESTART ) > 0._dp ! Check whether a parameter next/restart is required if ( p_restart . or . p_next ) then ! Calculate norm: ||f_k|| dnorm = norm ( n , F , F ) #ifdef MPI dtmp = dnorm call MPI_AllReduce ( dtmp , dnorm , 1 , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) #endif ! Calculate the relative difference dtmp = abs ( dnorm / mix % rv ( I_PREVIOUS_RES ) - 1._dp ) ! We first check for next, that has precedence if ( p_next ) then if ( dtmp < mix % rv ( I_P_NEXT ) ) then ! Signal stepping mixer mix % action = IOR ( mix % action , ACTION_NEXT ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < np  :  ' , & dtmp , ' < ' , mix % rv ( I_P_NEXT ) end if if ( p_restart ) then if ( dtmp < mix % rv ( I_P_RESTART ) ) then ! Signal restart mix % action = IOR ( mix % action , ACTION_RESTART ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < rp  :  ' , & dtmp , ' < ' , mix % rv ( I_P_RESTART ) end if ! Store the new residual norm mix % rv ( I_PREVIOUS_RES ) = dnorm end if ! Push information to the stack select case ( mix % m ) case ( MIX_LINEAR ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' linear' call init_linear () case ( MIX_PULAY ) if ( debug_mix ) then select case ( mix % v ) case ( 0 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay' case ( 1 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay, GR' case ( 2 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD' case ( 3 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD, GR' end select end if call init_pulay () case ( MIX_BROYDEN ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' Broyden' call init_broyden () end select contains subroutine init_linear () ! information for this depends on the ! following method call fake_history_from_linear ( mix % next ) call fake_history_from_linear ( mix % next_conv ) end subroutine init_linear subroutine init_pulay () logical :: GR_linear select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do end if case ( 1 , 3 ) ! Whether this is the linear cycle... GR_linear = mod ( current_itt ( mix ), 2 ) == 1 ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F , mix % rv ( 1 )) ns = n_items ( mix % stack ( 1 )) if ( GR_linear . and . current_itt ( mix ) > 1 . and . & ns > 1 ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = rres ( i ) + res ( i ) end do !$OMP end parallel do else if ( ns > 1 . and . . not . GR_linear ) then ! now we can calculate RRes[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) end if end select end subroutine init_pulay subroutine init_broyden () ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do else ! Store F[x_in] (used to create the input residual) call push_stack_data ( mix % stack ( 3 ), n ) res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if end subroutine init_broyden subroutine fake_history_from_linear ( next ) type ( tMixer ), pointer :: next real ( dp ), pointer :: t1 (:), t2 (:) integer :: ns , nh , i , nhl if ( . not . associated ( next ) ) return ! Reduce to # history of linear nhl = mix % n_hist ! if the number of fake-history steps saved is ! zero we immediately return. ! Only if mix%n_hist > 0 will the below ! occur. if ( nhl == 0 ) return ! Check for the type of following method select case ( next % m ) case ( MIX_PULAY ) ! Here it depends on the variant select case ( next % v ) case ( 0 , 2 ) ! stable pulay mixing ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select case ( MIX_BROYDEN ) ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns >= 2 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select end subroutine fake_history_from_linear end subroutine mixing_init","tags":"","loc":"proc/mixing_init.html","title":"mixing_init – SIESTA"},{"text":"private subroutine mixing_coeff(mix, n, xin, F, coeff) Uses parallel proc~~mixing_coeff~~UsesGraph proc~mixing_coeff mixing_coeff parallel parallel proc~mixing_coeff->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Calculate the mixing coefficients for the\n current mixer @param[in] mix the current mixer\n @param[in] n the number of elements used to calculate\n           the coefficients\n @param[in] xin the input value\n @param[in] F xout - xin, (residual)\n @param[out] coeff the coefficients Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: coeff (:) Calls proc~~mixing_coeff~~CallsGraph proc~mixing_coeff mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_coeff~~CalledByGraph proc~mixing_coeff mixing_coeff proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_coeff interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_coeff Source Code subroutine mixing_coeff ( mix , n , xin , F , coeff ) use parallel , only : IONode type ( tMixer ), intent ( inout ) :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), intent ( out ) :: coeff (:) integer :: ncoeff ncoeff = size ( coeff ) if ( ncoeff < mixing_ncoeff ( mix ) ) then write ( * , '(a)' ) 'mix: Error in calculating coefficients' ! Do not allow this... return end if select case ( mix % m ) case ( MIX_LINEAR ) call linear_coeff () case ( MIX_PULAY ) call pulay_coeff () case ( MIX_BROYDEN ) call broyden_coeff () end select contains subroutine linear_coeff () integer :: i do i = 1 , ncoeff coeff ( i ) = 0._dp end do end subroutine linear_coeff subroutine pulay_coeff () integer :: ns , nh , nmax integer :: i , j , info logical :: lreturn ! Calculation quantities real ( dp ) :: dnorm , G real ( dp ), pointer :: res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) lreturn = . false . ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) return ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! Allocate arrays for calculating the ! coefficients allocate ( A ( nh , nh ), Ainv ( nh , nh )) ! Calculate A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = norm(RRes[i],RRes[j]) A ( i , j ) = norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Diagonal A ( i , i ) = norm ( n , rres1 , rres1 ) end do #ifdef MPI ! Global operations, but only for the non-extended entries call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) ! copy over reduced arrays A = Ainv #endif ! Get inverse of matrix select case ( mix % v ) case ( 0 , 1 ) call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if case ( 2 , 3 ) ! We forcefully use the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end select ! NOTE, although mix%stack(1) contains ! the x[i] - x[i-1], the tip of the stack ! contains F[i]! ! res == F[i] res => getstackval ( mix , 1 ) ! Initialize the coefficients do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients on all processors do j = 1 , nh ! res  == F[i] ! rres == F[j+1] - F[j] rres => getstackval ( mix , 2 , j ) dnorm = norm ( n , rres , res ) do i = 1 , nh coeff ( i ) = coeff ( i ) - Ainv ( i , j ) * dnorm end do end do #ifdef MPI ! Reduce the coefficients call MPI_AllReduce ( coeff ( 1 ), A ( 1 , 1 ), nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh coeff ( i ) = A ( i , 1 ) end do #endif else info = 0 ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, SVD failed, > linear' end if ! Clean up memory deallocate ( A , Ainv ) end subroutine pulay_coeff subroutine broyden_coeff () integer :: ns , nh , nmax integer :: i , j , k , info ! Calculation quantities real ( dp ) :: dnorm , dtmp real ( dp ), pointer :: w (:), res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: c (:), A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) ! Easy check for initial step... if ( ns == 1 ) then ! reset coeff = 0._dp return end if ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! This is the modified Broyden algorithm... ! Retrieve the previous weights w => mix % rv ( 2 : 1 + nh ) select case ( mix % v ) case ( 2 ) ! Unity Broyden w ( nh ) = 1._dp case ( 1 ) ! RMS Broyden dnorm = norm ( n , F , F ) #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif w (:) = 1._dp / sqrt ( dnorm ) if ( debug_mix ) & write ( * , '(2(a,e10.4))' ) & trim ( debug_msg ) // ' weight = ' , w ( 1 ), & ' , norm = ' , dnorm case ( 0 ) ! Varying weight dnorm = 0._dp !$OMP parallel do default(shared), private(i), & !$OMP& reduction(max:dnorm) do i = 1 , n dnorm = max ( dnorm , abs ( F ( i )) ) end do !$OMP end parallel do #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif ! Problay 0.2 should be changed to user-defined w ( nh ) = exp ( 1._dp / ( dnorm + 0.2_dp ) ) if ( debug_mix ) & write ( * , '(2a,1000(tr1,e10.4))' ) & trim ( debug_msg ), ' weights = ' , w ( 1 : nh ) end select ! Allocate arrays used allocate ( c ( nh )) allocate ( A ( nh , nh ), Ainv ( nh , nh )) !  < RRes[i] | Res[n] > do i = 1 , nh rres => getstackval ( mix , 2 , i ) c ( i ) = norm ( n , rres , F ) end do #ifdef MPI call MPI_AllReduce ( c ( 1 ), A ( 1 , 1 ), nh , & MPI_Double_Precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh c ( i ) = A ( i , 1 ) end do #endif ! Create A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = dot_product(RRes[i],RRes[j]) A ( i , j ) = w ( i ) * w ( j ) * norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Do the diagonal term A ( i , i ) = w ( i ) * w ( i ) * norm ( n , rres1 , rres1 ) end do #ifdef MPI call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) A = Ainv #endif ! Add the diagonal term ! This should also prevent it from being ! singular (unless mix%w == 0) do i = 1 , nh A ( i , i ) = mix % rv ( 1 ) ** 2 + A ( i , i ) end do ! Calculate the inverse call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients... do i = 1 , nh do j = 1 , nh ! Ainv should be symmetric (A is) coeff ( i ) = coeff ( i ) + w ( j ) * c ( j ) * Ainv ( j , i ) end do ! Calculate correct weight... coeff ( i ) = - w ( i ) * coeff ( i ) end do else ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, SVD failed, > linear' end if deallocate ( A , Ainv ) end subroutine broyden_coeff end subroutine mixing_coeff","tags":"","loc":"proc/mixing_coeff.html","title":"mixing_coeff – SIESTA"},{"text":"private subroutine mixing_calc_next(mix, n, xin, F, xnext, coeff) Calculate the guess for the next iteration Note this gets passed the coefficients. Hence,\n they may be calculated from another set of history\n steps.\n This may be useful in certain situations. @param[in] mix the current mixer\n @param[in] n the number of elements used to calculate\n           the coefficients\n @param[in] xin the input value\n @param[in] F the xin residual\n @param[out] xnext the input for the following iteration\n @param[in] coeff the coefficients Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: xnext (n) real(kind=dp), intent(in) :: coeff (:) Called by proc~~mixing_calc_next~~CalledByGraph proc~mixing_calc_next mixing_calc_next proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_calc_next interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_calc_next Source Code subroutine mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ) real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( out ) :: xnext ( n ) real ( dp ), intent ( in ) :: coeff (:) select case ( mix % m ) case ( MIX_LINEAR ) call mixing_linear () case ( MIX_PULAY ) call mixing_pulay () case ( MIX_BROYDEN ) call mixing_broyden () end select contains subroutine mixing_linear () integer :: i real ( dp ) :: w w = mix % w if ( debug_mix ) write ( * , '(2a,e10.4)' ) & trim ( debug_msg ), ' alpha = ' , w !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + w * F ( i ) end do !$OMP end parallel do end subroutine mixing_linear subroutine mixing_pulay () integer :: ns , nh integer :: i , j logical :: lreturn real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Pulay' xnext = 0._dp return end if ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Pulay (initial), alpha = ' , mix % rv ( 1 ) case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Direct mixing, alpha = ' , mix % rv ( 1 ) end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) then ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ),& ' G = ' , G , ', sum(alpha) = ' , sum ( coeff ), & ', alpha = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_pulay subroutine mixing_broyden () integer :: ns , nh integer :: i , j real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Broyden' xnext = 0._dp return end if if ( ns == 1 ) then if ( debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Broyden (initial), alpha = ' , mix % rv ( 1 ) ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' G = ' , G , & ', sum(coeff) = ' , sum ( coeff ), & ', coeff = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_broyden end subroutine mixing_calc_next","tags":"","loc":"proc/mixing_calc_next.html","title":"mixing_calc_next – SIESTA"},{"text":"private subroutine mixing_finalize(mix, n, xin, F, xnext) Uses parallel proc~~mixing_finalize~~UsesGraph proc~mixing_finalize mixing_finalize parallel parallel proc~mixing_finalize->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finalize the mixing algorithm @param[inout] mix mixer to be finalized\n @param[in] n size of the input arrays\n @param[in] xin the input for this iteration\n @param[in] F the residual for this iteration\n @param[in] xnext the optimized input for the next iteration Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in) :: xnext (n) Calls proc~~mixing_finalize~~CallsGraph proc~mixing_finalize mixing_finalize reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step n_items n_items proc~mixing_finalize->n_items proc~current_itt current_itt proc~mixing_finalize->proc~current_itt proc~mixing_step->reset proc~mixing_step->n_items push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_finalize~~CalledByGraph proc~mixing_finalize mixing_finalize proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_finalize Source Code subroutine mixing_finalize ( mix , n , xin , F , xnext ) use parallel , only : IONode type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ), xnext ( n ) integer :: rsave select case ( mix % m ) case ( MIX_LINEAR ) call fin_linear () case ( MIX_PULAY ) call fin_pulay () case ( MIX_BROYDEN ) call fin_broyden () end select ! Fix the action to finalize it.. if ( mix % restart > 0 . and . & mod ( current_itt ( mix ), mix % restart ) == 0 ) then mix % action = IOR ( mix % action , ACTION_RESTART ) end if ! Check the actual finalization... ! First check whether we should restart history if ( IAND ( mix % action , ACTION_RESTART ) == ACTION_RESTART ) then ! The user has requested to restart the ! mixing scheme now rsave = mix % restart_save select case ( mix % m ) case ( MIX_PULAY ) if ( IONode ) then write ( * , '(a)' ) 'mix: Pulay -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if case ( MIX_BROYDEN ) if ( IONode ) then write ( * , '(a)' ) 'mix: Broyden -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if end select if ( allocated ( mix % stack ) ) then if ( debug_mix ) & write ( * , '(a,a,i0)' ) trim ( debug_msg ), & ' saved hist = ' , n_items ( mix % stack ( 1 )) end if end if ! check whether we should change the mixer if ( IAND ( mix % action , ACTION_NEXT ) == ACTION_NEXT ) then call mixing_step ( mix ) end if contains subroutine fin_linear () ! do nothing... end subroutine fin_linear subroutine fin_pulay () integer :: ns integer :: i logical :: GR_linear real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) select case ( mix % v ) case ( 0 , 2 ) ! stable Pulay if ( n_items ( mix % stack ( 3 )) == 0 ) then call push_stack_data ( mix % stack ( 3 ), n ) end if res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do case ( 1 , 3 ) ! GR Pulay GR_linear = mod ( current_itt ( mix ), 2 ) == 1 if ( n_items ( mix % stack ( 2 )) > 0 . and . & . not . GR_linear ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  rres == F[i] - F[i-1] rres ( i ) = rres ( i ) - res ( i ) ! Output: !  rres == - F[i-1] end do !$OMP end parallel do call pop ( mix % stack ( 1 )) ! Note that this is Res[i-1] = (F&#94;i-1_out - F&#94;i-1_in) res => getstackval ( mix , 1 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - xin ( i ) + xnext ( i ) end do !$OMP end parallel do end if end select end subroutine fin_pulay subroutine fin_broyden () integer :: ns , nh integer :: i real ( dp ), pointer :: res (:), rres (:) ns = current_itt ( mix ) nh = n_items ( mix % stack ( 2 )) if ( ns >= 2 . and . n_items ( mix % stack ( 3 )) > 0 ) then ! Update the residual to reflect the input residual res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do end if ! Update weights (if necessary) if ( nh > 1 ) then do i = 2 , nh mix % rv ( i ) = mix % rv ( i + 1 ) end do end if end subroutine fin_broyden end subroutine mixing_finalize","tags":"","loc":"proc/mixing_finalize.html","title":"mixing_finalize – SIESTA"},{"text":"private subroutine mixing_1d(mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub Calls proc~~mixing_1d~~CallsGraph proc~mixing_1d mixing_1d proc~mixing_calc_next mixing_calc_next proc~mixing_1d->proc~mixing_calc_next proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d->proc~mixing_ncoeff proc~mixing_init mixing_init proc~mixing_1d->proc~mixing_init proc~mixing_finalize mixing_finalize proc~mixing_1d->proc~mixing_finalize proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step proc~mixing_finalize->n_items proc~mixing_finalize->proc~current_itt proc~mixing_step->reset proc~mixing_step->n_items push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_1d~~CalledByGraph proc~mixing_1d mixing_1d interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_1d Source Code subroutine mixing_1d ( mix , n , xin , F , xnext , nsub ) ! The current mixing method type ( tMixer ), pointer :: mix ! The current step in the SCF and size of arrays integer , intent ( in ) :: n ! x1 == Input function, ! F1 == Residual from x1 real ( dp ), intent ( in ) :: xin ( n ), F ( n ) ! x2 == Next input function real ( dp ), intent ( inout ) :: xnext ( n ) ! Number of elements used for calculating the mixing ! coefficients integer , intent ( in ), optional :: nsub ! Coefficients integer :: ncoeff real ( dp ), allocatable :: coeff (:) call mixing_init ( mix , n , xin , F ) ncoeff = mixing_ncoeff ( mix ) allocate ( coeff ( ncoeff )) ! Calculate coefficients if ( present ( nsub ) ) then call mixing_coeff ( mix , nsub , xin , F , coeff ) else call mixing_coeff ( mix , n , xin , F , coeff ) end if ! Calculate the following output call mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! Coefficients are not needed anymore... deallocate ( coeff ) ! Finalize the mixer call mixing_finalize ( mix , n , xin , F , xnext ) end subroutine mixing_1d","tags":"","loc":"proc/mixing_1d.html","title":"mixing_1d – SIESTA"},{"text":"private subroutine mixing_2d(mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub Called by proc~~mixing_2d~~CalledByGraph proc~mixing_2d mixing_2d interface~mixing mixing interface~mixing->proc~mixing_2d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_2d Source Code subroutine mixing_2d ( mix , n1 , n2 , xin , F , xnext , nsub ) type ( tMixer ), pointer :: mix integer , intent ( in ) :: n1 , n2 real ( dp ), intent ( in ) :: xin ( n1 , n2 ), F ( n1 , n2 ) real ( dp ), intent ( inout ) :: xnext ( n1 , n2 ) integer , intent ( in ), optional :: nsub ! Simple wrapper for 1D if ( present ( nsub ) ) then call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 ) ,& nsub = n1 * nsub ) else call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 )) end if end subroutine mixing_2d","tags":"","loc":"proc/mixing_2d.html","title":"mixing_2d – SIESTA"},{"text":"private subroutine mixing_step(mix) Uses parallel proc~~mixing_step~~UsesGraph proc~mixing_step mixing_step parallel parallel proc~mixing_step->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix Calls proc~~mixing_step~~CallsGraph proc~mixing_step mixing_step push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer n_items n_items proc~mixing_step->n_items reset reset proc~mixing_step->reset Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_step~~CalledByGraph proc~mixing_step mixing_step proc~mixing_finalize mixing_finalize proc~mixing_finalize->proc~mixing_step proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_step Source Code subroutine mixing_step ( mix ) use parallel , only : IONode type ( tMixer ), pointer :: mix type ( tMixer ), pointer :: next => null () type ( dData1D ), pointer :: d1D integer :: i , is , n , init_itt logical :: reset_stack , copy_stack ! First try and next => mix % next if ( associated ( next ) ) then ! Whether or not the two methods are allowed ! to share history copy_stack = mix % m == next % m select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) copy_stack = . true . end select end select copy_stack = copy_stack . and . allocated ( mix % stack ) ! If the two methods are similar if ( copy_stack ) then ! They are similar, copy over the history stack do is = 1 , size ( mix % stack ) ! Get maximum size of the current stack, n = n_items ( mix % stack ( is )) ! Note that this will automatically take care of ! wrap-arounds and delete the unneccesry elements do i = 1 , n d1D => get_pointer ( mix % stack ( is ), i ) call push ( next % stack ( is ), d1D ) end do ! nullify nullify ( d1D ) end do end if end if reset_stack = . true . if ( associated ( next ) ) then if ( associated ( next % next , mix ) . and . & next % n_itt > 0 ) then ! if this is a circular mixing routine ! we should not reset the history... reset_stack = . false . end if end if if ( reset_stack ) then select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) n = size ( mix % stack ) do is = 1 , n call reset ( mix % stack ( is )) end do end select end if if ( associated ( next ) ) then init_itt = 0 ! Set-up the next mixer select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) init_itt = n_items ( next % stack ( 1 )) end select next % start_itt = init_itt next % cur_itt = init_itt if ( IONode ) then write ( * , '(3a)' ) trim ( debug_msg ), ' switching mixer --> ' , & trim ( next % name ) end if mix => mix % next end if end subroutine mixing_step","tags":"","loc":"proc/mixing_step.html","title":"mixing_step – SIESTA"},{"text":"private subroutine inverse(n, A, B, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) integer, intent(out) :: info Contents Source Code inverse Source Code subroutine inverse ( n , A , B , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) integer , intent ( out ) :: info integer :: i , j ! Local arrays real ( dp ) :: pm ( n , n ), work ( n * 4 ), err ! Relative tolerance dependent on the magnitude ! For now we retain the old tolerance real ( dp ), parameter :: etol = 1.e-4_dp integer :: ipiv ( n ) ! initialize info info = 0 ! simple check and fast return if ( n == 1 ) then B ( 1 , 1 ) = 1._dp / A ( 1 , 1 ) return end if call lapack_inv () if ( info /= 0 ) call simple_inv () contains subroutine lapack_inv () B = A call dgetrf ( n , n , B , n , ipiv , info ) if ( info /= 0 ) return call dgetri ( n , B , n , ipiv , work , n * 4 , info ) if ( info /= 0 ) return ! This sets info appropriately call check_inv () end subroutine lapack_inv subroutine simple_inv () real ( dp ) :: x integer :: k ! Copy over A B = A do i = 1 , n if ( B ( i , i ) == 0._dp ) then info = - n return end if x = 1._dp / B ( i , i ) B ( i , i ) = 1._dp do j = 1 , n B ( j , i ) = B ( j , i ) * x end do do k = 1 , n if ( ( k - i ) /= 0 ) then x = B ( i , k ) B ( i , k ) = 0._dp do j = 1 , n B ( j , k ) = B ( j , k ) - B ( j , i ) * x end do end if end do end do ! This sets info appropriately call check_inv () end subroutine simple_inv subroutine check_inv () ! Check correcteness pm = matmul ( A , B ) do j = 1 , n do i = 1 , n if ( i == j ) then err = pm ( i , j ) - 1._dp else err = pm ( i , j ) end if ! This is pretty strict tolerance! if ( abs ( err ) > etol ) then ! Signal failure in inversion info = - n - 1 return end if end do end do end subroutine check_inv end subroutine inverse","tags":"","loc":"proc/inverse.html","title":"inverse – SIESTA"},{"text":"private subroutine svd(n, A, B, cond, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) real(kind=dp), intent(in) :: cond integer, intent(out) :: info Calls proc~~svd~~CallsGraph proc~svd svd dgelss dgelss proc~svd->dgelss Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code svd Source Code subroutine svd ( n , A , B , cond , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) real ( dp ), intent ( in ) :: cond integer , intent ( out ) :: info ! Local arrays integer :: rank , i character ( len = 50 ) :: fmt real ( dp ) :: AA ( n , n ), S ( n ), work ( n * 5 ) ! Copy A matrix AA = A ! setup pseudo inverse solution for minimizing ! constraints B = 0._dp do i = 1 , n B ( i , i ) = 1._dp end do call dgelss ( n , n , n , AA , n , B , n , S , cond , rank , work , n * 5 , info ) ! if debugging print out the different variables if ( debug_mix ) then ! also mark the rank if ( rank == n ) then ! complete rank write ( * , '(2a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' SVD singular = ' , S else ! this prints the location of the SVD rank, if not full write ( fmt , '(i0,2a)' ) rank , '(tr1,e10.4),'' >'',100(tr1,e10.4)' write ( * , '(2a,' // trim ( fmt ) // ')' ) & trim ( debug_msg ), ' SVD singular = ' , S end if end if end subroutine svd","tags":"","loc":"proc/svd.html","title":"svd – SIESTA"},{"text":"private subroutine push_stack_data(s_F, n) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n Calls proc~~push_stack_data~~CallsGraph proc~push_stack_data push_stack_data push push proc~push_stack_data->push die die proc~push_stack_data->die newddata1d newddata1d proc~push_stack_data->newddata1d proc~stack_check stack_check proc~push_stack_data->proc~stack_check delete delete proc~push_stack_data->delete get_pointer get_pointer proc~stack_check->get_pointer n_items n_items proc~stack_check->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_stack_data Source Code subroutine push_stack_data ( s_F , n ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n type ( dData1D ) :: dD1 if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if call newdData1D ( dD1 , n , '(F)' ) ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_stack_data","tags":"","loc":"proc/push_stack_data.html","title":"push_stack_data – SIESTA"},{"text":"private subroutine push_F(s_F, n, F, fact) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in), optional :: fact Calls proc~~push_f~~CallsGraph proc~push_f push_F proc~stack_check stack_check proc~push_f->proc~stack_check val val proc~push_f->val get get proc~push_f->get die die proc~push_f->die newddata1d newddata1d proc~push_f->newddata1d max_size max_size proc~push_f->max_size push push proc~push_f->push n_items n_items proc~push_f->n_items dcopy dcopy proc~push_f->dcopy delete delete proc~push_f->delete proc~stack_check->n_items get_pointer get_pointer proc~stack_check->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~push_f~~CalledByGraph proc~push_f push_F proc~update_f update_F proc~update_f->proc~push_f Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_F Source Code subroutine push_F ( s_F , n , F , fact ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( in ), optional :: fact type ( dData1D ) :: dD1 real ( dp ), pointer :: sF (:) integer :: in , ns integer :: i if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) ns = max_size ( s_F ) if ( in == ns ) then ! we have to cycle the storage call get ( s_F , 1 , dD1 ) else call newdData1D ( dD1 , n , '(F)' ) end if sF => val ( dD1 ) if ( present ( fact ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n sF ( i ) = F ( i ) * fact end do !$OMP end parallel do else call dcopy ( n , F , 1 , sF , 1 ) end if ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_F","tags":"","loc":"proc/push_f.html","title":"push_F – SIESTA"},{"text":"private subroutine update_F(s_F, n, F) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) Calls proc~~update_f~~CallsGraph proc~update_f update_F proc~stack_check stack_check proc~update_f->proc~stack_check val val proc~update_f->val die die proc~update_f->die proc~push_f push_F proc~update_f->proc~push_f get_pointer get_pointer proc~update_f->get_pointer n_items n_items proc~update_f->n_items dcopy dcopy proc~update_f->dcopy proc~stack_check->get_pointer proc~stack_check->n_items proc~push_f->proc~stack_check proc~push_f->val proc~push_f->die proc~push_f->n_items proc~push_f->dcopy push push proc~push_f->push delete delete proc~push_f->delete newddata1d newddata1d proc~push_f->newddata1d max_size max_size proc~push_f->max_size get get proc~push_f->get Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code update_F Source Code subroutine update_F ( s_F , n , F ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) type ( dData1D ), pointer :: dD1 real ( dp ), pointer :: FF (:) integer :: in if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) if ( in == 0 ) then ! We need to add it as it does not exist call push_F ( s_F , n , F ) else ! we have an entry, update the latest dD1 => get_pointer ( s_F , in ) FF => val ( dD1 ) call dcopy ( n , F , 1 , FF , 1 ) end if end subroutine update_F","tags":"","loc":"proc/update_f.html","title":"update_F – SIESTA"},{"text":"private subroutine push_diff(s_rres, s_res, alpha) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_rres type(Fstack_dData1D), intent(in) :: s_res real(kind=dp), intent(in), optional :: alpha Calls proc~~push_diff~~CallsGraph proc~push_diff push_diff val val proc~push_diff->val get get proc~push_diff->get die die proc~push_diff->die get_pointer get_pointer proc~push_diff->get_pointer push push proc~push_diff->push n_items n_items proc~push_diff->n_items max_size max_size proc~push_diff->max_size delete delete proc~push_diff->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_diff Source Code subroutine push_diff ( s_rres , s_res , alpha ) type ( Fstack_dData1D ), intent ( inout ) :: s_rres type ( Fstack_dData1D ), intent ( in ) :: s_res real ( dp ), intent ( in ), optional :: alpha type ( dData1D ) :: dD1 type ( dData1D ), pointer :: pD1 real ( dp ), pointer :: res1 (:), res2 (:), rres (:) integer :: in , ns , i , n if ( n_items ( s_res ) < 2 ) then call die ( 'mixing: Residual residuals cannot be calculated, & &inferior residual size.' ) end if in = n_items ( s_res ) ! First get the value of in pD1 => get_pointer ( s_res , in - 1 ) res1 => val ( pD1 ) ! get the value of in pD1 => get_pointer ( s_res , in ) res2 => val ( pD1 ) in = n_items ( s_rres ) ns = max_size ( s_rres ) if ( in == ns ) then ! we have to cycle the storage call get ( s_rres , 1 , dD1 ) else call newdData1D ( dD1 , size ( res1 ), '(res)' ) end if ! Get the residual of the residual rres => val ( dD1 ) n = size ( rres ) if ( present ( alpha ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = ( res2 ( i ) - res1 ( i )) * alpha end do !$OMP end parallel do else !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = res2 ( i ) - res1 ( i ) end do !$OMP end parallel do end if ! Push the data to the stack call push ( s_rres , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_diff","tags":"","loc":"proc/push_diff.html","title":"push_diff – SIESTA"},{"text":"public interface mixing Calls interface~~mixing~~CallsGraph interface~mixing mixing proc~mixing_1d mixing_1d interface~mixing->proc~mixing_1d proc~mixing_2d mixing_2d interface~mixing->proc~mixing_2d proc~mixing_calc_next mixing_calc_next proc~mixing_1d->proc~mixing_calc_next proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d->proc~mixing_ncoeff proc~mixing_init mixing_init proc~mixing_1d->proc~mixing_init proc~mixing_finalize mixing_finalize proc~mixing_1d->proc~mixing_finalize proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step proc~mixing_finalize->n_items proc~mixing_finalize->proc~current_itt proc~mixing_step->reset proc~mixing_step->n_items push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures mixing_1d mixing_2d Module Procedures private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub","tags":"","loc":"interface/mixing.html","title":"mixing – SIESTA"},{"text":"public subroutine compute_max_diff_2d(X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff Calls proc~~compute_max_diff_2d~~CallsGraph proc~compute_max_diff_2d compute_max_diff_2d die die proc~compute_max_diff_2d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_max_diff_2d~~CalledByGraph proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff compute_max_diff interface~compute_max_diff->proc~compute_max_diff_2d proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_max_diff_2d Source Code subroutine compute_max_diff_2d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:,:), X2 (:,:) real ( dp ), intent ( out ) :: max_diff integer :: n1 , n2 integer :: i1 , i2 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) n2 = size ( X1 , 2 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if if ( size ( X2 , 2 ) /= n2 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (2-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i2,i1), & !$OMP& reduction(max:max_diff), collapse(2) do i2 = 1 , n2 do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 , i2 ) - X2 ( i1 , i2 )) ) end do end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_2d","tags":"","loc":"proc/compute_max_diff_2d.html","title":"compute_max_diff_2d – SIESTA"},{"text":"public subroutine compute_max_diff_1d(X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff Calls proc~~compute_max_diff_1d~~CallsGraph proc~compute_max_diff_1d compute_max_diff_1d die die proc~compute_max_diff_1d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_max_diff_1d~~CalledByGraph proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff compute_max_diff interface~compute_max_diff->proc~compute_max_diff_1d proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_max_diff_1d Source Code subroutine compute_max_diff_1d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:), X2 (:) real ( dp ), intent ( out ) :: max_diff integer :: n1 integer :: i1 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i1), reduction(max:max_diff) do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 ) - X2 ( i1 )) ) end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_1d","tags":"","loc":"proc/compute_max_diff_1d.html","title":"compute_max_diff_1d – SIESTA"},{"text":"public interface compute_max_diff Calls interface~~compute_max_diff~~CallsGraph interface~compute_max_diff compute_max_diff proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff->proc~compute_max_diff_1d proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff->proc~compute_max_diff_2d die die proc~compute_max_diff_1d->die proc~compute_max_diff_2d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by interface~~compute_max_diff~~CalledByGraph interface~compute_max_diff compute_max_diff proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures compute_max_diff_1d compute_max_diff_2d Module Procedures public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff","tags":"","loc":"interface/compute_max_diff.html","title":"compute_max_diff – SIESTA"},{"text":"public subroutine setup_hamiltonian(iscf) Uses siesta_options sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices class_dSpData1D class_dSpData2D class_zSpData2D siesta_geom atmfuncs atomlist metaforce molecularmechanics ldau_specs m_ldau m_dhscf m_stress m_energies parallel m_steps m_ntm m_spin m_dipol alloc m_gamma m_hsx sys m_partial_charges files m_rhog m_mpi_utils proc~~setup_hamiltonian~~UsesGraph proc~setup_hamiltonian setup_hamiltonian siesta_options siesta_options proc~setup_hamiltonian->siesta_options m_rhog m_rhog proc~setup_hamiltonian->m_rhog m_partial_charges m_partial_charges proc~setup_hamiltonian->m_partial_charges ldau_specs ldau_specs proc~setup_hamiltonian->ldau_specs m_energies m_energies proc~setup_hamiltonian->m_energies m_ldau m_ldau proc~setup_hamiltonian->m_ldau m_steps m_steps proc~setup_hamiltonian->m_steps class_dSpData2D class_dSpData2D proc~setup_hamiltonian->class_dSpData2D siesta_geom siesta_geom proc~setup_hamiltonian->siesta_geom m_stress m_stress proc~setup_hamiltonian->m_stress m_ntm m_ntm proc~setup_hamiltonian->m_ntm m_mpi_utils m_mpi_utils proc~setup_hamiltonian->m_mpi_utils m_dipol m_dipol proc~setup_hamiltonian->m_dipol files files proc~setup_hamiltonian->files m_hsx m_hsx proc~setup_hamiltonian->m_hsx sys sys proc~setup_hamiltonian->sys m_spin m_spin proc~setup_hamiltonian->m_spin parallel parallel proc~setup_hamiltonian->parallel atmfuncs atmfuncs proc~setup_hamiltonian->atmfuncs class_dSpData1D class_dSpData1D proc~setup_hamiltonian->class_dSpData1D module~m_dhscf m_dhscf proc~setup_hamiltonian->module~m_dhscf alloc alloc proc~setup_hamiltonian->alloc m_gamma m_gamma proc~setup_hamiltonian->m_gamma metaforce metaforce proc~setup_hamiltonian->metaforce sparse_matrices sparse_matrices proc~setup_hamiltonian->sparse_matrices atomlist atomlist proc~setup_hamiltonian->atomlist molecularmechanics molecularmechanics proc~setup_hamiltonian->molecularmechanics class_zSpData2D class_zSpData2D proc~setup_hamiltonian->class_zSpData2D precision precision module~m_dhscf->precision m_dfscf m_dfscf module~m_dhscf->m_dfscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~setup_hamiltonian~~CallsGraph proc~setup_hamiltonian setup_hamiltonian proc~dhscf dhscf proc~setup_hamiltonian->proc~dhscf hold hold proc~setup_hamiltonian->hold globalize_sum globalize_sum proc~setup_hamiltonian->globalize_sum val val proc~setup_hamiltonian->val h h proc~setup_hamiltonian->h timer timer proc~setup_hamiltonian->timer write_hsx write_hsx proc~setup_hamiltonian->write_hsx de_alloc de_alloc proc~setup_hamiltonian->de_alloc re_alloc re_alloc proc~setup_hamiltonian->re_alloc dimag dimag proc~setup_hamiltonian->dimag dscf dscf proc~setup_hamiltonian->dscf update_e0 update_e0 proc~setup_hamiltonian->update_e0 die die proc~setup_hamiltonian->die bye bye proc~setup_hamiltonian->bye hubbard_term hubbard_term proc~setup_hamiltonian->hubbard_term proc~dhscf->timer proc~dhscf->de_alloc proc~dhscf->re_alloc proc~dhscf->die proc~dhscf->bye elecs elecs proc~dhscf->elecs ts_voltage ts_voltage proc~dhscf->ts_voltage dfscf dfscf proc~dhscf->dfscf ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix volcel volcel proc~dhscf->volcel ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp rhoofd rhoofd proc~dhscf->rhoofd hartree_add hartree_add proc~dhscf->hartree_add add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field proc~reord reord proc~dhscf->proc~reord compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho mpi_barrier mpi_barrier proc~dhscf->mpi_barrier write_rho write_rho proc~dhscf->write_rho rhoofdsp rhoofdsp proc~dhscf->rhoofdsp mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce forhar forhar proc~dhscf->forhar vacuum_level vacuum_level proc~dhscf->vacuum_level localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh write_debug write_debug proc~dhscf->write_debug vmat vmat proc~dhscf->vmat meshlim meshlim proc~setmeshdistr->meshlim proc~reord->timer proc~reord->de_alloc proc~reord->re_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->timer proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->die proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->de_alloc proc~distmeshdata_int->re_alloc proc~distmeshdata_int->die proc~distmeshdata_int->proc~boxintersection var panprocsetup_hamiltonianCallsGraph = svgPanZoom('#procsetup_hamiltonianCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~setup_hamiltonian~~CalledByGraph proc~setup_hamiltonian setup_hamiltonian proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setup_hamiltonian Source Code subroutine setup_hamiltonian ( iscf ) USE siesta_options use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H , S , Hold use sparse_matrices , only : Dscf , Escf , xijo use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , . lastkb , no_s , rmaxv , indxua , iphorb , lasto , . rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use ldau_specs , only : switch_ldau ! This variable determines whether !   the subroutine to compute the !   Hubbard terms should be called !   or not use m_ldau , only : hubbard_term ! Subroutine that compute the !   Hubbard terms use m_dhscf , only : dhscf use m_stress use m_energies use parallel , only : Node use m_steps , only : istp use m_ntm use m_spin , only : spin use m_dipol use alloc , only : re_alloc , de_alloc use m_gamma use m_hsx , only : write_hsx use sys , only : die , bye use m_partial_charges , only : want_partial_charges use files , only : filesOut_t ! derived type for output file names use m_rhog , only : rhog_in , rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none integer , intent ( in ) :: iscf real ( dp ) :: stressl ( 3 , 3 ) real ( dp ), pointer :: fal (:,:) ! Local-node part of atomic F #ifdef MPI real ( dp ) :: buffer1 #endif integer :: io , is , ispin integer :: ifa ! Calc. forces?      0=>no, 1=>yes integer :: istr ! Calc. stress?      0=>no, 1=>yes integer :: ihmat ! Calc. hamiltonian? 0=>no, 1=>yes real ( dp ) :: g2max type ( filesOut_t ) :: filesOut ! blank output file names logical :: use_rhog_in real ( dp ), pointer :: H_vkb (:), H_kin (:), H_ldau (:,:) real ( dp ), pointer :: H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) complex ( dp ) :: Dc integer :: ind , i , j !------------------------------------------------------------------------- BEGIN call timer ( 'setup_H' , 1 ) ! Nullify pointers nullify ( fal ) !$OMP parallel default(shared), private(ispin,io) !     Save present H matrix !$OMP do collapse(2) do ispin = 1 , spin % H do io = 1 , maxnh Hold ( io , ispin ) = H ( io , ispin ) end do end do !$OMP end do !$OMP single H_kin => val ( H_kin_1D ) H_vkb => val ( H_vkb_1D ) if ( spin % SO_onsite ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so_on => val ( H_so_on_2D ) else if ( spin % SO_offsite ) then H_so_off => val ( H_so_off_2D ) end if !$OMP end single ! keep wait ! Initialize diagonal Hamiltonian do ispin = 1 , spin % spinor !$OMP do do io = 1 , maxnh H ( io , ispin ) = H_kin ( io ) + H_vkb ( io ) end do !$OMP end do nowait end do if ( spin % SO_onsite ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = H_so_on ( io , ispin - 2 ) end do end do !$OMP end do nowait else !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = 0._dp end do end do !$OMP end do nowait end if ! .................. ! Non-SCF part of total energy ....................................... ! Note that these will be \"impure\" for a mixed Dscf ! If mixing the charge, Dscf is the previous step's DM_out. Since ! the \"scf\" components of the energy are computed with the (mixed) ! charge, this introduces an inconsistency. In this case the energies ! coming out of this routine need to be corrected. ! !$OMP single Ekin = 0.0_dp Enl = 0.0_dp Eso = 0.0_dp !$OMP end single ! keep wait !$OMP do collapse(2), reduction(+:Ekin,Enl) do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) end do end do !$OMP end do nowait ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! if ( spin % SO_offsite ) then do io = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ), dp ) Eso = Eso + real ( H_so_off ( io , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ), dp ) Eso = Eso + real ( H_so_off ( io , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( io , 3 ), Dscf ( io , 4 ), dp ) Eso = Eso + real ( H_so_off ( io , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( io , 7 ), - Dscf ( io , 8 ), dp ) Eso = Eso + real ( H_so_off ( io , 3 ) * Dc , dp ) end do else if ( spin % SO_onsite ) then !$OMP do reduction(+:Eso) do io = 1 , maxnh Eso = Eso + H_so_on ( io , 1 ) * Dscf ( io , 7 ) + & H_so_on ( io , 2 ) * Dscf ( io , 8 ) + H_so_on ( io , 5 ) * Dscf ( io , 3 ) + & H_so_on ( io , 6 ) * Dscf ( io , 4 ) - H_so_on ( io , 3 ) * Dscf ( io , 5 ) - & H_so_on ( io , 4 ) * Dscf ( io , 6 ) end do !$OMP end do nowait end if !$OMP end parallel #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 end if #endif !     Non-SCF part of total energy call update_E0 () ! Hubbard term for LDA+U: energy, forces, stress and matrix elements .... if ( switch_ldau ) then if ( spin % NCol ) then call die ( 'LDA+U cannot be used with non-collinear spin.' ) end if if ( spin % SO ) then call die ( 'LDA+U cannot be used with spin-orbit coupling.' ) end if call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) H_ldau => val ( H_ldau_2D ) call hubbard_term ( scell , na_u , na_s , isa , xa , indxua , . maxnh , maxnh , lasto , iphorb , no_u , no_l , . numh , listhptr , listh , numh , listhptr , listh , . spin % spinor , Dscf , Eldau , DEldau , H_ldau , . fal , stressl , H , iscf , . matrix_elements_only = . true .) #ifdef MPI ! Global reduction of energy terms call globalize_sum ( Eldau , buffer1 ) Eldau = buffer1 ! DEldau should not be globalized ! as it is based on globalized occupations #endif Eldau = Eldau + DEldau call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) endif ! .................. ! Add SCF contribution to energy and matrix elements .................. g2max = g2cut call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) ifa = 0 istr = 0 ihmat = 1 if (( hirshpop . or . voropop ) $ . and . partial_charges_at_every_scf_step ) then want_partial_charges = . true . endif use_rhog_in = ( mix_charge . and . iscf > 1 ) call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa , indxua , . ntm , ifa , istr , ihmat , filesOut , . maxnh , numh , listhptr , listh , Dscf , Datm , . maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , . Exc , Dxc , dipol , stress , fal , stressl , . use_rhog_in ) if ( spin % SO_offsite ) then ! H(:, [5, 6]) are not updated in dhscf, see vmat for details. !------- H(u,u) H (:, 1 ) = H (:, 1 ) + real ( H_so_off (:, 1 ), dp ) H (:, 5 ) = dimag ( H_so_off (:, 1 )) !------- H(d,d) H (:, 2 ) = H (:, 2 ) + real ( H_so_off (:, 2 ), dp ) H (:, 6 ) = dimag ( H_so_off (:, 2 )) !------- H(u,d) H (:, 3 ) = H (:, 3 ) + real ( H_so_off (:, 3 ), dp ) H (:, 4 ) = H (:, 4 ) + dimag ( H_so_off (:, 3 )) !------- H(d,u) H (:, 7 ) = H (:, 7 ) + real ( H_so_off (:, 4 ), dp ) H (:, 8 ) = H (:, 8 ) - dimag ( H_so_off (:, 4 )) endif ! This statement will apply to iscf = 1, for example, when ! we do not use rhog_in. Rhog here is always the charge used to ! build H, that is, rhog_in. if ( mix_charge ) rhog_in = rhog want_partial_charges = . false . call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) !  It is wasteful to write over and over H and S, as there are !  no different files. ! Save Hamiltonian and overlap matrices ............................ ! Only in HSX format now.  Use Util/HSX/hsx2hs to generate an HS file if ( savehs . or . write_coop ) then call write_hsx ( gamma , no_u , no_s , spin % H , indxuo , & maxnh , numh , listhptr , listh , H , S , qtot , & temp , xijo ) endif call timer ( 'setup_H' , 2 ) #ifdef SIESTA__PEXSI if ( node == 0 ) call memory_snapshot ( \"after setup_H\" ) #endif if ( h_setup_only ) then call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call bye ( \"H-Setup-Only requested\" ) STOP endif !------------------------------------------------------------------------- END END subroutine setup_hamiltonian","tags":"","loc":"proc/setup_hamiltonian.html","title":"setup_hamiltonian – SIESTA"},{"text":"public subroutine compute_dm(iscf) Uses precision units siesta_options class_dSpData1D sparse_matrices siesta_geom atomlist sys kpoint_scf_m m_energies m_energies m_rmaxh m_eo m_spin m_diagon m_gamma parallel parallel m_compute_ebs_shift m_pexsi_solver m_hsx mpi_siesta iodmhs_netcdf m_dminim m_zminim m_ordern m_steps m_normalize_dm m_chess m_energies m_ts_global_vars m_transiesta proc~~compute_dm~~UsesGraph proc~compute_dm compute_dm m_diagon m_diagon proc~compute_dm->m_diagon m_eo m_eo proc~compute_dm->m_eo m_ts_global_vars m_ts_global_vars proc~compute_dm->m_ts_global_vars m_energies m_energies proc~compute_dm->m_energies m_ordern m_ordern proc~compute_dm->m_ordern kpoint_scf_m kpoint_scf_m proc~compute_dm->kpoint_scf_m atomlist atomlist proc~compute_dm->atomlist m_steps m_steps proc~compute_dm->m_steps m_normalize_dm m_normalize_dm proc~compute_dm->m_normalize_dm m_compute_ebs_shift m_compute_ebs_shift proc~compute_dm->m_compute_ebs_shift siesta_geom siesta_geom proc~compute_dm->siesta_geom iodmhs_netcdf iodmhs_netcdf proc~compute_dm->iodmhs_netcdf units units proc~compute_dm->units siesta_options siesta_options proc~compute_dm->siesta_options m_pexsi_solver m_pexsi_solver proc~compute_dm->m_pexsi_solver m_hsx m_hsx proc~compute_dm->m_hsx m_dminim m_dminim proc~compute_dm->m_dminim precision precision proc~compute_dm->precision m_chess m_chess proc~compute_dm->m_chess sys sys proc~compute_dm->sys mpi_siesta mpi_siesta proc~compute_dm->mpi_siesta m_spin m_spin proc~compute_dm->m_spin m_transiesta m_transiesta proc~compute_dm->m_transiesta parallel parallel proc~compute_dm->parallel class_dSpData1D class_dSpData1D proc~compute_dm->class_dSpData1D m_gamma m_gamma proc~compute_dm->m_gamma sparse_matrices sparse_matrices proc~compute_dm->sparse_matrices m_rmaxh m_rmaxh proc~compute_dm->m_rmaxh m_zminim m_zminim proc~compute_dm->m_zminim Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~compute_dm~~CallsGraph proc~compute_dm compute_dm ordern ordern proc~compute_dm->ordern write_hs_formatted write_hs_formatted proc~compute_dm->write_hs_formatted zminim zminim proc~compute_dm->zminim eold eold proc~compute_dm->eold val val proc~compute_dm->val escf escf proc~compute_dm->escf transiesta transiesta proc~compute_dm->transiesta chess_wrapper chess_wrapper proc~compute_dm->chess_wrapper pexsi_solver pexsi_solver proc~compute_dm->pexsi_solver write_dmh_netcdf write_dmh_netcdf proc~compute_dm->write_dmh_netcdf timer timer proc~compute_dm->timer dold dold proc~compute_dm->dold diagon diagon proc~compute_dm->diagon write_orb_indx write_orb_indx proc~compute_dm->write_orb_indx mpi_bcast mpi_bcast proc~compute_dm->mpi_bcast dscf dscf proc~compute_dm->dscf normalize_dm normalize_dm proc~compute_dm->normalize_dm bye bye proc~compute_dm->bye dminim dminim proc~compute_dm->dminim compute_ebs_shift compute_ebs_shift proc~compute_dm->compute_ebs_shift Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_dm~~CalledByGraph proc~compute_dm compute_dm proc~siesta_forces siesta_forces proc~siesta_forces->proc~compute_dm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_dm Source Code subroutine compute_dm ( iscf ) use precision use units , only : eV USE siesta_options use class_dSpData1D , only : val use sparse_matrices use siesta_geom use atomlist , only : qa , lasto , iphorb , iaorb , no_u , no_s , indxuo , & qtot , Qtots , no_l use sys , only : die , bye use kpoint_scf_m , only : kpoints_scf use m_energies , only : Ebs , Ecorrec , Entropy , DE_NEGF use m_energies , only : Ef , Efs use m_rmaxh use m_eo use m_spin , only : spin use m_diagon , only : diagon use m_gamma use parallel , only : IONode use parallel , only : SIESTA_worker use m_compute_ebs_shift , only : compute_ebs_shift #ifdef SIESTA__PEXSI use m_pexsi_solver , only : pexsi_solver #endif use m_hsx , only : write_hs_formatted #ifdef MPI use mpi_siesta #endif #ifdef CDF use iodmhs_netcdf , only : write_dmh_netcdf #endif use m_dminim , only : dminim use m_zminim , only : zminim use m_ordern , only : ordern use m_steps , only : istp use m_normalize_dm , only : normalize_dm #ifdef SIESTA__CHESS use m_chess , only : CheSS_wrapper #endif use m_energies , only : DE_NEGF use m_ts_global_vars , only : TSmode , TSinit , TSrun use m_transiesta , only : transiesta implicit none !     Input variables integer , intent ( in ) :: iscf real ( dp ) :: delta_Ebs , delta_Ef logical :: CallDiagon integer :: nnz real ( dp ), pointer :: H_kin (:) ! e1>e2 to signal that we do not want DOS weights real ( dp ), parameter :: e1 = 1.0_dp , e2 = - 1.0_dp real ( dp ) :: buffer1 integer :: mpierr !       character(15)            :: filename, indexstr !       character(15), parameter :: fnameform = '(A,A,A)' !-------------------------------------------------------------------- BEGIN if ( SIESTA_worker ) call timer ( 'compute_dm' , 1 ) #ifdef MPI call MPI_Bcast ( isolve , 1 , MPI_integer , 0 , true_MPI_Comm_World , mpierr ) #endif if ( SIESTA_worker ) then ! Save present density matrix !$OMP parallel default(shared) if ( converge_EDM ) then !$OMP workshare Eold (:,:) = Escf (:,:) Dold (:,:) = Dscf (:,:) !$OMP end workshare else !$OMP workshare Dold (:,:) = Dscf (:,:) !$OMP end workshare end if !$OMP end parallel end if ! Compute shift in Tr(H*DM) for fermi-level bracketting ! Use the current H, the previous iteration H, and the ! previous iteration DM if ( SIESTA_worker ) then if ( iscf > 1 ) then call compute_Ebs_shift ( Dscf , H , Hold , delta_Ebs ) delta_Ef = delta_Ebs / qtot if ( ionode . and . isolve . eq . SOLVE_PEXSI ) then write ( 6 , \"(a,f16.5)\" ) $ \"Estimated change in band-structure energy:\" , $ delta_Ebs / eV , \"Estimated shift in E_fermi: \" , $ delta_Ef / eV endif else delta_Ebs = 0.0_dp delta_Ef = 0.0_dp endif endif #ifdef SIESTA__PEXSI if ( isolve . eq . SOLVE_PEXSI ) then ! This test done in node 0 since NonCol and SpOrb ! are not set for PEXSI-solver-only processes if ( ionode ) then if ( spin % NCol . or . spin % SO ) call die ( $ \"The PEXSI solver does not implement \" // $ \"non-coll spins or Spin-orbit yet\" ) endif call pexsi_solver ( iscf , no_u , no_l , spin % spinor , $ maxnh , numh , listhptr , listh , $ H , S , qtot , Dscf , Escf , $ ef , Entropy , temp , delta_Ef ) endif if (. not . SIESTA_worker ) RETURN #endif ! Here we decide if we want to calculate one or more SCF steps by ! diagonalization before proceeding with the OMM routine CallDiagon = . false . if ( isolve . eq . SOLVE_MINIM ) then if ( istp . eq . 1 ) then if (( iscf . le . call_diagon_first_step ) . or . & ( call_diagon_first_step < 0 )) CallDiagon = . true . else if (( iscf . le . call_diagon_default ) . or . & ( call_diagon_default < 0 )) CallDiagon = . true . endif endif if ( isolve . eq . MATRIX_WRITE ) then !             write(indexstr,'(I15)') iscf !             write(filename,fnameform) 'H_', trim(adjustl(indexstr)), !      &                                '.matrix' !             call write_global_matrix( no_s, no_l, maxnh, numh, listh, !      &           H(1:maxnh,1), filename ) ! !             write(filename,fnameform) 'S_', trim(adjustl(indexstr)), !      &                                '.matrix' !        Note: only one-shot for now call write_hs_formatted ( no_u , spin % H , $ maxnh , numh , listhptr , listh , H , S ) call bye ( \"End of run after writing H.matrix and S.matrix\" ) c$ call write_global_matrix_singlenodewrite ( c$ & no_u , no_s , maxnh , numh , listhptr , listh , c$ & H (:, 1 ), 'H.matrix' ) c$ c$ call write_global_matrix_singlenodewrite ( c$ & no_u , no_s , maxnh , numh , listhptr , listh , c$ & S , 'S.matrix' ) elseif (( isolve . eq . SOLVE_DIAGON ) . or . ( CallDiagon )) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0.0_dp PreviousCallDiagon = . true . elseif ( isolve . eq . SOLVE_ORDERN ) then if (. not . gamma ) call die ( \"Cannot do O(N) with k-points.\" ) if ( spin % NCol . or . spin % SO ) . call die ( \"Cannot do O(N) with non-coll spins or Spin-orbit\" ) call ordern ( usesavelwf , ioptlwf , na_u , no_u , no_l , lasto , & isa , qa , rcoor , rmaxh , ucell , xa , iscf , & istp , ncgmax , etol , eta , qtot , maxnh , numh , & listhptr , listh , H , S , chebef , noeta , rcoorcp , & beta , pmax , Dscf , Escf , Ecorrec , spin % H , qtots ) Entropy = 0.0_dp elseif ( isolve . eq . SOLVE_MINIM ) then if ( spin % NCol . or . spin % SO ) & call die ( 'ERROR: Non-collinear spin calculations &                       not yet implemented with OMM!' ) H_kin => val ( H_kin_1D ) if ( gamma ) then call dminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , H , S , H_kin ) else call zminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , no_s , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & H , S , H_kin ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #ifdef SIESTA__CHESS elseif ( isolve . eq . SOLVE_CHESS ) then ! FOE solver from the CheSS library if ( gamma ) then call CheSS_wrapper (. false ., PreviousCallDiagon , & iscf , istp , no_l , & spin % spinor , no_u , maxnh , numh , listhptr , listh , & qs , h , s , & Dscf , Escf , Ef ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #endif elseif ( TSmode . and . TSinit ) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0._dp else if ( TSrun ) then call transiesta ( iscf , spin % H , block_dist , sparse_pattern , & Gamma , ucell , nsc , isc_off , no_u , na_u , lasto , xa , maxnh , & H , S , Dscf , Escf , Ef , Qtot , . false ., DE_NEGF ) Ecorrec = 0._dp Entropy = 0.0_dp else !call die('siesta: ERROR: wrong solution method') endif #ifdef CDF if ( writedmhs_cdf_history ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf ) else if ( writedmhs_cdf ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf , & overwrite = . true . ) endif #endif ! Write orbital indexes. JMS Dec.2009 if ( IOnode . and . iscf == 1 ) then call write_orb_indx ( na_u , na_s , no_u , no_s , isa , xa , . iaorb , iphorb , indxuo , nsc , ucell ) endif !     Normalize density matrix to exact charge !     Placed here for now to avoid disturbing EHarris if ( . not . TSrun ) then call normalize_dm ( first = . false . ) end if call timer ( 'compute_dm' , 2 ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after compute_DM\" ) #endif !-----------------------------------------------------------------------END END subroutine compute_dm","tags":"","loc":"proc/compute_dm.html","title":"compute_dm – SIESTA"},{"text":"public subroutine mixers_scf_init(nspin, Comm) Uses fdf precision m_mixing m_mixing m_mixing m_mixing proc~~mixers_scf_init~~UsesGraph proc~mixers_scf_init mixers_scf_init fdf fdf proc~mixers_scf_init->fdf precision precision proc~mixers_scf_init->precision module~m_mixing m_mixing proc~mixers_scf_init->module~m_mixing module~m_mixing->precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in), optional :: Comm Calls proc~~mixers_scf_init~~CallsGraph proc~mixers_scf_init mixers_scf_init proc~mixers_init mixers_init proc~mixers_scf_init->proc~mixers_init die die proc~mixers_scf_init->die proc~mix_method mix_method proc~mixers_scf_init->proc~mix_method proc~mixers_history_init mixers_history_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_reset mixers_reset proc~mixers_scf_init->proc~mixers_reset fdf_get fdf_get proc~mixers_scf_init->fdf_get proc~mix_method_variant mix_method_variant proc~mixers_scf_init->proc~mix_method_variant leqi leqi proc~mixers_scf_init->leqi proc~mixers_init->die proc~mixers_init->proc~mixers_history_init proc~mixers_init->proc~mixers_reset proc~mixers_init->fdf_get fdf_block fdf_block proc~mixers_init->fdf_block fdf_bnnames fdf_bnnames proc~mixers_init->fdf_bnnames fdf_bline fdf_bline proc~mixers_init->fdf_bline fdf_bnames fdf_bnames proc~mixers_init->fdf_bnames fdf_brewind fdf_brewind proc~mixers_init->fdf_brewind proc~mix_method->die proc~mix_method->leqi proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new delete delete proc~mixers_history_init->delete proc~mixers_reset->delete proc~mix_method_variant->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_init Source Code subroutine mixers_scf_init ( nspin , Comm ) use fdf use precision , only : dp #ifdef MPI use mpi_siesta , only : MPI_Comm_World #endif use m_mixing , only : mixers_reset , mixers_init use m_mixing , only : mix_method , mix_method_variant use m_mixing , only : mixer_init use m_mixing , only : mixers_history_init ! The number of spin-components integer , intent ( in ) :: nspin ! The communicator used for the mixer integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf ! Get number of history steps integer :: n_hist , n_kick , n_restart , n_save real ( dp ) :: w , w_kick integer :: n_lin_after real ( dp ) :: w_lin_after logical :: lin_after ! number of history steps saved type ( tMixer ), pointer :: m integer :: nm , im , im2 , tmp logical :: is_broyden character ( len = 70 ) :: method , variant , opt ! If the mixers are denoted by a block, then ! the entire logic *MUST* be defined in the blocks opt = fdf_get ( 'SCF.Mix.Spin' , 'all' ) if ( leqi ( opt , 'all' ) ) then mix_spin = MIX_SPIN_ALL else if ( leqi ( opt , 'spinor' ) ) then mix_spin = MIX_SPIN_SPINOR else if ( leqi ( opt , 'sum' ) ) then mix_spin = MIX_SPIN_SUM else if ( leqi ( opt , 'sum+diff' ) ) then mix_spin = MIX_SPIN_SUM_DIFF else call die ( \"Unknown option given for SCF.Mix.Spin & &all|spinor|sum|sum+diff\" ) end if ! If there is only one spinor we should mix all... if ( nspin == 1 ) mix_spin = MIX_SPIN_ALL ! Initialize to ensure debug stuff read call mixers_init ( 'SCF' , scf_mixs , Comm = Comm ) ! Check for existance of the SCF.Mix block if ( associated ( scf_mixs ) ) then if ( size ( scf_mixs ) > 0 ) then return end if ! Something has gone wrong... ! The user has supplied a block, but ! haven't added any content to the block... ! However, we fall-back to the default mechanism end if ! ensure nullification call mixers_reset ( scf_mixs ) ! >>>*** FIRST ***<<< ! Read in compatibility options ! Figure out if we are dealing with ! Broyden or Pulay n_hist = fdf_get ( 'DM.NumberPulay' , 2 ) tmp = fdf_get ( 'DM.NumberBroyden' , 0 ) is_broyden = tmp > 0 if ( is_broyden ) then n_hist = tmp end if ! Define default mixing weight (used for ! Pulay, Broyden and linear mixing) w = fdf_get ( 'DM.MixingWeight' , 0.25_dp ) ! Default kick-options n_kick = fdf_get ( 'DM.NumberKick' , 0 ) w_kick = fdf_get ( 'DM.KickMixingWeight' , 0.5_dp ) lin_after = fdf_get ( 'SCF.LinearMixingAfterPulay' , . false .) w_lin_after = fdf_get ( 'SCF.MixingWeightAfterPulay' , w ) ! >>>*** END ***<<< ! Read options in new format ! Get history length n_hist = fdf_get ( 'SCF.Mixer.History' , n_hist ) ! update mixing weight and kick mixing weight w = fdf_get ( 'SCF.Mixer.Weight' , w ) n_kick = fdf_get ( 'SCF.Mixer.Kick' , n_kick ) w_kick = fdf_get ( 'SCF.Mixer.Kick.Weight' , w_kick ) ! Restart after this number of iterations n_restart = fdf_get ( 'SCF.Mixer.Restart' , 0 ) n_save = fdf_get ( 'SCF.Mixer.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Get the variant of the mixing method if ( is_broyden ) then method = 'Broyden' else if ( n_hist > 0 ) then method = 'Pulay' else method = 'Linear' end if method = fdf_get ( 'SCF.Mixer.Method' , trim ( method )) variant = fdf_get ( 'SCF.Mixer.Variant' , 'original' ) ! Determine whether linear mixing should be ! performed after the \"advanced\" mixing n_lin_after = fdf_get ( 'SCF.Mixer.Linear.After' , - 1 ) w_lin_after = fdf_get ( 'SCF.Mixer.Linear.After.Weight' , w_lin_after ) ! Determine total number of mixers nm = 1 if ( n_lin_after >= 0 . or . lin_after ) nm = nm + 1 if ( n_kick > 0 ) nm = nm + 1 ! Initiailaze all mixers allocate ( scf_mixs ( nm )) scf_mixs (:)% w = w scf_mixs (:)% n_hist = n_hist scf_mixs (:)% restart = n_restart scf_mixs (:)% restart_save = n_save ! 1. Current mixing index im = 1 ! Store the advanced mixer index (for references to ! later mixers) im2 = im m => scf_mixs ( im ) m % name = method m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! 2. Setup the linear mixing after the actual mixing if ( n_lin_after > 0 . or . lin_after ) then im = im + 1 m => scf_mixs ( im ) ! Signal to switch to this mixer after ! convergence scf_mixs ( im2 )% next_conv => m m % name = 'Linear-After' m % m = mix_method ( 'linear' ) m % w = w_lin_after m % n_itt = n_lin_after ! jump back to previous after having run a ! few iterations m % next => scf_mixs ( im2 ) end if ! In case we have a kick, apply the kick here ! This overrides the \"linear.after\" option if ( n_kick > 0 ) then im = im + 1 m => scf_mixs ( im ) m % name = 'Linear-Kick' m % n_itt = 1 m % n_hist = 0 m % m = mix_method ( 'linear' ) m % w = w_kick m % next => scf_mixs ( im2 ) ! set the default mixer to kick scf_mixs ( im2 )% n_itt = n_kick - 1 scf_mixs ( im2 )% next => m scf_mixs ( im2 )% restart = n_kick - 1 end if ! Correct the input do im = 1 , nm call mixer_init ( scf_mixs ( im ) ) end do ! Initialize the allocation of each mixer call mixers_history_init ( scf_mixs ) #ifdef MPI if ( present ( Comm ) ) then scf_mixs (:)% Comm = Comm else scf_mixs (:)% Comm = MPI_Comm_World end if #endif end subroutine mixers_scf_init","tags":"","loc":"proc/mixers_scf_init.html","title":"mixers_scf_init – SIESTA"},{"text":"public subroutine mixers_scf_print(nspin) Uses parallel m_mixing proc~~mixers_scf_print~~UsesGraph proc~mixers_scf_print mixers_scf_print parallel parallel proc~mixers_scf_print->parallel module~m_mixing m_mixing proc~mixers_scf_print->module~m_mixing class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Calls proc~~mixers_scf_print~~CallsGraph proc~mixers_scf_print mixers_scf_print proc~mixers_print mixers_print proc~mixers_scf_print->proc~mixers_print die die proc~mixers_scf_print->die proc~mixers_print->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_print Source Code subroutine mixers_scf_print ( nspin ) use parallel , only : IONode use m_mixing , only : mixers_print integer , intent ( in ) :: nspin ! Print mixing options call mixers_print ( 'SCF' , scf_mixs ) if ( IONode . and . nspin > 1 ) then select case ( mix_spin ) case ( MIX_SPIN_ALL ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'all' case ( MIX_SPIN_SPINOR ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'spinor' if ( nspin <= 2 ) then call die ( \"SCF.Mixer.Spin spinor option only valid for & &non-collinear and spin-orbit calculations\" ) end if case ( MIX_SPIN_SUM ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum' case ( MIX_SPIN_SUM_DIFF ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum and diff' end select end if end subroutine mixers_scf_print","tags":"","loc":"proc/mixers_scf_print.html","title":"mixers_scf_print – SIESTA"},{"text":"public subroutine mixers_scf_print_block() Uses m_mixing proc~~mixers_scf_print_block~~UsesGraph proc~mixers_scf_print_block mixers_scf_print_block module~m_mixing m_mixing proc~mixers_scf_print_block->module~m_mixing class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_print_block~~CallsGraph proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_print_block mixers_print_block proc~mixers_scf_print_block->proc~mixers_print_block die die proc~mixers_print_block->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_print_block Source Code subroutine mixers_scf_print_block ( ) use m_mixing , only : mixers_print_block ! Print mixing options call mixers_print_block ( 'SCF' , scf_mixs ) end subroutine mixers_scf_print_block","tags":"","loc":"proc/mixers_scf_print_block.html","title":"mixers_scf_print_block – SIESTA"},{"text":"public subroutine mixing_scf_converged(SCFconverged) Uses parallel proc~~mixing_scf_converged~~UsesGraph proc~mixing_scf_converged mixing_scf_converged parallel parallel proc~mixing_scf_converged->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name logical, intent(inout) :: SCFconverged Calls proc~~mixing_scf_converged~~CallsGraph proc~mixing_scf_converged mixing_scf_converged reset reset proc~mixing_scf_converged->reset Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_scf_converged~~CalledByGraph proc~mixing_scf_converged mixing_scf_converged proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixing_scf_converged Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_scf_converged Source Code subroutine mixing_scf_converged ( SCFconverged ) use parallel , only : IONode logical , intent ( inout ) :: SCFconverged integer :: i ! Return if no convergence if ( . not . SCFconverged ) return if ( associated ( scf_mix % next_conv ) ) then ! this means that we skip to the ! following algorithm scf_mix => scf_mix % next_conv SCFconverged = . false . if ( allocated ( scf_mix % stack ) ) then do i = 1 , size ( scf_mix % stack ) ! delete all but one history ! This should be fine call reset ( scf_mix % stack ( i ), - 1 ) end do end if if ( IONode ) then write ( * , '(/,2a)' ) ':!: SCF cycle continuation mixer: ' , & trim ( scf_mix % name ) end if end if end subroutine mixing_scf_converged","tags":"","loc":"proc/mixing_scf_converged.html","title":"mixing_scf_converged – SIESTA"},{"text":"public subroutine mixers_scf_reset() Uses m_mixing proc~~mixers_scf_reset~~UsesGraph proc~mixers_scf_reset mixers_scf_reset module~m_mixing m_mixing proc~mixers_scf_reset->module~m_mixing class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_reset~~CallsGraph proc~mixers_scf_reset mixers_scf_reset proc~mixers_reset mixers_reset proc~mixers_scf_reset->proc~mixers_reset delete delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_reset Source Code subroutine mixers_scf_reset () use m_mixing , only : mixers_reset nullify ( scf_mix ) call mixers_reset ( scf_mixs ) end subroutine mixers_scf_reset","tags":"","loc":"proc/mixers_scf_reset.html","title":"mixers_scf_reset – SIESTA"},{"text":"public subroutine mixers_scf_history_init() Uses m_mixing proc~~mixers_scf_history_init~~UsesGraph proc~mixers_scf_history_init mixers_scf_history_init module~m_mixing m_mixing proc~mixers_scf_history_init->module~m_mixing class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_history_init~~CallsGraph proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_history_init mixers_history_init proc~mixers_scf_history_init->proc~mixers_history_init new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt delete delete proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_scf_history_init~~CalledByGraph proc~mixers_scf_history_init mixers_scf_history_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_history_init Source Code subroutine mixers_scf_history_init ( ) use m_mixing , only : mixers_history_init call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) end subroutine mixers_scf_history_init","tags":"","loc":"proc/mixers_scf_history_init.html","title":"mixers_scf_history_init – SIESTA"},{"text":"public subroutine siesta_analysis(relaxd) Uses band writewave writewave m_ksvinit m_ksv m_projected_DOS m_local_DOS m_pexsi_local_DOS m_pexsi_dos siesta_options units sparse_matrices sparse_matrices siesta_geom m_dhscf atomlist atomlist fdf writewave siesta_cml files files zmatrix kpoint_scf_m parallel parallel files m_energies m_steps m_ntm m_spin m_spin m_dipol m_eo m_forces m_gamma alloc basis_enthalpy m_partial_charges m_iodm_old m_siesta2wannier90 m_mpi_utils flook_siesta proc~~siesta_analysis~~UsesGraph proc~siesta_analysis siesta_analysis siesta_options siesta_options proc~siesta_analysis->siesta_options flook_siesta flook_siesta proc~siesta_analysis->flook_siesta m_partial_charges m_partial_charges proc~siesta_analysis->m_partial_charges sparse_matrices sparse_matrices proc~siesta_analysis->sparse_matrices m_projected_DOS m_projected_DOS proc~siesta_analysis->m_projected_DOS siesta_cml siesta_cml proc~siesta_analysis->siesta_cml m_energies m_energies proc~siesta_analysis->m_energies kpoint_scf_m kpoint_scf_m proc~siesta_analysis->kpoint_scf_m atomlist atomlist proc~siesta_analysis->atomlist m_steps m_steps proc~siesta_analysis->m_steps basis_enthalpy basis_enthalpy proc~siesta_analysis->basis_enthalpy m_iodm_old m_iodm_old proc~siesta_analysis->m_iodm_old siesta_geom siesta_geom proc~siesta_analysis->siesta_geom m_ksvinit m_ksvinit proc~siesta_analysis->m_ksvinit units units proc~siesta_analysis->units m_ntm m_ntm proc~siesta_analysis->m_ntm m_ksv m_ksv proc~siesta_analysis->m_ksv files files proc~siesta_analysis->files m_pexsi_dos m_pexsi_dos proc~siesta_analysis->m_pexsi_dos module~m_dhscf m_dhscf proc~siesta_analysis->module~m_dhscf band band proc~siesta_analysis->band m_local_DOS m_local_DOS proc~siesta_analysis->m_local_DOS m_spin m_spin proc~siesta_analysis->m_spin writewave writewave proc~siesta_analysis->writewave parallel parallel proc~siesta_analysis->parallel m_siesta2wannier90 m_siesta2wannier90 proc~siesta_analysis->m_siesta2wannier90 m_forces m_forces proc~siesta_analysis->m_forces alloc alloc proc~siesta_analysis->alloc m_gamma m_gamma proc~siesta_analysis->m_gamma fdf fdf proc~siesta_analysis->fdf m_eo m_eo proc~siesta_analysis->m_eo m_mpi_utils m_mpi_utils proc~siesta_analysis->m_mpi_utils m_dipol m_dipol proc~siesta_analysis->m_dipol zmatrix zmatrix proc~siesta_analysis->zmatrix m_pexsi_local_DOS m_pexsi_local_DOS proc~siesta_analysis->m_pexsi_local_DOS precision precision module~m_dhscf->precision m_dfscf m_dfscf module~m_dhscf->m_dfscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Check that we are converged in geometry,\n if strictly required,\n before carrying out any analysis. @code\n@endcode Arguments Type Intent Optional Attributes Name logical :: relaxd Calls proc~~siesta_analysis~~CallsGraph proc~siesta_analysis siesta_analysis eo eo proc~siesta_analysis->eo re_alloc re_alloc proc~siesta_analysis->re_alloc xa xa proc~siesta_analysis->xa scell scell proc~siesta_analysis->scell proc~dhscf dhscf proc~siesta_analysis->proc~dhscf siesta_write_energies siesta_write_energies proc~siesta_analysis->siesta_write_energies de_alloc de_alloc proc~siesta_analysis->de_alloc timer timer proc~siesta_analysis->timer siesta_write_forces siesta_write_forces proc~siesta_analysis->siesta_write_forces message message proc~siesta_analysis->message fdf_boolean fdf_boolean proc~siesta_analysis->fdf_boolean dipol dipol proc~siesta_analysis->dipol xa_last xa_last proc~siesta_analysis->xa_last print_spin print_spin proc~siesta_analysis->print_spin local_dos local_dos proc~siesta_analysis->local_dos scell_last scell_last proc~siesta_analysis->scell_last siesta_write_stress_pressure siesta_write_stress_pressure proc~siesta_analysis->siesta_write_stress_pressure pexsi_local_dos pexsi_local_dos proc~siesta_analysis->pexsi_local_dos projected_dos projected_dos proc~siesta_analysis->projected_dos slua_call slua_call proc~siesta_analysis->slua_call write_basis_enthalpy write_basis_enthalpy proc~siesta_analysis->write_basis_enthalpy barrier barrier proc~siesta_analysis->barrier bands bands proc~siesta_analysis->bands bk bk proc~siesta_analysis->bk optical optical proc~siesta_analysis->optical pexsi_dos pexsi_dos proc~siesta_analysis->pexsi_dos cmladdproperty cmladdproperty proc~siesta_analysis->cmladdproperty cmlstartmodule cmlstartmodule proc~siesta_analysis->cmlstartmodule outcoor outcoor proc~siesta_analysis->outcoor die die proc~siesta_analysis->die fdf_get fdf_get proc~siesta_analysis->fdf_get setup_wfs_list setup_wfs_list proc~siesta_analysis->setup_wfs_list siesta_write_positions siesta_write_positions proc~siesta_analysis->siesta_write_positions ksv_pol ksv_pol proc~siesta_analysis->ksv_pol ucell ucell proc~siesta_analysis->ucell ucell_last ucell_last proc~siesta_analysis->ucell_last read_spmatrix read_spmatrix proc~siesta_analysis->read_spmatrix wwave wwave proc~siesta_analysis->wwave proc~dhscf->re_alloc proc~dhscf->de_alloc proc~dhscf->timer proc~dhscf->die elecs elecs proc~dhscf->elecs ts_voltage ts_voltage proc~dhscf->ts_voltage dfscf dfscf proc~dhscf->dfscf ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix volcel volcel proc~dhscf->volcel ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofd rhoofd proc~dhscf->rhoofd bye bye proc~dhscf->bye get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field hartree_add hartree_add proc~dhscf->hartree_add proc~reord reord proc~dhscf->proc~reord rhoofdsp rhoofdsp proc~dhscf->rhoofdsp compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho mpi_barrier mpi_barrier proc~dhscf->mpi_barrier write_rho write_rho proc~dhscf->write_rho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce forhar forhar proc~dhscf->forhar vacuum_level vacuum_level proc~dhscf->vacuum_level localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh write_debug write_debug proc~dhscf->write_debug vmat vmat proc~dhscf->vmat meshlim meshlim proc~setmeshdistr->meshlim proc~reord->re_alloc proc~reord->de_alloc proc~reord->timer proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->timer proc~distmeshdata_rea->die proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->die proc~distmeshdata_int->proc~boxintersection var panprocsiesta_analysisCallsGraph = svgPanZoom('#procsiesta_analysisCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code siesta_analysis Source Code subroutine siesta_analysis ( relaxd ) USE band , only : nbk , bk , maxbk , bands USE writewave , only : nwk , wfk , wwave USE writewave , only : setup_wfs_list , wfs_filename USE m_ksvinit , only : nkpol , kpol , wgthpol use m_ksv USE m_projected_DOS , only : projected_DOS USE m_local_DOS , only : local_DOS #ifdef SIESTA__PEXSI USE m_pexsi_local_DOS , only : pexsi_local_DOS USE m_pexsi_dos , only : pexsi_dos #endif USE siesta_options use units , only : Debye , eV use sparse_matrices , only : maxnh , listh , listhptr , numh use sparse_matrices , only : H , S , Dscf , xijo use siesta_geom use m_dhscf , only : dhscf use atomlist , only : indxuo , iaorb , lastkb , lasto , datm , no_l , & iphkb , no_u , no_s , iza , iphorb , rmaxo , indxua use atomlist , only : qtot use fdf use writewave , only : wwave use siesta_cml use files , only : slabel use files , only : filesOut_t ! derived type for output file names use zmatrix , only : lUseZmatrix , write_zmatrix use kpoint_scf_m , only : kpoints_scf use parallel , only : IOnode use parallel , only : SIESTA_worker use files , only : label_length use m_energies use m_steps , only : final use m_ntm use m_spin , only : nspin , spinor_dim , h_spin_dim use m_spin , only : SpOrb , NonCol , SPpol , NoMagn use m_dipol use m_eo use m_forces , only : fa use m_gamma use alloc , only : re_alloc , de_alloc use basis_enthalpy , only : write_basis_enthalpy use m_partial_charges , only : want_partial_charges use m_iodm_old , only : read_spmatrix use m_siesta2wannier90 , only : siesta2wannier90 use m_mpi_utils , only : barrier #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_ANALYSIS #endif implicit none logical :: relaxd , getPSI , quenched_MD , found real ( dp ) :: dummy_str ( 3 , 3 ) real ( dp ) :: dummy_strl ( 3 , 3 ) real ( dp ) :: qspin ( 4 ) ! Local real ( dp ) :: polxyz ( 3 , nspin ) ! Autom., small real ( dp ) :: polR ( 3 , nspin ) ! Autom., small real ( dp ) :: qaux real ( dp ), pointer :: ebk (:,:,:) ! Band energies integer :: j , ix , ind , ik , io , ispin integer :: wfs_band_min , wfs_band_max real ( dp ) :: g2max , current_ef type ( filesOut_t ) :: filesOut ! blank output file names !-----------------------------------------------------------------------BEGIN if ( SIESTA_worker ) call timer ( \"Analysis\" , 1 ) !! Check that we are converged in geometry, !! if strictly required, !! before carrying out any analysis. !!@code quenched_MD = ( ( iquench > 0 ) . and . $ (( idyn . eq . 1 ) . or . ( idyn . eq . 3 )) ) if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( SIESTA_worker ) then ! For timing ops and associated barrier if ( GeometryMustConverge . and . (. not . relaxd )) then call message ( \"FATAL\" , $ \"GEOM_NOT_CONV: Geometry relaxation not converged\" ) call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call barrier () call die ( \"ABNORMAL_TERMINATION\" ) endif endif endif !!@endcode !     All the comments below assume that this compatibility option !     is not used. !     Note also that full compatibility cannot be guaranteed if (. not . compat_pre_v4_dynamics ) then !     This is a sanity safeguard: we reset the geometry (which might !     have been moved by the relaxation or MD routines) to the one used !     in the last computation of the electronic structure. !     See the comments below for explanation !$OMP parallel workshare default(shared) xa ( 1 : 3 , 1 : na_s ) = xa_last ( 1 : 3 , 1 : na_s ) ucell ( 1 : 3 , 1 : 3 ) = ucell_last ( 1 : 3 , 1 : 3 ) scell ( 1 : 3 , 1 : 3 ) = scell_last ( 1 : 3 , 1 : 3 ) !$OMP end parallel workshare endif ! zmatrix info reset?? if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then if ( SIESTA_worker ) then call read_spmatrix ( maxnh , no_l , h_spin_dim , numh , . listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) current_ef = ef ef = fdf_get ( \"Manual-Fermi-Level\" , current_ef , \"Ry\" ) endif endif #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.DOS\" ,. false .)) then call pexsi_dos ( no_u , no_l , spinor_dim , $ maxnh , numh , listhptr , listh , H , S , qtot , ef ) endif #endif ! section done by Siesta subset of nodes if ( SIESTA_worker ) then final = . true . if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'Finalization' ) endif #ifdef SIESTA__FLOOK ! Call lua right before doing the analysis, ! possibly changing some of the variables call slua_call ( LUA , LUA_ANALYSIS ) #endif ! !     NOTE that the geometry output by the following sections !     used to be that \"predicted\" for the next MD or relaxation step. !     This is now changed ! if ( IOnode ) then ! Print atomic coordinates ! This covers CG and MD-quench (verlet, pr), instances in which ! \"relaxd\" is meaningful if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( relaxd ) then ! xa = xa_last ! The \"relaxation\" routines do not update ! the coordinates if relaxed, so this behavior is unchanged call outcoor ( ucell , xa , na_u , 'Relaxed' , . true . ) else ! Since xa = xa_last now, this will just repeat the ! last set of coordinates used, not the predicted ones. call outcoor ( ucell , xa , na_u , 'Final (unrelaxed)' , . true . ) endif endif ! This call will write xa_last to the .STRUCT_OUT file ! (again, since it has already been written by state_init), ! CML records of the latest processed structure, and ! possibly zmatrix info.  *** unmoved?? how? ! Note that the .STRUCT_NEXT_ITER file is produced ! in siesta_move for checkpointing of relaxations and MD runs. ! If all we want are the CML records (to satisfy some expectation ! of appearance in the \"finalization\" section, we might put the ! cml call explicitly and forget about the rest. if ( compat_pre_v4_dynamics ) then call siesta_write_positions ( moved = . true .) else call siesta_write_positions ( moved = . false .) endif ! ??  Clarify Zmatrix behavior **** if ( lUseZmatrix ) call write_Zmatrix ! Print unit cell (cell_last) for variable cell and server operation if ( varcel . or . ( idyn . eq . 8 )) call outcell ( ucell ) !------------------------------------------------------------------ ! It can be argued that these needed the xa_last coordinates ! all along !       Print coordinates in xmol format in a separate file if ( fdf_boolean ( 'WriteCoorXmol' ,. false .)) & call coxmol ( iza , xa , na_u ) !       Print coordinates in cerius format in a separate file if ( fdf_boolean ( 'WriteCoorCerius' ,. false .)) & call coceri ( iza , xa , ucell , na_u , sname ) !       Find interatomic distances (output in file BONDS_FINAL) call bonds ( ucell , na_u , isa , xa , & rmax_bonds , trim ( slabel ) // \".BONDS_FINAL\" ) endif ! IONode !--- end output of geometry information ! ! ! NOTE: In the following sections, wavefunction generation, computation !       of band structure, etc, are carried out using the last Hamiltonian !       generated in the SCF run for the last geometry considered. !   But, if xa /= xa_last, the computation of, say, bands, will use !      H phases which are not the same as those producing the final !      ground-state electronic structure. ! !    Also, since we have removed the replication (superx call) !      of \"moved\" coordinates !      into the auxiliary supercell from 'siesta_move' (recall that it is !      done always in state_init for every new geometry), the \"moved unit !      cell coordinates\" could coexist here with \"unmoved non-unit cell SC coords\", !      which is wrong. !      For all of the above, we should put here a sanity safeguard !        (if we have not done so already at the top of this routine) !        xa(1:3,1:na_s) = xa_last(1:3,1:na_s) !        ucell(1:3,1:3) = ucell_last(1:3,1:3) !        scell(1:3,1:3) = scell_last(1:3,1:3) !        DM and H issues ! !        Some of the routines that follow use H and S, and some use the DM. !        Those which use the DM should work with the final \"out\" DM for !        consistency. !        Those which use H,S should work with the latest diagonalized H,S pair. ! !      If mixing the DM during the scf loop we should avoid mixing it one more time !        after convergence (or restoring Dold) !        If mixing H, we should avoid mixing it one more time !        after convergence (and restoring Hold to have the exact H that generated the !        latest DM, although this is probably too much). !        See the logic in siesta_forces. !     Find and print wavefunctions at selected k-points !   This uses H,S, and xa if ( nwk . gt . 0 ) then wfs_filename = trim ( slabel ) // \".selected.WFSX\" if ( IONode ) print \"(a)\" , $ \"Writing WFSX for selected k-points in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , & nwk , & numh , listhptr , listh , H , S , Ef , xijo , indxuo , & nwk , wfk , no_u , gamma , occtol ) endif !   This uses H,S, and xa if ( write_coop ) then ! Output the wavefunctions for the kpoints in the SCF set ! Note that we allow both a band number and an energy range ! The user is responsible for using appropriate values. wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( kpoints_scf % N , no_u , & wfs_band_min , wfs_band_max , $ use_scf_weights = . true ., $ use_energy_window = . true .) wfs_filename = trim ( slabel ) // \".fullBZ.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for COOP/COHP in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . kpoints_scf % N , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . kpoints_scf % N , kpoints_scf % k , no_u , gamma , occtol ) endif !     Compute bands !   This uses H,S, and xa nullify ( ebk ) call re_alloc ( ebk , 1 , no_u , 1 , spinor_dim , 1 , maxbk , & 'ebk' , 'siesta_analysis' ) if ( nbk . gt . 0 ) then if ( IONode ) print \"(a)\" , \"Computing bands...\" getPSI = fdf_get ( 'WFS.Write.For.Bands' , . false .) if ( getPSI ) then wfs_filename = trim ( slabel ) // \".bands.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for bands in \" $ // trim ( wfs_filename ) wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nbk , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . false ., $ use_energy_window = . false .) endif call bands ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . maxbk , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . . true ., nbk , bk , ebk , occtol , getPSI ) if ( IOnode ) then if ( writbk ) then write ( 6 , '(/,a,/,a4,a12)' ) & 'siesta: Band k vectors (Bohr**-1):' , 'ik' , 'k' do ik = 1 , nbk write ( 6 , '(i4,3f12.6)' ) ik , ( bk ( ix , ik ), ix = 1 , 3 ) enddo endif if ( writb ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Band energies (eV):' , 'ik' , 'is' , 'eps' do ispin = 1 , spinor_dim do ik = 1 , nbk write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin , ( ebk ( io , ispin , ik ) / eV , io = 1 , min ( 10 , no_u )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( ebk ( io , ispin , ik ) / eV , io = 11 , no_u ) enddo enddo endif endif endif !     Print eigenvalues if ( IOnode . and . writeig ) then if (( isolve . eq . SOLVE_DIAGON . or . & (( isolve . eq . SOLVE_MINIM ) . and . minim_calc_eigenvalues )) & . and . no_l . lt . 1000 ) then if ( h_spin_dim <= 2 ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Eigenvalues (eV):' , 'ik' , 'is' , 'eps' do ik = 1 , kpoints_scf % N do ispin = 1 , spinor_dim write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin ,( eo ( io , ispin , ik ) / eV , io = 1 , min ( 10 , neigwanted )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( eo ( io , ispin , ik ) / eV , io = 11 , neigwanted ) enddo enddo else write ( 6 , '(/,a)' ) 'siesta: Eigenvalues (eV):' do ik = 1 , kpoints_scf % N write ( 6 , '(a,i6)' ) 'ik =' , ik write ( 6 , '(10f7.2)' ) & (( eo ( io , ispin , ik ) / eV , io = 1 , neigwanted ), ispin = 1 , 2 ) enddo endif write ( 6 , '(a,f15.6,a)' ) 'siesta: Fermi energy =' , ef / eV , ' eV' endif endif if ((( isolve . eq . SOLVE_DIAGON ). or . & (( isolve . eq . SOLVE_MINIM ). and . minim_calc_eigenvalues )) & . and . IOnode ) & call ioeig ( eo , ef , neigwanted , nspin , kpoints_scf % N , & no_u , spinor_dim , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w ) !   This uses H,S, and xa, as it diagonalizes them again call projected_DOS () !     Print program's energy decomposition and final forces if ( IOnode ) then call siesta_write_energies ( iscf = 0 , dDmax = 0._dp , dHmax = 0._dp ) ! final == .true. which makes the step counter irrelevant call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () call write_basis_enthalpy ( FreeE , FreeEHarris ) endif ! NOTE: Here, the spin polarization is computed using Dscf, which is !       a density matrix obtained after mixing the \"in\" and \"out\" !       DMs of the SCF run for the last geometry considered. !       This can be considered a feature or a bug. call print_spin ( qspin ) ! qspin returned for use below !     This uses the last computed dipole in dhscf during the scf cycle, !     which is in fact derived from the \"in\" DM. !     Perhaps this section should be moved after the call to dhscf below !     AND use the DM_out of the last step (but there might not be a call !     to dhscf if there are no files to output, and the computation of the !     charge density is expensive... !     Print electric dipole if ( shape . ne . 'bulk' ) then if ( IOnode ) then write ( 6 , '(/,a,3f12.6)' ) & 'siesta: Electric dipole (a.u.)  =' , dipol write ( 6 , '(a,3f12.6)' ) & 'siesta: Electric dipole (Debye) =' , & ( dipol ( ix ) / Debye , ix = 1 , 3 ) endif if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = dipol / Debye , & title = 'Electric dipole' , dictref = 'siesta:dipol' , . units = 'siestaUnits:Debye' ) endif !cml_p endif ! NOTE: The use of *_last geometries in the following sections !       guarantees that the analysis of the electronic structure !       is done for the geometry for which it was computed. !  BUT these routines need H,S, so H should not be mixed after !       convergence. !     Calculation of the bulk polarization using the Berry phase !     formulas by King-Smith and Vanderbilt if ( nkpol . gt . 0 . and . . not . bornz ) then if ( NonCol . or . SpOrb ) then if ( IOnode ) then write ( 6 , '(/a)' ) . 'siesta_analysis: bulk polarization implemented only for' write ( 6 , '(/a)' ) . 'siesta_analysis: paramagnetic or collinear spin runs' endif else call KSV_pol ( na_u , na_s , xa_last , rmaxo , scell_last , & ucell_last , no_u , no_l , no_s , nspin , qspin , & maxnh , nkpol , numh , listhptr , listh , & H , S , xijo , indxuo , isa , iphorb , & iaorb , lasto , shape , & nkpol , kpol , wgthpol , polR , polxyz ) endif endif !     Calculation of the optical conductivity call optical ( na_u , na_s , xa_last , scell_last , ucell_last , & no_u , no_l , no_s , nspin , qspin , & maxnh , numh , listhptr , listh , H , S , xijo , $ indxuo , ebk , ef , temp , & isa , iphorb , iphKB , lasto , lastkb , shape ) call de_alloc ( ebk , 'ebk' , 'siesta_analysis' ) !................................... ! !  NOTE: Dscf here might be the mixed one (see above). ! want_partial_charges = ( hirshpop . or . voropop ) . AND . $ (. not . partial_charges_at_every_geometry ) !     Save electron density and potential if ( saverho . or . savedrho . or . saverhoxc . or . & savevh . or . savevt . or . savevna . or . & savepsch . or . savetoch . or . & save_ebs_dens . or . & want_partial_charges ) then if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( save_ebs_dens ) filesOut % ebs_dens = trim ( slabel ) // '.EBS' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' g2max = g2cut dummy_str = 0.0 dummy_strl = 0.0 call dhscf ( nspin , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa_last , indxua , & ntm , 0 , 0 , 0 , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_str , fa , dummy_strl ) ! next to last argument is dummy here, ! as no forces are calculated ! todo: make all these optional endif C C Call the wannier90 interface here , as local_DOS destroys the DM ... C if ( w90_processing ) call siesta2wannier90 () C Find local density of states !  It needs H,S, and xa, as it diagonalizes them again !  NOTE: This call will obliterate Dscf !  It is better to put a explicit out argument for the partial DM computed. call local_DOS () ! In summary, it is better to: ! !   -- Avoid (or warn the user about) doing any analysis if the calculation is not converged !   -- Avoid mixing DM or H after scf convergence !   -- Set xa to xa_last at the top of this file. Write any \"next iter\" coordinate file !      in 'siesta_move' endif ! SIESTA_worker #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.LDOS\" ,. false .)) then call pexsi_local_DOS () endif #endif if ( SIESTA_worker ) call timer ( \"Analysis\" , 2 ) !------------------------------------------------------------------------- END END subroutine siesta_analysis","tags":"","loc":"proc/siesta_analysis.html","title":"siesta_analysis – SIESTA"},{"text":"public subroutine state_init(istep) Uses kpoint_scf_m kpoint_t_m m_os m_new_dm m_proximity_check siesta_options units sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices create_Sparsity_SC m_sparsity_handling m_sparsity_handling m_pivot_methods siesta_geom atomlist alloc m_hsparse m_overlap m_supercell siesta_cml siesta_cml siesta_cml siesta_cml zmatrix m_energies write_subs m_ioxv m_iotdxv m_steps parallel m_spin m_rmaxh m_mixing m_mixing_scf m_normalize_dm m_eo m_gamma files m_mpi_utils m_mpi_utils domain_decom ldau_specs fdf sys m_sparse ts_kpoint_scf_m m_ts_charge m_ts_options m_ts_options m_ts_options m_ts_electype m_ts_global_vars sys m_ts_io m_ts_sparse m_ts_tri_init files m_chess iodm_netcdf iodmhs_netcdf class_Sparsity class_dSpData1D class_dSpData2D class_zSpData2D class_dData2D m_test_io siesta_dicts proc~~state_init~~UsesGraph proc~state_init state_init m_sparse m_sparse proc~state_init->m_sparse siesta_options siesta_options proc~state_init->siesta_options m_os m_os proc~state_init->m_os m_test_io m_test_io proc~state_init->m_test_io ts_kpoint_scf_m ts_kpoint_scf_m proc~state_init->ts_kpoint_scf_m ldau_specs ldau_specs proc~state_init->ldau_specs write_subs write_subs proc~state_init->write_subs m_ts_options m_ts_options proc~state_init->m_ts_options class_dData2D class_dData2D proc~state_init->class_dData2D m_supercell m_supercell proc~state_init->m_supercell m_proximity_check m_proximity_check proc~state_init->m_proximity_check m_overlap m_overlap proc~state_init->m_overlap m_energies m_energies proc~state_init->m_energies domain_decom domain_decom proc~state_init->domain_decom kpoint_scf_m kpoint_scf_m proc~state_init->kpoint_scf_m atomlist atomlist proc~state_init->atomlist m_steps m_steps proc~state_init->m_steps m_ts_electype m_ts_electype proc~state_init->m_ts_electype m_normalize_dm m_normalize_dm proc~state_init->m_normalize_dm class_dSpData2D class_dSpData2D proc~state_init->class_dSpData2D siesta_geom siesta_geom proc~state_init->siesta_geom m_sparsity_handling m_sparsity_handling proc~state_init->m_sparsity_handling m_ts_charge m_ts_charge proc~state_init->m_ts_charge m_iotdxv m_iotdxv proc~state_init->m_iotdxv m_ts_tri_init m_ts_tri_init proc~state_init->m_ts_tri_init iodmhs_netcdf iodmhs_netcdf proc~state_init->iodmhs_netcdf units units proc~state_init->units kpoint_t_m kpoint_t_m proc~state_init->kpoint_t_m m_mpi_utils m_mpi_utils proc~state_init->m_mpi_utils class_zSpData2D class_zSpData2D proc~state_init->class_zSpData2D files files proc~state_init->files m_pivot_methods m_pivot_methods proc~state_init->m_pivot_methods siesta_dicts siesta_dicts proc~state_init->siesta_dicts class_Sparsity class_Sparsity proc~state_init->class_Sparsity m_ts_global_vars m_ts_global_vars proc~state_init->m_ts_global_vars m_ts_io m_ts_io proc~state_init->m_ts_io m_chess m_chess proc~state_init->m_chess sys sys proc~state_init->sys m_ioxv m_ioxv proc~state_init->m_ioxv siesta_cml siesta_cml proc~state_init->siesta_cml m_spin m_spin proc~state_init->m_spin module~m_mixing m_mixing proc~state_init->module~m_mixing m_new_dm m_new_dm proc~state_init->m_new_dm m_hsparse m_hsparse proc~state_init->m_hsparse parallel parallel proc~state_init->parallel class_dSpData1D class_dSpData1D proc~state_init->class_dSpData1D sparse_matrices sparse_matrices proc~state_init->sparse_matrices alloc alloc proc~state_init->alloc m_gamma m_gamma proc~state_init->m_gamma fdf fdf proc~state_init->fdf create_Sparsity_SC create_Sparsity_SC proc~state_init->create_Sparsity_SC m_eo m_eo proc~state_init->m_eo m_rmaxh m_rmaxh proc~state_init->m_rmaxh m_ts_sparse m_ts_sparse proc~state_init->m_ts_sparse module~m_mixing_scf m_mixing_scf proc~state_init->module~m_mixing_scf iodm_netcdf iodm_netcdf proc~state_init->iodm_netcdf zmatrix zmatrix proc~state_init->zmatrix class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D module~m_mixing_scf->module~m_mixing module~m_mixing_scf->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: istep Calls proc~~state_init~~CallsGraph proc~state_init state_init volcel volcel proc~state_init->volcel kpoint_delete kpoint_delete proc~state_init->kpoint_delete mscell mscell proc~state_init->mscell escf escf proc~state_init->escf re_alloc re_alloc proc~state_init->re_alloc newddata2d newddata2d proc~state_init->newddata2d nsc nsc proc~state_init->nsc proc~mixers_history_init mixers_history_init proc~state_init->proc~mixers_history_init globalize_or globalize_or proc~state_init->globalize_or iotdxv iotdxv proc~state_init->iotdxv timer timer proc~state_init->timer time_io time_io proc~state_init->time_io message message proc~state_init->message proc~check_cohp check_cohp proc~state_init->proc~check_cohp file_exist file_exist proc~state_init->file_exist listhptr listhptr proc~state_init->listhptr crtsparsity_sc crtsparsity_sc proc~state_init->crtsparsity_sc setup_kpoint_scf setup_kpoint_scf proc~state_init->setup_kpoint_scf proximity_check proximity_check proc~state_init->proximity_check siesta_write_positions siesta_write_positions proc~state_init->siesta_write_positions superc superc proc~state_init->superc attach attach proc~state_init->attach cmlendpropertylist cmlendpropertylist proc~state_init->cmlendpropertylist newdspdata1d newdspdata1d proc~state_init->newdspdata1d ts_sparse_init ts_sparse_init proc~state_init->ts_sparse_init setup_dm_netcdf_file setup_dm_netcdf_file proc~state_init->setup_dm_netcdf_file sp_to_spglobal sp_to_spglobal proc~state_init->sp_to_spglobal bye bye proc~state_init->bye globalize_sum globalize_sum proc~state_init->globalize_sum ioxv ioxv proc~state_init->ioxv setup_ts_kpoint_scf setup_ts_kpoint_scf proc~state_init->setup_ts_kpoint_scf init_val init_val proc~state_init->init_val newsparsity newsparsity proc~state_init->newsparsity setup_ordern_indexes setup_ordern_indexes proc~state_init->setup_ordern_indexes setup_dmhs_netcdf_file setup_dmhs_netcdf_file proc~state_init->setup_dmhs_netcdf_file cmlstartpropertylist cmlstartpropertylist proc~state_init->cmlstartpropertylist newdspdata2d newdspdata2d proc~state_init->newdspdata2d new_dm new_dm proc~state_init->new_dm madelung madelung proc~state_init->madelung chess_init chess_init proc~state_init->chess_init exact_sc_ag exact_sc_ag proc~state_init->exact_sc_ag normalize_dm normalize_dm proc~state_init->normalize_dm get_chess_parameter get_chess_parameter proc~state_init->get_chess_parameter numh numh proc~state_init->numh write_zmatrix write_zmatrix proc~state_init->write_zmatrix hsparse hsparse proc~state_init->hsparse dict_repopulate_md dict_repopulate_md proc~state_init->dict_repopulate_md cmladdproperty cmladdproperty proc~state_init->cmladdproperty val val proc~state_init->val outcoor outcoor proc~state_init->outcoor die die proc~state_init->die sporb_to_spatom sporb_to_spatom proc~state_init->sporb_to_spatom xij_offset xij_offset proc~state_init->xij_offset fdf_get fdf_get proc~state_init->fdf_get newzspdata2d newzspdata2d proc~state_init->newzspdata2d ts_tri_analyze ts_tri_analyze proc~state_init->ts_tri_analyze kpoint_nullify kpoint_nullify proc~state_init->kpoint_nullify ucell ucell proc~state_init->ucell overlap overlap proc~state_init->overlap write_debug write_debug proc~state_init->write_debug ts_write_tshs ts_write_tshs proc~state_init->ts_write_tshs fname_tshs fname_tshs proc~state_init->fname_tshs nscold nscold proc~state_init->nscold isc_off isc_off proc~state_init->isc_off delete delete proc~state_init->delete domaindecom domaindecom proc~state_init->domaindecom proc~mixers_history_init->delete new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt proc~check_cohp->message Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~state_init~~CalledByGraph proc~state_init state_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code state_init Source Code subroutine state_init ( istep ) use kpoint_scf_m , only : setup_kpoint_scf , kpoints_scf use kpoint_t_m , only : kpoint_delete , kpoint_nullify use m_os , only : file_exist use m_new_dm , only : new_dm use m_proximity_check , only : proximity_check use siesta_options use units , only : Ang use sparse_matrices , only : maxnh , numh , listh , listhptr use sparse_matrices , only : Dold , Dscf , DM_2D use sparse_matrices , only : Eold , Escf , EDM_2D use sparse_matrices , only : Hold , H , H_2D use sparse_matrices , only : xijo , xij_2D use sparse_matrices , only : S , S_1D use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : sparse_pattern use sparse_matrices , only : block_dist , single_dist use sparse_matrices , only : DM_history use create_Sparsity_SC , only : crtSparsity_SC use m_sparsity_handling , only : SpOrb_to_SpAtom use m_sparsity_handling , only : Sp_to_Spglobal use m_pivot_methods , only : sp2graphviz use siesta_geom use atomlist , only : iphorb , iphkb , indxua , & rmaxo , rmaxkb , rmaxv , rmaxldau , & lastkb , lasto , superc , indxuo , & no_u , no_s , no_l , iza , qtots use alloc , only : re_alloc , de_alloc , alloc_report use m_hsparse , only : hsparse use m_overlap , only : overlap use m_supercell , only : exact_sc_ag use siesta_cml , only : cml_p , cmlStartStep , mainXML use siesta_cml , only : cmlStartPropertyList use siesta_cml , only : cmlEndPropertyList use siesta_cml , only : cmlAddProperty use zmatrix , only : lUseZmatrix , write_zmatrix use m_energies , only : Emad use write_subs use m_ioxv , only : ioxv use m_iotdxv , only : iotdxv use m_steps use parallel , only : IOnode , node , nodes , BlockSize use m_spin , only : spin use m_rmaxh use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_normalize_dm , only : normalize_dm use m_eo use m_gamma use files , only : slabel use m_mpi_utils , only : globalize_or use m_mpi_utils , only : globalize_sum use domain_decom , only : domainDecom , use_dd , use_dd_perm use ldau_specs , only : switch_ldau , ldau_init use fdf , only : fdf_get use sys , only : message , die use m_sparse , only : xij_offset use ts_kpoint_scf_m , only : setup_ts_kpoint_scf , ts_kpoints_scf use m_ts_charge , only : TS_RHOCORR_METHOD , TS_RHOCORR_FERMI use m_ts_options , only : BTD_method use m_ts_options , only : TS_Analyze use m_ts_options , only : N_Elec , Elecs , IsVolt use m_ts_electype use m_ts_global_vars , only : TSrun , TSmode , onlyS use sys , only : bye use m_ts_io , only : fname_TSHS , ts_write_tshs use m_ts_sparse , only : ts_sparse_init use m_ts_tri_init , only : ts_tri_init , ts_tri_analyze use files , only : slabel , label_length #ifdef SIESTA__CHESS use m_chess , only : CheSS_init , get_CheSS_parameter #endif #ifdef CDF use iodm_netcdf , only : setup_dm_netcdf_file use iodmhs_netcdf , only : setup_dmhs_netcdf_file #endif use class_Sparsity use class_dSpData1D use class_dSpData2D use class_zSpData2D use class_dData2D #ifdef TEST_IO use m_test_io #endif #ifdef SIESTA__FLOOK use siesta_dicts , only : dict_repopulate_MD #endif implicit none integer :: istep , nnz real ( dp ) :: veclen ! Length of a unit-cell vector real ( dp ) :: rmax logical :: cell_can_change integer :: i , ix , iadispl , ixdispl logical :: auxchanged ! Auxiliary supercell changed? logical :: folding , folding1 logical :: diag_folding , diag_folding1 logical :: foundxv ! dummy for call to ioxv external :: madelung , timer real ( dp ), external :: volcel integer :: ts_kscell_file ( 3 , 3 ) = 0 real ( dp ) :: ts_kdispl_file ( 3 ) = 0.0 logical :: ts_Gamma_file = . true . character ( len = label_length + 6 ) :: fname real ( dp ) :: dummyef = 0.0 , dummyqtot = 0.0 #ifdef SIESTA__CHESS integer :: maxnh_kernel , maxnh_mult , no_l_kernel , no_l_mult integer , dimension (:), allocatable :: listh_kernel , listh_mult integer , dimension (:), allocatable :: numh_kernel , numh_mult real ( dp ) :: chess_value #endif type ( Sparsity ) :: g_Sp character ( len = 256 ) :: oname type ( dData2D ) :: tmp_2D real ( dp ) :: dummy_qspin ( 8 ) !------------------------------------------------------------------- BEGIN call timer ( 'IterGeom' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_init' ) #endif call timer ( 'state_init' , 1 ) istp = istp + 1 if ( IOnode ) then write ( 6 , '(/,t22,a)' ) repeat ( '=' , 36 ) select case ( idyn ) case ( 0 ) if ( nmove == 0 ) then write ( 6 , '(t25,a)' ) 'Single-point calculation' if ( cml_p ) call cmlStartStep ( mainXML , type = 'Single-Point' , $ index = istp ) else if ( broyden_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin Broyden opt. move = ' , $ istep else if ( fire_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin FIRE opt. move = ' , $ istep else write ( 6 , '(t25,a,i6)' ) 'Begin CG opt. move = ' , $ istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'Geom. Optim' , $ index = istp ) endif !        Print Z-matrix coordinates if ( lUseZmatrix ) then call write_Zmatrix () endif case ( 1 , 3 ) if ( iquench > 0 ) then write ( 6 , '(t25,a,i6)' ) 'Begin MD quenched step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD-quenched' , $ index = istep ) else write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , $ index = istep ) endif case ( 2 , 4 , 5 ) write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , index = istep ) case ( 6 ) write ( 6 , '(t25,a,i6)' ) 'Begin FC step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FC' , index = istep ) if ( istep . eq . 0 ) then write ( 6 , '(t25,a)' ) 'Undisplaced coordinates' else iadispl = ( istep - mod ( istep - 1 , 6 )) / 6 + ia1 ix = mod ( istep - 1 , 6 ) + 1 ixdispl = ( ix - mod ( ix - 1 , 2 ) + 1 ) / 2 write ( 6 , '(t26,a,i0,/,t26,a,i1,a,f10.6,a)' ) 'displace atom ' , & iadispl , 'in direction ' , ixdispl , ' by' , dx / Ang , ' Ang' endif case ( 8 ) write ( 6 , '(t25,a,i6)' ) 'Begin Server step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FS' , index = istep ) case ( 9 ) if ( istep == 0 ) then write ( 6 , '(t25,a,i7)' ) 'Explicit coord. initialization' else write ( 6 , '(t25,a,i7)' ) 'Explicit coord. step =' , istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'ECS' , index = istep ) case ( 10 ) write ( 6 , '(t25,a,i7)' ) 'LUA coord. step =' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'LUA' , index = istep ) end select write ( 6 , '(t22,a)' ) repeat ( '=' , 36 ) !     Print atomic coordinates call outcoor ( ucell , xa , na_u , ' ' , writec ) !     Save structural information in crystallographic format !     (in file SystemLabel.STRUCT_OUT), !     canonical Zmatrix (if applicable), and CML record call siesta_write_positions ( moved = . false .) endif ! IONode ! Write the XV file for single-point calculations, so that ! it is there at the end for those users who rely on it call ioxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , & foundxv ) ! Write TDXV file for TDDFT restart. if ( writetdwf . or . td_elec_dyn ) then call iotdxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , foundxv ) end if !     Actualize things if variable cell auxchanged = . false . cell_can_change = ( varcel . or . & ( idyn . eq . 8 ) ! Force/stress evaluation & ) if ( change_kgrid_in_md ) then cell_can_change = cell_can_change . or . & ( idyn . eq . 3 ) . or . ! Parrinello-Rahman & ( idyn . eq . 4 ) . or . ! Nose-Parrinello-Rahman & ( idyn . eq . 5 ) ! Anneal endif if ( cell_can_change . and . & ( istep . ne . inicoor ) . and . (. not . gamma ) ) then !       Will print k-points also call kpoint_delete ( kpoints_scf ) call setup_kpoint_scf ( ucell ) if ( TSmode ) then call kpoint_delete ( ts_kpoints_scf ) else call kpoint_nullify ( ts_kpoints_scf ) end if call setup_ts_kpoint_scf ( ucell , kpoints_scf ) call re_alloc ( eo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'eo' , 'state_init' ) call re_alloc ( qo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'qo' , 'state_init' ) !       Find required supercell if ( gamma ) then nsc ( 1 : 3 ) = 1 else do i = 1 , 3 veclen = sqrt ( ucell ( 1 , i ) ** 2 + ucell ( 2 , i ) ** 2 + ucell ( 3 , i ) ** 2 ) nsc ( i ) = 1 + 2 * ceiling ( rmaxh / veclen ) end do ! The above is kept for historical reasons, ! but a tight supercell can be found from the atom-graph info: call exact_sc_ag ( negl , ucell , na_u , isa , xa , nsc ) endif mscell = 0.0_dp do i = 1 , 3 mscell ( i , i ) = nsc ( i ) if ( nsc ( i ) /= nscold ( i )) auxchanged = . true . nscold ( i ) = nsc ( i ) enddo !       Madelung correction for charged systems if ( charnet . ne . 0.0_dp ) then call madelung ( ucell , shape , charnet , Emad ) endif endif !     End variable cell actualization !     Auxiliary supercell !     Do not move from here, as the coordinates might have changed !     even if not the unit cell call superc ( ucell , scell , nsc ) #ifdef SIESTA__FLOOK call dict_repopulate_MD () #endif !     Print unit cell and compute cell volume !     Possible BUG: !     Note that this volume is later used in write_subs and the md output !     routines, even if the cell later changes. if ( IOnode ) call outcell ( ucell ) volume_of_some_cell = volcel ( ucell ) !     Use largest possible range in program, except hsparse... !     2 * rmaxv: Vna overlap !     rmaxo + rmaxkb: Non-local KB action !     2 * (rmaxo + rmaxldau): Interaction through LDAU projector !     2.0_dp * (rmaxo+rmaxkb) : Orbital interaction through KB projectors rmax = max ( 2._dp * rmaxv , 2._dp * ( rmaxo + rmaxldau ), rmaxo + rmaxkb ) if ( . not . negl ) then rmax = max ( rmax , 2.0_dp * ( rmaxo + rmaxkb ) ) endif !     Check if any two atoms are unreasonably close call proximity_check ( rmax ) ! Clear history of mixing parameters call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) ! Ensure sparsity pattern is empty call delete ( sparse_pattern ) ! sadly deleting the sparse pattern does not necessarily ! mean that the arrays are de-associated. ! Remember that the reference counter could (in MD) ! be higher than 1, hence we need to create \"fake\" ! containers and let the new<class> delete the old ! sparsity pattern nullify ( numh , listhptr , listh ) allocate ( numh ( no_l ), listhptr ( no_l )) ! We do not need to allocate listh ! that will be allocated in hsparse #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then !         Calculate a sparsity pattern with some buffers... Only required !         for CheSS chess_value = get_chess_parameter ( 'chess_buffer_kernel' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_kernel = maxnh no_l_kernel = no_l allocate ( listh_kernel ( maxnh_kernel )) allocate ( numh_kernel ( no_l_kernel )) listh_kernel = listh numh_kernel = numh chess_value = get_chess_parameter ( 'chess_buffer_mult' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_mult = maxnh no_l_mult = no_l allocate ( listh_mult ( maxnh_mult )) allocate ( numh_mult ( no_l_mult )) listh_mult = listh numh_mult = numh end if #endif /* CHESS */ !     List of nonzero Hamiltonian matrix elements !     and, if applicable,  vectors between orbital centers !     Listh and xijo are allocated inside hsparse !     Note: We always generate xijo now, for COOP and other !           analyses. call delete ( xij_2D ) ! as xijo will be reallocated nullify ( xijo ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , $ set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ debug_folding = fdf_get ( 'debug-folding' ,. false .)) ! call globalize_or ( diag_folding1 , diag_folding ) call globalize_or ( folding1 , folding ) if ( diag_folding . and . gamma ) then call message ( \"WARNING\" , \"Gamma-point calculation \" // $ \"with interaction between periodic images\" ) call message ( \"WARNING\" , $ \"Some features might not work optimally:\" ) call message ( \"WARNING\" , $ \"e.g. DM initialization from atomic data\" ) if ( harrisfun ) call die ( \"Harris functional run needs \" // $ \"'force-aux-cell T'\" ) else if ( folding ) then if ( gamma ) then call message ( \"INFO\" , \"Gamma-point calculation \" // $ \"with multiply-connected orbital pairs\" ) call message ( \"INFO\" , $ \"Folding of H and S implicitly performed\" ) call check_cohp () else write ( 6 , \"(a,/,a)\" ) \"Non Gamma-point calculation \" // $ \"with multiply-connected orbital pairs \" // $ \"in auxiliary supercell.\" , $ \"Possible internal error. \" // $ \"Use 'debug-folding T' to debug.\" call die ( \"Inadequate auxiliary supercell\" ) endif endif ! call globalize_sum ( maxnh , nnz ) if ( cml_p ) then call cmlStartPropertyList ( mainXML , title = 'Orbital info' ) call cmlAddProperty ( xf = mainXML , value = no_u , $ title = 'Number of orbitals in unit cell' , $ dictref = 'siesta:no_u' , units = \"cmlUnits:countable\" ) call cmlAddProperty ( xf = mainXML , value = nnz , $ title = 'Number of non-zeros' , $ dictref = 'siesta:nnz' , units = \"cmlUnits:countable\" ) call cmlEndPropertyList ( mainXML ) endif ! #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then call CheSS_init ( node , nodes , maxnh , maxnh_kernel , maxnh_mult , & no_u , no_l , no_l_kernel , no_l_mult , BlockSize , & spin % spinor , qtots , listh , listh_kernel , listh_mult , & numh , numh_kernel , numh_mult ) deallocate ( listh_kernel ) deallocate ( numh_kernel ) deallocate ( listh_mult ) deallocate ( numh_mult ) end if #endif /* CHESS */ ! ! If using domain decomposition, redistribute orbitals ! for this geometry, based on the hsparse info. ! The first time round, the initial distribution is a ! simple block one (given by preSetOrbitLimits). ! ! Any DM, etc, read from file will be redistributed according ! to the new pattern. ! Inherited DMs from a previous geometry cannot be used if the ! orbital distribution changes. For now, we avoid changing the ! distribution (the variable use_dd_perm is .true. if domain ! decomposition is in effect). Names should be changed... if ( use_dd . and . (. not . use_dd_perm )) then call domainDecom ( no_u , no_l , maxnh ) ! maxnh intent(in) here maxnh = sum ( numh ( 1 : no_l )) ! We still need to re-create Julian Gale's ! indexing for O(N) in parallel. print \"(a5,i3,a20,3i8)\" , $ \"Node: \" , Node , \"no_u, no_l, maxnh: \" , no_u , no_l , maxnh call setup_ordern_indexes ( no_l , no_u , Nodes ) endif ! I would like to skip this alloc/move/dealloc/attach ! by allowing sparsity to have pointer targets. ! However, this poses a problem with intel compilers, ! as it apparently errors out when de-allocating a target pointer write ( oname , \"(a,i0)\" ) \"sparsity for geom step \" , istep call newSparsity ( sparse_pattern , no_l , no_u , maxnh , & numh , listhptr , listh , name = oname ) deallocate ( numh , listhptr , listh ) call attach ( sparse_pattern , & n_col = numh , list_ptr = listhptr , list_col = listh ) ! In case the user requests to create the connectivity graph if ( write_GRAPHVIZ > 0 ) then ! first create the unit-cell sparsity pattern call crtSparsity_SC ( sparse_pattern , g_Sp , UC = . true .) ! next move to global sparsity pattern call Sp_to_Spglobal ( block_dist , g_Sp , g_Sp ) if ( IONode ) then if ( write_GRAPHVIZ /= 2 ) & call sp2graphviz ( trim ( slabel ) // '.ORB.gv' , g_Sp ) ! Convert to atomic if ( write_GRAPHVIZ /= 1 ) then call SpOrb_to_SpAtom ( single_dist , g_Sp , na_u , lasto , g_Sp ) call sp2graphviz ( trim ( slabel ) // '.ATOM.gv' , g_Sp ) end if end if call delete ( g_Sp ) end if ! Copy over xijo array (we can first do it here... :( ) call newdData2D ( tmp_2D , xijo , 'xijo' ) deallocate ( xijo ) write ( oname , \"(a,i0)\" ) \"xijo at geom step \" , istep call newdSpData2D ( sparse_pattern , tmp_2D , block_dist , xij_2D , & name = oname ) call delete ( tmp_2D ) ! decrement container... xijo => val ( xij_2D ) ! Calculate the super-cell offsets... if ( Gamma ) then ! Here we create the super-cell offsets call re_alloc ( isc_off , 1 , 3 , 1 , 1 ) isc_off (:,:) = 0 else call xij_offset ( ucell , nsc , na_u , xa , lasto , & xij_2D , isc_off , & Bcast = . true .) end if ! When the user requests to only do an analyzation, we can call ! appropriate routines and quit if ( TS_Analyze ) then ! Force the creation of the full sparsity pattern call ts_sparse_init ( slabel , IsVolt , N_Elec , Elecs , & ucell , nsc , na_u , xa , lasto , block_dist , sparse_pattern , & Gamma , isc_off ) ! create the tri-diagonal matrix call ts_tri_analyze ( block_dist , sparse_pattern , N_Elec , & Elecs , ucell , na_u , lasto , nsc , isc_off , & BTD_method ) ! Print-out timers call timer ( 'TS-rgn2tri' , 3 ) ! Bye also waits for all processors call bye ( 'transiesta analyzation performed' ) end if write ( oname , \"(a,i0)\" ) \"EDM at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % EDM , block_dist , EDM_2D , & name = oname ) !if (ionode) call print_type(EDM_2D) Escf => val ( EDM_2D ) call re_alloc ( Dold , 1 , maxnh , 1 , spin % DM , name = 'Dold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) call re_alloc ( Hold , 1 , maxnh , 1 , spin % H , name = 'Hold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) if ( converge_EDM ) then call re_alloc ( Eold , 1 , maxnh , 1 , spin % EDM , name = 'Eold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) end if !     Allocate/reallocate storage associated with Hamiltonian/Overlap matrix write ( oname , \"(a,i0)\" ) \"H at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H , block_dist , H_2D , & name = oname ) !if (ionode) call print_type(H_2D) H => val ( H_2D ) write ( oname , \"(a,i0)\" ) \"H_vkb at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_vkb_1D , name = oname ) !if (ionode) call print_type(H_vkb_1D) write ( oname , \"(a,i0)\" ) \"H_kin at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_kin_1D , name = oname ) !if (ionode) call print_type(H_kin_1D) if ( switch_ldau ) then write ( oname , \"(a,i0)\" ) \"H_ldau at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % spinor , & block_dist , H_ldau_2D , name = oname ) ! Initialize to 0, LDA+U may re-calculate !   this matrix sporadically doing the SCF. ! Hence initialization MUST be performed upon ! re-allocation. call init_val ( H_ldau_2D ) if ( inicoor /= istep ) then ! Force initialization of the LDA+U ! when changing geometry ! For the first geometry this is controlled ! by the user via an fdf-key ldau_init = . true . end if end if if ( spin % SO_onsite ) then write ( oname , \"(a,i0)\" ) \"H_so (onsite) at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H - 2 , & block_dist , H_so_on_2D , name = oname ) else if ( spin % SO_offsite ) then write ( oname , \"(a,i0)\" ) \"H_so (offsite) at geom step \" , istep call newzSpData2D ( sparse_pattern , 4 , & block_dist , H_so_off_2D , name = oname ) endif write ( oname , \"(a,i0)\" ) \"S at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , S_1D , name = oname ) if ( ionode ) call print_type ( S_1D ) S => val ( S_1D ) !     Find overlap matrix call overlap ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , maxnh , & lasto , iphorb , isa , numh , listhptr , listh , S ) ! !     Here we could also read a Hamiltonian, either to proceed to !     the analysis section (with nscf=0) or to start a mix-H scf cycle. ! !     Initialize density matrix ! The resizing of Dscf is done inside new_dm call new_DM ( auxchanged , DM_history , DM_2D , EDM_2D ) Dscf => val ( DM_2D ) Escf => val ( EDM_2D ) if ( spin % H > 1 ) call print_spin ( dummy_qspin ) ! Initialize energy-density matrix to zero for first call to overfsm ! Only part of Escf is updated in TS, so if it is put as zero here ! a continuation run gives bad forces. if ( . not . TSrun ) then call normalize_DM ( first = . true . ) !$OMP parallel workshare default(shared) Escf (:,:) = 0.0_dp !$OMP end parallel workshare end if #ifdef TEST_IO ! We test the io-performance here call time_io ( spin % H , H_2D ) #endif !     If onlyS, Save overlap matrix and exit if ( onlyS ) then fname = fname_TSHS ( slabel , onlyS = . true . ) ! We include H as S, well-knowing that we only write one of ! them, there is no need to allocate space for no reason! call ts_write_tshs ( fname , & . true ., Gamma , ts_Gamma_file , & ucell , nsc , isc_off , na_u , no_s , spin % H , & ts_kscell_file , ts_kdispl_file , & xa , lasto , & H_2D , S_1D , indxuo , & dummyEf , dummyQtot , Temp , 0 , 0 ) call bye ( 'Save overlap matrix and exit' ) ! Exit siesta endif ! In case the user is requesting a Fermi-correction ! we need to delete the TS_FERMI file after each iteration if ( TSmode . and . TS_RHOCORR_METHOD == TS_RHOCORR_FERMI & . and . IONode ) then ! Delete the TS_FERMI file (enables ! reading it in and improve on the convergence) if ( file_exist ( 'TS_FERMI' ) ) then i = 23455 ! this should just not be used any were... ! Delete the file... open ( unit = i , file = 'TS_FERMI' ) close ( i , status = 'delete' ) end if end if #ifdef CDF if ( writedm_cdf ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh ) endif if ( writedm_cdf_history ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & istep ) endif if ( writedmhs_cdf ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s ) endif if ( writedmhs_cdf_history ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s , & istep ) endif #endif call timer ( 'state_init' , 2 ) END subroutine state_init","tags":"","loc":"proc/state_init.html","title":"state_init – SIESTA"},{"text":"private subroutine check_cohp() Uses siesta_options sys proc~~check_cohp~~UsesGraph proc~check_cohp check_cohp siesta_options siesta_options proc~check_cohp->siesta_options sys sys proc~check_cohp->sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~check_cohp~~CallsGraph proc~check_cohp check_cohp message message proc~check_cohp->message Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~check_cohp~~CalledByGraph proc~check_cohp check_cohp proc~state_init state_init proc~state_init->proc~check_cohp proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code check_cohp Source Code subroutine check_cohp () use siesta_options , only : write_coop use sys , only : message if ( write_coop ) then call message ( \"WARNING\" , \"There are multiply-connected \" // $ \"orbitals.\" ) call message ( \"WARNING\" , \"Your COOP/COHP analysis might \" // $ \"be affected by folding.\" ) call message ( \"WARNING\" , 'Use \"force-aux-cell T \"' // $ 'or k-point sampling' ) endif end subroutine check_cohp","tags":"","loc":"proc/check_cohp.html","title":"check_cohp – SIESTA"},{"text":"public subroutine state_analysis(istep) Uses siesta_cml m_born_charge parallel m_wallclock zmatrix atomlist atomlist m_spin m_fixed sparse_matrices siesta_geom siesta_options units m_stress m_energies m_energies m_ntm m_forces m_energies m_intramol_pressure flook_siesta proc~~state_analysis~~UsesGraph proc~state_analysis state_analysis siesta_options siesta_options proc~state_analysis->siesta_options m_fixed m_fixed proc~state_analysis->m_fixed flook_siesta flook_siesta proc~state_analysis->flook_siesta siesta_geom siesta_geom proc~state_analysis->siesta_geom sparse_matrices sparse_matrices proc~state_analysis->sparse_matrices siesta_cml siesta_cml proc~state_analysis->siesta_cml atomlist atomlist proc~state_analysis->atomlist m_forces m_forces proc~state_analysis->m_forces m_born_charge m_born_charge proc~state_analysis->m_born_charge m_energies m_energies proc~state_analysis->m_energies m_stress m_stress proc~state_analysis->m_stress m_wallclock m_wallclock proc~state_analysis->m_wallclock m_spin m_spin proc~state_analysis->m_spin units units proc~state_analysis->units m_intramol_pressure m_intramol_pressure proc~state_analysis->m_intramol_pressure m_ntm m_ntm proc~state_analysis->m_ntm parallel parallel proc~state_analysis->parallel zmatrix zmatrix proc~state_analysis->zmatrix Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: istep Calls proc~~state_analysis~~CallsGraph proc~state_analysis state_analysis amass amass proc~state_analysis->amass va va proc~state_analysis->va volcel volcel proc~state_analysis->volcel moments moments proc~state_analysis->moments kin_stress kin_stress proc~state_analysis->kin_stress siesta_write_forces siesta_write_forces proc~state_analysis->siesta_write_forces cartesianforce_to_zmatforce cartesianforce_to_zmatforce proc~state_analysis->cartesianforce_to_zmatforce print_spin print_spin proc~state_analysis->print_spin wallclock wallclock proc~state_analysis->wallclock siesta_write_stress_pressure siesta_write_stress_pressure proc~state_analysis->siesta_write_stress_pressure remove_intramol_pressure remove_intramol_pressure proc~state_analysis->remove_intramol_pressure slua_call slua_call proc~state_analysis->slua_call update_freeeharris update_freeeharris proc~state_analysis->update_freeeharris update_freee update_freee proc~state_analysis->update_freee born_charge born_charge proc~state_analysis->born_charge cmladdproperty cmladdproperty proc~state_analysis->cmladdproperty cmlstartmodule cmlstartmodule proc~state_analysis->cmlstartmodule mulliken mulliken proc~state_analysis->mulliken eggbox eggbox proc~state_analysis->eggbox timer timer proc~state_analysis->timer cmlendmodule cmlendmodule proc~state_analysis->cmlendmodule write_debug write_debug proc~state_analysis->write_debug fixed fixed proc~state_analysis->fixed Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~state_analysis~~CalledByGraph proc~state_analysis state_analysis proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_analysis Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code state_analysis Source Code subroutine state_analysis ( istep ) use siesta_cml use m_born_charge , only : born_charge use parallel , only : IOnode use m_wallclock , only : wallclock use zmatrix , only : lUseZmatrix , iofaZmat , & CartesianForce_to_ZmatForce use atomlist , only : iaorb , iphorb , amass , no_u , lasto use atomlist , only : indxuo use m_spin , only : spin use m_fixed , only : fixed use sparse_matrices use siesta_geom USE siesta_options use units , only : amu , eV use m_stress use m_energies , only : Etot , FreeE , Eharrs , FreeEHarris , Entropy use m_energies , only : Ebs , Ef use m_ntm use m_forces use m_energies , only : update_FreeE , update_FreeEHarris use m_intramol_pressure , only : remove_intramol_pressure #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_FORCES #endif implicit none integer :: istep integer :: ia , jx , ix real ( dp ) :: volume logical :: eggbox_block = . true . ! Read eggbox info from data file? real ( dp ) :: qspin external :: eggbox , mulliken , moments real ( dp ), external :: volcel !------------------------------------------------------------------------- BEGIN call timer ( 'state_analysis' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_analysis' ) #endif if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'SCF Finalization' ) endif !     Write final Kohn-Sham and Free Energy FreeE = Etot - Temp * Entropy FreeEHarris = Eharrs - Temp * Entropy if ( cml_p ) call cmlStartPropertyList ( mainXML , & title = 'Energies and spin' ) if ( IOnode ) then if ( . not . harrisfun ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS(eV) =        ' , Etot / eV if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = FreeE / eV , & dictref = 'siesta:FreeE' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ebs / eV , & dictref = 'siesta:Ebs' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ef / eV , & dictref = 'siesta:E_Fermi' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif endif !     Substract egg box effect from energy if ( eggbox_block ) then call eggbox ( 'energy' , ucell , na_u , isa , ntm , xa , fa , Etot , & eggbox_block ) FreeE = Etot - Temp * Entropy if ( IOnode ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS - E_eggbox = ' , Etot / eV if ( cml_p ) call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS_egg' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif call update_FreeE ( Temp ) call update_FreeEHarris ( Temp ) call print_spin ( qspin ) if ( cml_p ) call cmlEndPropertyList ( mainXML ) !     Substract egg box effect from the forces if ( eggbox_block ) then call eggbox ( 'forces' , ucell , na_u , isa , ntm , xa , fa , Etot , eggbox_block ) endif if ( IOnode ) call write_raw_efs ( stress , na_u , fa , FreeE ) !     Compute stress without internal molecular pressure call remove_intramol_pressure ( ucell , stress , na_u , xa , fa , mstress ) !     Impose constraints to atomic movements by changing forces ........... if ( RemoveIntraMolecularPressure ) then !        Consider intramolecular pressure-removal as another !        kind of constraint call fixed ( ucell , mstress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) else call fixed ( ucell , stress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) endif #ifdef SIESTA__FLOOK ! We call it right after using the ! geometry constraints. ! In that way we can use both methods on top ! of each other! ! The easy, already implemented methods in fixed, ! and custom ones in Lua :) call slua_call ( LUA , LUA_FORCES ) #endif !     Calculate and output Zmatrix forces if ( lUseZmatrix . and . ( idyn . eq . 0 )) then call CartesianForce_to_ZmatForce ( na_u , xa , fa ) if ( IOnode ) call iofaZmat () endif !     Compute kinetic contribution to stress kin_stress ( 1 : 3 , 1 : 3 ) = 0.0_dp volume = volcel ( ucell ) do ia = 1 , na_u do jx = 1 , 3 do ix = 1 , 3 kin_stress ( ix , jx ) = kin_stress ( ix , jx ) - & amu * amass ( ia ) * va ( ix , ia ) * va ( jx , ia ) / volume enddo enddo enddo !     Add kinetic term to stress tensor tstress = stress + kin_stress !     Force output if ( IOnode ) then call siesta_write_forces ( istep ) call siesta_write_stress_pressure () call wallclock ( '--- end of geometry step' ) endif !     Population and moment analysis if ( spin % SO . and . orbmoms ) then call moments ( 1 , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) endif ! Call this unconditionally call mulliken ( mullipop , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) ! !     Call the born effective charge routine only in those steps (even) !     in which the dx  is positive. if ( bornz . and . ( mod ( istep , 2 ) . eq . 0 )) then call born_charge () endif !     End the xml module corresponding to the analysis if ( cml_p ) then call cmlEndModule ( mainXML ) endif call timer ( 'state_analysis' , 2 ) !--------------------------------------------------------------------------- END END subroutine state_analysis","tags":"","loc":"proc/state_analysis.html","title":"state_analysis – SIESTA"},{"text":"public subroutine dhscf_init(nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ucell, mscell, g2max, ntm, maxnd, numd, listdptr, listd, datm, Fal, stressl) Uses precision parallel atmfuncs fdf sys mesh parsing siestaXC bsc_xcmod alloc siesta_options meshsubs meshsubs meshsubs meshsubs meshsubs meshsubs meshsubs moreMeshSubs moreMeshSubs moreMeshSubs meshdscf iogrid_netcdf m_ncdf_io cellxc_mod m_efield m_efield m_efield m_efield m_doping_uniform m_doping_uniform m_rhog m_rhog siesta_options mpi_siesta m_mesh_node m_charge_add m_hartree_add m_ts_global_vars m_ts_options m_ts_voltage m_ts_hartree proc~~dhscf_init~~UsesGraph proc~dhscf_init dhscf_init m_charge_add m_charge_add proc~dhscf_init->m_charge_add m_ncdf_io m_ncdf_io proc~dhscf_init->m_ncdf_io m_ts_options m_ts_options proc~dhscf_init->m_ts_options parsing parsing proc~dhscf_init->parsing meshdscf meshdscf proc~dhscf_init->meshdscf m_ts_voltage m_ts_voltage proc~dhscf_init->m_ts_voltage siestaXC siestaXC proc~dhscf_init->siestaXC m_mesh_node m_mesh_node proc~dhscf_init->m_mesh_node cellxc_mod cellxc_mod proc~dhscf_init->cellxc_mod m_efield m_efield proc~dhscf_init->m_efield m_doping_uniform m_doping_uniform proc~dhscf_init->m_doping_uniform siesta_options siesta_options proc~dhscf_init->siesta_options m_ts_global_vars m_ts_global_vars proc~dhscf_init->m_ts_global_vars module~moremeshsubs moreMeshSubs proc~dhscf_init->module~moremeshsubs precision precision proc~dhscf_init->precision meshsubs meshsubs proc~dhscf_init->meshsubs sys sys proc~dhscf_init->sys mesh mesh proc~dhscf_init->mesh mpi_siesta mpi_siesta proc~dhscf_init->mpi_siesta m_rhog m_rhog proc~dhscf_init->m_rhog bsc_xcmod bsc_xcmod proc~dhscf_init->bsc_xcmod m_ts_hartree m_ts_hartree proc~dhscf_init->m_ts_hartree iogrid_netcdf iogrid_netcdf proc~dhscf_init->iogrid_netcdf alloc alloc proc~dhscf_init->alloc atmfuncs atmfuncs proc~dhscf_init->atmfuncs fdf fdf proc~dhscf_init->fdf parallel parallel proc~dhscf_init->parallel m_hartree_add m_hartree_add proc~dhscf_init->m_hartree_add module~moremeshsubs->precision module~moremeshsubs->sys module~moremeshsubs->alloc module~moremeshsubs->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in) :: norb integer, intent(in) :: iaorb (norb) integer, intent(in) :: iphorb (norb) integer, intent(in) :: nuo integer, intent(in) :: nuotot integer, intent(in) :: nua integer, intent(in) :: na integer, intent(in) :: isa (na) real(kind=dp), intent(in) :: xa (3,na) integer, intent(in) :: indxua (na) real(kind=dp), intent(in) :: ucell (3,3) integer, intent(in) :: mscell (3,3) real(kind=dp), intent(inout) :: g2max integer, intent(inout) :: ntm (3) integer, intent(in) :: maxnd integer, intent(in) :: numd (nuo) integer, intent(in) :: listdptr (nuo) integer, intent(in) :: listd (maxnd) real(kind=dp), intent(in) :: datm (norb) real(kind=dp), intent(inout) :: Fal (3,nua) real(kind=dp), intent(inout) :: stressl (3,3) Calls proc~~dhscf_init~~CallsGraph proc~dhscf_init dhscf_init volcel volcel proc~dhscf_init->volcel ts_init_voltage ts_init_voltage proc~dhscf_init->ts_init_voltage re_alloc re_alloc proc~dhscf_init->re_alloc rcut rcut proc~dhscf_init->rcut de_alloc de_alloc proc~dhscf_init->de_alloc fdf_get fdf_get proc~dhscf_init->fdf_get setupextmesh setupextmesh proc~dhscf_init->setupextmesh distriphionmesh distriphionmesh proc~dhscf_init->distriphionmesh initmesh initmesh proc~dhscf_init->initmesh leqi leqi proc~dhscf_init->leqi proc~setmeshdistr setMeshDistr proc~dhscf_init->proc~setmeshdistr initatommesh initatommesh proc~dhscf_init->initatommesh proc~reord reord proc~dhscf_init->proc~reord init_hartree_add init_hartree_add proc~dhscf_init->init_hartree_add ts_init_hartree_fix ts_init_hartree_fix proc~dhscf_init->ts_init_hartree_fix partialcoreonmesh partialcoreonmesh proc~dhscf_init->partialcoreonmesh digcel digcel proc~dhscf_init->digcel rcore rcore proc~dhscf_init->rcore fdf_integer fdf_integer proc~dhscf_init->fdf_integer init_mesh_node init_mesh_node proc~dhscf_init->init_mesh_node setgga setgga proc~dhscf_init->setgga phionmesh phionmesh proc~dhscf_init->phionmesh initialize_efield initialize_efield proc~dhscf_init->initialize_efield getxc getxc proc~dhscf_init->getxc createlocaldscfpointers createlocaldscfpointers proc~dhscf_init->createlocaldscfpointers init_charge_add init_charge_add proc~dhscf_init->init_charge_add neutralatomonmesh neutralatomonmesh proc~dhscf_init->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata die die proc~dhscf_init->die elecs elecs proc~dhscf_init->elecs timer timer proc~dhscf_init->timer shaper shaper proc~dhscf_init->shaper user_specified_field user_specified_field proc~dhscf_init->user_specified_field write_debug write_debug proc~dhscf_init->write_debug cdf_init_mesh cdf_init_mesh proc~dhscf_init->cdf_init_mesh set_box_limits set_box_limits proc~dhscf_init->set_box_limits rhooda rhooda proc~dhscf_init->rhooda compute_doping_structs_uniform compute_doping_structs_uniform proc~dhscf_init->compute_doping_structs_uniform ddot ddot proc~dhscf_init->ddot meshlim meshlim proc~setmeshdistr->meshlim proc~reord->re_alloc proc~reord->de_alloc proc~reord->timer proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->die proc~distmeshdata_rea->timer proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->die proc~distmeshdata_int->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~dhscf_init~~CalledByGraph proc~dhscf_init dhscf_init proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code dhscf_init Source Code subroutine dhscf_init ( nspin , norb , iaorb , iphorb , & nuo , nuotot , nua , na , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnd , numd , listdptr , listd , datm , & Fal , stressl ) use precision , only : dp , grid_p use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use fdf use sys , only : die use mesh , only : xdsp , nsm , nsp , meshLim use parsing #ifndef BSC_CELLXC use siestaXC , only : getXC ! Returns the XC functional used #else /* BSC_CELLXC */ use bsc_xcmod , only : nXCfunc , XCauth #endif /* BSC_CELLXC */ use alloc , only : re_alloc , de_alloc use siesta_options , only : harrisfun use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use meshsubs , only : PhiOnMesh use meshsubs , only : InitMesh use meshsubs , only : InitAtomMesh use meshsubs , only : setupExtMesh use meshsubs , only : distriPhiOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use meshdscf , only : createLocalDscfPointers use iogrid_netcdf , only : set_box_limits #ifdef NCDF_4 use m_ncdf_io , only : cdf_init_mesh #endif #ifdef BSC_CELLXC use cellxc_mod , only : setGGA #endif /* BSC_CELLXC */ use m_efield , only : initialize_efield , acting_efield use m_efield , only : get_field_from_dipole use m_efield , only : dipole_correction use m_efield , only : user_specified_field use m_doping_uniform , only : initialize_doping_uniform use m_doping_uniform , only : compute_doping_structs_uniform , $ doping_active use m_rhog , only : rhog , rhog_in use m_rhog , only : order_rhog use siesta_options , only : mix_charge #ifdef MPI use mpi_siesta #endif use m_mesh_node , only : init_mesh_node use m_charge_add , only : init_charge_add use m_hartree_add , only : init_hartree_add use m_ts_global_vars , only : TSmode use m_ts_options , only : IsVolt , N_Elec , Elecs use m_ts_voltage , only : ts_init_voltage use m_ts_hartree , only : ts_init_hartree_fix implicit none integer , intent ( in ) :: nspin , norb , iaorb ( norb ), iphorb ( norb ), & nuo , nuotot , nua , na , isa ( na ), & indxua ( na ), mscell ( 3 , 3 ), maxnd , & numd ( nuo ), listdptr ( nuo ), listd ( maxnd ) real ( dp ), intent ( in ) :: xa ( 3 , na ), ucell ( 3 , 3 ), datm ( norb ) real ( dp ), intent ( inout ) :: g2max integer , intent ( inout ) :: ntm ( 3 ) real ( dp ), intent ( inout ) :: Fal ( 3 , nua ), stressl ( 3 , 3 ) real ( dp ), parameter :: tiny = 1.e-12_dp integer :: io , ia , iphi , is , n , i , j integer :: nsc ( 3 ), nbcell , nsd real ( dp ) :: DStres ( 3 , 3 ), volume real ( dp ), external :: volcel , ddot real ( grid_p ) :: dummy_Drho ( 1 , 1 ), dummy_Vaux ( 1 ), & dummy_Vscf ( 1 ) logical , save :: frstme = . true . ! Keeps state real ( grid_p ), pointer :: Vscf (:,:), rhoatm_par (:) integer , pointer :: numphi (:), numphi_par (:) integer :: nm ( 3 ) ! For call to initMesh #ifndef BSC_CELLXC integer :: nXCfunc character ( len = 20 ) :: XCauth ( 10 ), XCfunc ( 10 ) #endif /* ! BSC_CELLXC */ ! Transport direction (unit-cell aligned) integer :: iE real ( dp ) :: ortho , field ( 3 ), field2 ( 3 ) !--------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE dhscf_init' ) #endif ! ---------------------------------------------------------------------- !     General initialisation ! ---------------------------------------------------------------------- !     Start time counter call timer ( 'DHSCF_Init' , 1 ) nsd = min ( nspin , 2 ) nullify ( Vscf , rhoatm_par ) if ( frstme ) then debug_dhscf = fdf_get ( 'Debug.DHSCF' , . false .) nullify ( xdsp , rhopcc , Vna , rhoatm ) !       nsm lives in module m_dhscf now    !! AG** nsm = fdf_integer ( 'MeshSubDivisions' , 2 ) nsm = max ( nsm , 1 ) !       Set mesh sub-division variables & perform one off allocation nsp = nsm * nsm * nsm call re_alloc ( xdsp , 1 , 3 , 1 , nsp , 'xdsp' , 'dhscf_init' ) !       Check spin-spiral wavevector (if defined) if ( spiral . and . nspin . lt . 4 ) & call die ( 'dhscf: ERROR: spiral defined but nspin < 4' ) endif ! First time #ifndef BSC_CELLXC ! Get functional(s) being used call getXC ( nXCfunc , XCfunc , XCauth ) #endif /* ! BSC_CELLXC */ if ( harrisfun ) then do n = 1 , nXCfunc if (. not .( leqi ( XCauth ( n ), 'PZ' ). or . leqi ( XCauth ( n ), 'CA' ))) then call die ( \"** Harris forces not implemented for non-LDA XC\" ) endif enddo endif ! ---------------------------------------------------------------------- !     Orbital initialisation : part 1 ! ---------------------------------------------------------------------- !     Find the maximum orbital radius rmax = 0.0_dp do io = 1 , norb ia = iaorb ( io ) ! Atomic index of each orbital iphi = iphorb ( io ) ! Orbital index of each  orbital in its atom is = isa ( ia ) ! Species index of each atom rmax = max ( rmax , rcut ( is , iphi ) ) enddo !     Start time counter for mesh initialization call timer ( 'DHSCF1' , 1 ) ! ---------------------------------------------------------------------- !     Unit cell handling ! ---------------------------------------------------------------------- !     Find diagonal unit cell and supercell call digcel ( ucell , mscell , cell , scell , nsc , IsDiag ) if (. not . IsDiag ) then if ( Node . eq . 0 ) then write ( 6 , '(/,a,3(/,a,3f12.6,a,i6))' ) & 'DHSCF: WARNING: New shape of unit cell and supercell:' , & ( 'DHSCF:' ,( cell ( i , j ), i = 1 , 3 ), '   x' , nsc ( j ), j = 1 , 3 ) endif endif !     Find the system shape call shaper ( cell , nua , isa , xa , shape , nbcell , bcell ) !     Find system volume volume = volcel ( cell ) ! ---------------------------------------------------------------------- !     Mesh initialization ! ---------------------------------------------------------------------- call InitMesh ( na , cell , norb , iaorb , iphorb , isa , rmax , & G2max , G2mesh , nsc , nmpl , nm , & nml , ntm , ntml , ntpl , dvol ) !     Setup box descriptors for each processor, !     held in module iogrid_netcdf call set_box_limits ( ntm , nsm ) ! Initialize information on local mesh for each node call init_mesh_node ( cell , ntm , meshLim , nsm ) ! Setup charge additions in the mesh call init_charge_add ( cell , ntm ) ! Setup Hartree additions in the mesh call init_hartree_add ( cell , ntm ) #ifdef NCDF_4 ! Initialize the box for each node... call cdf_init_mesh ( ntm , nsm ) #endif !     Stop time counter for mesh initialization call timer ( 'DHSCF1' , 2 ) ! ---------------------------------------------------------------------- !     End of mesh initialization ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     Initialize atomic orbitals, density and potential ! ---------------------------------------------------------------------- !     Start time counter for atomic initializations call timer ( 'DHSCF2' , 1 ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Initialise quantities relating to the atom-mesh positioning call InitAtomMesh ( UNIFORM , na , xa ) #ifdef BSC_CELLXC !     Check if we need extencils in cellxc call setGGA ( ) #endif /* BSC_CELLXC */ !     Compute the number of orbitals on the mesh and recompute the !     partions for every processor in order to have a similar load !     in each of them. nullify ( numphi ) call re_alloc ( numphi , 1 , nmpl , 'numphi' , 'dhscf_init' ) !$OMP parallel do default(shared), private(i) do i = 1 , nmpl numphi ( i ) = 0 enddo !$OMP end parallel do call distriPhiOnMesh ( nm , nmpl , norb , iaorb , iphorb , & isa , numphi ) !     Find if there are partial-core-corrections for any atom npcc = 0 do ia = 1 , na if ( rcore ( isa ( ia )) . gt . tiny ) npcc = 1 enddo !     Find partial-core-correction energy density !     Vscf and Vaux are not used here call re_alloc ( rhopcc , 1 , ntpl * npcc + 1 , 'rhopcc' , 'dhscf_init' ) if ( npcc . eq . 1 ) then call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , & nsd , dvol , volume , dummy_Vscf , dummy_Vaux , Fal , stressl , & . false ., . false . ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhopcc' , sqrt ( sum ( rhopcc ** 2 )) end if endif !     Find neutral-atom potential !     Drho is not used here call re_alloc ( Vna , 1 , ntpl , 'Vna' , 'dhscf_init' ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , dummy_DRho , Fal , stressl , & . false ., . false . ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Vna' , sqrt ( sum ( Vna ** 2 )) end if if ( nodes . gt . 1 ) then if ( node . eq . 0 ) then write ( 6 , \"(a)\" ) \"Setting up quadratic distribution...\" endif call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) !       Create extended mesh arrays for the second data distribution call setupExtMesh ( QUADRATIC , rmax ) !       Compute atom positions for the second data distribution call InitAtomMesh ( QUADRATIC , na , xa ) endif !     Calculate orbital values on mesh !     numphi has already been computed in distriPhiOnMesh !     in the UNIFORM distribution if ( nodes . eq . 1 ) then numphi_par => numphi else nullify ( numphi_par ) call re_alloc ( numphi_par , 1 , nmpl , 'numphi_par' , & 'dhscf_init' ) call distMeshData ( UNIFORM , numphi , QUADRATIC , & numphi_par , KEEP ) endif call PhiOnMesh ( nmpl , norb , iaorb , iphorb , isa , numphi_par ) if ( nodes . gt . 1 ) then call de_alloc ( numphi_par , 'numphi_par' , 'dhscf_init' ) endif call de_alloc ( numphi , 'numphi' , 'dhscf_init' ) ! ---------------------------------------------------------------------- !       Create sparse indexing for Dscf as needed for local mesh !       Note that this is done in the QUADRATIC distribution !       since 'endpht' (computed finally in PhiOnMesh and stored in !       meshphi module) is in that distribution. ! ---------------------------------------------------------------------- if ( Nodes . gt . 1 ) then call CreateLocalDscfPointers ( nmpl , nuotot , numd , listdptr , & listd ) endif ! ---------------------------------------------------------------------- !     Calculate terms relating to the neutral atoms on the mesh ! ---------------------------------------------------------------------- !     Find Harris (sum of atomic) electron density call re_alloc ( rhoatm_par , 1 , ntpl , 'rhoatm_par' , 'dhscf_init' ) call rhooda ( norb , nmpl , datm , rhoatm_par , iaorb , iphorb , isa ) !     rhoatm_par comes out of here in clustered form in QUADRATIC dist !     Routine Poison should use the uniform data distribution if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Create Rhoatm using UNIFORM distr, in sequential form call re_alloc ( rhoatm , 1 , ntpl , 'rhoatm' , 'dhscf_init' ) call distMeshData ( QUADRATIC , rhoatm_par , & UNIFORM , rhoatm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhoatm' , sqrt ( sum ( rhoatm ** 2 )) end if ! !  AG: The initialization of doping structs could be done here now, !      in the uniform distribution, and with a simple loop over !      rhoatm. if ( frstme ) call initialize_doping_uniform () if ( doping_active ) then call compute_doping_structs_uniform ( ntpl , rhoatm , nsd ) ! Will get the global number of hit points ! Then, the doping density to be added can be simply computed endif !     Allocate Temporal array call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf_init' ) !     Vscf is filled here but not used later !     Uharrs is computed (and saved) !     DStres is computed but not used later call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , rhoatm , & Uharrs , Vscf , DStres , nsm ) call de_alloc ( Vscf , 'Vscf' , 'dhscf_init' ) !     Always deallocate rhoatm_par, as it was used even if nodes=1 call de_alloc ( rhoatm_par , 'rhoatm_par' , 'dhscf_init' ) if ( mix_charge ) then call re_alloc ( rhog , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog' , 'dhscf_init' ) call re_alloc ( rhog_in , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog_in' , 'dhscf_init' ) call order_rhog ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nsm ) endif !     Stop time counter for atomic initializations call timer ( 'DHSCF2' , 2 ) ! ---------------------------------------------------------------------- !     At the end of initializations: !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution ! ---------------------------------------------------------------------- if ( frstme ) then call initialize_efield () end if ! Check if we need to add the potential ! corresponding to the voltage-drop. if ( TSmode ) then ! These routines are important if there are cell-changes call ts_init_hartree_fix ( cell , nua , xa , ntm , ntml ) if ( IsVolt ) then call ts_init_voltage ( cell , nua , xa , ntm ) end if if ( acting_efield ) then ! We do not allow the electric field for ! transiesta runs with V = 0, either. ! It does not make sense, only for fields perpendicular ! to the applied bias. ! We need to check that the e-field is perpendicular ! to the transport direction, and that the system is ! either a chain, or a slab. ! However, due to the allowance of a dipole correction ! along the transport direction for buffer calculations ! we have to allow all shapes. (atom is not transiesta ! compatible anyway) ! check that we do not violate the periodicity if ( Node . eq . 0 ) then write ( * , '(/,2(2a,/))' ) 'ts-WARNING: ' , & 'E-field/dipole-correction! ' , & 'ts-WARNING: ' , & 'I hope you know what you are doing!' end if ! This is either dipole or user, or both field (:) = user_specified_field (:) do iE = 1 , N_Elec field2 = Elecs ( iE )% cell (:, Elecs ( iE )% t_dir ) ortho = ddot ( 3 , field2 , 1 , field , 1 ) if ( abs ( ortho ) > 1.e-9_dp ) then call die ( 'User defined E-field must be &perpendicular to semi-infinite directions' ) end if end do end if ! acting_efield ! We know that we currently allow people to do more than ! they probably should be allowed. However, there are many ! corner cases that may require dipole corrections, or ! electric fields to \"correct\" an intrinsic dipole. ! For instance, what should we do with a dipole in a transiesta ! calculation? ! Should we apply a field to counter act it in a device ! calculation? end if frstme = . false . call timer ( 'DHSCF_Init' , 2 ) #ifdef DEBUG call write_debug ( '    POS dhscf_init' ) #endif !------------------------------------------------------------------------- END end subroutine dhscf_init","tags":"","loc":"proc/dhscf_init.html","title":"dhscf_init – SIESTA"},{"text":"public subroutine dhscf(nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ntm, ifa, istr, iHmat, filesOut, maxnd, numd, listdptr, listd, Dscf, datm, maxnh, Hmat, Enaatm, Enascf, Uatm, Uscf, DUscf, DUext, Exc, Dxc, dipol, stress, Fal, stressl, use_rhog_in, charge_density_only) Uses precision parallel parallel atmfuncs units fdf sys mesh parsing m_iorho m_forhar alloc files files siesta_options siesta_options meshsubs meshsubs meshsubs moreMeshSubs moreMeshSubs moreMeshSubs m_partial_charges m_partial_charges siestaXC siestaXC siestaXC m_vmat m_rhoofd mpi_siesta iogrid_netcdf iogrid_netcdf siesta_options siesta_options siesta_options siesta_options m_efield m_efield m_efield m_doping_uniform m_charge_add m_hartree_add siesta_options m_ncdf_siesta m_rhofft m_rhog m_spin m_spin m_iotddft m_ts_global_vars m_ts_options m_ts_voltage m_ts_hartree proc~~dhscf~~UsesGraph proc~dhscf dhscf files files proc~dhscf->files m_charge_add m_charge_add proc~dhscf->m_charge_add m_rhog m_rhog proc~dhscf->m_rhog m_partial_charges m_partial_charges proc~dhscf->m_partial_charges m_ts_options m_ts_options proc~dhscf->m_ts_options parsing parsing proc~dhscf->parsing m_iorho m_iorho proc~dhscf->m_iorho m_ts_voltage m_ts_voltage proc~dhscf->m_ts_voltage m_rhoofd m_rhoofd proc~dhscf->m_rhoofd siestaXC siestaXC proc~dhscf->siestaXC m_forhar m_forhar proc~dhscf->m_forhar m_iotddft m_iotddft proc~dhscf->m_iotddft units units proc~dhscf->units m_efield m_efield proc~dhscf->m_efield m_doping_uniform m_doping_uniform proc~dhscf->m_doping_uniform siesta_options siesta_options proc~dhscf->siesta_options m_rhofft m_rhofft proc~dhscf->m_rhofft m_ts_global_vars m_ts_global_vars proc~dhscf->m_ts_global_vars m_ncdf_siesta m_ncdf_siesta proc~dhscf->m_ncdf_siesta module~moremeshsubs moreMeshSubs proc~dhscf->module~moremeshsubs precision precision proc~dhscf->precision meshsubs meshsubs proc~dhscf->meshsubs sys sys proc~dhscf->sys mesh mesh proc~dhscf->mesh mpi_siesta mpi_siesta proc~dhscf->mpi_siesta m_spin m_spin proc~dhscf->m_spin parallel parallel proc~dhscf->parallel m_vmat m_vmat proc~dhscf->m_vmat m_ts_hartree m_ts_hartree proc~dhscf->m_ts_hartree iogrid_netcdf iogrid_netcdf proc~dhscf->iogrid_netcdf alloc alloc proc~dhscf->alloc atmfuncs atmfuncs proc~dhscf->atmfuncs fdf fdf proc~dhscf->fdf m_hartree_add m_hartree_add proc~dhscf->m_hartree_add module~moremeshsubs->precision module~moremeshsubs->sys module~moremeshsubs->parallel module~moremeshsubs->alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Calculates the self-consistent field contributions to Hamiltonian\n matrix elements, total energy and atomic forces. Coded by J.M. Soler, August 1996. July 1997. Modified by J.D. Gale, February 2000. Units Energies in Rydbergs. Distances in Bohr. Routines called internally cellxc(...)    : Finds total exch-corr energy and potential CROSS : Finds the cross product of two vectors dfscf(...)     : Finds SCF contribution to atomic forces dipole(...)    : Finds electric dipole moment doping(...)    : Adds a background charge for doped systems write_rho(...)     : Saves electron density on a file poison(...)    : Solves Poisson equation reord(...)     : Reorders electron density and potential arrays rhooda(...)    : Finds Harris electron density in the mesh rhoofd(...)    : Finds SCF electron density in the mesh rhoofdsp(...)  : Finds SCF electron density in the mesh for spiral arrangement of spins timer(...)     : Finds CPU times vmat(...)      : Finds matrix elements of SCF potential vmatsp(...)    : Finds matrix elements of SCF potential for\n                    spiral arrangement of spins delk(...)      : Finds matrix elements of exp(i \\vec{k} \\cdot \\vec{r}) real*8 volcel( cell ) : Returns volume of unit cell Internal variables and arrays real*8  bcell(3,3) : Bulk lattice vectors real*8  cell(3,3) : Auxiliary lattice vectors (same as ucell) real*8  const : Auxiliary variable (constant within a loop) real*8  DEc : Auxiliary variable to call cellxc real*8  DEx : Auxiliary variable to call cellxc real*8  dvol : Mesh-cell volume real*8  Ec : Correlation energy real*8  Ex : Exchange energy real*8  field(3) : External electric field integer i : General-purpose index integer ia : Atom index integer io : Orbital index integer ip : Point index integer is : Species index logical IsDiag : Is supercell diagonal? integer ispin : Spin index integer j : General-purpose index integer JDGdistr : J.D.Gale's parallel distribution of mesh points integer myBox(2,3) : My processor's mesh box integer nbcell : Number of independent bulk lattice vectors integer npcc : Partial core corrections? (0=no, 1=yes) integer nsd : Number of diagonal spin values (1 or 2) integer ntpl : Number of mesh Total Points in unit cell\n                           (including subpoints) locally real*4  rhoatm(ntpl) : Harris electron density real*4  rhopcc(ntpl) : Partial-core-correction density for xc real*4  DRho(ntpl) : Selfconsistent electron density difference real*8  rhotot : Total density at one point real*8  rmax : Maximum orbital radius real*8  scell(3,3) : Supercell vectors character shape*10 : Name of system shape real*4  Vaux(ntpl) : Auxiliary potential array real*4  Vna(ntpl) : Sum of neutral-atom potentials real*8  volume : Unit cell volume real*4  Vscf(ntpl) : Hartree potential of selfconsistent density real*8  x0(3) : Center of molecule logical harrisfun : Harris functional or Kohn-Sham? Use the functionality in the first block\n of the routine to get charge files and partial charges Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Number of different spin polarisations: nspin=1 => Unpolarized, nspin=2 => polarized nspin=4 => Noncollinear spin or spin-orbit. integer, intent(in) :: norb Total number of basis orbitals in supercell integer, intent(in) :: iaorb (norb) Atom to which each orbital belongs integer, intent(in) :: iphorb (norb) Orbital index (within atom) of each orbital integer, intent(in) :: nuo Number of orbitals in a unit cell in this node integer, intent(in) :: nuotot Number of orbitals in a unit cell integer, intent(in) :: nua Number of atoms in unit cell integer, intent(in) :: na Number of atoms in supercell integer, intent(in) :: isa (na) Species index of all atoms in supercell real(kind=dp), intent(in) :: xa (3,na) Atomic positions of all atoms in supercell integer, intent(in) :: indxua (na) Index of equivalent atom in unit cell integer, intent(inout) :: ntm (3) Number of mesh divisions of each cell\n vector, including subgrid. integer, intent(in) :: ifa Switch which fixes whether the SCF contrib: to atomic forces is calculated and added to fa. integer, intent(in) :: istr Switch which fixes whether the SCF contrib: to stress is calculated and added to stress. integer, intent(in) :: iHmat Switch which fixes whether the Hmat matrix\n elements are calculated or not. type(filesOut_t), intent(inout) :: filesOut Output file names (If blank => not saved) integer, intent(in) :: maxnd First dimension of listd and Dscf integer, intent(in) :: numd (nuo) Number of nonzero density-matrix\n elements for each matrix row integer, intent(in) :: listdptr (nuo) Pointer to start of rows of density-matrix integer, intent(in) :: listd (*) listd(maxnd) : Nonzero-density-matrix-element column\n indexes for each matrix row real(kind=dp), intent(in) :: Dscf (:,:) Dscf(maxnd,h_spin_dim) : SCF density-matrix elements real(kind=dp), intent(in) :: datm (norb) Harris density-matrix diagonal elements (atomic occupation charges of orbitals) integer, intent(in) :: maxnh First dimension of listh and Hmat real(kind=dp), intent(in) :: Hmat (:,:) Hmat(maxnh,h_spin_dim) : Hamiltonian matrix in sparse form, to which are added the matrix elements <ORB_I | DeltaV | ORB_J> , where DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris) real(kind=dp), intent(out) :: Enaatm Integral of Vna * rhoatm real(kind=dp), intent(out) :: Enascf Integral of Vna * rhoscf real(kind=dp), intent(out) :: Uatm Harris hartree electron-interaction energy real(kind=dp), intent(out) :: Uscf SCF hartree electron-interaction energy real(kind=dp), intent(out) :: DUscf Electrostatic (Hartree) energy of (rhoscf - rhoatm) density real(kind=dp), intent(out) :: DUext Interaction energy with external electric field real(kind=dp), intent(out) :: Exc SCF exchange-correlation energy real(kind=dp), intent(out) :: Dxc SCF double-counting correction to Exc Dxc = integral of ( (epsxc - Vxc) * Rho ) All energies in Rydbergs real(kind=dp), intent(out) :: dipol (3) Electric dipole (in a.u.)\n only when the system is a molecule real(kind=dp) :: stress (3,3) real(kind=dp), intent(inout) :: Fal (3,nua) Atomic forces, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative\n of (Enascf - Enaatm + DUscf + Exc) with\n respect to atomic positions, in Ry/Bohr.\n Contributions local to this node. real(kind=dp), intent(inout) :: stressl (3,3) Stress tensor, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative of (Enascf - Enaatm + DUscf + Exc) / volume with respect to the strain tensor, in Ry.\n Contributions local to this node. logical, intent(in), optional :: use_rhog_in logical, intent(in), optional :: charge_density_only Calls proc~~dhscf~~CallsGraph proc~dhscf dhscf elecs elecs proc~dhscf->elecs re_alloc re_alloc proc~dhscf->re_alloc ts_voltage ts_voltage proc~dhscf->ts_voltage mpi_barrier mpi_barrier proc~dhscf->mpi_barrier ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix volcel volcel proc~dhscf->volcel ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofd rhoofd proc~dhscf->rhoofd bye bye proc~dhscf->bye get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field hartree_add hartree_add proc~dhscf->hartree_add proc~reord reord proc~dhscf->proc~reord rhoofdsp rhoofdsp proc~dhscf->rhoofdsp compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho dfscf dfscf proc~dhscf->dfscf write_rho write_rho proc~dhscf->write_rho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata die die proc~dhscf->die timer timer proc~dhscf->timer mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce forhar forhar proc~dhscf->forhar vacuum_level vacuum_level proc~dhscf->vacuum_level de_alloc de_alloc proc~dhscf->de_alloc localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh write_debug write_debug proc~dhscf->write_debug vmat vmat proc~dhscf->vmat meshlim meshlim proc~setmeshdistr->meshlim proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->die proc~distmeshdata_rea->timer proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event nmeshg nmeshg proc~distmeshdata_rea->nmeshg proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->re_alloc proc~distmeshdata_int->die proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~boxintersection Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~dhscf~~CalledByGraph proc~dhscf dhscf proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code dhscf Source Code subroutine dhscf ( nspin , norb , iaorb , iphorb , nuo , & nuotot , nua , na , isa , xa , indxua , & ntm , ifa , istr , iHmat , & filesOut , maxnd , numd , & listdptr , listd , Dscf , datm , maxnh , Hmat , & Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , stress , Fal , stressl , & use_rhog_in , charge_density_only ) !! author: J.M. Soler !! date: August 1996 !! !! Calculates the self-consistent field contributions to Hamiltonian !! matrix elements, total energy and atomic forces. !! !! Coded by J.M. Soler, August 1996. July 1997. !! Modified by J.D. Gale, February 2000. !! !! !!### Units !! Energies in Rydbergs. !! Distances in Bohr. !! !!### Routines called internally !! * cellxc(...)    : Finds total exch-corr energy and potential !! * [[cross(proc)]]   : Finds the cross product of two vectors !! * dfscf(...)     : Finds SCF contribution to atomic forces !! * dipole(...)    : Finds electric dipole moment !! * doping(...)    : Adds a background charge for doped systems !! * write_rho(...)     : Saves electron density on a file !! * poison(...)    : Solves Poisson equation !! * reord(...)     : Reorders electron density and potential arrays !! * rhooda(...)    : Finds Harris electron density in the mesh !! * rhoofd(...)    : Finds SCF electron density in the mesh !! * rhoofdsp(...)  : Finds SCF electron density in the mesh for !! *                  spiral arrangement of spins !! * timer(...)     : Finds CPU times !! * vmat(...)      : Finds matrix elements of SCF potential !! * vmatsp(...)    : Finds matrix elements of SCF potential for !!                    spiral arrangement of spins !! * delk(...)      : Finds matrix elements of  exp(i \\vec{k} \\cdot \\vec{r})  !! * real*8 volcel( cell ) : Returns volume of unit cell !! !!### Internal variables and arrays !! * `real*8  bcell(3,3)`    : Bulk lattice vectors !! * `real*8  cell(3,3)`     : Auxiliary lattice vectors (same as ucell) !! * `real*8  const`         : Auxiliary variable (constant within a loop) !! * `real*8  DEc`           : Auxiliary variable to call cellxc !! * `real*8  DEx`           : Auxiliary variable to call cellxc !! * `real*8  dvol`          : Mesh-cell volume !! * `real*8  Ec`            : Correlation energy !! * `real*8  Ex`            : Exchange energy !! * `real*8  field(3)`      : External electric field !! * `integer i`             : General-purpose index !! * `integer ia`            : Atom index !! * `integer io`            : Orbital index !! * `integer ip`            : Point index !! * `integer is`            : Species index !! * `logical IsDiag`        : Is supercell diagonal? !! * `integer ispin`         : Spin index !! * `integer j`             : General-purpose index #ifndef BSC_CELLXC !! * `integer JDGdistr`      : J.D.Gale's parallel distribution of mesh points !! * `integer myBox(2,3)`    : My processor's mesh box #endif /* ! BSC_CELLXC */ !! * `integer nbcell`        : Number of independent bulk lattice vectors !! * `integer npcc`          : Partial core corrections? (0=no, 1=yes) !! * `integer nsd`           : Number of diagonal spin values (1 or 2) !! * `integer ntpl`          : Number of mesh Total Points in unit cell !!                           (including subpoints) locally !! * `real*4  rhoatm(ntpl)`  : Harris electron density !! * `real*4  rhopcc(ntpl)`  : Partial-core-correction density for xc !! * `real*4  DRho(ntpl)`    : Selfconsistent electron density difference !! * `real*8  rhotot`        : Total density at one point !! * `real*8  rmax`          : Maximum orbital radius !! * `real*8  scell(3,3)`    : Supercell vectors !! * `character shape*10`    : Name of system shape !! * `real*4  Vaux(ntpl)`    : Auxiliary potential array !! * `real*4  Vna(ntpl)`     : Sum of neutral-atom potentials !! * `real*8  volume`        : Unit cell volume !! * `real*4  Vscf(ntpl)`    : Hartree potential of selfconsistent density !! * `real*8  x0(3)`         : Center of molecule !! * `logical harrisfun`     : Harris functional or Kohn-Sham? use precision , only : dp , grid_p #ifndef BSC_CELLXC use parallel , only : ProcessorY #endif /* ! BSC_CELLXC */ !     Number of Mesh divisions of each cell vector (global) !     The status of this variable is confusing use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use units , only : Debye , eV , Ang use fdf use sys , only : die , bye use mesh , only : nsm , nsp use parsing use m_iorho , only : write_rho use m_forhar , only : forhar use alloc , only : re_alloc , de_alloc use files , only : slabel use files , only : filesOut_t ! derived type for output file names use siesta_options , only : harrisfun , save_initial_charge_density use siesta_options , only : analyze_charge_density_only use meshsubs , only : LocalChargeOnMesh use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use m_partial_charges , only : compute_partial_charges use m_partial_charges , only : want_partial_charges #ifndef BSC_CELLXC use siestaXC , only : cellXC ! Finds xc energy and potential use siestaXC , only : myMeshBox ! Returns my processor mesh box use siestaXC , only : jms_setMeshDistr => setMeshDistr ! Sets a distribution of mesh ! points over parallel processors #endif /* BSC_CELLXC */ use m_vmat , only : vmat use m_rhoofd , only : rhoofd #ifdef MPI use mpi_siesta #endif use iogrid_netcdf , only : write_grid_netcdf use iogrid_netcdf , only : read_grid_netcdf use siesta_options , only : read_charge_cdf use siesta_options , only : savebader use siesta_options , only : read_deformation_charge_cdf use siesta_options , only : mix_charge use m_efield , only : get_field_from_dipole , dipole_correction use m_efield , only : add_potential_from_field use m_efield , only : user_specified_field , acting_efield use m_doping_uniform , only : doping_active , doping_uniform use m_charge_add , only : charge_add use m_hartree_add , only : hartree_add #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif use m_rhofft , only : rhofft , FORWARD , BACKWARD use m_rhog , only : rhog_in , rhog use m_spin , only : spin use m_spin , only : Spiral , qSpiral use m_iotddft , only : write_tdrho use m_ts_global_vars , only : TSmode , TSrun use m_ts_options , only : IsVolt , Elecs , N_elec use m_ts_voltage , only : ts_voltage use m_ts_hartree , only : ts_hartree_fix implicit none integer , intent ( in ) :: nspin !! Number of different spin polarisations: !! nspin=1 => Unpolarized, nspin=2 => polarized !! nspin=4 => Noncollinear spin or spin-orbit. integer , intent ( in ) :: norb !! Total number of basis orbitals in supercell integer , intent ( in ) :: iaorb ( norb ) !! Atom to which each orbital belongs integer , intent ( in ) :: iphorb ( norb ) !! Orbital index (within atom) of each orbital integer , intent ( in ) :: nuo !! Number of orbitals in a unit cell in this node integer , intent ( in ) :: nuotot !! Number of orbitals in a unit cell integer , intent ( in ) :: nua !! Number of atoms in unit cell integer , intent ( in ) :: na !! Number of atoms in supercell integer , intent ( in ) :: isa ( na ) !! Species index of all atoms in supercell integer , intent ( in ) :: indxua ( na ) !! Index of equivalent atom in unit cell integer , intent ( in ) :: ifa !! Switch which fixes whether the SCF contrib: !! to atomic forces is calculated and added to fa. integer , intent ( in ) :: istr !! Switch which fixes whether the SCF contrib: !! to stress is calculated and added to stress. integer , intent ( in ) :: iHmat !! Switch which fixes whether the Hmat matrix !! elements are calculated or not. integer , intent ( in ) :: maxnd !! First dimension of listd and Dscf integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero density-matrix !! elements for each matrix row integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of rows of density-matrix integer , intent ( in ) :: listd ( * ) !! `listd(maxnd)`: Nonzero-density-matrix-element column !! indexes for each matrix row integer , intent ( in ) :: maxnh !! First dimension of listh and Hmat real ( dp ), intent ( in ) :: xa ( 3 , na ) !! Atomic positions of all atoms in supercell real ( dp ), intent ( in ) :: Dscf (:,:) !! `Dscf(maxnd,h_spin_dim)`: !! SCF density-matrix elements real ( dp ), intent ( in ) :: datm ( norb ) !! Harris density-matrix diagonal elements !! (atomic occupation charges of orbitals) real ( dp ), intent ( in ) :: Hmat (:,:) !! `Hmat(maxnh,h_spin_dim)`: !! Hamiltonian matrix in sparse form, !! to which are added the matrix elements !! `<ORB_I | DeltaV | ORB_J>`, where !! `DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris)` type ( filesOut_t ), intent ( inout ) :: filesOut !! Output file names (If blank => not saved) integer , intent ( inout ) :: ntm ( 3 ) !! Number of mesh divisions of each cell !! vector, including subgrid. real ( dp ), intent ( inout ) :: Fal ( 3 , nua ) !! Atomic forces, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative !! of `(Enascf - Enaatm + DUscf + Exc)` with !! respect to atomic positions, in Ry/Bohr. !! Contributions local to this node. real ( dp ), intent ( inout ) :: stressl ( 3 , 3 ) !! Stress tensor, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative of !! `(Enascf - Enaatm + DUscf + Exc) / volume` !! with respect to the strain tensor, in Ry. !! Contributions local to this node. real ( dp ) :: stress ( 3 , 3 ) real ( dp ), intent ( out ) :: Enaatm !! Integral of `Vna * rhoatm` real ( dp ), intent ( out ) :: Enascf !! Integral of `Vna * rhoscf` real ( dp ), intent ( out ) :: Uatm !! Harris hartree electron-interaction energy real ( dp ), intent ( out ) :: Uscf !! SCF hartree electron-interaction energy real ( dp ), intent ( out ) :: DUscf !! Electrostatic (Hartree) energy of !! `(rhoscf - rhoatm)` density real ( dp ), intent ( out ) :: DUext !! Interaction energy with external electric field real ( dp ), intent ( out ) :: Exc !! SCF exchange-correlation energy real ( dp ), intent ( out ) :: Dxc !! SCF double-counting correction to Exc !! `Dxc = integral of ( (epsxc - Vxc) * Rho )` !! All energies in Rydbergs real ( dp ), intent ( out ) :: dipol ( 3 ) !! Electric dipole (in a.u.) !! only when the system is a molecule logical , intent ( in ), optional :: use_rhog_in logical , intent ( in ), optional :: charge_density_only !     Local variables integer :: i , ia , ip , ispin , nsd , np_vac #ifndef BSC_CELLXC !     Interface to JMS's SiestaXC integer :: myBox ( 2 , 3 ) integer , save :: JDGdistr =- 1 real ( dp ) :: stressXC ( 3 , 3 ) #endif /* ! BSC_CELLXC */ real ( dp ) :: b1Xb2 ( 3 ), const , DEc , DEx , DStres ( 3 , 3 ), & Ec , Ex , rhotot , x0 ( 3 ), volume , Vmax_vac , Vmean_vac #ifdef BSC_CELLXC !     Dummy arrays for cellxc call real ( grid_p ) :: aux3 ( 3 , 1 ) real ( grid_p ) :: dummy_DVxcdn ( 1 , 1 , 1 ) #endif /* BSC_CELLXC */ logical :: use_rhog real ( dp ), external :: volcel , ddot external & cross , & dipole , & poison , & reord , rhooda , rhoofdsp , & timer , vmatsp , & readsp #ifdef BSC_CELLXC external bsc_cellxc #endif /* BSC_CELLXC */ !     Work arrays real ( grid_p ), pointer :: Vscf (:,:), Vscf_par (:,:), & DRho (:,:), DRho_par (:,:), & Vaux (:), Vaux_par (:), Chlocal (:), & Totchar (:), fsrc (:), fdst (:), & rhoatm_quad (:) => null (), & DRho_quad (:,:) => null () ! Temporary reciprocal spin quantity real ( grid_p ) :: rnsd #ifdef BSC_CELLXC real ( grid_p ), pointer :: Vscf_gga (:,:), DRho_gga (:,:) #endif /* BSC_CELLXC */ #ifdef MPI integer :: MPIerror real ( dp ) :: sbuffer ( 7 ), rbuffer ( 7 ) #endif #ifdef DEBUG call write_debug ( '    PRE DHSCF' ) #endif if ( spin % H /= size ( Dscf , dim = 2 ) ) then call die ( 'Spin components is not equal to options.' ) end if if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DM' , & ( sqrt ( sum ( Dscf (:, ispin ) ** 2 )), ispin = 1 , spin % H ) write ( * , debug_fmt ) Node , 'H' , & ( sqrt ( sum ( Hmat (:, ispin ) ** 2 )), ispin = 1 , spin % H ) end if !-------------------------------------------------------------------- BEGIN ! ---------------------------------------------------------------------- ! Start of SCF iteration part ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     At the end of DHSCF_INIT, and also at the end of any previous !     call to dhscf, we were in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna were in UNIFORM dist, sequential form !     The index array endpht was in the QUADRATIC distribution ! ---------------------------------------------------------------------- #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_restart ( ) #endif call timer ( 'DHSCF' , 1 ) call timer ( 'DHSCF3' , 1 ) nullify ( Vscf , Vscf_par , DRho , DRho_par , & Vaux , Vaux_par , Chlocal , Totchar ) #ifdef BSC_CELLXC nullify ( Vscf_gga , DRho_gga ) #endif /* BSC_CELLXC */ volume = volcel ( cell ) !------------------------------------------------------------------------- if ( analyze_charge_density_only ) then !! Use the functionality in the first block !! of the routine to get charge files and partial charges call setup_analysis_options () endif if ( filesOut % vna . ne . ' ' ) then ! Uniform dist, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vna' , 1 , ntml , Vna ) else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) end if #else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) #endif endif !     Allocate memory for DRho using the UNIFORM data distribution call re_alloc ( DRho , 1 , ntpl , 1 , nspin , 'DRho' , 'dhscf' ) ! Find number of diagonal spin values nsd = min ( nspin , 2 ) if ( nsd == 1 ) then rnsd = 1._grid_p else rnsd = 1._grid_p / nsd end if ! ---------------------------------------------------------------------- ! Find SCF electron density at mesh points. Store it in array DRho ! ---------------------------------------------------------------------- ! !     The reading routine works in the uniform distribution, in !     sequential form ! if ( present ( use_rhog_in )) then use_rhog = use_rhog_in else use_rhog = . false . endif if ( use_rhog ) then ! fourier transform back into drho call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog_in , BACKWARD ) else if ( read_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"Rho\" ) read_charge_cdf = . false . else if ( read_deformation_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"DeltaRho\" ) ! Add to diagonal components only do ispin = 1 , nsd do ip = 1 , ntpl !             rhoatm and Drho are in sequential mode DRho ( ip , ispin ) = DRho ( ip , ispin ) + rhoatm ( ip ) * rnsd enddo enddo read_deformation_charge_cdf = . false . else ! Set the QUADRATIC distribution and allocate memory for DRho_par ! since the construction of the density from the DM and orbital ! data needs that distribution if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_par , 1 , ntpl , 1 , nspin , & 'DRho_par' , 'dhscf' ) if ( Spiral ) then call rhoofdsp ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , nuo , nuotot , iaorb , & iphorb , isa , qspiral ) else call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , & nuo , nuotot , iaorb , iphorb , isa ) endif ! DRHO_par is here in QUADRATIC, clustered form !       Set the UNIFORM distribution again and copy DRho to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => DRho_par (:, ispin ) fdst => DRho (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( DRho_par , 'DRho_par' , 'dhscf' ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DRho' , & ( sqrt ( sum ( DRho (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if if ( save_initial_charge_density ) then ! This section is to be deprecated in favor ! of \"analyze_charge_density_only\" ! (except for the special name for the .nc file) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoInit' , nspin , & ntml , DRho ) else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) end if #else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) #endif call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after producing RHO_INIT from input DM\" ) endif endif if ( mix_charge ) then ! Save fourier transform of charge density call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog , FORWARD ) endif ! !     Proper place to integrate Hirshfeld and Voronoi code, !     since we have just computed rhoatm and Rho. if ( want_partial_charges ) then ! The endpht array is in the quadratic distribution, so ! we need to use it for this... if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_quad , 1 , ntpl , 1 , nspin , & 'DRho_quad' , 'dhscf' ) call re_alloc ( rhoatm_quad , 1 , ntpl , & 'rhoatm_quad' , 'dhscf' ) ! Redistribute grid-density do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_quad (:, ispin ) ! if nodes==1, this call will just reorder call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo call distMeshData ( UNIFORM , rhoatm , & QUADRATIC , rhoatm_quad , TO_CLUSTER ) call compute_partial_charges ( DRho_quad , rhoatm_quad , . nspin , iaorb , iphorb , . isa , nmpl , dvol ) call de_alloc ( rhoatm_quad , 'rhoatm_quad' , 'dhscf' ) call de_alloc ( Drho_quad , 'DRho_quad' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif ! ---------------------------------------------------------------------- ! Save electron density ! ---------------------------------------------------------------------- if ( filesOut % rho . ne . ' ' ) then !  DRho is already using a uniform, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Rho' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) end if #else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) #endif endif !----------------------------------------------------------------------- ! Save TD-electron density after every given number of steps- Rafi, Jan 2016 !----------------------------------------------------------------------- call write_tdrho ( filesOut ) if ( filesOut % tdrho . ne . ' ' ) then !  DRho is already using a uniform, sequential form call write_rho ( filesOut % tdrho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"TDRho\" ) endif ! ---------------------------------------------------------------------- ! Save the diffuse ionic charge and/or the total (ionic+electronic) charge ! ---------------------------------------------------------------------- if ( filesOut % psch . ne . ' ' . or . filesOut % toch . ne . ' ' ) then !       Find diffuse ionic charge on mesh ! Note that the *OnMesh routines, except PhiOnMesh, ! work with any distribution, thanks to the fact that ! the ipa, idop, and indexp arrays are distro-specific call re_alloc ( Chlocal , 1 , ntpl , 'Chlocal' , 'dhscf' ) call LocalChargeOnMesh ( na , isa , ntpl , Chlocal , indxua ) ! Chlocal comes out in clustered form, so we convert it call reord ( Chlocal , Chlocal , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Chlocal' , sqrt ( sum ( Chlocal ** 2 )) end if !       Save diffuse ionic charge if ( filesOut % psch . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Chlocal' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & 'Chlocal' ) end if #else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , 'Chlocal' ) #endif endif !       Save total (ionic+electronic) charge if ( filesOut % toch . ne . ' ' ) then ! ***************** ! **  IMPORTANT  ** ! The Chlocal array is re-used to minimize memory ! usage. In the this small snippet the Chlocal ! array will contain the total charge, and ! if the logic should change, (i.e. should Chlocal ! be retained) is the Totchar needed to be re-instantiated. ! ***************** !$OMP parallel default(shared), private(ispin,ip) do ispin = 1 , nsd !$OMP do do ip = 1 , ntpl Chlocal ( ip ) = Chlocal ( ip ) + DRho ( ip , ispin ) end do !$OMP end do end do !$OMP end parallel ! See note above #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoTot' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & \"TotalCharge\" ) end if #else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , \"TotalCharge\" ) #endif end if call de_alloc ( Chlocal , 'Chlocal' , 'dhscf' ) endif ! ---------------------------------------------------------------------- ! Save the total charge (model core + valence) for Bader analysis ! ---------------------------------------------------------------------- ! The test for toch guarantees that we are in \"analysis mode\" if ( filesOut % toch . ne . ' ' . and . savebader ) then call save_bader_charge () endif ! Find difference between selfconsistent and atomic densities !Both DRho and rhoatm are using a UNIFORM, sequential form !$OMP parallel do default(shared), private(ispin,ip), !$OMP&collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do ! ---------------------------------------------------------------------- ! Save electron density difference ! ---------------------------------------------------------------------- if ( filesOut % drho . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoDelta' , nspin , ntml , & DRho ) else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"DeltaRho\" ) end if #else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & DRho , \"DeltaRho\" ) #endif endif if ( present ( charge_density_only )) then if ( charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) RETURN endif endif ! End of analysis section ! Can exit now, if requested if ( analyze_charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after analyzing charge from input DM\" ) endif !------------------------------------------------------------- !     Transform spin density into sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif ! Add a background charge to neutralize the net charge, to ! model doped systems. It only adds the charge at points ! where there are atoms (i.e., not in vacuum). ! First, call with 'task=0' to add background charge if ( doping_active ) call doping_uniform ( cell , ntpl , 0 , $ DRho (:, 1 ), rhoatm ) ! Add doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '+' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Calculate the dipole moment ! ---------------------------------------------------------------------- dipol ( 1 : 3 ) = 0.0_dp if ( shape . ne . 'bulk' ) then ! Find center of system x0 ( 1 : 3 ) = 0.0_dp do ia = 1 , nua x0 ( 1 : 3 ) = x0 ( 1 : 3 ) + xa ( 1 : 3 , ia ) / nua enddo ! Find dipole ! This routine is distribution-blind ! and will reduce over all processors. call dipole ( cell , ntm , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), nsm , & DRho , x0 , dipol ) ! Orthogonalize dipole to bulk directions if ( shape . eq . 'chain' ) then const = ddot ( 3 , dipol , 1 , bcell , 1 ) / ddot ( 3 , bcell , 1 , bcell , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * bcell ( 1 : 3 , 1 ) else if ( shape . eq . 'slab' ) then call cross ( bcell ( 1 , 1 ), bcell ( 1 , 2 ), b1Xb2 ) const = ddot ( 3 , dipol , 1 , b1Xb2 , 1 ) / ddot ( 3 , b1Xb2 , 1 , b1Xb2 , 1 ) dipol ( 1 : 3 ) = const * b1Xb2 ( 1 : 3 ) end if if ( TSmode ) then if ( N_elec > 1 ) then ! Orthogonalize dipole to electrode transport directions do ia = 1 , N_Elec x0 = Elecs ( ia )% cell (:, Elecs ( ia )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * x0 end do else if ( ( shape == 'molecule' ) . or . ( shape == 'chain' ) ) then ! Only allow dipole correction for chains and molecules ! along the semi-infinite direciton. ! Note this is *only* for 1-electrode setups ! Note that since the above removes the periodic directions ! this should not do anything for 'chain' with the same semi-infinite ! direction x0 = Elecs ( 1 )% cell (:, Elecs ( 1 )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = const * x0 end if end if endif ! ---------------------------------------------------------------------- !     Find Hartree potential of DRho = rhoscf-rhoatm. Store it in Vaux ! ---------------------------------------------------------------------- !     Solve Poisson's equation call re_alloc ( Vaux , 1 , ntpl , 'Vaux' , 'dhscf' ) call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , DRho , & DUscf , Vaux , DStres , nsm ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Poisson' , sqrt ( sum ( Vaux (:) ** 2 )) end if ! Vscf is in the UNIFORM, sequential form, and only using ! the first spin index ! We require that even the SIESTA potential is \"fixed\" ! NOTE, this will only do something if !   TS.Hartree.Fix is set call ts_hartree_fix ( ntm , ntml , Vaux ) ! Add contribution to stress from electrostatic energy of rhoscf-rhoatm if ( istr . eq . 1 ) then stressl ( 1 : 3 , 1 : 3 ) = stressl ( 1 : 3 , 1 : 3 ) + DStres ( 1 : 3 , 1 : 3 ) endif ! ---------------------------------------------------------------------- !     Find electrostatic (Hartree) energy of full SCF electron density !     using the original data distribution ! ---------------------------------------------------------------------- Uatm = Uharrs Uscf = 0._dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Uscf) do ip = 1 , ntpl Uscf = Uscf + Vaux ( ip ) * rhoatm ( ip ) enddo !$OMP end parallel do Uscf = Uscf * dVol + Uatm + DUscf ! Call doping with 'task=1' to remove background charge added previously ! The extra charge thus only affects the Hartree energy and potential, ! but not the contribution to Enascf ( = \\Int_{Vna*\\rho}) if ( doping_active ) call doping_uniform ( cell , ntpl , 1 , $ DRho (:, 1 ), rhoatm ) ! Remove doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '-' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Add neutral-atom potential to Vaux ! ---------------------------------------------------------------------- Enaatm = 0.0_dp Enascf = 0.0_dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Enaatm,Enascf) do ip = 1 , ntpl Enaatm = Enaatm + Vna ( ip ) * rhoatm ( ip ) Enascf = Enascf + Vna ( ip ) * DRho ( ip , 1 ) Vaux ( ip ) = Vaux ( ip ) + Vna ( ip ) enddo !$OMP end parallel do Enaatm = Enaatm * dVol Enascf = Enaatm + Enascf * dVol ! ---------------------------------------------------------------------- ! Add potential from external electric field (if present) ! ---------------------------------------------------------------------- if ( acting_efield ) then if ( dipole_correction ) then field = get_field_from_dipole ( dipol , cell ) if ( Node == 0 ) then write ( 6 , '(a,3f12.4,a)' ) $ 'Dipole moment in unit cell   =' , dipol / Debye , ' D' write ( 6 , '(a,3f12.6,a)' ) $ 'Electric field for dipole correction =' , $ field / eV * Ang , ' eV/Ang/e' end if ! The dipole correction energy has an extra factor ! of one half because the field involved is internal. ! See the paper by Bengtsson DOI:10.1103/PhysRevB.59.12301 ! Hence we compute this part separately DUext = - 0.5_dp * ddot ( 3 , field , 1 , dipol , 1 ) else field = 0._dp DUext = 0._dp end if ! Add the external electric field field = field + user_specified_field ! This routine expects a sequential array, ! but it is distribution-blind call add_potential_from_field ( field , cell , nua , isa , xa , & ntm , nsm , Vaux ) ! Add energy of external electric field DUext = DUext - ddot ( 3 , user_specified_field , 1 , dipol , 1 ) endif ! --------------------------------------------------------------------- !     Transiesta: !     add the potential corresponding to the (possible) voltage-drop. !     note that ts_voltage is not sharing the reord wih efield since !     we should not encounter both at the same time. ! --------------------------------------------------------------------- if ( TSmode . and . IsVolt . and . TSrun ) then ! This routine expects a sequential array, ! in whatever distribution #ifdef TRANSIESTA_VOLTAGE_DEBUG !$OMP parallel workshare default(shared) Vaux (:) = 0._dp !$OMP end parallel workshare #endif call ts_voltage ( cell , ntm , ntml , Vaux ) #ifdef TRANSIESTA_VOLTAGE_DEBUG call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"TransiestaHartreePotential\" ) call timer ( 'ts_volt' , 3 ) call bye ( 'transiesta debug for Hartree potential' ) #endif endif ! ---------------------------------------------------------------------- ! Add potential from user defined geometries (if present) ! ---------------------------------------------------------------------- call hartree_add ( cell , ntpl , Vaux ) ! ---------------------------------------------------------------------- !     Save electrostatic potential ! ---------------------------------------------------------------------- if ( filesOut % vh . ne . ' ' ) then ! Note that only the first spin component is used #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vh' , 1 , ntml , & Vaux ) else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"ElectrostaticPotential\" ) end if #else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Vaux , \"ElectrostaticPotential\" ) #endif endif !     Get back spin density from sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(ip,rhotot) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) DRho ( ip , 1 ) = 0.5_dp * ( rhotot - DRho ( ip , 2 )) DRho ( ip , 2 ) = 0.5_dp * ( rhotot + DRho ( ip , 2 )) enddo !$OMP end parallel do endif ! ---------------------------------------------------------------------- #ifndef BSC_CELLXC ! Set uniform distribution of mesh points and find my processor mesh box ! This is the interface to JM Soler's own cellxc routine, which sets ! up the right distribution internally. ! ---------------------------------------------------------------------- call jms_setMeshDistr ( distrID = JDGdistr , nMesh = ntm , . nNodesX = 1 , nNodesY = ProcessorY , nBlock = nsm ) call myMeshBox ( ntm , JDGdistr , myBox ) ! ---------------------------------------------------------------------- #endif /* ! BSC_CELLXC */ ! Exchange-correlation energy ! ---------------------------------------------------------------------- call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf' ) if ( npcc . eq . 1 ) then !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & ( rhopcc ( ip ) + rhoatm ( ip )) * rnsd enddo enddo !$OMP end parallel do else !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do end if ! Write the electron density used by cellxc if ( filesOut % rhoxc . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoXC' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) end if #else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) #endif endif !     Everything now is in UNIFORM, sequential form call timer ( \"CellXC\" , 1 ) #ifdef BSC_CELLXC if ( nodes . gt . 1 ) then call setMeshDistr ( LINEAR , nsm , nsp , nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_gga , 1 , ntpl , 1 , nspin , 'Vscf_gga' , 'dhscf' ) call re_alloc ( DRho_gga , 1 , ntpl , 1 , nspin , 'DRho_gga' , 'dhscf' ) ! Redistribute all spin densities do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_gga (:, ispin ) call distMeshData ( UNIFORM , fsrc , LINEAR , fdst , KEEP ) enddo call bsc_cellxc ( 0 , 0 , cell , ntml , ntml , ntpl , 0 , aux3 , nspin , & DRho_gga , Ex , Ec , DEx , DEc , Vscf_gga , & dummy_DVxcdn , stressl ) #endif /* BSC_CELLXC */ #ifndef BSC_CELLXC call cellXC ( 0 , cell , ntm , myBox ( 1 , 1 ), myBox ( 2 , 1 ), . myBox ( 1 , 2 ), myBox ( 2 , 2 ), . myBox ( 1 , 3 ), myBox ( 2 , 3 ), nspin , . DRho , Ex , Ec , DEx , DEc , stressXC , Vscf ) #else /* BSC_CELLXC */ if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif ! Redistribute to the Vxc array do ispin = 1 , nspin fsrc => Vscf_gga (:, ispin ) fdst => Vscf (:, ispin ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) enddo #endif /* BSC_CELLXC */ if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'XC' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if #ifndef BSC_CELLXC !     Vscf is still sequential after the call to JMS's cellxc #else /* BSC_CELLXC */ call de_alloc ( DRho_gga , 'DRho_gga' , 'dhscf' ) call de_alloc ( Vscf_gga , 'Vscf_gga' , 'dhscf' ) #endif /* BSC_CELLXC */ Exc = Ex + Ec Dxc = DEx + DEc call timer ( \"CellXC\" , 2 ) !     Vscf contains only Vxc, and is UNIFORM and sequential !     Now we add up the other contributions to it, at !     the same time that we get DRho back to true DeltaRho form !$OMP parallel default(shared), private(ip,ispin) ! Hartree potential only has diagonal components do ispin = 1 , nsd if ( npcc . eq . 1 ) then !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - & ( rhoatm ( ip ) + rhopcc ( ip )) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do else !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do endif enddo !$OMP end parallel #ifndef BSC_CELLXC stress = stress + stressXC #endif /* ! BSC_CELLXC */ ! ---------------------------------------------------------------------- !     Save total potential ! ---------------------------------------------------------------------- if ( filesOut % vt . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vt' , nspin , ntml , & Vscf ) else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , & Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Vscf , & \"TotalPotential\" ) end if #else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Vscf , \"TotalPotential\" ) #endif endif ! ---------------------------------------------------------------------- ! Print vacuum level ! ---------------------------------------------------------------------- if ( filesOut % vt /= ' ' . or . filesOut % vh /= ' ' ) then forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) + rhoatm (:) * rnsd call vacuum_level ( ntpl , nspin , DRho , Vscf , . np_vac , Vmax_vac , Vmean_vac ) forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) - rhoatm (:) * rnsd if ( np_vac > 0 . and . Node == 0 ) print '(/,a,2f12.6,a)' , . 'dhscf: Vacuum level (max, mean) =' , . Vmax_vac / eV , Vmean_vac / eV , ' eV' endif if ( filesOut % ebs_dens /= '' ) then call save_ebs_density () endif ! ---------------------------------------------------------------------- !     Find SCF contribution to hamiltonian matrix elements ! ---------------------------------------------------------------------- if ( iHmat . eq . 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !        This is a work array, to which we copy Vscf call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo if ( Spiral ) then call vmatsp ( norb , nmpl , dvol , nspin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa , qspiral ) else call vmat ( norb , nmpl , dvol , spin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa ) endif call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then !          Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif #ifdef MPI !     Global reduction of Uscf/DUscf/Uatm/Enaatm/Enascf #ifndef BSC_CELLXC !     Note that Exc and Dxc are already reduced in the new cellxc #endif /* ! BSC_CELLXC */ sbuffer ( 1 ) = Uscf sbuffer ( 2 ) = DUscf sbuffer ( 3 ) = Uatm sbuffer ( 4 ) = Enaatm sbuffer ( 5 ) = Enascf #ifdef BSC_CELLXC sbuffer ( 6 ) = Exc sbuffer ( 7 ) = Dxc #else sbuffer ( 6 : 7 ) = 0._dp #endif /* BSC_CELLXC */ call MPI_AllReduce ( sbuffer , rbuffer , 7 , MPI_double_precision , & MPI_Sum , MPI_Comm_World , MPIerror ) Uscf = rbuffer ( 1 ) DUscf = rbuffer ( 2 ) Uatm = rbuffer ( 3 ) Enaatm = rbuffer ( 4 ) Enascf = rbuffer ( 5 ) #ifdef BSC_CELLXC Exc = rbuffer ( 6 ) Dxc = rbuffer ( 7 ) #endif /* BSC_CELLXC */ #endif /* MPI */ !     Add contribution to stress from the derivative of the Jacobian of --- !     r->r' (strained r) in the integral of Vna*(rhoscf-rhoatm) if ( istr . eq . 1 ) then do i = 1 , 3 stress ( i , i ) = stress ( i , i ) + ( Enascf - Enaatm ) / volume enddo endif !     Stop time counter for SCF iteration part call timer ( 'DHSCF3' , 2 ) ! ---------------------------------------------------------------------- !     End of SCF iteration part ! ---------------------------------------------------------------------- if ( ifa . eq . 1 . or . istr . eq . 1 ) then ! ---------------------------------------------------------------------- ! Forces and stress : SCF contribution ! ---------------------------------------------------------------------- !       Start time counter for force calculation part call timer ( 'DHSCF4' , 1 ) !       Find contribution of partial-core-correction if ( npcc . eq . 1 ) then call reord ( rhopcc , rhopcc , nml , nsm , TO_CLUSTER ) call reord ( Vaux , Vaux , nml , nsm , TO_CLUSTER ) ! The partial core calculation only acts on ! the diagonal spin-components (no need to ! redistribute un-used elements) do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_CLUSTER ) enddo call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , nsd , & dvol , volume , Vscf , Vaux , Fal , & stressl , ifa . ne . 0 , istr . ne . 0 ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) call reord ( Vaux , Vaux , nml , nsm , TO_SEQUENTIAL ) ! ** see above do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_SEQUENTIAL ) enddo if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'PartialCore' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nsd ) end if endif if ( harrisfun ) then !         Forhar deals internally with its own needs !         for distribution changes #ifndef BSC_CELLXC call forhar ( ntpl , nspin , nml , ntml , ntm , npcc , cell , #else /* BSC_CELLXC */ call forhar ( ntpl , nspin , nml , ntml , npcc , cell , #endif /* BSC_CELLXC */ & rhoatm , rhopcc , Vna , DRho , Vscf , Vaux ) !         Upon return, everything is UNIFORM, sequential form endif !     Transform spin density into sum and difference ! TODO NC/SO ! Should we perform local diagonalization? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif !       Find contribution of neutral-atom potential call reord ( Vna , Vna , nml , nsm , TO_CLUSTER ) call reord ( DRho , DRho , nml , nsm , TO_CLUSTER ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , DRho , Fal , stressl , & ifa . ne . 0 , istr . ne . 0 ) call reord ( DRho , DRho , nml , nsm , TO_SEQUENTIAL ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo !       Remember that Vaux contains everything except Vxc call re_alloc ( Vaux_par , 1 , ntpl , 'Vaux_par' , 'dhscf' ) call distMeshData ( UNIFORM , Vaux , & QUADRATIC , Vaux_par , TO_CLUSTER ) call dfscf ( ifa , istr , na , norb , nuo , nuotot , nmpl , nspin , & indxua , isa , iaorb , iphorb , & maxnd , numd , listdptr , listd , Dscf , datm , & Vscf_par , Vaux_par , dvol , volume , Fal , stressl ) call de_alloc ( Vaux_par , 'Vaux_par' , 'dhscf' ) call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif !       Stop time counter for force calculation part call timer ( 'DHSCF4' , 2 ) ! ---------------------------------------------------------------------- !       End of force and stress calculation ! ---------------------------------------------------------------------- endif !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution !     Stop time counter call timer ( 'DHSCF' , 2 ) ! ---------------------------------------------------------------------- !     Free locally allocated memory ! ---------------------------------------------------------------------- call de_alloc ( Vaux , 'Vaux' , 'dhscf' ) call de_alloc ( Vscf , 'Vscf' , 'dhscf' ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) #ifdef DEBUG call write_debug ( '    POS DHSCF' ) #endif !------------------------------------------------------------------------ END CONTAINS subroutine save_bader_charge () use meshsubs , only : ModelCoreChargeOnMesh #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif ! Auxiliary routine to output the Bader Charge ! real ( grid_p ), pointer :: BaderCharge (:) => null () call re_alloc ( BaderCharge , 1 , ntpl , name = 'BaderCharge' , & routine = 'dhscf' ) ! Find a model core charge by re-scaling the local charge call ModelCoreChargeOnMesh ( na , isa , ntpl , BaderCharge , indxua ) ! It comes out in clustered form, so we convert it call reord ( BaderCharge , BaderCharge , nml , nsm , TO_SEQUENTIAL ) do ispin = 1 , nsd BaderCharge ( 1 : ntpl ) = BaderCharge ( 1 : ntpl ) + DRho ( 1 : ntpl , ispin ) enddo #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoBader' , 1 , ntml , & BaderCharge ) else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) end if #else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) #endif call de_alloc ( BaderCharge , name = 'BaderCharge' ) end subroutine save_bader_charge subroutine setup_analysis_options () !! For the analyze-charge-density-only case, !! avoiding any diagonalization use siesta_options , only : hirshpop , voropop use siesta_options , only : saverho , savedrho , saverhoxc use siesta_options , only : savevh , savevt , savevna use siesta_options , only : savepsch , savetoch want_partial_charges = ( hirshpop . or . voropop ) if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' end subroutine setup_analysis_options subroutine save_ebs_density () !! Optional output of the \"band-structure energy density\", which !! is just the charge density weighted by the eigenvalues, i.e., !! using EDM instead of DM in rhoofd use sparse_matrices , only : Escf real ( grid_p ), pointer :: Ebs_dens (:,:) => null (), & Ebs_dens_quad (:,:) => null () !     Allocate memory for Ebs_dens using the UNIFORM data distribution call re_alloc ( Ebs_dens , 1 , ntpl , 1 , nspin , 'Ebs_dens' , 'dhscf' ) !     Switch to quadratic distribution for call to rhoofd if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( ebs_dens_quad , 1 , ntpl , 1 , nspin , & 'Ebs_dens_quad' , 'dhscf' ) call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Escf , Ebs_dens_quad , & nuo , nuotot , iaorb , iphorb , isa ) !     Ebs_dens_par is here in QUADRATIC, clustered form !     Set the UNIFORM distribution again and copy Ebs_dens to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => Ebs_dens_quad (:, ispin ) fdst => Ebs_dens (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( Ebs_dens_quad , 'Ebs_dens_quad' , 'dhscf' ) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Ebs_density' , $ nspin , ntml , Ebs_dens ) else call write_rho ( filesOut % ebs_dens , $ cell , ntm , nsm , ntpl , nspin , Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Ebs_dens , & \"Ebs_density\" ) end if #else call write_rho ( filesOut % ebs_dens , cell , ntm , nsm , ntpl , nspin , $ Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Ebs_dens , \"Ebs_density\" ) #endif call de_alloc ( Ebs_dens , 'Ebs_dens' , 'dhscf' ) end subroutine save_ebs_density end subroutine dhscf","tags":"","loc":"proc/dhscf.html","title":"dhscf – SIESTA"},{"text":"public subroutine delk_wrapper(isigneikr, norb, maxnd, numd, listdptr, listd, nuo, nuotot, iaorb, iphorb, isa) Uses m_delk moreMeshSubs moreMeshSubs parallel mesh proc~~delk_wrapper~~UsesGraph proc~delk_wrapper delk_wrapper m_delk m_delk proc~delk_wrapper->m_delk mesh mesh proc~delk_wrapper->mesh module~moremeshsubs moreMeshSubs proc~delk_wrapper->module~moremeshsubs parallel parallel proc~delk_wrapper->parallel module~moremeshsubs->parallel sys sys module~moremeshsubs->sys alloc alloc module~moremeshsubs->alloc precision precision module~moremeshsubs->precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. This is a wrapper to call delk, using some of the module\n variables of m_dhscf, but from outside dhscf itself. The dhscf module variables used are: nmpl dvol nml nmpl ntml ntpl Some of them might be put somewhere else (mesh?) to allow some\n of the kitchen-sink functionality of dhscf to be made more modular.\n For example, this wrapper might live independently if enough mesh\n information is made available to it. Arguments Type Intent Optional Attributes Name integer :: isigneikr integer :: norb integer :: maxnd integer :: numd (nuo) integer :: listdptr (nuo) integer :: listd (maxnd) integer :: nuo integer :: nuotot integer :: iaorb (*) integer :: iphorb (*) integer :: isa (*) Calls proc~~delk_wrapper~~CallsGraph proc~delk_wrapper delk_wrapper proc~setmeshdistr setMeshDistr proc~delk_wrapper->proc~setmeshdistr delk delk proc~delk_wrapper->delk meshlim meshlim proc~setmeshdistr->meshlim Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code delk_wrapper Source Code subroutine delk_wrapper ( isigneikr , norb , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) !! This is a wrapper to call delk, using some of the module !! variables of m_dhscf, but from outside dhscf itself. !! !! The dhscf module variables used are: !! !! * nmpl !! * dvol !! * nml !! * nmpl !! * ntml !! * ntpl !! !! Some of them might be put somewhere else (mesh?) to allow some !! of the kitchen-sink functionality of dhscf to be made more modular. !! For example, this wrapper might live independently if enough mesh !! information is made available to it. use m_delk , only : delk ! The real workhorse, similar to vmat use moreMeshSubs , only : setMeshDistr use moreMeshSubs , only : UNIFORM , QUADRATIC use parallel , only : Nodes use mesh , only : nsm , nsp integer :: isigneikr , & norb , nuo , nuotot , maxnd , & iaorb ( * ), iphorb ( * ), isa ( * ), & numd ( nuo ), & listdptr ( nuo ), listd ( maxnd ) ! ---------------------------------------------------------------------- ! Calculate matrix elements of exp(i \\vec{k} \\cdot \\vec{r}) ! ---------------------------------------------------------------------- if ( isigneikr . eq . 1 . or . isigneikr . eq . - 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call delk ( isigneikr , norb , nmpl , dvol , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) if ( nodes . gt . 1 ) then !           Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif end subroutine delk_wrapper","tags":"","loc":"proc/delk_wrapper.html","title":"delk_wrapper – SIESTA"},{"text":"Used by module~~m_setup_h0~~UsedByGraph module~m_setup_h0 m_setup_H0 proc~siesta_forces siesta_forces proc~siesta_forces->module~m_setup_h0 Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines setup_H0 Subroutines public subroutine setup_H0 (g2max) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(inout) :: g2max","tags":"","loc":"module/m_setup_h0.html","title":"m_setup_H0 – SIESTA"},{"text":"Uses precision parallel sys alloc module~~moremeshsubs~~UsesGraph module~moremeshsubs moreMeshSubs sys sys module~moremeshsubs->sys alloc alloc module~moremeshsubs->alloc precision precision module~moremeshsubs->precision parallel parallel module~moremeshsubs->parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~moremeshsubs~~UsedByGraph module~moremeshsubs moreMeshSubs proc~dhscf_init dhscf_init proc~dhscf_init->module~moremeshsubs proc~dhscf dhscf proc~dhscf->module~moremeshsubs proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~moremeshsubs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables UNIFORM QUADRATIC LINEAR TO_SEQUENTIAL TO_CLUSTER KEEP moduName maxDistr gp meshDistr meshCommu exteCommu tBuff1 tBuff2 Interfaces distMeshData Derived Types meshDisType meshCommType Subroutines initMeshDistr allocASynBuffer allocExtMeshDistr allocIpaDistr setMeshDistr resetMeshDistr distMeshData_rea distMeshData_rea distMeshData_int boxIntersection initMeshExtencil distExtMeshData gathExtMeshData splitwload reduce3Dto1D vecBisec reordMeshNumbering reordMeshNumbering compMeshComm Variables Type Visibility Attributes Name Initial integer, public, parameter :: UNIFORM = 1 integer, public, parameter :: QUADRATIC = 2 integer, public, parameter :: LINEAR = 3 integer, public, parameter :: TO_SEQUENTIAL = +1 alias to translation direction for reord procedure integer, public, parameter :: TO_CLUSTER = -1 alias to translation direction for reord procedure integer, public, parameter :: KEEP = 0 character(len=*), private, parameter :: moduName = 'moreMeshSubs' Name of the module. integer, private, parameter :: maxDistr = 5 Maximum number of data distribution that can be handled integer, private, parameter :: gp = grid_p Alias of the grid precision type( meshDisType ), private, target, save :: meshDistr (maxDistr) Contains information of the several data distributions type( meshCommType ), private, target, save :: meshCommu ((maxDistr*(maxDistr-1))/2) Contains all the communications to move among the\n several data distributions type( meshCommType ), private, target, save :: exteCommu (maxDistr,3) Contains all the needed communications to compute\n the extencil real(kind=grid_p), private, pointer :: tBuff1 (:) Memory buffer for asynchronous communications real(kind=grid_p), private, pointer :: tBuff2 (:) Memory buffer for asynchronous communications Interfaces public interface distMeshData Move data from vector fsrc , that uses distribution iDistr , to vector fdst , that uses distribution oDistr . It also re-orders a clustered\n data array into a sequential one and viceversa.\n If this is a sequencial execution, it only reorders the data. Read more… private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr Derived Types type, private :: meshDisType Private type to hold mesh distribution data. Components Type Visibility Attributes Name Initial integer, public :: nMesh (3) Number of mesh div. in each axis. integer, public, pointer :: box (:,:,:) Mesh box bounds of each node: box(1,iAxis,iNode)=lower bounds box(2,iAxis,iNode)=upper bounds integer, public, pointer :: indexp (:) integer, public, pointer :: idop (:) real(kind=dp), public, pointer :: xdop (:,:) integer, public, pointer :: ipa (:) type, private :: meshCommType Private type to hold communications to move data from one\n distribution to another. Components Type Visibility Attributes Name Initial integer, public :: ncom Number of needed communications integer, public, pointer :: src (:) Sources of communications integer, public, pointer :: dst (:) Destination of communications Subroutines public subroutine initMeshDistr (iDistr, oDistr, nm, wload) Computes a new data distribution and the communications needed to\n move data from/to the current distribution to the existing ones. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index of the input vector integer, intent(in) :: oDistr The new data distribution index integer, intent(in) :: nm (3) Number of Mesh divisions of each cell vector integer, intent(in), optional :: wload (*) Weights of every point of the mesh using the input distribut      !ion public subroutine allocASynBuffer (ndistr) Allocate memory buffers for asynchronous communications.\n It does nothing for synchronous communications. The output values are stored in the current module: tBuff1 : Buffer for distribution 1 tBuff2 : Buffer for other distributions Arguments Type Intent Optional Attributes Name integer :: ndistr Total number of distributions public subroutine allocExtMeshDistr (iDistr, nep, mop) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nep integer, intent(in) :: mop public subroutine allocIpaDistr (iDistr, na) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: na public subroutine setMeshDistr (iDistr, nsm, nsp, nml, nmpl, ntml, ntpl) Fixes the new data limits and dimensions of the mesh to those of\n the data distribution iDistr . Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index of the input vector integer, intent(in) :: nsm Number of mesh sub-divisions in each direction integer, intent(in) :: nsp Number of sub-points of each mesh point integer, intent(out) :: nml (3) Local number of Mesh divisions in each cell vector integer, intent(out) :: nmpl Local number of Mesh divisions integer, intent(out) :: ntml (3) Local number of Mesh points in each cell vector integer, intent(out) :: ntpl Local number of Mesh points public subroutine resetMeshDistr (iDistr) Reset the data of the distribution iDistr .\n Deallocate associated arrays of the current distribution. Modifies data of the current module. Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index to be reset private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine boxIntersection (ibox1, ibox2, obox, inters) Checks the three axis of the input boxes to see if there is\n intersection between the input boxes. If it exists, returns\n the resulting box. Arguments Type Intent Optional Attributes Name integer, intent(in) :: ibox1 (2,3) Input box integer, intent(in) :: ibox2 (2,3) Input box integer, intent(out) :: obox (2,3) Intersection between ibox1 and ibox2 logical, intent(out) :: inters TRUE , if there is an intersection. Otherwise FALSE . public subroutine initMeshExtencil (iDistr, nm) Compute the needed communications in order to send/receive the\n extencil (when the data is ordered in the distribution iDistr )\n The results are stored in the variable exteCommu (iDistr,1:3) of the current module. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: nm (3) Number of Mesh divisions in each cell vector public subroutine distExtMeshData (iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, DENS, BDENS) Send/receive the extencil information from the DENS matrix to the\n temporal array BDENS . Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: DENS (maxp,NSPIN) Electron density matrix real(kind=gp), intent(out) :: BDENS (BS,2*NN,NSPIN) Auxiliary arrays to store the extencil from other partitions public subroutine gathExtMeshData (iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, BVXC, VXC) Send/receive the extencil information from the BVXC temporal array\n to the array VXC . Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: BVXC (BS,2*NN,NSPIN) Auxiliar array that contains the extencil of the\n exch-corr potential real(kind=gp), intent(out) :: VXC (maxp,NSPIN) Exch-corr potential private subroutine splitwload (Nodes, Node, nm, wload, iDistr, oDistr) Compute the limits of a new distribution, trying to split the load\n of the array wload . We use the nested disection algorithm in\n order to split the mesh in the 3 dimensions. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes Total number of nodes integer, intent(in) :: Node Current process ID (from 1 to Node) integer, intent(in) :: nm (3) Number of mesh sub-divisions in each direction integer, intent(in) :: wload (*) Weights of every point of the mesh. type( meshDisType ), intent(in) :: iDistr Input distribution type( meshDisType ), intent(out) :: oDistr Output distribution private subroutine reduce3Dto1D (iaxis, Ibox, Lbox, wload, lwload) Given a 3-D array, wload , we will make a reduction of its values\n to one of its dimensions ( iaxis ). Ibox gives the limits of the\n input array wload and Lbox gives the limits of the part that we\n want to reduce. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iaxis Axe to be reduced integer, intent(in) :: Ibox (2,3) Limits of the input array integer, intent(in) :: Lbox (2,3) Limits of the intersection that we want to reduce integer, intent(in) :: wload (*) 3-D array that we want to reduce to one of\n its dimensions integer(kind=i8b), intent(out) :: lwload (*) 1-D array. Reduction of the intersected part\n of wload private subroutine vecBisec (nval, values, nparts, pos, h1, h2) Bisection of the load associated to an array. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: nval Dimension of the input array integer(kind=i8b), intent(in) :: values (nval) Input array integer, intent(in) :: nparts Numbers of partitions that we want to make from\n the input array (in this call we only make one cut) integer, intent(out) :: pos Position of the cut integer(kind=i8b), intent(out) :: h1 Load of the first part integer(kind=i8b), intent(out) :: h2 Load of the second part private subroutine reordMeshNumbering (distr1, distr2) Read more… Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution private subroutine reordMeshNumbering (distr1, distr2) Read more… Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution private subroutine compMeshComm (distr1, distr2, mcomm) Find the communications needed to transform one array that uses\n distribution distr1 to distribution distr2 Read more… Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 Source distribution type( meshDisType ), intent(in) :: distr2 Destination distribution type( meshCommType ), intent(out) :: mcomm Communications needed","tags":"","loc":"module/moremeshsubs.html","title":"moreMeshSubs – SIESTA"},{"text":"Contents Subroutines siesta_forces Subroutines public subroutine siesta_forces (istep) This subroutine represents central SIESTA operation logic. Read more… Arguments Type Intent Optional Attributes Name integer, intent(inout) :: istep","tags":"","loc":"module/m_siesta_forces.html","title":"m_siesta_forces – SIESTA"},{"text":"Uses precision class_dData1D class_Fstack_dData1D module~~m_mixing~~UsesGraph module~m_mixing m_mixing class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_mixing~~UsedByGraph module~m_mixing m_mixing proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->module~m_mixing proc~state_init state_init proc~state_init->module~m_mixing module~m_mixing_scf m_mixing_scf proc~state_init->module~m_mixing_scf proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->module~m_mixing proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->module~m_mixing module~m_mixing_scf->module~m_mixing proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->module~m_mixing proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->module~m_mixing proc~siesta_forces siesta_forces proc~siesta_forces->module~m_mixing_scf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables MIX_LINEAR MIX_PULAY MIX_BROYDEN MIX_FIRE ACTION_MIX ACTION_RESTART ACTION_NEXT I_PREVIOUS_RES I_P_RESTART I_P_NEXT I_SVD_COND debug_mix debug_msg Interfaces mixing Derived Types tMixer Functions mix_method mix_method_variant mixing_ncoeff getstackval is_next current_itt stack_check norm Subroutines mixers_init mixer_init mixers_history_init mixers_reset mixers_print mixers_print_block mixing_init mixing_coeff mixing_calc_next mixing_finalize mixing_1d mixing_2d mixing_step inverse svd push_stack_data push_F update_F push_diff Variables Type Visibility Attributes Name Initial integer, public, parameter :: MIX_LINEAR = 1 integer, public, parameter :: MIX_PULAY = 2 integer, public, parameter :: MIX_BROYDEN = 3 integer, public, parameter :: MIX_FIRE = 4 integer, private, parameter :: ACTION_MIX = 0 integer, private, parameter :: ACTION_RESTART = 1 integer, private, parameter :: ACTION_NEXT = 2 integer, private, parameter :: I_PREVIOUS_RES = 0 integer, private, parameter :: I_P_RESTART = -1 integer, private, parameter :: I_P_NEXT = -2 integer, private, parameter :: I_SVD_COND = -3 logical, private :: debug_mix = .false. character(len=20), private :: debug_msg = 'mix:' Interfaces public interface mixing private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub Derived Types type, public :: tMixer Components Type Visibility Attributes Name Initial character(len=24), public :: name type(Fstack_dData1D), public, allocatable :: stack (:) integer, public :: m = MIX_PULAY integer, public :: v = 0 integer, public :: cur_itt = 0 integer, public :: start_itt = 0 integer, public :: n_hist = 2 integer, public :: n_itt = 0 integer, public :: restart = 0 integer, public :: restart_save = 0 integer, public :: action = ACTION_MIX type( tMixer ), public, pointer :: next => null() type( tMixer ), public, pointer :: next_conv => null() real(kind=dp), public :: w = 0._dp real(kind=dp), public, pointer :: rv (:) => null() integer, public, pointer :: iv (:) => null() Functions public function mix_method (str) result(m) Return the integer specification of the mixing type Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: str Return Value integer public function mix_method_variant (m, str) result(v) Return the variant of the mixing method Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: m character(len=*), intent(in) :: str Return Value integer private function mixing_ncoeff (mix) result(n) Function to retrieve the number of coefficients\n calculated in this iteration.\n This is so external routines can query the size\n of the arrays used. Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer private function getstackval (mix, sidx, hidx) result(d1) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix integer, intent(in) :: sidx integer, intent(in), optional :: hidx Return Value real(kind=dp),\n  pointer, (:) private function is_next (mix, method, next) result(bool) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in), target :: mix integer, intent(in) :: method type( tMixer ), optional pointer :: next Return Value logical private function current_itt (mix) result(itt) Get current iteration count Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer private function stack_check (stack, n) result(check) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: stack integer, intent(in) :: n Return Value logical private function norm (n, x1, x2) Calculate the norm of two arrays Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: x1 (n) real(kind=dp), intent(in) :: x2 (n) Return Value real(kind=dp) Subroutines public subroutine mixers_init (prefix, mixers, Comm) Initialize a set of mixers by reading in fdf information.\n @param[in] prefix the fdf-label prefixes\n @param[pointer] mixers the mixers that are to be initialized\n @param[in] Comm @opt optional MPI-communicator Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), pointer :: mixers (:) integer, intent(in), optional :: Comm public subroutine mixer_init (mix) Initialize a single mixer depending on the preset\n options. Useful for external correct setup. Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix public subroutine mixers_history_init (mixers) Initialize all history for the mixers Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout), target :: mixers (:) public subroutine mixers_reset (mixers) Reset the mixers, i.e. clean everything Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mixers (:) public subroutine mixers_print (prefix, mixers) Print (to std-out) information regarding the mixers Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) public subroutine mixers_print_block (prefix, mixers) Print (to std-out) the fdf-blocks that recreate the mixer settings Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) private subroutine mixing_init (mix, n, xin, F) Initialize the mixing algorithm Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) private subroutine mixing_coeff (mix, n, xin, F, coeff) Calculate the mixing coefficients for the\n current mixer Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: coeff (:) private subroutine mixing_calc_next (mix, n, xin, F, xnext, coeff) Calculate the guess for the next iteration Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: xnext (n) real(kind=dp), intent(in) :: coeff (:) private subroutine mixing_finalize (mix, n, xin, F, xnext) Finalize the mixing algorithm Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in) :: xnext (n) private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub private subroutine mixing_step (mix) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix private subroutine inverse (n, A, B, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) integer, intent(out) :: info private subroutine svd (n, A, B, cond, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) real(kind=dp), intent(in) :: cond integer, intent(out) :: info private subroutine push_stack_data (s_F, n) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n private subroutine push_F (s_F, n, F, fact) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in), optional :: fact private subroutine update_F (s_F, n, F) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) private subroutine push_diff (s_rres, s_res, alpha) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_rres type(Fstack_dData1D), intent(in) :: s_res real(kind=dp), intent(in), optional :: alpha","tags":"","loc":"module/m_mixing.html","title":"m_mixing – SIESTA"},{"text":"Uses precision module~~m_compute_max_diff~~UsesGraph module~m_compute_max_diff m_compute_max_diff precision precision module~m_compute_max_diff->precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_compute_max_diff~~UsedByGraph module~m_compute_max_diff m_compute_max_diff proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables dDmax_current Interfaces compute_max_diff Subroutines compute_max_diff_2d compute_max_diff_1d Variables Type Visibility Attributes Name Initial real(kind=dp), public, save :: dDmax_current Temporary for storing the old maximum change Interfaces public interface compute_max_diff public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff Subroutines public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff","tags":"","loc":"module/m_compute_max_diff.html","title":"m_compute_max_diff – SIESTA"},{"text":"Used by module~~m_setup_hamiltonian~~UsedByGraph module~m_setup_hamiltonian m_setup_hamiltonian proc~siesta_forces siesta_forces proc~siesta_forces->module~m_setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines setup_hamiltonian Subroutines public subroutine setup_hamiltonian (iscf) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_setup_hamiltonian.html","title":"m_setup_hamiltonian – SIESTA"},{"text":"Used by module~~m_compute_dm~~UsedByGraph module~m_compute_dm m_compute_dm proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_dm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables PreviousCallDiagon Subroutines compute_dm Variables Type Visibility Attributes Name Initial logical, public, save :: PreviousCallDiagon = .false. Subroutines public subroutine compute_dm (iscf) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_compute_dm.html","title":"m_compute_dm – SIESTA"},{"text":"Uses class_Fstack_dData1D m_mixing module~~m_mixing_scf~~UsesGraph module~m_mixing_scf m_mixing_scf class_Fstack_dData1D class_Fstack_dData1D module~m_mixing_scf->class_Fstack_dData1D module~m_mixing m_mixing module~m_mixing_scf->module~m_mixing module~m_mixing->class_Fstack_dData1D precision precision module~m_mixing->precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_mixing_scf~~UsedByGraph module~m_mixing_scf m_mixing_scf proc~state_init state_init proc~state_init->module~m_mixing_scf proc~siesta_forces siesta_forces proc~siesta_forces->module~m_mixing_scf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables scf_mixs scf_mix MIX_SPIN_ALL MIX_SPIN_SPINOR MIX_SPIN_SUM MIX_SPIN_SUM_DIFF mix_spin Subroutines mixers_scf_init mixers_scf_print mixers_scf_print_block mixing_scf_converged mixers_scf_reset mixers_scf_history_init Variables Type Visibility Attributes Name Initial type( tMixer ), public, pointer :: scf_mixs (:) => null() type( tMixer ), public, pointer :: scf_mix => null() integer, public, parameter :: MIX_SPIN_ALL = 1 integer, public, parameter :: MIX_SPIN_SPINOR = 2 integer, public, parameter :: MIX_SPIN_SUM = 3 integer, public, parameter :: MIX_SPIN_SUM_DIFF = 4 integer, public :: mix_spin = MIX_SPIN_ALL Subroutines public subroutine mixers_scf_init (nspin, Comm) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in), optional :: Comm public subroutine mixers_scf_print (nspin) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin public subroutine mixers_scf_print_block () Arguments None public subroutine mixing_scf_converged (SCFconverged) Arguments Type Intent Optional Attributes Name logical, intent(inout) :: SCFconverged public subroutine mixers_scf_reset () Arguments None public subroutine mixers_scf_history_init () Arguments None","tags":"","loc":"module/m_mixing_scf.html","title":"m_mixing_scf – SIESTA"},{"text":"Uses write_subs module~~m_siesta_analysis~~UsesGraph module~m_siesta_analysis m_siesta_analysis write_subs write_subs module~m_siesta_analysis->write_subs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines siesta_analysis Subroutines public subroutine siesta_analysis (relaxd) Check that we are converged in geometry,\n if strictly required,\n before carrying out any analysis. Read more… Arguments Type Intent Optional Attributes Name logical :: relaxd","tags":"","loc":"module/m_siesta_analysis.html","title":"m_siesta_analysis – SIESTA"},{"text":"Used by module~~m_state_init~~UsedByGraph module~m_state_init m_state_init proc~siesta_forces siesta_forces proc~siesta_forces->module~m_state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines state_init check_cohp Subroutines public subroutine state_init (istep) Arguments Type Intent Optional Attributes Name integer :: istep private subroutine check_cohp () Arguments None","tags":"","loc":"module/m_state_init.html","title":"m_state_init – SIESTA"},{"text":"Uses write_subs module~~m_state_analysis~~UsesGraph module~m_state_analysis m_state_analysis write_subs write_subs module~m_state_analysis->write_subs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_state_analysis~~UsedByGraph module~m_state_analysis m_state_analysis proc~siesta_forces siesta_forces proc~siesta_forces->module~m_state_analysis Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines state_analysis Subroutines public subroutine state_analysis (istep) Arguments Type Intent Optional Attributes Name integer :: istep","tags":"","loc":"module/m_state_analysis.html","title":"m_state_analysis – SIESTA"},{"text":"To facilitate the communication among dhscf_init and dhscf ,\n some arrays that hold data which do not change during the SCF loop\n have been made into module variables Some others are scratch, such as nmpl , ntpl , etc Uses precision m_dfscf module~~m_dhscf~~UsesGraph module~m_dhscf m_dhscf precision precision module~m_dhscf->precision m_dfscf m_dfscf module~m_dhscf->m_dfscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_dhscf~~UsedByGraph module~m_dhscf m_dhscf proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~m_dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~m_dhscf proc~setup_h0 setup_H0 proc~setup_h0->module~m_dhscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables rhopcc rhoatm Vna Uharrs IsDiag spiral shape nml ntml npcc nmpl ntpl bcell cell dvol field rmax scell G2mesh debug_dhscf debug_fmt Subroutines dhscf_init dhscf delk_wrapper Variables Type Visibility Attributes Name Initial real(kind=grid_p), public, pointer :: rhopcc (:) real(kind=grid_p), public, pointer :: rhoatm (:) real(kind=grid_p), public, pointer :: Vna (:) real(kind=dp), public :: Uharrs Harris energy logical, public :: IsDiag logical, public :: spiral character(len=10), public :: shape integer, public :: nml (3) integer, public :: ntml (3) integer, public :: npcc integer, public :: nmpl integer, public :: ntpl real(kind=dp), public :: bcell (3,3) real(kind=dp), public :: cell (3,3) real(kind=dp), public :: dvol real(kind=dp), public :: field (3) real(kind=dp), public :: rmax real(kind=dp), public :: scell (3,3) real(kind=dp), public :: G2mesh = 0.0_dp logical, public :: debug_dhscf = .false. character(len=*), public, parameter :: debug_fmt = '('' -- dhscf : Node '',i3,tr1,a,4(tr1,e20.7))' Subroutines public subroutine dhscf_init (nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ucell, mscell, g2max, ntm, maxnd, numd, listdptr, listd, datm, Fal, stressl) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in) :: norb integer, intent(in) :: iaorb (norb) integer, intent(in) :: iphorb (norb) integer, intent(in) :: nuo integer, intent(in) :: nuotot integer, intent(in) :: nua integer, intent(in) :: na integer, intent(in) :: isa (na) real(kind=dp), intent(in) :: xa (3,na) integer, intent(in) :: indxua (na) real(kind=dp), intent(in) :: ucell (3,3) integer, intent(in) :: mscell (3,3) real(kind=dp), intent(inout) :: g2max integer, intent(inout) :: ntm (3) integer, intent(in) :: maxnd integer, intent(in) :: numd (nuo) integer, intent(in) :: listdptr (nuo) integer, intent(in) :: listd (maxnd) real(kind=dp), intent(in) :: datm (norb) real(kind=dp), intent(inout) :: Fal (3,nua) real(kind=dp), intent(inout) :: stressl (3,3) public subroutine dhscf (nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ntm, ifa, istr, iHmat, filesOut, maxnd, numd, listdptr, listd, Dscf, datm, maxnh, Hmat, Enaatm, Enascf, Uatm, Uscf, DUscf, DUext, Exc, Dxc, dipol, stress, Fal, stressl, use_rhog_in, charge_density_only) Author J.M. Soler Date August 1996 Calculates the self-consistent field contributions to Hamiltonian\n matrix elements, total energy and atomic forces. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Number of different spin polarisations: nspin=1 => Unpolarized, nspin=2 => polarized nspin=4 => Noncollinear spin or spin-orbit. integer, intent(in) :: norb Total number of basis orbitals in supercell integer, intent(in) :: iaorb (norb) Atom to which each orbital belongs integer, intent(in) :: iphorb (norb) Orbital index (within atom) of each orbital integer, intent(in) :: nuo Number of orbitals in a unit cell in this node integer, intent(in) :: nuotot Number of orbitals in a unit cell integer, intent(in) :: nua Number of atoms in unit cell integer, intent(in) :: na Number of atoms in supercell integer, intent(in) :: isa (na) Species index of all atoms in supercell real(kind=dp), intent(in) :: xa (3,na) Atomic positions of all atoms in supercell integer, intent(in) :: indxua (na) Index of equivalent atom in unit cell integer, intent(inout) :: ntm (3) Number of mesh divisions of each cell\n vector, including subgrid. integer, intent(in) :: ifa Switch which fixes whether the SCF contrib: to atomic forces is calculated and added to fa. integer, intent(in) :: istr Switch which fixes whether the SCF contrib: to stress is calculated and added to stress. integer, intent(in) :: iHmat Switch which fixes whether the Hmat matrix\n elements are calculated or not. type(filesOut_t), intent(inout) :: filesOut Output file names (If blank => not saved) integer, intent(in) :: maxnd First dimension of listd and Dscf integer, intent(in) :: numd (nuo) Number of nonzero density-matrix\n elements for each matrix row integer, intent(in) :: listdptr (nuo) Pointer to start of rows of density-matrix integer, intent(in) :: listd (*) listd(maxnd) : Nonzero-density-matrix-element column\n indexes for each matrix row real(kind=dp), intent(in) :: Dscf (:,:) Dscf(maxnd,h_spin_dim) : SCF density-matrix elements real(kind=dp), intent(in) :: datm (norb) Harris density-matrix diagonal elements (atomic occupation charges of orbitals) integer, intent(in) :: maxnh First dimension of listh and Hmat real(kind=dp), intent(in) :: Hmat (:,:) Hmat(maxnh,h_spin_dim) : Hamiltonian matrix in sparse form, to which are added the matrix elements <ORB_I | DeltaV | ORB_J> , where DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris) real(kind=dp), intent(out) :: Enaatm Integral of Vna * rhoatm real(kind=dp), intent(out) :: Enascf Integral of Vna * rhoscf real(kind=dp), intent(out) :: Uatm Harris hartree electron-interaction energy real(kind=dp), intent(out) :: Uscf SCF hartree electron-interaction energy real(kind=dp), intent(out) :: DUscf Electrostatic (Hartree) energy of (rhoscf - rhoatm) density real(kind=dp), intent(out) :: DUext Interaction energy with external electric field real(kind=dp), intent(out) :: Exc SCF exchange-correlation energy real(kind=dp), intent(out) :: Dxc SCF double-counting correction to Exc Dxc = integral of ( (epsxc - Vxc) * Rho ) All energies in Rydbergs real(kind=dp), intent(out) :: dipol (3) Electric dipole (in a.u.)\n only when the system is a molecule real(kind=dp) :: stress (3,3) real(kind=dp), intent(inout) :: Fal (3,nua) Atomic forces, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative\n of (Enascf - Enaatm + DUscf + Exc) with\n respect to atomic positions, in Ry/Bohr.\n Contributions local to this node. real(kind=dp), intent(inout) :: stressl (3,3) Stress tensor, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative of (Enascf - Enaatm + DUscf + Exc) / volume with respect to the strain tensor, in Ry.\n Contributions local to this node. logical, intent(in), optional :: use_rhog_in logical, intent(in), optional :: charge_density_only public subroutine delk_wrapper (isigneikr, norb, maxnd, numd, listdptr, listd, nuo, nuotot, iaorb, iphorb, isa) This is a wrapper to call delk, using some of the module\n variables of m_dhscf, but from outside dhscf itself. Read more… Arguments Type Intent Optional Attributes Name integer :: isigneikr integer :: norb integer :: maxnd integer :: numd (nuo) integer :: listdptr (nuo) integer :: listd (maxnd) integer :: nuo integer :: nuotot integer :: iaorb (*) integer :: iphorb (*) integer :: isa (*)","tags":"","loc":"module/m_dhscf.html","title":"m_dhscf – SIESTA"},{"text":"SIESTA is both a method and its computer program implementation, to perform efficient electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SIESTA's efficiency stems from the use of strictly localized basis sets and from the implementation of linear-scaling algorithms which can be applied to suitable systems. A very important feature of the code is that its accuracy and cost can be tuned in a wide range, from quick exploratory calculations to highly accurate simulations matching the quality of other approaches, such as plane-wave and all-electron methods. This is a documentation project for SIESTA's source code. Unlike the underlying\nprinciples of SIESTA and its user options, the codebase was not well documented,\nuntil recently, and here we make an attempt to do so. This project is in active stage of development, and thus should not be considered a reliable source of information. Please, consult the manual on SIESTA's official repository .","tags":"","loc":"page//index.html","title":"Program Overview – SIESTA"},{"text":"The general logic of the SIESTA's operation cycle is performed by siesta_forces . After initialization that includes setup of non-scf part of Hamiltonian setup_H0 (see subsections below) as long as initialization of\nparameters for MPI and TranSiesta, SIESTA enters the main Self-Consistent Field loop, that can be schematically represented: Within each SCF cycle normally we have setup_hamiltonian and compute_dm executed subsequently. \nOne of the two convergence criteria is checked at the beginning and\nat the end of each scf cycle. Hamiltonian setup DM computation Check for Convergence Results analysis step","tags":"","loc":"page/01_calculation_flow/index.html","title":"SIESTA Calculation Flow – SIESTA"},{"text":"","tags":"","loc":"page/01_calculation_flow/01_hamiltonian_setup.html","title":"Hamiltonian setup – SIESTA"},{"text":"Performed in the compute_dm subroutine. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/02_dm_computation.html","title":"DM computation – SIESTA"},{"text":"Performed in the compute_max_diff subroutine. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/03_check_convergence.html","title":"SCF Check for Convergence – SIESTA"},{"text":"Results analysis is performed in siesta_analysis and state_analysis routines. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/04_analyze_results.html","title":"SCF Results analysis – SIESTA"}]}