var tipuesearch = {"pages":[{"text":"SIESTA Note This is an early stage work-in-progress build of documentation for SIESTA code. At the moment it should not by any means be considered a reliable source of information. Please, consult the manual on SIESTA's official repository . Project Dashboard Todo Ebs Density Developer Info SIESTA Group E. Artacho, J.D. Gale, A. Garcia, J. Junquera, P. Ordejon, D. Sanchez-Portal, J.M. Cela and J.M. Soler","tags":"home","loc":"index.html","title":" SIESTA "},{"text":"This file depends on sourcefile~~rhoofd.f90~~EfferentGraph sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~atm_types.f atm_types.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~precision.f precision.F sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~sys.f sys.F sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~mesh.f mesh.F sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~parallel.f parallel.F sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~radial.f radial.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~radial.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~rhoofd.f90~~AfferentGraph sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_rhoofd Source Code rhoofd.F90 Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module m_rhoofd implicit none private public :: rhoofd contains subroutine rhoofd ( no , np , maxnd , numd , listdptr , listd , nspin , & Dscf , rhoscf , nuo , nuotot , iaorb , iphorb , isa ) !! author: P.Ordejon and J.M.Soler !! date: May 1995 !! license: GNU GPL !! !! Finds the SCF density at the mesh points from the density matrix. !! !! Re-ordered so that mesh is the outer loop and the orbitals are !! handled as lower-half triangular. J.D.Gale and J.M.Soler, Feb'99 !! !! Version of rhoofd that optionally uses a direct algorithm to save !! memory. Modified by J.D.Gale, November'99 use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo use m_spin , only : SpOrb use listsc_module , only : LISTSC use mesh , only : nsp , dxa , xdop , xdsp , meshLim use meshdscf , only : matrixOtoM use meshdscf , only : nrowsDscfL , listdl , listdlptr , NeedDscfL , & numdl , DscfL use meshphi , only : DirectPhi , endpht , lstpht , listp2 , phi use parallel , only : Nodes , node use sys , only : die use alloc , only : re_alloc , de_alloc use parallelsubs , only : GlobalToLocalOrb #ifdef MPI use mpi_siesta #endif implicit none integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of mesh points integer , intent ( in ) :: maxnd !! First dimension of `listd` and `Dscf`, and !! maximum number of nonzero elements in !! any row of `Dscf` integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero elemts in each row of `Dscf` integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of rows in `listd` integer , intent ( in ) :: listd ( maxnd ) !! List of nonzero elements in each row of `Dscf` integer , intent ( in ) :: nspin !! Number of spin components integer , intent ( in ) :: nuo !! Number of orbitals in unit cell locally integer , intent ( in ) :: nuotot !! Number of orbitals in unit cell in total integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms real ( dp ), intent ( in ) :: Dscf (:,:) !! `real*8  Dscf(maxnd)` - Rows of `Dscf` that are non-zero real ( grid_p ), intent ( out ) :: rhoscf ( nsp , np , nspin ) !! SCF density at mesh points external :: memory , timer !     Internal variables and arrays integer , parameter :: minloc = 1000 ! Min buffer size for local copy of Dscf integer , parameter :: maxoa = 100 ! Max # of orbitals per atom logical :: ParallelLocal integer :: i , ia , ic , ii , ijl , il , imp , ind integer :: ispin , io , iop , ip , iphi , is integer :: isp , iu , iul , j , jc , last , lasta integer :: lastop , maxloc , maxloc2 , triang , nc integer :: maxndl , nphiloc , lenx , leny , lenxy , lenz ! Total hamiltonian size integer :: h_spin_dim real ( dp ) :: r2sp , dxsp ( 3 ) integer , pointer :: ilc (:), ilocal (:), iorb (:) real ( dp ), pointer :: r2cut (:), Clocal (:,:), Dlocal (:,:), phia (:,:) #ifdef _TRACE_ integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE rhoofd' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 1 ) #endif !     Start time counter call timer ( 'rhoofd' , 1 ) ! Get spin-size h_spin_dim = size ( Dscf , 2 ) !     Set algorithm logical ParallelLocal = ( Nodes > 1 ) if ( ParallelLocal ) then if ( nrowsDscfL > 0 ) then maxndl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else maxndl = 1 end if nullify ( DscfL ) call re_alloc ( DscfL , 1 , maxndl , 1 , h_spin_dim , 'DscfL' , 'rhoofd' ) !       Redistribute Dscf to DscfL form call matrixOtoM ( maxnd , numd , listdptr , maxndl , nuo , & h_spin_dim , Dscf , DscfL ) end if !     Find atomic cutoff radii nullify ( r2cut ) call re_alloc ( r2cut , 1 , nsmax , 'r2cut' , 'rhoofd' ) r2cut = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) end do ! Find size of buffers to store partial copies of Dscf and C maxloc2 = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxloc = maxloc2 + minloc maxloc = min ( maxloc , no ) triang = ( maxloc + 1 ) * ( maxloc + 2 ) / 2 lenx = meshLim ( 2 , 1 ) - meshLim ( 1 , 1 ) + 1 leny = meshLim ( 2 , 2 ) - meshLim ( 1 , 2 ) + 1 lenz = meshLim ( 2 , 3 ) - meshLim ( 1 , 3 ) + 1 lenxy = lenx * leny !$OMP parallel default(shared), & !$OMP&shared(rhoscf), & !$OMP&private(ilocal,ilc,iorb,Dlocal,Clocal,phia), & !$OMP&private(ip,nc,ic,imp,i,il,last,j,iu,iul,ii,ind,io), & !$OMP&private(ijl,ispin,lasta,lastop,ia,is,iop,isp,iphi), & !$OMP&private(jc,nphiloc,dxsp,r2sp) ! Allocate local memory nullify ( ilocal , ilc , iorb , Dlocal , Clocal , phia ) !$OMP critical allocate ( ilocal ( no ), ilc ( maxloc2 ), iorb ( maxloc ) ) allocate ( Dlocal ( triang , nspin ), Clocal ( nsp , maxloc2 ) ) if ( DirectPhi ) allocate ( phia ( maxoa , nsp ) ) !$OMP end critical ! Initializations Dlocal (:,:) = 0.0_dp ilocal (:) = 0 iorb (:) = 0 last = 0 !$OMP do do ip = 1 , np !    Initializations rhoscf (:, ip ,:) = 0.0_grid_p !    Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !       iorb(il)>0 means that row il of Dlocal must not be overwritten !       iorb(il)=0 means that row il of Dlocal is empty !       iorb(il)<0 means that row il of Dlocal contains a valid row of !             Dscf, but which is not required at this point do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) if ( il > 0 ) iorb ( il ) = i end do !    Look for required rows of Dscf not yet stored in Dlocal do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) then !          Look for an available row in Dlocal do il = 1 , maxloc !             last runs circularly over rows of Dlocal last = last + 1 if ( last > maxloc ) last = 1 if ( iorb ( last ) <= 0 ) goto 10 end do call die ( 'rhoofd: no slot available in Dlocal' ) 10 continue !          Copy row i of Dscf into row last of Dlocal j = abs ( iorb ( last )) if ( j /= 0 ) ilocal ( j ) = 0 ilocal ( i ) = last iorb ( last ) = i il = last iu = indxuo ( i ) if ( ParallelLocal ) then iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = DscfL ( ind , 1 ) Dlocal ( ijl , 2 ) = DscfL ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( DscfL ( ind , 3 ) + DscfL ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( DscfL ( ind , 4 ) + DscfL ( ind , 8 )) else Dlocal ( ijl ,:) = DscfL ( ind ,:) end if end do else do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = LISTSC ( i , iu , listdl ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = DscfL ( ind , 1 ) Dlocal ( ijl , 2 ) = DscfL ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( DscfL ( ind , 3 ) + DscfL ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( DscfL ( ind , 4 ) + DscfL ( ind , 8 )) else Dlocal ( ijl ,:) = DscfL ( ind ,:) end if end do end if else call GlobalToLocalOrb ( iu , Node , Nodes , iul ) if ( i == iu ) then do ii = 1 , numd ( iul ) ind = listdptr ( iul ) + ii j = listd ( ind ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = Dscf ( ind , 1 ) Dlocal ( ijl , 2 ) = Dscf ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( Dscf ( ind , 3 ) + Dscf ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( Dscf ( ind , 4 ) + Dscf ( ind , 8 )) else Dlocal ( ijl ,:) = Dscf ( ind ,:) end if end do else do ii = 1 , numd ( iul ) ind = listdptr ( iul ) + ii j = LISTSC ( i , iu , listd ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = Dscf ( ind , 1 ) Dlocal ( ijl , 2 ) = Dscf ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( Dscf ( ind , 3 ) + Dscf ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( Dscf ( ind , 4 ) + Dscf ( ind , 8 )) else Dlocal ( ijl ,:) = Dscf ( ind ,:) end if end do end if end if end if end do !    Check algorithm if ( DirectPhi ) then lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !          Generate or retrieve phi values if ( ia /= lasta . or . iop /= lastop ) then lasta = ia lastop = iop is = isa ( ia ) do isp = 1 , nsp dxsp (:) = xdsp (:, isp ) + xdop (:, iop ) - dxa (:, ia ) r2sp = sum ( dxsp ** 2 ) if ( r2sp < r2cut ( is ) ) then !$OMP critical call all_phi ( is , + 1 , dxsp , nphiloc , phia (:, isp ) ) !$OMP end critical else phia (:, isp ) = 0.0_dp end if end do end if iphi = iphorb ( i ) !          Retrieve phi values Clocal (:, ic ) = dsqrt ( 2._dp ) * phia ( iphi ,:) !          Loop on second orbital of mesh point do jc = 1 , ic - 1 ijl = idx_ijl ( il , ilc ( jc )) do ispin = 1 , nspin !                Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * Clocal ( isp , ic ) * Clocal ( isp , jc ) end do end do end do !          ilc(ic) == il ijl = idx_ijl ( il , ilc ( ic )) do ispin = 1 , nspin !             Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * 0.5_dp * Clocal ( isp , ic ) ** 2 end do end do end do else !       Store values do ic = 1 , nc imp = endpht ( ip - 1 ) + ic il = ilocal ( lstpht ( imp )) ilc ( ic ) = il !          Retrieve phi values Clocal (:, ic ) = dsqrt ( 2._dp ) * phi (:, imp ) !          Loop on second orbital of mesh point do jc = 1 , ic - 1 ijl = idx_ijl ( il , ilc ( jc )) do ispin = 1 , nspin !                Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * Clocal ( isp , ic ) * Clocal ( isp , jc ) end do end do end do !          ilc(ic) == il ijl = idx_ijl ( il , ilc ( ic )) do ispin = 1 , nspin !             Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * 0.5_dp * Clocal ( isp , ic ) ** 2 end do end do end do end if !    Restore iorb for next point do imp = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( imp ) il = ilocal ( i ) iorb ( il ) = - i end do end do !$OMP end do ! Free local memory !$OMP critical deallocate ( ilocal , ilc , iorb , Dlocal , Clocal ) if ( DirectPhi ) deallocate ( phia ) !$OMP end critical !$OMP end parallel call de_alloc ( r2cut , 'r2cut' , 'rhoofd' ) if ( ParallelLocal ) then call de_alloc ( DscfL , 'DscfL' , 'rhoofd' ) end if #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'rhoofd' , 2 ) #ifdef DEBUG call write_debug ( '    POS rhoofd' ) #endif contains ! In any case will the compiler most likely inline this ! small routine. So it should not pose any problem. pure function idx_ijl ( i , j ) result ( ij ) integer , intent ( in ) :: i , j integer :: ij if ( i > j ) then ij = i * ( i + 1 ) / 2 + j + 1 else ij = j * ( j + 1 ) / 2 + i + 1 end if end function idx_ijl end subroutine rhoofd end module m_rhoofd","tags":"","loc":"sourcefile/rhoofd.f90.html","title":"rhoofd.F90 – SIESTA"},{"text":"This file depends on sourcefile~~setup_h0.f~~EfferentGraph sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~setup_h0.f->sourcefile~siesta_options.f90 sourcefile~atmfuncs.f atmfuncs.f sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~dhscf.f dhscf.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~precision.f precision.F sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~sys.f sys.F sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~atm_types.f atm_types.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~precision.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~dhscf.f->sourcefile~sys.f sourcefile~units.f90 units.f90 sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~mesh.f mesh.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~parallel.f parallel.F sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~delk.f90->sourcefile~parallel.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~units.f90->sourcefile~precision.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~forhar.f->sourcefile~mesh.f sourcefile~forhar.f->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~radial.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f var pansourcefilesetup_h0fEfferentGraph = svgPanZoom('#sourcefilesetup_h0fEfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~setup_h0.f~~AfferentGraph sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_setup_H0 Source Code setup_H0.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_setup_H0 private public :: setup_H0 CONTAINS subroutine setup_H0 ( G2max ) C     Computes non-self-consistent part of the Hamiltonian C     and initializes data structures on the grid. USE siesta_options , only : g2cut use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : Dscf use m_nlefsm , only : nlefsm_SO_off use m_spin , only : spin use sparse_matrices , only : listh , listhptr , numh , maxnh use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use m_nlefsm , only : nlefsm use m_kinefsm , only : kinefsm use m_naefs , only : naefs use m_dnaefs , only : dnaefs use m_dhscf , only : dhscf_init use m_energies , only : Eions , Ena , DEna , Emm , Emeta , Eso use m_ntm use m_spin , only : spin use spinorbit , only : spinorb use alloc , only : re_alloc , de_alloc use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none real ( dp ), intent ( inout ) :: g2max real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ), dummy_dm ( 1 , 1 ) real ( dp ) :: dummy_E integer :: ia , is real ( dp ) :: dummy_Eso integer :: ispin , i , j complex ( dp ) :: Dc #ifdef MPI real ( dp ) :: buffer1 #endif real ( dp ), pointer :: H_val (:), H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) #ifdef DEBUG call write_debug ( '    PRE setup_H0' ) #endif !----------------------------------------------------------------------BEGIN call timer ( 'Setup_H0' , 1 ) C     Self-energy of isolated ions Eions = 0.0_dp do ia = 1 , na_u is = isa ( ia ) Eions = Eions + uion ( is ) enddo !     In these routines, add a flag to tell them NOT to compute !     forces and stresses in this first pass, only energies. !     Neutral-atom: energy call naefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , Ena , dummy_fa , dummy_stress , & forces_and_stress = . false .) call dnaefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , DEna , dummy_fa , dummy_stress , & forces_and_stress = . false .) Ena = Ena + DEna C     Metadynamics energy if ( lMetaForce ) then call meta ( xa , na_u , ucell , Emeta , dummy_fa , dummy_stress , $ . false .,. false .) endif C     Add on force field contribution to energy call twobody ( na_u , xa , isa , ucell , Emm , & ifa = 0 , fa = dummy_fa , istr = 0 , stress = dummy_stress ) ! !     Now we compute matrix elements of the Kinetic and Non-local !     parts of H !     Kinetic: matrix elements only H_val => val ( H_kin_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call kinefsm ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , & maxnh , maxnh , lasto , iphorb , isa , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !     Non-local-pseudop:  matrix elements only H_val => val ( H_vkb_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare Eso = 0.0d0 if ( . not . spin % SO_offsite ) then call nlefsm ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) else H_so_off => val ( H_so_off_2D ) H_so_off = dcmplx ( 0._dp , 0._dp ) call nlefsm_SO_off ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & spin % Grid , & dummy_E , dummy_Eso , dummy_fa , & dummy_stress , H_val , H_so_off , & matrix_elements_only = . true .) ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! do i = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( i , 1 ), Dscf ( i , 5 ), dp ) Eso = Eso + real ( H_so_off ( i , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( i , 2 ), Dscf ( i , 6 ), dp ) Eso = Eso + real ( H_so_off ( i , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( i , 3 ), Dscf ( i , 4 ), dp ) Eso = Eso + real ( H_so_off ( i , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( i , 7 ), - Dscf ( i , 8 ), dp ) Eso = Eso + real ( H_so_off ( i , 3 ) * Dc , dp ) enddo #ifdef MPI ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 #endif endif ! .................. ! If in the future the spin-orbit routine is able to compute ! forces and stresses, then \"last\" will be needed. If we are not ! computing forces and stresses, calling it in the first iteration ! should be enough ! if ( spin % SO_onsite ) then H_so_on => val ( H_so_on_2D ) !$OMP parallel workshare default(shared) H_so_on (:,:) = 0._dp !$OMP end parallel workshare call spinorb ( no_u , no_l , iaorb , iphorb , isa , indxuo , & maxnh , numh , listhptr , listh , Dscf , H_so_on , Eso ) end if C     This will take care of possible changes to the mesh and atomic-related C     mesh structures for geometry changes g2max = g2cut call dhscf_init ( spin % Grid , no_s , iaorb , iphorb , & no_l , no_u , na_u , na_s , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnh , numh , listhptr , listh , datm , & dummy_fa , dummy_stress ) call timer ( 'Setup_H0' , 2 ) #ifdef DEBUG call write_debug ( '    POS setup_H0' ) #endif !---------------------------------------------------------------------- END END subroutine setup_H0 END module m_setup_H0","tags":"","loc":"sourcefile/setup_h0.f.html","title":"setup_H0.F – SIESTA"},{"text":"This file depends on sourcefile~~m_rhog.f90~~EfferentGraph sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~precision.f precision.F sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~parallel.f parallel.F sourcefile~m_rhog.f90->sourcefile~parallel.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_rhog.f90~~AfferentGraph sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_rhog.f90 sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~setup_hamiltonian.f->sourcefile~m_rhog.f90 sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~compute_energies.f90->sourcefile~m_rhog.f90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_rhog Source Code m_rhog.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_rhog !! This module implements most of the functionality necessary to !! to mix the Fourier components of the charge density !! !!#### Theory !! As presented in Kresse and Furthmuller !! The most basic form is the Kerker mixing, with a parameter related !! to the Thomas Fermi screening wavevector. !! There is also a Pulay (DIIS) mixer. !! !! This module is not completely self-contained. It interacts with: !! !! * [[dhscf_init(proc)]]: the allocation of the rhog arrays is done there once !! the mesh-related parameters are known. !! * [[dhscf(proc)]]: It can optionally use rhog_in instead of Dscf as starting !! point. It also computes and stores rhog by default. !! * [[compute_energies(proc)]]: Apart from correcting EKS when mixing the charge, !! it generates rhog from DM_out. !! * [[siesta_forces(proc)]]: In transiesta runs the history is reset upon starting !!   transiesta !! * [[setup_hamiltonian(proc)]]: it saves rhog_in. use precision use class_dData1D use class_Pair_dData1D use class_Fstack_Pair_dData1D use m_spin , only : nspin implicit none ! The (complex) fourier components of rho(G) as stored in a real ! array, with dimensions: !   complex (2) !   np = product(ntml) (local mesh divisions) !   nspin real ( grid_p ), pointer , public :: rhog (:,:,:) => null () real ( grid_p ), pointer , public :: rhog_in (:,:,:) => null () ! these are auxiliary arrays real ( dp ), pointer :: g2 (:) => null () logical , pointer :: g2mask (:) => null () integer , pointer :: gindex (:) => null () integer , pointer :: star_index (:) => null () ! Small cutoff for mixing with DIIS/Pulay method real ( dp ) :: rhog_cutoff integer :: ng_diis !  Number of G's in Pulay scheme !  (in this node) real ( dp ), pointer :: g2_diis (:) => null () logical :: using_diis_for_rhog ! rg_in and rg_diff are one-dimensional arrays holding the ! fourier components for selected G-vectors (those with a ! norm smaller than rhog_cutoff real ( dp ), pointer :: rg_in (:) => null () real ( dp ), pointer :: rg_diff (:) => null () real ( dp ) :: q0sq !  Thomas-Fermi K2 for damping real ( dp ) :: q1sq !  For scalar product type ( Fstack_Pair_dData1D ), save :: rhog_stack integer :: jg0 ! Index of G=0 vector ! Callable routines: ! Sets up the auxiliary arrays public :: order_rhog ! Performs the mixing public :: mix_rhog ! Computes the mismatch between in and out charge densities ! (in Fourier space) public :: compute_charge_diff ! Deallocates rhog storage public :: resetRhog private CONTAINS subroutine mix_rhog ( iscf ) use siesta_options , only : wmix use precision , only : dp use fdf , only : fdf_get use m_diis , only : diis integer , intent ( in ) :: iscf integer :: j , i real ( dp ) :: alpha real ( dp ), allocatable :: coeff (:) logical :: mix_first_iter if ( using_diis_for_rhog ) then ! Do not use the first diff in the cycle for DIIS if ( iscf > 1 ) call add_rhog_in_and_diff_to_stack () !call print_type(rhog_stack) if ( n_items ( rhog_stack ) > 1 ) then allocate ( coeff ( n_items ( rhog_stack ))) ! Get the DIIS coefficients call diis ( rhog_stack , scalar_product , coeff ) ! This will replace the small-G set coefficients ! by the DIIS-optimal ones call get_optimal_rhog_in () deallocate ( coeff ) endif endif ! Do Kerker mixing on the whole fourier series ! Check whether we want to mix in the first step mix_first_iter = fdf_get ( \"SCF.MixCharge.SCF1\" ,. false .) if (( iscf == 1 ) . and . (. not . mix_first_iter )) then ! Do not mix. Take the output density rhog_in (:,:,:) = rhog (:,:,:) else do j = 1 , size ( rhog_in , dim = 2 ) !!! maybe           if (g2(j) <= certain cutoff) then if (. true .) then alpha = wmix * g2 ( j ) / ( g2 ( j ) + q0sq ) if ( alpha == 0 ) alpha = wmix ! for G=0 rhog_in (:, j ,:) = alpha * rhog (:, j ,:) + & ( 1.0_dp - alpha ) * rhog_in (:, j ,:) else rhog_in (:, j ,:) = rhog (:, j ,:) endif enddo endif CONTAINS subroutine add_rhog_in_and_diff_to_stack () ! Store rho_in(G) and rho_diff(G) as single vectors ! in a circular stack of the appropriate size type ( dData1D ) :: vin , vdiff type ( Pair_dData1D ) :: pair integer :: ip , i , j , ispin character ( len = 20 ) :: msg ip = 0 do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then ! if this G is treated within DIIS do i = 1 , 2 ip = ip + 1 rg_in ( ip ) = rhog_in ( i , j , ispin ) rg_diff ( ip ) = rhog ( i , j , ispin ) - rg_in ( ip ) enddo endif enddo enddo write ( msg , \"(a,i3)\" ) \"scf step: \" , iscf call newdData1D ( vin , rg_in , name = \"(rhog_in -- \" // trim ( msg ) // \")\" ) call newdData1D ( vdiff , rg_diff , name = \"(rhog_diff -- \" // trim ( msg ) // \")\" ) call new ( pair , vin , vdiff , \"(pair in-diff -- \" // trim ( msg ) // \")\" ) call push ( rhog_stack , pair ) ! Store in stack call delete ( vin ) call delete ( vdiff ) call delete ( pair ) end subroutine add_rhog_in_and_diff_to_stack subroutine get_optimal_rhog_in () ! Synthesize the DIIS-optimal rho_in(G) and rho_out(G) ! from the DIIS coefficients. real ( dp ), dimension (:), pointer :: vin , vdiff type ( Pair_dData1D ), pointer :: pairp type ( dData1D ), pointer :: vp integer :: ip , i , j , ispin , k ! zero-out the components treated with DIIS do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then rhog_in (:, j , ispin ) = 0.0_dp rhog (:, j , ispin ) = 0.0_dp endif enddo enddo do k = 1 , n_items ( rhog_stack ) pairp => get_pointer ( rhog_stack , k ) call firstp ( pairp , vp ) vin => val ( vp ) call secondp ( pairp , vp ) vdiff => val ( vp ) ip = 0 do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then do i = 1 , 2 ip = ip + 1 rhog_in ( i , j , ispin ) = rhog_in ( i , j , ispin ) + & coeff ( k ) * vin ( ip ) rhog ( i , j , ispin ) = rhog ( i , j , ispin ) + & coeff ( k ) * ( vdiff ( ip ) + vin ( ip )) enddo endif enddo enddo enddo end subroutine get_optimal_rhog_in end subroutine mix_rhog !---------------------------------------------------------------- subroutine compute_charge_diff ( drhog ) use precision use m_spin , only : nspin use fdf , only : fdf_get use parallel , only : Node #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( out ) :: drhog #ifdef MPI real ( dp ) :: buffer1 #endif integer :: j , js , i , ispin logical :: debug_stars real ( dp ) :: ss ! Note that we now use the complex norm instead of the ! abs-norm... drhog = - huge ( 1.0_dp ) do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) ss = 0.0_dp do i = 1 , 2 ss = ss + ( rhog ( i , j , ispin ) - rhog_in ( i , j , ispin )) ** 2 !drhog = max(drhog,abs(rhog(i,j,ispin)-rhog_in(i,j,ispin))) enddo drhog = max ( drhog , ss ) enddo enddo drhog = sqrt ( drhog ) if ( Node == 0 ) print \"(a,f12.6)\" , \" Max |\\Delta rho(G)|: \" , drhog ! Print info about the first 10 stars debug_stars = fdf_get ( \"SCF.DebugRhogMixing\" ,. false .) if ( debug_stars . and . Node == 0 ) then print \"(a8,2x,a20,4x,a20,2x,a8)\" , \"G2\" , & \"rho_in(G) (R, C)\" , \"Diff_rho(G) (R, C)\" , \"damping\" do ispin = 1 , nspin do js = 1 , 10 j = gindex ( star_index ( js )) print \"(f8.4,2x,2f10.5,4x,2f10.5,2x,f8.4)\" , g2 ( j ), & rhog_in (:, j , ispin ) , ( rhog (:, j , ispin ) - rhog_in (:, j , ispin )), & g2 ( j ) / ( g2 ( j ) + q0sq ) enddo enddo endif #ifdef MPI !     Ensure that drhog is the same on all nodes call globalize_max ( drhog , buffer1 ) drhog = buffer1 #endif end subroutine compute_charge_diff subroutine order_rhog ( cell , n1 , n2 , n3 , mesh , nsm ) ! Sets up indexes for the handling of rho(G) use parallel , only : Node , Nodes , ProcessorY use alloc , only : re_alloc use fdf , only : fdf_get , fdf_defined use sorting , only : ordix use m_mpi_utils , only : globalize_max use m_mpi_utils , only : globalize_min use m_mpi_utils , only : globalize_sum use atomlist , only : qtot implicit none real ( dp ), intent ( in ) :: cell ( 3 , 3 ) integer , intent ( in ) :: n1 , n2 , n3 integer , intent ( in ) :: mesh ( 3 ) integer , intent ( in ) :: nsm ! Local variables real ( dp ) :: B ( 3 , 3 ), g ( 3 ), celvol integer :: I , I1 , I2 , I3 , IX , J , J1 , J2 , J3 , JX , & NP , NG , NG2 , NG3 , & ProcessorZ , Py , Pz , J2min , J2max , & J3min , J3max , J2L , J3L , NRemY , NRemZ , & BlockSizeY , BlockSizeZ external :: reclat real ( dp ), external :: volcel real ( dp ), parameter :: tiny = 1.e-10_dp ! for regularization integer :: n_rhog_depth integer :: ng_diis_min , ng_diis_max , ng_diis_sum real ( dp ) :: pi , qtf2 , length , length_max , q0_size ! !     Find the genuine thomas fermi k0&#94;2. This does not !     seem to be too relevant for the preconditioning, as !     it tends to be too big. ! pi = 4 * atan ( 1.0_dp ) celvol = volcel ( cell ) qtf2 = 4 * ( 3 * qtot / ( pi * celvol )) ** ( 1.0_dp / 3.0_dp ) ! Find the maximum length of a lattice vector ! This could set a better scale for the Kerker preconditioning length_max = 0.0_dp do i = 1 , 3 length = sqrt ( dot_product ( cell (:, i ), cell (:, i ))) length_max = max ( length , length_max ) enddo q0_size = ( 2 * pi / length_max ) if ( Node == 0 ) then print \"(a,f12.6)\" , \"Thomas-Fermi K2 (Ry):\" , qtf2 print \"(a,f12.6)\" , \"L max (bohr):\" , length_max print \"(a,f12.6)\" , \"q0_size = 2pi/L (Bohr&#94;-1):\" , q0_size print \"(a,f12.6)\" , \"q0_size&#94;2 (Ry) :\" , q0_size ** 2 endif if ( fdf_defined ( \"Thomas.Fermi.K2\" )) then if ( Node == 0 ) then print \"(a,f12.6)\" , \"Please use 'SCF.Kerker.q0sq' \" // & \"instead of 'Thomas.Fermi.K2'\" endif endif q0sq = fdf_get ( \"SCF.Kerker.q0sq\" , 0.0_dp , \"Ry\" ) ! in Ry if ( Node == 0 ) then print \"(a,f12.6)\" , \"Kerker preconditioner q0&#94;2 (Ry):\" , q0sq endif q0sq = q0sq + tiny call re_alloc ( g2 , 1 , n1 * n2 * n3 , \"g2\" , \"order_rhog\" ) call re_alloc ( g2mask , 1 , n1 * n2 * n3 , \"g2mask\" , \"order_rhog\" ) call re_alloc ( gindex , 1 , n1 * n2 * n3 , \"gindex\" , \"order_rhog\" ) rhog_cutoff = fdf_get ( \"SCF.RhoG-DIIS-Cutoff\" , 9.0_dp , \"Ry\" ) ! in Ry ng_diis = 0 !     Find reciprocal lattice vectors call reclat ( CELL , B , 1 ) !     Work out processor grid dimensions jg0 = - 1 ProcessorZ = Nodes / ProcessorY Py = ( Node / ProcessorZ ) + 1 Pz = Node - ( Py - 1 ) * ProcessorZ + 1 NG2 = Mesh ( 2 ) NG3 = Mesh ( 3 ) BlockSizeY = (( NG2 / NSM ) / ProcessorY ) * NSM BlockSizeZ = (( NG3 / NSM ) / ProcessorZ ) * NSM NRemY = ( NG2 - BlockSizeY * ProcessorY ) / NSM NRemZ = ( NG3 - BlockSizeZ * ProcessorZ ) / NSM J2min = ( Py - 1 ) * BlockSizeY + NSM * min ( Py - 1 , NRemY ) J2max = J2min + BlockSizeY - 1 if ( Py - 1. lt . NRemY ) J2max = J2max + NSM J2max = min ( J2max , NG2 - 1 ) J3min = ( Pz - 1 ) * BlockSizeZ + NSM * min ( Pz - 1 , NRemZ ) J3max = J3min + BlockSizeZ - 1 if ( Pz - 1. lt . NRemZ ) J3max = J3max + NSM J3max = min ( J3max , NG3 - 1 ) do J3 = J3min , J3max if ( J3 . gt . NG3 / 2 ) then I3 = J3 - NG3 else I3 = J3 endif do J2 = J2min , J2max if ( J2 . gt . NG2 / 2 ) then I2 = J2 - NG2 else I2 = J2 endif do J1 = 0 , N1 - 1 if ( J1 . gt . N1 / 2 ) then I1 = J1 - N1 else I1 = J1 endif G ( 1 ) = B ( 1 , 1 ) * I1 + B ( 1 , 2 ) * I2 + B ( 1 , 3 ) * I3 G ( 2 ) = B ( 2 , 1 ) * I1 + B ( 2 , 2 ) * I2 + B ( 2 , 3 ) * I3 G ( 3 ) = B ( 3 , 1 ) * I1 + B ( 3 , 2 ) * I2 + B ( 3 , 3 ) * I3 J2L = J2 - J2min J3L = J3 - J3min J = 1 + J1 + N1 * J2L + N1 * N2 * J3L G2 ( J ) = G ( 1 ) ** 2 + G ( 2 ) ** 2 + G ( 3 ) ** 2 if ( max ( abs ( i1 ), abs ( i2 ), abs ( i3 )) == 0 ) then jg0 = j endif ! Do not include G=0 in the DIIS subset g2mask ( j ) = (( g2 ( j ) <= rhog_cutoff ) . and . ( g2 ( j ) > 0 )) !   if (g2mask(j)) then !      print \"(i5,f10.4,4x,3i6)\", j, g2(j), i1, i2, i3 !   endif enddo enddo enddo ! This will work only in serial form for now ! Sort by module of G call ordix ( g2 , 1 , n1 * n2 * n3 , gindex ) ! Get index of star representatives call get_star_reps ( g2 , gindex , star_index ) ng_diis = count ( g2mask ) call globalize_min ( ng_diis , ng_diis_min ) call globalize_max ( ng_diis , ng_diis_max ) call globalize_sum ( ng_diis , ng_diis_sum ) if ( Node == 0 ) then print \"(a,i10)\" , \"Number of G's in DIIS: \" , ng_diis_sum #ifdef MPI print \"(a,3i10)\" , \"Distrib of G's in DIIS (min, ave, max): \" , & ng_diis_min , nint ( dble ( ng_diis_sum ) / Nodes ), ng_diis_max #endif endif n_rhog_depth = fdf_get ( \"SCF.RhoG-DIIS-Depth\" , 0 ) ! Note that some nodes might not have any Gs in the DIIS procedure ! But we still go ahead using_diis_for_rhog = (( ng_diis_max > 1 ) . and . ( n_rhog_depth > 1 )) if ( using_diis_for_rhog ) call set_up_diis () CONTAINS subroutine get_star_reps ( a , aindex , star_index ) ! Determine star representatives by checking ! when the (sorted) modulus of G changes... real ( dp ), intent ( in ) :: a (:) integer , intent ( in ) :: aindex (:) integer , pointer :: star_index (:) integer :: j , ng , jj , jp1 , ns ng = size ( a ) ns = 0 do jj = 1 , ng - 1 j = aindex ( jj ) jp1 = aindex ( jj + 1 ) if ( abs ( a ( j ) - a ( jp1 )) > 1.e-7_dp ) ns = ns + 1 enddo ! call re_alloc ( star_index , 1 , ns , \"star_index\" , \"m_rhog\" ) ! ns = 0 do jj = 1 , ng - 1 j = aindex ( jj ) jp1 = aindex ( jj + 1 ) if ( abs ( a ( j ) - a ( jp1 )) > 1.e-7_dp ) then ns = ns + 1 star_index ( ns ) = jj endif enddo if ( Node == 0 ) print * , \"Number of stars: \" , ns end subroutine get_star_reps subroutine set_up_diis () #ifdef MPI use m_mpi_utils , only : globalize_max , globalize_min #endif integer :: ip , ispin , n real ( dp ) :: max_g2 , min_g2 , q1sq_def #ifdef MPI real ( dp ) :: global_max , global_min #endif if ( Node == 0 ) print \"(/,a)\" , \"Setting up DIIS for rho(G) -------\" if ( Node == 0 ) print \"(a,i6)\" , & \"Number of G's treated with DIIS in Node 0: \" , ng_diis call re_alloc ( g2_diis , 1 , 2 * ng_diis * nspin , \"g2_diis\" , \"order_rhog\" ) call re_alloc ( rg_in , 1 , 2 * ng_diis * nspin , \"rg_in\" , \"order_rhog\" ) call re_alloc ( rg_diff , 1 , 2 * ng_diis * nspin , \"rg_diff\" , \"order_rhog\" ) ! Create the g2_diis array holding |G|&#94;2 for each G in the DIIS set ip = 0 do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then do i = 1 , 2 ! A bit superfluous ip = ip + 1 g2_diis ( ip ) = g2 ( j ) enddo endif enddo enddo min_g2 = minval ( g2_diis ) max_g2 = maxval ( g2_diis ) #ifdef MPI call globalize_min ( min_g2 , global_min ) min_g2 = global_min call globalize_max ( max_g2 , global_max ) max_g2 = global_max #endif if ( Node == 0 ) then print \"(a,2f10.3)\" , & \"Minimum and maximum g2 in DIIS: \" , min_g2 , max_g2 endif ! KF parameter for scalar-product weight ! Weight smallest g 20 times more than largest g ! (when possible -- if not, set factor n<20) n = floor ( max_g2 / min_g2 ) - 1 n = min ( n , 20 ) q1sq_def = ( n - 1 ) * max_g2 / (( max_g2 / min_g2 ) - n ) if ( Node == 0 ) then print \"(a,f10.3,a,i3)\" , & \"Metric preconditioner cutoff default (Ry): \" , q1sq_def , & \" n:\" , n endif q1sq = fdf_get ( \"SCF.RhoG.Metric.Preconditioner.Cutoff\" , q1sq_def , \"Ry\" ) if ( Node == 0 ) then print \"(a,f10.3)\" , & \"Metric preconditioner cutoff (Ry): \" , q1sq print \"(a,2f10.4)\" , \"Max and min weights: \" , & ( min_g2 + q1sq ) / min_g2 , & ( max_g2 + q1sq ) / max_g2 endif call new ( rhog_stack , n_rhog_depth , \"(rhog DIIS stack)\" ) end subroutine set_up_diis end subroutine order_rhog !--- subroutine resetRhog ( continuation ) use alloc , only : de_alloc !< If .true. we don't de-allocate anything, we only reset the history logical , intent ( in ), optional :: continuation logical :: lcontinuation lcontinuation = . false . if ( present ( continuation ) ) lcontinuation = continuation if ( lcontinuation ) then call reset ( rhog_stack ) else call de_alloc ( rhog_in ) call de_alloc ( rhog ) call de_alloc ( g2 ) call de_alloc ( g2mask ) call de_alloc ( gindex ) call de_alloc ( star_index ) call de_alloc ( g2_diis ) call de_alloc ( rg_in ) call de_alloc ( rg_diff ) call delete ( rhog_stack ) end if end subroutine resetRhog ! Auxiliary function to perform the scalar product of residual ! vectors in the DIIS method function scalar_product ( a , b ) result ( sp ) real ( dp ), intent ( in ) :: a (:), b (:) real ( dp ) :: sp integer :: ip , ispin , j real ( dp ) :: weight sp = 0 ip = 1 ! Use standard definition of the scalar product as conjg(A)*B, ! but take the real part, as per DIIS equations ! If q1sq is not zero, g2_diis(ip) determines the weight, as in KF do ispin = 1 , nspin do j = 1 , ng_diis weight = ( g2_diis ( ip ) + q1sq ) / g2_diis ( ip ) sp = sp + ( a ( ip ) * b ( ip ) + a ( ip + 1 ) * b ( ip + 1 )) * weight ip = ip + 2 enddo enddo ! Note: This is a \"local scalar product\" ! For efficiency reasons, the All_reduce call is done ! on the whole matrix in m_diis end function scalar_product end module m_rhog","tags":"","loc":"sourcefile/m_rhog.f90.html","title":"m_rhog.F90 – SIESTA"},{"text":"This file contains module moreMeshSubs. It defines and handles\n  different parallel mesh distributions. Includes the following subroutines connected to the mesh : initMeshDistr - Precompute a new data distribution for the\n    grid mesh to be used in Hamiltonian construction. setMeshDistr - Select a mesh distribution and set the grid sizes. distMeshData - Move data from one data distribution to another. allocASynBuffer - Allocate buffer for asynchronous\n    communications  if necessary. This file depends on sourcefile~~moremeshsubs.f~~EfferentGraph sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~sys.f sys.F sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~parallel.f parallel.F sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~mesh.f mesh.F sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~precision.f precision.F sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~mesh.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~moremeshsubs.f~~AfferentGraph sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~forhar.f forhar.F sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules moreMeshSubs Source Code moremeshsubs.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! #ifdef ASYNCHRONOUS_GRID_COMMS   /* Compile-time option */ # ifdef MPI #   define ASYNCHRONOUS           /* Internal symbol */ # endif #endif !!  author: Rogeli Grima (BSC) !!  date: December 2007 !!  license: GNU GPL !! !!  This file contains module moreMeshSubs. It defines and handles !!  different parallel mesh distributions. !! !!  Includes the following subroutines connected to the mesh : !! !!  * `[[initMeshDistr(proc)]]` - Precompute a new data distribution for the !!    grid mesh to be used in Hamiltonian construction. !!  * `[[setMeshDistr(proc)]]`- Select a mesh distribution and set the grid sizes. !!  * `[[distMeshData(proc)]]`- Move data from one data distribution to another. !!  * `[[allocASynBuffer(proc)]]` - Allocate buffer for asynchronous !!    communications  if necessary. MODULE moreMeshSubs !! author: Rogeli Grima (BSC) !! date: December 2007 use precision , only : grid_p , dp , i8b use parallel , only : node , Nodes , ProcessorY use sys , only : die use alloc , only : re_alloc , de_alloc implicit none PUBLIC :: initMeshDistr , setMeshDistr , allocExtMeshDistr PUBLIC :: allocIpaDistr , distMeshData , resetMeshDistr #ifdef MPI PUBLIC :: initMeshExtencil , distExtMeshData , gathExtMeshData #endif PUBLIC :: allocASynBuffer !     Symbolic names for parallel mesh distributions integer , parameter , public :: UNIFORM = 1 integer , parameter , public :: QUADRATIC = 2 integer , parameter , public :: LINEAR = 3 ! !     Symbolic names for \"reord\"-type operations ! integer , parameter , public :: TO_SEQUENTIAL = + 1 !! alias to translation direction for `[[reord(proc)]]` procedure integer , parameter , public :: TO_CLUSTER = - 1 !! alias to translation direction for `[[reord(proc)]]` procedure integer , parameter , public :: KEEP = 0 PRIVATE interface distMeshData !! Move data from vector `fsrc`, that uses distribution `iDistr`, to vector !! `fdst`, that uses distribution `oDistr`. It also re-orders a clustered !! data array into a sequential one and viceversa. !! If this is a sequencial execution, it only reorders the data. !!@note !! There are two subroutines: one to deal with real data and !! the other with integers. Both are called using the same interface. !!@endnote !!@note !! *AG*: Note that the integer version does NOT have the exact functionality !! of the real version. In particular, the integer version has no provision !! for a \"serial fallback\", and so this case has been trapped. !!@endnote !! Check the communications that this process should do to move data !! from `iDistr` to `oDistr`. We have 3 kind of communications (send, receive !! and keep on the same node). We have 3 kind of reorderings (clustered to !! sequential, sequential to clustered and keep the same ordering). !! For the sequencial code we call subroutine `[[reord(proc)]]` !!#### INPUT !! !! * `iDistr` : Distribution index of the input vector. !! * `fsrc`   : Input vector. !! * `oDistr` : Distribution index of the output vector. !! * `itr`    : TRanslation-direction switch: !!     `itr`=+1 => From clustered to sequential !!     `itr`=-1 => From sequential to clustered !!     `itr`=0  => Keep the status !! !! !!#### OUTPUT !! !! * `fdst`   : Output vector. ! module procedure distMeshData_rea , distMeshData_int end interface distMeshData TYPE meshDisType !! Private type to hold mesh distribution data. integer :: nMesh ( 3 ) !! Number of mesh div. in each axis. integer , pointer :: box (:,:,:) !! Mesh box bounds of each node: !! box(1,iAxis,iNode)=lower bounds !! box(2,iAxis,iNode)=upper bounds integer , pointer :: indexp (:) integer , pointer :: idop (:) real ( dp ), pointer :: xdop (:,:) integer , pointer :: ipa (:) END TYPE meshDisType TYPE meshCommType !! Private type to hold communications to move data from one !! distribution to another. integer :: ncom !! Number of needed communications integer , pointer :: src (:) !! Sources of communications integer , pointer :: dst (:) !! Destination of communications END TYPE meshCommType character ( len =* ), parameter :: moduName = 'moreMeshSubs' !! Name of the module. integer , parameter :: maxDistr = 5 !! Maximum number of data distribution that can be handled integer , parameter :: gp = grid_p !! Alias of the grid precision type ( meshDisType ), target , save :: meshDistr ( maxDistr ) !! Contains information of the several data distributions type ( meshCommType ), target , save :: & meshCommu (( maxDistr * ( maxDistr - 1 )) / 2 ) !! Contains all the communications to move among the !! several data distributions type ( meshCommType ), target , save :: exteCommu ( maxDistr , 3 ) !! Contains all the needed communications to compute !! the extencil #ifdef ASYNCHRONOUS real ( grid_p ), pointer :: tBuff1 (:) !! Memory buffer for asynchronous communications real ( grid_p ), pointer :: tBuff2 (:) !! Memory buffer for asynchronous communications #endif CONTAINS subroutine initMeshDistr ( iDistr , oDistr , nm , wload ) !! Computes a new data distribution and the communications needed to !! move data from/to the current distribution to the existing ones. !! !! The limits of the new distributions are stored in the current module !! in `[[moreMeshSubs(module):meshDistr(variable)]]`: !! !!     meshDistr(oDistr) !!     meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) !! !! If this is the first distribution, we split the mesh uniformly among !! the several processes (we only split it in dimensions Y and Z). !! !! For the other data distributions we should split the vector wload. !! The subroutine splitwload will return the limits of the new data !! distribution. The subroutine compMeshComm will return the communications !! needed to move data from/to the current distribution to/from the !! previous ones. implicit none integer , optional , intent ( in ) :: iDistr !!  Distribution index of the input vector integer , intent ( in ) :: oDistr !!  The new data distribution index integer , intent ( in ) :: nm ( 3 ) !!  Number of Mesh divisions of each cell vector integer , optional , intent ( in ) :: wload ( * ) !!  Weights of every point of the mesh using the input distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) call timer ( 'INITMESH' , 1 ) !     Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) !     Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) !     Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) !     The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else !       In order to compute the other data distributions, we should split !       the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) !       Precompute the communications needed to move data between the new data !       distribution and the previous ones. jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) end subroutine initMeshDistr subroutine allocASynBuffer ( ndistr ) !! Allocate memory buffers for asynchronous communications. !! It does nothing for synchronous communications. !! The output values are stored in the current module: !! `[[moreMeshSubs(module):tBuff1(variable)]]` : Buffer for distribution 1 !! `[[moreMeshSubs(module):tBuff2(variable)]]` : Buffer for other distributions use mesh , only : nsm implicit none integer :: ndistr !! Total number of distributions integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters #ifdef ASYNCHRONOUS !     Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) !     Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo !     Check the size of the intersections between the first data distributions !     and the others data distributions. !     Buffers don't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr ( 1 )% box (:,:, node + 1 ) do ii = 2 , ndistr box2 => meshDistr ( ii )% box (:,:, node + 1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp else lsize = 0 endif imax1 = max ( imax1 , nsize ( 1 ) - lsize ) imax2 = max ( imax2 , nsize ( ii ) - lsize ) enddo !     Deallocate local memory call de_alloc ( nsize , 'nsize' , moduName ) !     Allocate memory for asynchronous communications call re_alloc ( tBuff1 , 1 , imax1 , 'tBuff1' , moduName ) call re_alloc ( tBuff2 , 1 , imax2 , 'tBuff2' , moduName ) #endif end subroutine allocASynBuffer subroutine allocExtMeshDistr ( iDistr , nep , mop ) use mesh , only : indexp , idop , xdop implicit none !     Input variables integer , intent ( in ) :: iDistr , nep , mop !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % indexp , 1 , nep , 'distr%indexp' , moduName ) call re_alloc ( distr % idop , 1 , mop , 'distr%idop' , moduName ) call re_alloc ( distr % xdop , 1 , 3 , 1 , mop , 'distr%xdop' , moduName ) indexp => distr % indexp idop => distr % idop xdop => distr % xdop end subroutine allocExtMeshDistr subroutine allocIpaDistr ( iDistr , na ) use mesh , only : ipa implicit none !     Input variables integer , intent ( in ) :: iDistr , na !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % ipa , 1 , na , 'distr%ipa' , moduName ) ipa => meshDistr ( iDistr )% ipa end subroutine allocIpaDistr subroutine setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) !! Fixes the new data limits and dimensions of the mesh to those of !! the data distribution `iDistr`. use mesh , only : meshLim , indexp , ipa , idop , xdop implicit none integer , intent ( in ) :: iDistr !! Distribution index of the input vector integer , intent ( in ) :: nsm !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: nsp !! Number of sub-points of each mesh point integer , intent ( out ) :: nml ( 3 ) !! Local number of Mesh divisions in each cell vector integer , intent ( out ) :: nmpl !! Local number of Mesh divisions integer , intent ( out ) :: ntml ( 3 ) !! Local number of Mesh points in each cell vector integer , intent ( out ) :: ntpl !! Local number of Mesh points !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) meshLim = distr % box ( 1 : 2 , 1 : 3 , node + 1 ) nml ( 1 ) = ( MeshLim ( 2 , 1 ) - MeshLim ( 1 , 1 )) + 1 nml ( 2 ) = ( MeshLim ( 2 , 2 ) - MeshLim ( 1 , 2 )) + 1 nml ( 3 ) = ( MeshLim ( 2 , 3 ) - MeshLim ( 1 , 3 )) + 1 nmpl = nml ( 1 ) * nml ( 2 ) * nml ( 3 ) ntml = nml * nsm ntpl = nmpl * nsp indexp => distr % indexp idop => distr % idop xdop => distr % xdop ipa => distr % ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr subroutine resetMeshDistr ( iDistr ) !! Reset the data of the distribution `iDistr`. !! Deallocate associated arrays of the current distribution. !! Modifies data of the current module. implicit none integer , optional , intent ( in ) :: iDistr !! Distribution index to be reset integer :: idis , ini , fin , icom type ( meshDisType ), pointer :: distr type ( meshCommType ), pointer :: mcomm if ( present ( iDistr )) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis = ini , fin distr => meshDistr ( idis ) distr % nMesh = 0 if ( associated ( distr % box )) then call de_alloc ( distr % box , 'distr%box' , 'moreMeshSubs' ) endif if ( associated ( distr % indexp )) then call de_alloc ( distr % indexp , 'distr%indexp' , & 'moreMeshSubs' ) endif if ( associated ( distr % idop )) then call de_alloc ( distr % idop , 'distr%idop' , & 'moreMeshSubs' ) endif if ( associated ( distr % xdop )) then call de_alloc ( distr % xdop , 'distr%xdop' , & 'moreMeshSubs' ) endif if ( associated ( distr % ipa )) then call de_alloc ( distr % ipa , 'distr%ipa' , & 'moreMeshSubs' ) endif do icom = 1 , 3 mcomm => exteCommu ( idis , icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo do icom = (( idis - 2 ) * ( idis - 1 )) / 2 + 1 , (( idis - 1 ) * idis ) / 2 mcomm => meshCommu ( icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if ( associated ( tBuff1 )) then call de_alloc ( tBuff1 , 'tBuff1' , 'moreMeshSubs' ) endif if ( associated ( tBuff2 )) then call de_alloc ( tBuff2 , 'tBuff2' , 'moreMeshSubs' ) endif #endif end subroutine resetMeshDistr #ifdef ASYNCHRONOUS subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea #else /* SYNCHRONOUS communications */ subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & N1 , N2 , N3 , NN , ind , ncom , & icom , NSP , NSRC ( 3 ), NDST ( 3 ), ME , & MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), JS (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters real ( grid_p ), pointer :: TBUF (:) integer :: nsize , nm ( 3 ) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif !----------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) if ( nodes == 1 ) then nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) NSP = NSM * NSM * NSM ME = Node + 1 nullify ( JS ) call re_alloc ( JS , 1 , NSP , 'JS' , 'moreMeshSubs' ) !       Compute the maximum size of the buffer needed to transfer data !       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo MaxSize = MaxSize * nsp if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP TBUF ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = TBUF ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP TBUF ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize * nsp , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_grid_real , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_grid_real , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif call de_alloc ( JS , 'JS' , 'moreMeshSubs' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea #endif subroutine distMeshData_int ( iDistr , fsrc , oDistr , fdst , itr ) #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr integer , intent ( in ) :: fsrc ( * ) integer , intent ( out ) :: fdst ( * ) !     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & ind , ncom , icom , NSRC ( 3 ), NDST ( 3 ), & ME , MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters integer , pointer :: TBUF (:) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif if ( nodes == 1 ) then call die ( \"Called _int version of distMeshData for n=1\" ) else !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) ME = Node + 1 !       Compute the maximum size of the buffer needed to transfer data !       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 NSRC ( 2 ) = Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 NSRC ( 3 ) = Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 Dbox => odis % box (:,:, ME ) NDST ( 1 ) = Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 NDST ( 2 ) = Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 NDST ( 3 ) = Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 if ( itr . eq . 0 ) then !         From sequencial to sequencial do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif endif end subroutine distMeshData_int subroutine boxIntersection ( ibox1 , ibox2 , obox , inters ) !! Checks the three axis of the input boxes to see if there is !! intersection between the input boxes. If it exists, returns !! the resulting box. implicit none !     Passed arguments integer , intent ( in ) :: ibox1 ( 2 , 3 ), ibox2 ( 2 , 3 ) !! Input box integer , intent ( out ) :: obox ( 2 , 3 ) !! Intersection between `ibox1` and `ibox2` logical , intent ( out ) :: inters !! `TRUE`, if there is an intersection. Otherwise `FALSE`. !     Local variables integer :: iaxis inters = . true . do iaxis = 1 , 3 obox ( 1 , iaxis ) = max ( ibox1 ( 1 , iaxis ), ibox2 ( 1 , iaxis )) obox ( 2 , iaxis ) = min ( ibox1 ( 2 , iaxis ), ibox2 ( 2 , iaxis )) if ( obox ( 2 , iaxis ). lt . obox ( 1 , iaxis )) inters = . false . enddo end subroutine boxIntersection #ifdef MPI subroutine initMeshExtencil ( iDistr , nm ) !! Compute the needed communications in order to send/receive the !! extencil (when the data is ordered in the distribution `iDistr`) !! The results are stored in the variable !! `[[moreMeshSubs(module):exteCommu(variable)]](iDistr,1:3)` !! of the current module. !! !! For every dimension of the problem, search all the neighbors that !! we have. Given the current data distribution we compute the limits !! of our extencil and we check its intersection with all the other !! processes. Once we know all our neighbors we call subroutine !! `[[scheduleComm(proc)]]` in order to minimize the number !! of communications steps. use scheComm implicit none !     Passed arguments integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: nm ( 3 ) !! Number of Mesh divisions in each cell vector !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), Ibox ( 2 , 3 ), & ii , iaxis , ncom , Gcom , Lcom , P1 , P2 integer , pointer :: src (:), dst (:), Dbox (:,:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm type ( COMM_T ) :: comm logical :: inters idis => meshDistr ( iDistr ) do iaxis = 1 , 3 !       One communication structure for every dimension mcomm => exteCommu ( iDistr , iaxis ) !       Count the number of communications needed to send/receive !       the extencil ncom = 0 do P1 = 1 , Nodes !         Create the extencil boxes for both sides of the current !         partition Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) ncom = ncom + 1 endif enddo enddo Gcom = ncom !       Create a list of communications needed to send/receive !       the extencil if ( Gcom . gt . 0 ) then nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) ncom = 0 do P1 = 1 , Nodes Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif endif enddo enddo comm % np = Nodes !         reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !         Count the number of communications needed by the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !         Store the ordered list of communications needed by the current !         process to send/receive the extencil. if ( Lcom . gt . 0 ) then nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , & 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , & 'moreMeshSubs' ) ncom = 0 do P1 = 1 , comm % ncol ii = comm % ind ( P1 , Node + 1 ) if ( ii . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( ii ) mcomm % dst ( ncom ) = dst ( ii ) endif enddo mcomm % ncom = Lcom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) endif call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) endif enddo end subroutine initMeshExtencil subroutine distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , dens , BDENS ) !! Send/receive the extencil information from the `DENS` matrix to the !! temporal array `BDENS`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: DENS ( maxp , NSPIN ) !! Electron density matrix real ( gp ), intent ( out ) :: BDENS ( BS , 2 * NN , NSPIN ) !! Auxiliary arrays to store the extencil from other partitions !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM if (. not . associated ( mcomm % dst )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif if (. not . associated ( mcomm % src )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = dimB ( 2 ) - NN + 1 , dimB ( 2 ) uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ) - NN + 1 , dimB ( 3 ) do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine distExtMeshData subroutine gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , BVXC , VXC ) !! Send/receive the extencil information from the `BVXC` temporal array !! to the array `VXC`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: BVXC ( BS , 2 * NN , NSPIN ) !! Auxiliar array that contains the extencil of the !! exch-corr potential real ( gp ), intent ( out ) :: VXC ( maxp , NSPIN ) !! Exch-corr potential !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = NN + 1 , 2 * NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine gathExtMeshData subroutine splitwload ( Nodes , Node , nm , wload , iDistr , oDistr ) !! Compute the limits of a new distribution, trying to split the load !! of the array `wload`. We use the nested disection algorithm in !! order to split the mesh in the 3 dimensions. !! !! We use the nested disection algorithm to split the load associated !! to the vector `wload` among all the processes. The problem is that !! every process have a different part of wload. Every time that we want !! to split a piece of the mesh, we should find which processors have that !! information. !! !! `wload` is a 3D array. In every iteration of the algorithm we should !! decide the direction of the cut. Then we should made a reduction of !! this 3-D array to a 1-D array (according to the selected direction). use mpi_siesta implicit none integer , intent ( in ) :: Nodes !! Total number of nodes integer , intent ( in ) :: Node !! Current process ID (from 1 to Node) integer , intent ( in ) :: nm ( 3 ) !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: wload ( * ) !! Weights of every point of the mesh. type ( meshDisType ), intent ( in ) :: iDistr !! Input distribution type ( meshDisType ), intent ( out ) :: oDistr !! Output distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'splitwload ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: PP , Lbox ( 2 , 3 ), Ldim , & QQ , P1 , P2 , POS , ini integer ( i8b ), pointer :: lwload (:), gwload (:), recvB (:) logical :: found , inters integer :: mGdim , mLdim , nAxis , nms ( 3 ) integer ( i8b ) :: h1 , h2 integer , pointer :: PROCS (:) integer :: MPIerror , Status ( MPI_Status_Size ) call timer ( 'SPLOAD' , 1 ) !     At the begining of the algorithm all the mesh is assigned to the !     first node:  oDistr%box(*,*,1) = nm oDistr % box ( 1 , 1 , 1 ) = 1 oDistr % box ( 2 , 1 , 1 ) = nm ( 1 ) oDistr % box ( 1 , 2 , 1 ) = 1 oDistr % box ( 2 , 2 , 1 ) = nm ( 2 ) oDistr % box ( 1 , 3 , 1 ) = 1 oDistr % box ( 2 , 3 , 1 ) = nm ( 3 ) oDistr % box ( 1 : 2 , 1 : 3 , 2 : Nodes ) = 0 nms = nm !     Array PROCS will contain the number of processes that are associated to !     every box. At the begining all the mesh is assigned to process 1, then !     PROCS(1)=Nodes, while the rest are equal to zero nullify ( PROCS , lwload , gwload , recvB ) call re_alloc ( PROCS , 1 , Nodes , 'PROCS' , 'moreMeshSubs' ) PROCS ( 1 ) = Nodes PROCS ( 2 : Nodes ) = 0 found = . true . do while ( found ) !       Choose the direction to cut the mesh nAxis = 3 if ( nms ( 2 ). gt . nms ( nAxis )) nAxis = 2 if ( nms ( 1 ). gt . nms ( nAxis )) nAxis = 1 nms ( nAxis ) = ( nms ( nAxis ) + 1 ) / 2 !       Check if we still have to keep cutting the mesh found = . false . do PP = Nodes , 1 , - 1 if ( PROCS ( PP ). GT . 1 ) then !           There are more than one processes associated to the mesh !           of process PP. We are going to split the mesh in two parts !           of p1 and p2 processors. p1 = PROCS ( PP ) / 2 p2 = PROCS ( PP ) - p1 found = . true . !           Check if the current partition has intersection with the piece of !           mesh that we want to cut. call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, Node ), & Lbox , inters ) if ( Node . eq . PP ) then mGdim = oDistr % box ( 2 , nAxis , PP ) - oDistr % box ( 1 , nAxis , PP ) + 1 call re_alloc ( gwload , 1 , mGdim , 'gwload' , 'moreMeshSubs' ) call re_alloc ( recvB , 1 , mGdim , 'recvB' , 'moreMeshSubs' ) endif if ( inters ) then !             If there is an intersection I should reduce the intersected part !             from a 3-D array to a 1-D array. mLdim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 call re_alloc ( lwload , 1 , mLdim , 'lwload' , & 'moreMeshSubs' ) call reduce3Dto1D ( nAxis , iDistr % box (:,:, Node ), Lbox , & wload , lwload ) endif if ( Node . eq . PP ) then !             If, I'm the process PP I should receive the information from other !             processes gwload = 0 do QQ = 1 , Nodes call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, QQ ), & Lbox , inters ) if ( inters ) then Ldim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 ini = Lbox ( 1 , nAxis ) - oDistr % box ( 1 , nAxis , PP ) if ( PP . eq . QQ ) then gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + lwload ( 1 : Ldim ) else call mpi_recv ( recvB , Ldim , MPI_INTEGER8 , QQ - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + recvB ( 1 : Ldim ) endif endif enddo call de_alloc ( recvB , 'recvB' , 'moreMeshSubs' ) !             Process PP computes where to cut the mesh call vecBisec ( mGdim , gwload ( 1 : mGdim ), & PROCS ( PP ), POS , h1 , h2 ) call de_alloc ( gwload , 'gwload' , 'moreMeshSubs' ) else if ( inters ) then !             If, I'm not the process PP I should send the information to !             the process PP call MPI_Send ( lwload , mLdim , & MPI_INTEGER8 , PP - 1 , 1 , MPI_Comm_World , & MPIerror ) endif if ( associated ( lwload )) & call de_alloc ( lwload , 'lwload' , 'moreMeshSubs' ) !           Process PP send the position of the cut to the rest of processes call MPI_Bcast ( pos , 1 , MPI_integer , PP - 1 , & MPI_Comm_World , MPIerror ) !           We have splitted the piece of mesh associated to process PP !           in two parts. One would be stored in position PP and the other !           would be stored in position PP+P1 QQ = PP + P1 oDistr % box ( 1 : 2 , 1 : 3 , QQ ) = oDistr % box ( 1 : 2 , 1 : 3 , PP ) pos = oDistr % box ( 1 , naxis , QQ ) + pos oDistr % box ( 1 , naxis , QQ ) = pos oDistr % box ( 2 , naxis , PP ) = pos - 1 !           We should actualize the numbers of processes associated to PP and QQ PROCS ( PP ) = P1 PROCS ( QQ ) = P2 endif enddo enddo call de_alloc ( PROCS , 'PROCS' , 'moreMeshSubs' ) call timer ( 'SPLOAD' , 2 ) end subroutine splitwload subroutine reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) !! Given a 3-D array, `wload`, we will make a reduction of its values !! to one of its dimensions (`iaxis`). `Ibox` gives the limits of the !! input array `wload` and `Lbox` gives the limits of the part that we !! want to reduce. !! !! First we compute the 3 dimensions of the input array and the !! intersection. We accumulate the values of the intersection into a !! 1-D array. !! !!     IF (iaxis=1) lwload(II) = SUM(wload(II,*,*)) !!     IF (iaxis=2) lwload(II) = SUM(wload(*,II,*)) !!     IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) implicit none integer , intent ( in ) :: iaxis !! Axe to be reduced integer , intent ( in ) :: Ibox ( 2 , 3 ) !! Limits of the input array integer , intent ( in ) :: Lbox ( 2 , 3 ) !! Limits of the intersection that we want to reduce integer , intent ( in ) :: wload ( * ) !! 3-D array that we want to reduce to one of !! its dimensions integer ( i8b ), intent ( out ) :: lwload ( * ) !! 1-D array. Reduction of the intersected part !! of wload !     Local variables integer :: Idim ( 3 ), Ldim ( 3 ), ind , ind1 , ind2 , ind3 , & I1 , I2 , I3 !     Dimensions of the input array Idim ( 1 ) = Ibox ( 2 , 1 ) - Ibox ( 1 , 1 ) + 1 Idim ( 2 ) = Ibox ( 2 , 2 ) - Ibox ( 1 , 2 ) + 1 Idim ( 3 ) = Ibox ( 2 , 3 ) - Ibox ( 1 , 3 ) + 1 !     Dimensions of the intersection. Ldim ( 1 ) = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ldim ( 2 ) = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Ldim ( 3 ) = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( iaxis . eq . 1 ) then !       Reduction into the X-axis lwload ( 1 : Ldim ( 1 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I1 ) = lwload ( I1 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else if ( iaxis . eq . 2 ) then !       Reduction into the Y-axis lwload ( 1 : Ldim ( 2 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I2 ) = lwload ( I2 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else !       Reduction into the Z-axis lwload ( 1 : Ldim ( 3 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I3 ) = lwload ( I3 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo endif end subroutine reduce3Dto1D subroutine vecBisec ( nval , values , nparts , pos , h1 , h2 ) !! Bisection of the load associated to an array. !! !! We want to split array `values` in `nparts`, but in this call to !! `vecBisec` we are going to make only one cut. First, we split `nparts` !! in two parts: `p1=nparts/2` and `p2=nparts-p1`. Then we compute the total !! load of the array `values` (`total`) and the desired load for the !! first part: !! !!     halfG = (total*p1)/nparts !! !! Finally, we try to find the position inside `values` where we are !! nearer of the the desired solution. implicit none integer , intent ( in ) :: nval !! Dimension of the input array integer ( i8b ), intent ( in ) :: values ( nval ) !! Input array integer , intent ( in ) :: nparts !! Numbers of partitions that we want to make from !! the input array (in this call we only make one cut) integer , intent ( out ) :: pos !! Position of the cut integer ( i8b ), intent ( out ) :: h1 !! Load of the first part integer ( i8b ), intent ( out ) :: h2 !! Load of the second part !     Local variables integer :: p1 , p2 , ii integer ( i8b ) :: total , halfG , halfL if ( nparts . gt . 1 ) then !       Split the number of parts in 2 p1 = nparts / 2 p2 = nparts - p1 !       Compute the total load of the array total = 0 do ii = 1 , nval total = total + values ( ii ) enddo !       Desired load of the first part halfG = ( total * p1 ) / nparts halfL = 0 pos = 0 !       Loop until we reach the solution do while ( halfL . lt . halfG ) pos = pos + 1 if ( pos . eq . nval + 1 ) STOP 'ERROR in vecBisec' halfL = halfL + values ( pos ) enddo !       Check if the previous position is better than the !       current position if (( halfL - values ( pos ) * p2 / nparts ). gt . halfG ) then halfL = halfL - values ( pos ) pos = pos - 1 endif h1 = halfL h2 = total - halfL endif end subroutine vecBisec #ifdef REORD1 subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is defined. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > Minimize the number of communications. Data don't need to !! > be communicated if it belongs to the same process in !! > different data distributions !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters !     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) !     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo !     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 !       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo !       Save the permutation for this node (a negative number means that !       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI !       Change the permutation of those nodes who pretend to use the !       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering #else subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is **NOT** defined. !! !! Here an integer parameter `PROCS_PER_NODE` is also !! used as an input. Value of `PROCS_PER_NODE` is either !! read from .fdf-file or set to 4 as default. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > * Minimize the number of communications. Data don't need to !! >   be communicated if it belongs to the same process in !! >   different data distributions !! > * Distribute memory needs among different NODES (group of processes !! >   that shares the same memory) !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` !! !!#### Behavior !! !! 1. Compute the size of all the boxes of the second distribution !! 2. Reorder the list of boxes according to its size !! 3. Create a list of buckets use fdf implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , I1 , I2 , J1 , J2 , J3 , & K1 , K2 , K3 , NN , NB , NM , SI , & SIMAX , Lbox ( 2 , 3 ) integer , pointer :: Nsiz (:), perm (:), Gprm (:), & chkb (:), box1 (:,:), box2 (:,:), & box (:,:,:) => null () integer :: PROCS_PER_NODE logical :: inters PROCS_PER_NODE = fdf_get ( 'PROCS_PER_NODE' , 4 ) !     Create groups of PROCS_PER_NODE processes NN = nodes + PROCS_PER_NODE - 1 NB = NN / PROCS_PER_NODE ! Number of buckets NM = MOD ( NN , PROCS_PER_NODE ) + 1 ! Size of the last bucket !     Allocate local arrays nullify ( Nsiz , perm , Gprm , chkb ) call re_alloc ( Nsiz , 1 , Nodes , 'Nsiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( Gprm , 1 , Nodes , 'Gprm' , 'moreMeshSubs' ) call re_alloc ( chkb , 1 , NB , 'chkb' , 'moreMeshSubs' ) call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) !     Compute the size of all the boxes of the second distribution do P1 = 1 , Nodes box2 => distr2 % box (:,:, P1 ) Nsiz ( P1 ) = ( box2 ( 2 , 1 ) - box2 ( 1 , 1 ) + 1 ) * & ( box2 ( 2 , 2 ) - box2 ( 1 , 2 ) + 1 ) * & ( box2 ( 2 , 3 ) - box2 ( 1 , 3 ) + 1 ) perm ( P1 ) = P1 enddo !     Reorder the list of boxes according to its size call myQsort ( Nodes , Nsiz , perm ) Gprm ( 1 : Nodes ) = 0 P1 = 0 !     We distribute processes in \"buckets\" of size PROCS_PER_NODE !     We have a total number of NB \"buckets\". DO I1 = 1 , PROCS_PER_NODE !       At every step of loop I1, we assign a box to every bucket if ( I1 . EQ . NM + 1 ) NB = NB - 1 !       Reset chkb. All buckets are empty. chkb ( 1 : NB ) = 0 DO I2 = 1 , NB !         At every step of loop I2, we assign a box to a different bucket P1 = P1 + 1 P2 = perm ( P1 ) ! P2 is the \"P1\"th biggest box box2 => distr2 % box (:,:, P2 ) J2 = 1 J3 = 1 SIMAX = - 1 DO J1 = 1 , Nodes !           J1=node; J2=position inside the bucket; J3=bucket index if ( chkb ( J3 ). eq . 0 . and . Gprm ( J1 ). eq . 0 ) then !             chkb(J3).eq.0 => The current bucket is not in use !             Gprm(J1).eq.0 => The node has not been permuted yet box1 => distr1 % box (:,:, J1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then SI = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) else SI = 0 endif if ( SI . gt . SIMAX ) then !               Save the information of the current node if it has the !               biggest intersection with box P2 SIMAX = SI K1 = J1 K3 = J3 endif endif !           Update information about bucket index and position J2 = J2 + 1 if ( J2 . gt . PROCS_PER_NODE ) then J2 = 1 J3 = J3 + 1 endif ENDDO !         box(P2) will be set to process K1 (that belongs to bucket K3) chkb ( K3 ) = 1 Gprm ( K1 ) = P2 ENDDO ENDDO !     Reorder boxes of the second distribution according to Gprm box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes P2 = Gprm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , P2 ) enddo !     Deallocate local arrays call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( chkb , 'chkb' , 'moreMeshSubs' ) call de_alloc ( Gprm , 'Gprm' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Nsiz , 'Nsiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering #endif subroutine compMeshComm ( distr1 , distr2 , mcomm ) !! Find the communications needed to transform one array that uses !! distribution `distr1` to distribution `distr2` !! !! Count the number of intersections between the source distribution !! and the destiny distribution. Every intersection represents a !! communication. Then we call [[scheduleComm(proc)]] !! to optimize the order of these !! communications. Finally, we save the communications that belongs to !! the current process in the variable `mcomm` use scheComm implicit none type ( meshDisType ), intent ( in ) :: distr1 !! Source distribution type ( meshDisType ), intent ( in ) :: distr2 !! Destination distribution type ( meshCommType ), intent ( out ) :: mcomm !! Communications needed !     Local variables integer :: P1 , P2 , ncom , Gcom , Lcom , & Lind , Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:) logical :: inters type ( COMM_T ) :: comm !     count the number of intersections between Source distribution and !     destiny distribution. Every intersection represents a communication. ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) ncom = ncom + 1 enddo enddo Gcom = ncom !     Allocate local arrays nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) !     Make a list of communications ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif enddo enddo comm % np = Nodes !     reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !     Count the number of communications of the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !     Allocate memory to store data of the communications of the !     current process. nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , 'moreMeshSubs' ) !     Save the list of communications for the current process ncom = 0 do P1 = 1 , comm % ncol Lind = comm % ind ( P1 , Node + 1 ) if ( Lind . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( Lind ) mcomm % dst ( ncom ) = dst ( Lind ) endif enddo mcomm % ncom = ncom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) end subroutine compMeshComm #endif END MODULE moreMeshSubs","tags":"","loc":"sourcefile/moremeshsubs.f.html","title":"moremeshsubs.F – SIESTA"},{"text":"This file depends on sourcefile~~forhar.f~~EfferentGraph sourcefile~forhar.f forhar.F sourcefile~parallel.f parallel.F sourcefile~forhar.f->sourcefile~parallel.f sourcefile~mesh.f mesh.F sourcefile~forhar.f->sourcefile~mesh.f sourcefile~precision.f precision.F sourcefile~forhar.f->sourcefile~precision.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~sys.f sys.F sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~sys.f->sourcefile~parallel.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~forhar.f~~AfferentGraph sourcefile~forhar.f forhar.F sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_forhar Source Code forhar.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module m_forhar use precision , only : dp , grid_p use alloc , only : re_alloc , de_alloc #ifndef BSC_CELLXC use parallel , only : ProcessorY use mesh , only : NSM use siestaXC , only : cellXC ! Finds xc energy and potential use siestaXC , only : myMeshBox ! Returns my processor mesh box use siestaXC , only : jms_setMeshDistr => setMeshDistr ! Sets a distribution of mesh ! points over parallel processors #else /* BSC_CELLXC */ use mesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , LINEAR use moreMeshSubs , only : KEEP #endif /* BSC_CELLXC */ implicit none public :: forhar private CONTAINS #ifndef BSC_CELLXC subroutine forhar ( NTPL , NSPIN , NML , NTML , NTM , NPCC , $ CELL , RHOATM , #else /* BSC_CELLXC */ subroutine forhar ( NTPL , NSPIN , NML , NTML , NPCC , CELL , RHOATM , #endif /* BSC_CELLXC */ & RHOPCC , VNA , DRHOOUT , VHARRIS1 , VHARRIS2 ) !! author: J.Junquera !! date: 09/00 !! !! Build the potentials needed for computing Harris forces: !! !!@todo !! Formulas in description !!@endtodo !! !! !! (V_{NA} + V_{Hartree}(DeltaRho_{in}) - DV_{xc}(Rho_{in})/Dn * (Rho_{out}-Rho_{in})) !! !! !! !! (V_{NA} + V_{Hartree}(DeltaRho_in) + V_{xc}(Rho_{in}) !! !! !! In the first SCF step,  V_{Hartree}(DeltaRho_{in})  is zero, because !! in that case,  Rho_{SCF}(r) = Rho_{atm}(r)  and therefore,  DeltaRho(r) = 0  !! This calculation will be skipped. !! !! If Harris + Spin polarized in the first SCF step, then Vharris2 will !! multiply to  D Rho(Harris)/D R  inside dfscf, and the change of !! the harris density respect the displacement !! on one atom will not depend on spin. We add in Vharris2 the !! contributions of both spins. INTEGER :: NTPL !! Number of Mesh Total Points in unit cell (including subpoints) locally. INTEGER :: NML ( 3 ), NTML ( 3 ) #ifndef BSC_CELLXC INTEGER , intent ( IN ) :: NTM ( 3 ) !! Number of mesh divisions of each cell !! vector, including subgrid INTEGER , INTENT ( IN ) :: NSPIN !! Spin polarizations INTEGER , INTENT ( IN ) :: NPCC !! Partial core corrections? (`0`=no,`1`=yes) #else /* BSC_CELLXC */ INTEGER , INTENT ( IN ) :: NSPIN , NPCC #endif /* BSC_CELLXC */ REAL ( dp ), INTENT ( IN ) :: CELL ( 3 , 3 ) !! Cell vectors REAL ( grid_p ), INTENT ( IN ) :: VNA ( NTPL ) !! Sum of neutral atoms potentials REAL ( grid_p ), INTENT ( IN ) :: RHOATM ( NTPL ) !! Harris density at mesh points REAL ( grid_p ), INTENT ( IN ) :: RHOPCC ( NTPL ) !! Partial-core-correction density for xc REAL ( grid_p ), INTENT ( INOUT ) :: DRHOOUT ( NTPL , NSPIN ) !! Charge density at the mesh points in current step. !! The charge density that enters in forhar is  Drho_{out} - Rho_{atm} . REAL ( grid_p ), TARGET , INTENT ( INOUT ) :: VHARRIS1 ( NTPL , NSPIN ) !!  V_{na} + V_{Hartree}(DeltaRho_{in}) + V_{xc}(Rho_{in})  real ( grid_p ), intent ( INOUT ) :: VHARRIS2 ( NTPL ) !!  !! V_{na} + V_{Hartree}(Rho_{in}) + !! DV_{xc}(Rho_{in})/DRho_{in} * (Rho_{out}-Rho_{in}) !!  !! !! If Harris forces are computed in the first SCF step, !! it does not depend on spin. #ifndef BSC_CELLXC EXTERNAL REORD #else /* BSC_CELLXC */ EXTERNAL bsc_cellxc #endif /* BSC_CELLXC */ ! AG: Note:  REAL*4 variables are really REAL(kind=grid_p) ! C ***** INTERNAL VARIABLES ********************************************* C REAL*4 DVXDN(NTPL,NSPIN,NSPIN): Derivative of exchange-correlation C                                potential respect the charge density C ********************************************************************** C ---------------------------------------------------------------------- C Internal variables and arrays C ---------------------------------------------------------------------- #ifndef BSC_CELLXC INTEGER IP , ISPIN , ISPIN2 , myBox ( 2 , 3 ) REAL ( dp ) EX , EC , DEX , DEC , STRESS ( 3 , 3 ) INTEGER , SAVE :: JDGdistr =- 1 real ( grid_p ), pointer :: drhoin (:,:), & dvxcdn (:,:,:) #else /* BSC_CELLXC */ INTEGER :: IP , ISPIN , ISPIN2 , NMPL REAL ( dp ) :: EX , EC , DEX , DEC , STRESSL ( 3 , 3 ) real ( grid_p ) :: aux3 ( 3 , 1 ) !! dummy arrays for cellxc real ( grid_p ), pointer :: drhoin (:,:), drhoin_par (:,:), & dvxcdn (:,:,:), dvxcdn_par (:,:,:), & vharris1_par (:,:), fsrc (:), fdst (:) INTEGER :: ntpl_3 #endif /* BSC_CELLXC */ nullify ( drhoin , dvxcdn ) call re_alloc ( drhoin , 1 , ntpl , 1 , nspin , 'drhoin' , 'forhar' ) call re_alloc ( dvxcdn , 1 , ntpl , 1 , nspin , 1 , nspin , & 'dvxcdn' , 'forhar' ) C ---------------------------------------------------------------------- C Initialize some variables C ---------------------------------------------------------------------- VHARRIS1 (:,:) = 0.0_grid_p VHARRIS2 (:) = 0.0_grid_p DRHOIN (:,:) = 0.0_grid_p DVXCDN (:,:,:) = 0.0_grid_p #ifndef BSC_CELLXC C ---------------------------------------------------------------------- C Set uniform distribution of mesh points and find my processor mesh box C ---------------------------------------------------------------------- call jms_setMeshDistr ( distrID = JDGdistr , nMesh = NTM , . nNodesX = 1 , nNodesY = ProcessorY , nBlock = NSM ) call myMeshBox ( NTM , JDGdistr , myBox ) #else /* BSC_CELLXC */ STRESSL (:,:) = 0.0_dp #endif /* BSC_CELLXC */ C ---------------------------------------------------------------------- C Compute exchange-correlation energy and potential and C their derivatives respect the input charge, that is, Harris charge C or the sum of atomic charges. C ---------------------------------------------------------------------- DO ISPIN = 1 , NSPIN DRHOIN ( 1 : NTPL , ISPIN ) = RHOATM ( 1 : NTPL ) / NSPIN IF ( NPCC . EQ . 1 ) . DRHOIN ( 1 : NTPL , ISPIN ) = DRHOIN ( 1 : NTPL , ISPIN ) + . RHOPCC ( 1 : NTPL ) / NSPIN ENDDO #ifdef BSC_CELLXC #ifdef MPI ! The input distribution is UNIFORM, but we need to work ! with the LINEAR one call setMeshDistr ( LINEAR , nsm , nsp , nml , nmpl , ntml , ntpl ) #endif ntpl_3 = ntpl nullify ( drhoin_par , vharris1_par , dvxcdn_par ) call re_alloc ( drhoin_par , 1 , ntpl_3 , 1 , nspin , & 'drhoin_par' , 'forhar' ) call re_alloc ( vharris1_par , 1 , ntpl_3 , 1 , nspin , & 'vharris1_par' , 'forhar' ) call re_alloc ( dvxcdn_par , 1 , ntpl_3 , 1 , nspin , 1 , nspin , & 'dvxcdn_par' , 'forhar' ) #endif /* BSC_CELLXC */ DO ISPIN = 1 , NSPIN #ifndef BSC_CELLXC CALL REORD ( DRHOIN ( 1 , ISPIN ), DRHOIN ( 1 , ISPIN ), NML , NSM , + 1 ) #else /* BSC_CELLXC */ fsrc => drhoin (:, ispin ) fdst => drhoin_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , LINEAR , fdst , KEEP ) #endif /* BSC_CELLXC */ ENDDO #ifndef BSC_CELLXC CALL CELLXC ( 0 , CELL , NTM , myBox ( 1 , 1 ), myBox ( 2 , 1 ), . myBox ( 1 , 2 ), myBox ( 2 , 2 ), . myBox ( 1 , 3 ), myBox ( 2 , 3 ), NSPIN , DRHOIN , . EX , EC , DEX , DEC , STRESS , VHARRIS1 , DVXCDN ) #else /* BSC_CELLXC */ CALL bsc_cellxc ( 0 , 1 , CELL , NTML , NTML , NTPL , 0 , AUX3 , NSPIN , & DRHOIN_PAR , EX , EC , DEX , DEC , VHARRIS1_PAR , & DVXCDN_PAR , STRESSL ) #endif /* BSC_CELLXC */ DO ISPIN = 1 , NSPIN #ifndef BSC_CELLXC CALL REORD ( DRHOIN ( 1 , ISPIN ), DRHOIN ( 1 , ISPIN ), NML , NSM , - 1 ) CALL REORD ( VHARRIS1 ( 1 , ISPIN ), VHARRIS1 ( 1 , ISPIN ), NML , NSM , - 1 ) #else /* BSC_CELLXC */ fsrc => VHARRIS1_PAR (:, ISPIN ) fdst => VHARRIS1 (:, ispin ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) #endif /* BSC_CELLXC */ DO ISPIN2 = 1 , NSPIN #ifndef BSC_CELLXC CALL REORD ( DVXCDN ( 1 , ISPIN , ISPIN2 ), DVXCDN ( 1 , ISPIN , ISPIN2 ), . NML , NSM , - 1 ) #else /* BSC_CELLXC */ fsrc => DVXCDN_PAR (:, ISPIN , ISPIN2 ) fdst => DVXCDN (:, ISPIN , ISPIN2 ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) #endif /* BSC_CELLXC */ ENDDO ENDDO #ifdef BSC_CELLXC call de_alloc ( dvxcdn_par , 'dvxcdn_par' , 'forhar' ) call de_alloc ( vharris1_par , 'vharris1_par' , 'forhar' ) call de_alloc ( drhoin_par , 'drhoin_par' , 'forhar' ) #ifdef MPI call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) #endif #endif /* BSC_CELLXC */ DO ISPIN = 1 , NSPIN IF ( NPCC . EQ . 1 ) & DRHOIN ( 1 : NTPL , ISPIN ) = DRHOIN ( 1 : NTPL , ISPIN ) - & RHOPCC ( 1 : NTPL ) / NSPIN DO IP = 1 , NTPL VHARRIS1 ( IP , ISPIN ) = VHARRIS1 ( IP , ISPIN ) + VNA ( IP ) ENDDO ENDDO C ---------------------------------------------------------------------- C Compute the product DV_xc(Rho_in)/DRho_in * (Rho_out - Rho_in). C Since the charge that enters into forhar is DRHOOUT = Rho_out-Rhoatm C no extra transformation on the charge density is needed. C ---------------------------------------------------------------------- DO ISPIN = 1 , NSPIN DO ISPIN2 = 1 , NSPIN DO IP = 1 , NTPL VHARRIS2 ( IP ) = VHARRIS2 ( IP ) + & DVXCDN ( IP , ISPIN2 , ISPIN ) * DRHOOUT ( IP , ISPIN2 ) ENDDO ENDDO ENDDO C ---------------------------------------------------------------------- C Since V_Hartree(DeltaRho_in) = 0.0, we only add to vharris2 the neutral C atom potential C ---------------------------------------------------------------------- DO IP = 1 , NTPL VHARRIS2 ( IP ) = VNA ( IP ) - VHARRIS2 ( IP ) ENDDO call de_alloc ( dvxcdn , 'dvxcdn' , 'forhar' ) call de_alloc ( drhoin , 'drhoin' , 'forhar' ) end subroutine forhar end module m_forhar","tags":"","loc":"sourcefile/forhar.f.html","title":"forhar.F – SIESTA"},{"text":"Files dependent on this one sourcefile~~parallel.f~~AfferentGraph sourcefile~parallel.f parallel.F sourcefile~delk.f90 delk.F90 sourcefile~delk.f90->sourcefile~parallel.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~state_init.f state_init.F sourcefile~state_init.f->sourcefile~parallel.f sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~sys.f sys.F sourcefile~state_init.f->sourcefile~sys.f sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~parallel.f sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~siesta_forces.f90->sourcefile~m_rhog.f90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~siesta_forces.f90->sourcefile~sys.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~state_analysis.f->sourcefile~parallel.f sourcefile~forhar.f forhar.F sourcefile~forhar.f->sourcefile~parallel.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~m_iorho.f m_iorho.F sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~vmat.f90 vmat.F90 sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~m_mixing.f90->sourcefile~parallel.f sourcefile~compute_dm.f->sourcefile~parallel.f sourcefile~compute_dm.f->sourcefile~sys.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~compute_energies.f90->sourcefile~parallel.f sourcefile~compute_energies.f90->sourcefile~m_rhog.f90 sourcefile~dhscf.f dhscf.F sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~dfscf.f dfscf.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~parallel.f sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~poison.f poison.F sourcefile~poison.f->sourcefile~parallel.f sourcefile~poison.f->sourcefile~sys.f sourcefile~setup_hamiltonian.f->sourcefile~parallel.f sourcefile~setup_hamiltonian.f->sourcefile~m_rhog.f90 sourcefile~setup_hamiltonian.f->sourcefile~sys.f sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~m_mixing_scf.f90->sourcefile~parallel.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dhscf.f->sourcefile~sys.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~atmfuncs.f var pansourcefileparallelfAfferentGraph = svgPanZoom('#sourcefileparallelfAfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules parallel Source Code parallel.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module parallel !! Parallelisation related global parameters implicit none integer , save :: SIESTA_group , SIESTA_comm logical , save :: SIESTA_worker = . false . integer , save :: Node = 0 integer , save :: Nodes = 1 integer , save :: PEXSINodes = 1 integer , save :: BlockSize = 24 !! This is the blocking factor used to divide up !! the arrays over the processes for the Scalapack !! routines. Setting this value is a compromise !! between dividing up the orbitals over the processors !! evenly to achieve load balancing and making the !! local work efficient. Typically a value of about !! 10 is good, but optimisation may be worthwhile. !! A value of 1 is very bad for any number of processors !! and a large value may also be less than ideal. integer , save :: ProcessorY = 1 !! Second dimension of processor grid in mesh point !! parallelisation - note that the first dimension !! is determined by the total number of processors !! in the current job. Also note that this number !! must be a factor of the total number of processors. !! Furthermore on many parallel machines (e.g. T3E) !! this number must also be a power of 2. logical , save :: IOnode = . true . public interface operator (. PARCOUNT .) module procedure parcount end interface private :: parcount contains subroutine parallel_init () !! Initializes Node, Nodes, and IOnode #ifdef MPI use mpi_siesta , only : MPI_Comm_World logical , save :: initialized = . false . integer :: MPIerror if (. not . initialized ) then call MPI_Comm_Rank ( MPI_Comm_World , Node , MPIerror ) call MPI_Comm_Size ( MPI_Comm_World , Nodes , MPIerror ) IOnode = ( Node == 0 ) initialized = . true . end if #endif end subroutine parallel_init elemental function parcount ( Nodes , N ) !! Convert a (positive) counter into a counter divisable by !! the number of Nodes. !! !! It works by this: !! `PN = Nodes .PARCOUNT. N` !! !! We make it elemental for obvious reasons integer , intent ( in ) :: Nodes , N integer :: parcount if ( mod ( N , Nodes ) == 0 ) then parcount = N else parcount = N + Nodes - mod ( N , Nodes ) end if end function parcount end module parallel","tags":"","loc":"sourcefile/parallel.f.html","title":"parallel.F – SIESTA"},{"text":"This file depends on sourcefile~~siesta_forces.f90~~EfferentGraph sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~precision.f precision.F sourcefile~siesta_forces.f90->sourcefile~precision.f sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 sourcefile~sys.f sys.F sourcefile~siesta_forces.f90->sourcefile~sys.f sourcefile~units.f90 units.f90 sourcefile~siesta_forces.f90->sourcefile~units.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~siesta_forces.f90->sourcefile~siesta_options.f90 sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~parallel.f parallel.F sourcefile~siesta_forces.f90->sourcefile~parallel.f sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~siesta_forces.f90->sourcefile~m_rhog.f90 sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~m_mixing.f90->sourcefile~precision.f sourcefile~m_mixing.f90->sourcefile~parallel.f sourcefile~compute_max_diff.f90->sourcefile~precision.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~units.f90->sourcefile~precision.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90->sourcefile~precision.f sourcefile~m_mixing_scf.f90->sourcefile~parallel.f sourcefile~setup_hamiltonian.f->sourcefile~sys.f sourcefile~setup_hamiltonian.f->sourcefile~siesta_options.f90 sourcefile~setup_hamiltonian.f->sourcefile~parallel.f sourcefile~setup_hamiltonian.f->sourcefile~m_rhog.f90 sourcefile~atmfuncs.f atmfuncs.f sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~setup_h0.f->sourcefile~siesta_options.f90 sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~compute_dm.f->sourcefile~precision.f sourcefile~compute_dm.f->sourcefile~sys.f sourcefile~compute_dm.f->sourcefile~units.f90 sourcefile~compute_dm.f->sourcefile~siesta_options.f90 sourcefile~compute_dm.f->sourcefile~parallel.f sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~sys.f sourcefile~state_init.f->sourcefile~units.f90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f->sourcefile~siesta_options.f90 sourcefile~state_init.f->sourcefile~parallel.f sourcefile~compute_energies.f90->sourcefile~precision.f sourcefile~compute_energies.f90->sourcefile~siesta_options.f90 sourcefile~compute_energies.f90->sourcefile~parallel.f sourcefile~compute_energies.f90->sourcefile~m_rhog.f90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~state_analysis.f->sourcefile~units.f90 sourcefile~state_analysis.f->sourcefile~siesta_options.f90 sourcefile~state_analysis.f->sourcefile~parallel.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~atm_types.f atm_types.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~dhscf.f->sourcefile~precision.f sourcefile~dhscf.f->sourcefile~sys.f sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~mesh.f mesh.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~radial.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~parallel.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~forhar.f->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~parallel.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f var pansourcefilesiesta_forcesf90EfferentGraph = svgPanZoom('#sourcefilesiesta_forcesf90EfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_siesta_forces Source Code siesta_forces.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_siesta_forces implicit none private public :: siesta_forces contains subroutine siesta_forces ( istep ) !! This subroutine represents central SIESTA operation logic. #ifdef MPI use mpi_siesta #endif use units , only : eV , Ang use precision , only : dp use sys , only : bye use files , only : slabel use siesta_cml #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call use flook_siesta , only : LUA_INIT_MD , LUA_SCF_LOOP use siesta_dicts , only : dict_variable_add use m_ts_options , only : ts_scf_mixs use variable , only : cunpack #ifndef NCDF_4 use dictionary , only : assign #endif use m_mixing , only : mixers_history_init #endif use m_state_init use m_setup_hamiltonian use m_setup_H0 use m_compute_dm use m_compute_max_diff use m_scfconvergence_test use m_post_scf_work use m_mixer , only : mixer use m_mixing_scf , only : mixing_scf_converged use m_mixing_scf , only : mixers_scf_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_rhog , only : mix_rhog , compute_charge_diff use siesta_options use parallel , only : IOnode , SIESTA_worker use m_state_analysis use m_steps use m_spin , only : spin use sparse_matrices , only : DM_2D , S_1D use sparse_matrices , only : H , Hold , Dold , Dscf , Eold , Escf , maxnh use m_convergence , only : converger_t use m_convergence , only : reset , set_tolerance use siesta_geom , only : na_u ! Number of atoms in unit cell use m_energies , only : Etot ! Total energy use m_forces , only : fa , cfa ! Forces and constrained forces use m_stress , only : cstress ! Constrained stress tensor use siesta_master , only : forcesToMaster ! Send forces to master prog use siesta_master , only : siesta_server ! Is siesta a server? use m_save_density_matrix , only : save_density_matrix use m_iodm_old , only : write_spmatrix use atomlist , only : no_u , lasto , Qtot use m_dm_charge , only : dm_charge use m_pexsi_solver , only : prevDmax use write_subs , only : siesta_write_forces use write_subs , only : siesta_write_stress_pressure #ifdef NCDF_4 use dictionary use m_ncdf_siesta , only : cdf_init_file , cdf_save_settings use m_ncdf_siesta , only : cdf_save_state , cdf_save_basis #endif use m_compute_energies , only : compute_energies use m_mpi_utils , only : broadcast , barrier use fdf #ifdef SIESTA__PEXSI use m_pexsi , only : pexsi_finalize_scfloop #endif use m_check_walltime use m_energies , only : DE_NEGF use m_ts_options , only : N_Elec use m_ts_method use m_ts_global_vars , only : TSmode , TSinit , TSrun use siesta_geom , only : nsc , xa , ucell , isc_off use sparse_matrices , only : sparse_pattern , block_dist use sparse_matrices , only : S use m_ts_charge , only : ts_get_charges use m_ts_charge , only : TS_RHOCORR_METHOD use m_ts_charge , only : TS_RHOCORR_FERMI use m_ts_charge , only : TS_RHOCORR_FERMI_TOLERANCE use m_transiesta , only : transiesta use kpoint_scf_m , only : gamma_scf use m_energies , only : Ef use m_initwf , only : initwf integer , intent ( inout ) :: istep integer :: iscf logical :: first_scf , SCFconverged real ( dp ) :: dDmax ! Max. change in DM elements real ( dp ) :: dHmax ! Max. change in H elements real ( dp ) :: dEmax ! Max. change in EDM elements real ( dp ) :: drhog ! Max. change in rho(G) (experimental) real ( dp ), target :: G2max ! actually used meshcutoff type ( converger_t ) :: conv_harris , conv_freeE ! For initwf integer :: istpp #ifdef SIESTA__FLOOK ! len=24 from m_mixing.F90 character ( len = 1 ), target :: next_mixer ( 24 ) character ( len = 24 ) :: nnext_mixer integer :: imix #endif logical :: time_is_up character ( len = 40 ) :: tmp_str real ( dp ) :: Qcur #ifdef NCDF_4 type ( dict ) :: d_sav #endif #ifdef MPI integer :: MPIerror #endif external :: die , message #ifdef DEBUG call write_debug ( '    PRE siesta_forces' ) #endif #ifdef SIESTA__PEXSI ! Broadcast relevant things for program logic ! These were set in read_options, called only by \"SIESTA_workers\". call broadcast ( nscf , comm = true_MPI_Comm_World ) #endif !  Initialization tasks for a given geometry: if ( SIESTA_worker ) then call state_init ( istep ) end if #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_init\" ) #endif if ( fdf_get ( \"Sonly\" ,. false .) ) then if ( SIESTA_worker ) then call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) end if call bye ( \"S only\" ) end if Qcur = Qtot #ifdef SIESTA__FLOOK ! Add the iscf constant to the list of variables ! that are available only in this part of the routine. call dict_variable_add ( 'SCF.iteration' , iscf ) call dict_variable_add ( 'SCF.converged' , SCFconverged ) call dict_variable_add ( 'SCF.charge' , Qcur ) call dict_variable_add ( 'SCF.dD' , dDmax ) call dict_variable_add ( 'SCF.dH' , dHmax ) call dict_variable_add ( 'SCF.dE' , dEmax ) call dict_variable_add ( 'SCF.drhoG' , drhog ) ! We have to set the meshcutoff here ! because the asked and required ones are not ! necessarily the same call dict_variable_add ( 'Mesh.Cutoff.Minimum' , G2cut ) call dict_variable_add ( 'Mesh.Cutoff.Used' , G2max ) if ( mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , wmix ) else call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) ! Just to populate the table in the dictionary call dict_variable_add ( 'SCF.Mixer.Switch' , next_mixer ) end if ! Initialize to no switch next_mixer = ' ' #endif !  This call computes the **non-scf** part of  H  and initializes the !  real-space grid structures: if ( SIESTA_worker ) call setup_H0 ( G2max ) !!@todo !* It might be better to split the two, !  putting the grid initialization into **state_init (link!)** and moving the !  calculation of  H_0  to the body of the loop, done `if first_scf=.true.` !  This would suit _analysis_ runs in which **nscf = 0** !!@endtodo #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after setup_H0\" ) #endif #ifdef SIESTA__FLOOK ! Communicate with lua, just before entering the SCF loop ! This is mainly to be able to communicate ! mesh-related quantities (g2max) call slua_call ( LUA , LUA_INIT_MD ) #endif #ifdef NCDF_4 ! Initialize the NC file if ( write_cdf ) then ! Initialize the file... call cdf_init_file ( trim ( slabel ) // '.nc' , is_MD = . false .) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif ! Save the settings call cdf_save_settings ( trim ( slabel ) // '.nc' ) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif d_sav = ( 'sp' . kv . 1 ) // ( 'S' . kv . 1 ) d_sav = d_sav // ( 'nsc' . kv . 1 ) // ( 'xij' . kv . 1 ) d_sav = d_sav // ( 'xa' . kv . 1 ) // ( 'cell' . kv . 1 ) d_sav = d_sav // ( 'isc_off' . kv . 1 ) call cdf_save_state ( trim ( slabel ) // '.nc' , d_sav ) call delete ( d_sav ) ! Save the basis set call cdf_save_basis ( trim ( slabel ) // '.nc' ) end if #endif !* The dHmax variable only has meaning for Hamiltonian !  mixing, or when requiring the Hamiltonian to be converged. dDmax = - 1._dp dHmax = - 1._dp dEmax = - 1._dp drhog = - 1._dp ! Setup convergence criteria: if ( SIESTA_worker ) then if ( converge_Eharr ) then call reset ( conv_harris ) call set_tolerance ( conv_harris , tolerance_Eharr ) end if if ( converge_FreeE ) then call reset ( conv_FreeE ) call set_tolerance ( conv_FreeE , tolerance_FreeE ) end if end if !!# SCF loop !* The current structure of the loop tries to reproduce the !  historical Siesta usage. It should be made more clear. !* Two changes: ! !  1. The number of scf iterations performed is exactly !     equal to the number specified (i.e., the \"forces\" !     phase is not counted as a final scf step) !  2. At the change to a TranSiesta GF run the variable \"first_scf\" !     is implicitly reset to \"true\". ! !!## Start of SCF cycle ! !* Conditions of exit: ! !  * At the top, to catch a non-positive nscf and # of iterations !  * At the bottom, based on convergence ! iscf = 0 do while ( iscf < nscf ) iscf = iscf + 1 !* Note implications for TranSiesta when mixing H. !  Now H will be recomputed instead of simply being !  inherited, however, this is required as if !  we have bias calculations as the electric !  field across the junction needs to be present. first_scf = ( iscf == 1 ) if ( SIESTA_worker ) then ! Check whether we are short of time to continue call check_walltime ( time_is_up ) if ( time_is_up ) then ! Save DM/H if we were not saving it... !   Do any other bookeeping not done by \"die\" call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge' // & ' before wall time exhaustion' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call barrier () ! A non-root node might get first to the 'die' call call die ( \"OUT_OF_TIME: Time is up.\" ) end if call timer ( 'IterSCF' , 1 ) if ( cml_p ) & call cmlStartStep ( xf = mainXML , type = 'SCF' , index = iscf ) if ( mixH ) then if ( first_scf ) then if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then call get_H_from_file () else call setup_hamiltonian ( iscf ) end if end if call compute_DM ( iscf ) ! Maybe set Dold to zero if reading charge or H... call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) else call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) call compute_DM ( iscf ) call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) end if ! This iteration has completed calculating the new DM call compute_energies ( iscf ) if ( mix_charge ) then call compute_charge_diff ( drhog ) end if ! Note: For DM and H convergence checks. At this point: ! If mixing the DM: !        Dscf=DM_out, Dold=DM_in(mixed), H=H_in, Hold=H_in(prev step) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H_in - H_in(prev step)) ! If mixing the Hamiltonian: !        Dscf=DM_out, Dold=DM_in, H=H_(DM_out), Hold=H_in(mixed) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H(DM_out),H_in) call scfconvergence_test ( first_scf , iscf , & dDmax , dHmax , dEmax , & conv_harris , conv_freeE , & SCFconverged ) ! ** Check this heuristic if ( mixH ) then prevDmax = dHmax else prevDmax = dDmax end if ! Calculate current charge based on the density matrix call dm_charge ( spin , DM_2D , S_1D , Qcur ) ! Check whether we should step to the next mixer call mixing_scf_converged ( SCFconverged ) if ( SCFconverged . and . iscf < min_nscf ) then SCFconverged = . false . if ( IONode ) then write ( * , \"(a,i0)\" ) & \"SCF cycle continued for minimum number of iterations: \" , & min_nscf end if end if ! In case the user has requested a Fermi-level correction ! Then we start by correcting the fermi-level if ( TSrun . and . SCFconverged . and . & TS_RHOCORR_METHOD == TS_RHOCORR_FERMI ) then if ( abs ( Qcur - Qtot ) > TS_RHOCORR_FERMI_TOLERANCE ) then ! Call transiesta with fermi-correct call transiesta ( iscf , spin % H , & block_dist , sparse_pattern , Gamma_Scf , ucell , nsc , & isc_off , no_u , na_u , lasto , xa , maxnh , H , S , & Dscf , Escf , Ef , Qtot , . true ., DE_NEGF ) ! We will not have not converged as we have just ! changed the Fermi-level SCFconverged = . false . end if end if if ( monitor_forces_in_scf ) call compute_forces () ! Mix_after_convergence preserves the old behavior of ! the program. if ( (. not . SCFconverged ) . or . mix_after_convergence ) then ! Mix for next step if ( mix_charge ) then call mix_rhog ( iscf ) else call mixer ( iscf ) end if ! Save for possible restarts if ( mixH ) then call write_spmatrix ( H , file = \"H_MIXED\" , when = writeH ) call save_density_matrix ( file = \"DM_OUT\" , when = writeDM ) else call save_density_matrix ( file = \"DM_MIXED\" , when = writeDM ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = writeH ) end if end if call timer ( 'IterSCF' , 2 ) call print_timings ( first_scf , istep == inicoor ) if ( cml_p ) call cmlEndStep ( mainXML ) #ifdef SIESTA__FLOOK ! Communicate with lua call slua_call ( LUA , LUA_SCF_LOOP ) ! Retrieve an easy character string nnext_mixer = cunpack ( next_mixer ) if ( len_trim ( nnext_mixer ) > 0 . and . . not . mix_charge ) then if ( TSrun ) then do imix = 1 , size ( ts_scf_mixs ) if ( ts_scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( ts_scf_mixs ) scf_mix => ts_scf_mixs ( imix ) exit end if end do else do imix = 1 , size ( scf_mixs ) if ( scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( imix ) exit end if end do end if ! Check that we indeed have changed the mixer if ( IONode . and . scf_mix % name /= nnext_mixer ) then write ( * , '(2a)' ) 'siesta-lua: WARNING: trying to change ' , & 'to a non-existing mixer! Not changing anything!' else if ( IONode ) then write ( * , '(2a)' ) 'siesta-lua: Switching mixer method to: ' , & trim ( nnext_mixer ) end if ! Reset for next loop next_mixer = ' ' ! Update the references call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif ! ... except that we might continue for TranSiesta if ( SCFconverged ) then call transiesta_switch () ! might reset SCFconverged and iscf end if else ! non-siesta worker call compute_DM ( iscf ) end if #ifdef SIESTA__PEXSI call broadcast ( iscf , comm = true_MPI_Comm_World ) call broadcast ( SCFconverged , comm = true_MPI_Comm_World ) #endif !  Exit if converged: if ( SCFconverged ) exit end do !! **end of SCF cycle** #ifdef SIESTA__PEXSI if ( isolve == SOLVE_PEXSI ) then call pexsi_finalize_scfloop () end if #endif if ( . not . SIESTA_worker ) return call end_of_cycle_save_operations () if ( . not . SCFconverged ) then if ( SCFMustConverge ) then call message ( 'FATAL' , 'SCF_NOT_CONV: SCF did not converge' // & ' in maximum number of steps (required).' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call barrier () call die ( 'ABNORMAL_TERMINATION' ) else if ( . not . harrisfun ) then call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge  in maximum number of steps.' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) end if end if ! To write the initial wavefunctions to be used in a ! consequent TDDFT run. if ( writetdwf ) then istpp = 0 call initwf ( istpp , totime ) end if if ( TSmode . and . TSinit . and .(. not . SCFConverged ) ) then ! Signal that the DM hasn't converged, so we cannot ! continue to the transiesta routines call die ( 'ABNORMAL_TERMINATION' ) end if ! Clean-up here to limit memory usage call mixers_scf_history_init ( ) ! End of standard SCF loop. ! Do one more pass to compute forces and stresses ! Note that this call will no longer overwrite H while computing the ! final energies, forces and stresses... if ( fdf_get ( \"compute-forces\" ,. true .) ) then call post_scf_work ( istep , iscf , SCFconverged ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after post_scf_work\" ) #endif end if ! ... so H at this point is the latest generator of the DM, except ! if mixing H beyond self-consistency or terminating the scf loop ! without convergence while mixing H call state_analysis ( istep ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_analysis\" ) #endif ! If siesta is running as a subroutine, send forces to master program if ( siesta_server ) & call forcesToMaster ( na_u , Etot , cfa , cstress ) #ifdef DEBUG call write_debug ( '    POS siesta_forces' ) #endif contains ! Read the Hamiltonian from a file subroutine get_H_from_file () use sparse_matrices , only : maxnh , numh , listh , listhptr use atomlist , only : no_l use m_spin , only : spin use m_iodm_old , only : read_spmatrix logical :: found call read_spmatrix ( maxnh , no_l , spin % H , numh , & listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) end subroutine get_H_from_file ! Computes forces and stresses with the current DM_out subroutine compute_forces () use siesta_options , only : recompute_H_after_scf use m_final_H_f_stress , only : final_H_f_stress use write_subs real ( dp ), allocatable :: fa_old (:,:), Hsave (:,:) allocate ( fa_old ( size ( fa , dim = 1 ), size ( fa , dim = 2 ))) fa_old (:,:) = fa (:,:) if ( recompute_H_after_scf ) then allocate ( Hsave ( size ( H , dim = 1 ), size ( H , dim = 2 ))) Hsave (:,:) = H (:,:) end if call final_H_f_stress ( istep , iscf , . false . ) if ( recompute_H_after_scf ) then H (:,:) = Hsave (:,:) deallocate ( Hsave ) end if if ( ionode ) then print * , \"Max diff in force (eV/Ang): \" , & maxval ( abs ( fa - fa_old )) * Ang / eV call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () endif deallocate ( fa_old ) end subroutine compute_forces ! Print out timings of the first SCF loop only subroutine print_timings ( first_scf , first_md ) use timer_options , only : use_tree_timer use m_ts_global_vars , only : TSrun logical , intent ( in ) :: first_scf , first_md character ( len = 20 ) :: routine ! If this is not the first iteration, ! we immediately return. if ( . not . first_scf ) return if ( . not . first_md ) return routine = 'IterSCF' if ( TSrun ) then ! with Green function generation ! The tree-timer requires direct ! children of the routine to be ! queried. ! This is not obeyed in the TS case... :( if ( . not . use_tree_timer ) then routine = 'TS' end if endif call timer ( routine , 3 ) end subroutine print_timings ! Depending on various conditions, save the DMin ! or the DMout, and possibly keep a copy of H ! NOTE: Only if the scf cycle converged before exit it ! is guaranteed that the DM is \"pure out\" and that ! we can recover the right H if mixing H. ! subroutine end_of_cycle_save_operations () logical :: DM_write , H_write ! Depending on the option we should overwrite the ! Hamiltonian if ( mixH . and . . not . mix_after_convergence ) then ! Make sure that we keep the H actually used ! to generate the last DM, if needed. H = Hold end if DM_write = write_DM_at_end_of_cycle . and . & . not . writeDM H_write = write_H_at_end_of_cycle . and . & . not . writeH if ( mix_after_convergence ) then ! If we have been saving them, there is no point in doing ! it one more time if ( mixH ) then call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_MIXED\" , when = H_write ) else call save_density_matrix ( file = \"DM_MIXED\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if else call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if end subroutine end_of_cycle_save_operations subroutine transiesta_switch () use precision , only : dp use parallel , only : IONode use class_dSpData2D use class_Fstack_dData1D use densematrix , only : resetDenseMatrix use siesta_options , only : fire_mix , broyden_maxit use siesta_options , only : dDtol , dHtol use sparse_matrices , only : DM_2D , EDM_2D use atomlist , only : lasto use siesta_geom , only : nsc , isc_off , na_u , xa , ucell use m_energies , only : Ef use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mix , scf_mixs use m_rhog , only : resetRhoG use m_ts_global_vars , only : TSinit , TSrun use m_ts_global_vars , only : ts_print_transiesta use m_ts_method use m_ts_options , only : N_Elec , Elecs use m_ts_options , only : DM_bulk use m_ts_options , only : val_swap use m_ts_options , only : ts_Dtol , ts_Htol use m_ts_options , only : ts_hist_keep use m_ts_options , only : ts_siesta_stop use m_ts_options , only : ts_scf_mixs use m_ts_electype integer :: iEl , na_a integer , allocatable :: allowed_a (:) real ( dp ), pointer :: DM (:,:), EDM (:,:) ! We are done with the initial diagon run ! Now we start the TRANSIESTA (Green functions) run if ( . not . TSmode ) return if ( . not . TSinit ) return ! whether we are in siesta initialization step TSinit = . false . ! whether transiesta is running TSrun = . true . ! If transiesta should stop immediately if ( ts_siesta_stop ) then if ( IONode ) then write ( * , '(a)' ) 'ts: Stopping transiesta (user option)!' end if return end if ! Reduce memory requirements call resetDenseMatrix () ! Signal to continue... ! These two variables are from the top-level ! routine (siesta_forces) SCFconverged = . false . iscf = 0 ! DANGER (when/if going back to the DIAGON run, we should ! re-instantiate the original mixing value) call val_swap ( dDtol , ts_Dtol ) call val_swap ( dHtol , ts_Htol ) ! Clean up mixing history if ( mix_charge ) then call resetRhoG (. true .) else if ( associated ( ts_scf_mixs , target = scf_mixs ) ) then do iel = 1 , size ( scf_mix % stack ) call reset ( scf_mix % stack ( iel ), - ts_hist_keep ) ! Reset iteration count as certain ! mixing schemes require this for consistency scf_mix % cur_itt = n_items ( scf_mix % stack ( iel )) end do else call mixers_history_init ( scf_mixs ) end if end if ! Transfer scf_mixing to the transiesta mixing routine scf_mix => ts_scf_mixs ( 1 ) #ifdef SIESTA__FLOOK if ( . not . mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif call ts_print_transiesta () ! In case of transiesta and DM_bulk. ! In case we ask for initialization of the DM in bulk ! we read in the DM files from the electrodes and ! initialize the bulk to those values if ( DM_bulk > 0 ) then if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Initializing bulk DM in electrodes.' end if na_a = 0 do iEl = 1 , na_u if ( . not . a_isDev ( iEl ) ) na_a = na_a + 1 end do allocate ( allowed_a ( na_a )) na_a = 0 do iEl = 1 , na_u ! We allow the buffer atoms as well (this will even out the ! potential around the back of the electrode) if ( . not . a_isDev ( iEl ) ) then na_a = na_a + 1 allowed_a ( na_a ) = iEl end if end do do iEl = 1 , N_Elec if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Reading in electrode TSDE for ' // & trim ( Elecs ( iEl )% Name ) end if ! Copy over the DM in the lead ! Notice that the EDM matrix that is copied over ! will be equivalent at Ef == 0 call copy_DM ( Elecs ( iEl ), na_u , xa , lasto , nsc , isc_off , & ucell , DM_2D , EDM_2D , na_a , allowed_a ) end do ! Clean-up deallocate ( allowed_a ) if ( IONode ) then write ( * , * ) ! new-line end if ! The electrode EDM is aligned at Ef == 0 ! We need to align the energy matrix DM => val ( DM_2D ) EDM => val ( EDM_2D ) iEl = size ( DM ) call daxpy ( iEl , Ef , DM ( 1 , 1 ), 1 , EDM ( 1 , 1 ), 1 ) end if end subroutine transiesta_switch end subroutine siesta_forces end module m_siesta_forces","tags":"","loc":"sourcefile/siesta_forces.f90.html","title":"siesta_forces.F90 – SIESTA"},{"text":"This file depends on sourcefile~~m_mixing.f90~~EfferentGraph sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~parallel.f parallel.F sourcefile~m_mixing.f90->sourcefile~parallel.f sourcefile~precision.f precision.F sourcefile~m_mixing.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_mixing.f90~~AfferentGraph sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_mixing Source Code m_mixing.F90 Source Code ! Module for all mixing methods in a standard way ! This module implements mixing of the Pulay and Broyden ! type. ! The Pulay method is implemented in the fast calculation ! setup and in the stable method. ! The stable method is executed if the inversion fails. !  - Stable: G.Kresse and J.Furthmuller, Comp. Mat. Sci. 6, 15, 1996 !  - gr (guarenteed-reduction) : http://arxiv.org/pdf/cond-mat/0005521.pdf ! All implemented methods employ a restart with variable ! history saving. module m_mixing use precision , only : dp #ifdef MPI ! MPI stuff use mpi_siesta #endif ! Intrinsic classes for retaining history use class_dData1D use class_Fstack_dData1D implicit none private save integer , parameter :: MIX_LINEAR = 1 integer , parameter :: MIX_PULAY = 2 integer , parameter :: MIX_BROYDEN = 3 integer , parameter :: MIX_FIRE = 4 ! Action tokens (binary: 0, 1, 2, 4, 8, ...!) integer , parameter :: ACTION_MIX = 0 integer , parameter :: ACTION_RESTART = 1 integer , parameter :: ACTION_NEXT = 2 type tMixer ! Name of mixer character ( len = 24 ) :: name ! The different saved variables per iteration ! and their respective stacks type ( Fstack_dData1D ), allocatable :: stack (:) ! The method of the mixer integer :: m = MIX_PULAY ! In case the mixing method has a variant ! this denote the variant ! This value is thus specific for each method integer :: v = 0 ! The currently reached iteration integer :: cur_itt = 0 , start_itt = 0 ! Different mixers may have different histories integer :: n_hist = 2 ! Number of iterations using this mixer ! There are a couple of signals here !  == 0 : !     only use this mixer until convergence !   > 0 : !     after having runned n_itt step to \"next\" integer :: n_itt = 0 ! When mod(cur_itt,restart_itt) == 0 the history will ! be _reset_ integer :: restart = 0 integer :: restart_save = 0 ! This is an action token specifying the current ! action integer :: action = ACTION_MIX ! The next mixing method following this method type ( tMixer ), pointer :: next => null () ! The next mixing method following this method ! Only used if mixing method achieved convergence ! using this method type ( tMixer ), pointer :: next_conv => null () ! ** Parameters specific for the method: ! The mixing parameter used for this mixer real ( dp ) :: w = 0._dp ! linear array of real variables used specifically ! for this mixing type real ( dp ), pointer :: rv (:) => null () integer , pointer :: iv (:) => null () #ifdef MPI ! In case we have MPI the mixing scheme ! can implement a reduction scheme. ! This can be MPI_Comm_Self to not employ any ! reductions integer :: Comm = MPI_Comm_Self #endif end type tMixer ! Indices for special constanst integer , parameter :: I_PREVIOUS_RES = 0 integer , parameter :: I_P_RESTART = - 1 integer , parameter :: I_P_NEXT = - 2 ! This index should always be the lowest index ! This is used to allocate the correct bounds for the ! additional array of information integer , parameter :: I_SVD_COND = - 3 ! Debug mixing runs logical :: debug_mix = . false . ! In case of parallel mixing this also contains the node number character ( len = 20 ) :: debug_msg = 'mix:' public :: tMixer ! Routines are divided in three sections ! 1. Routines used to construct the mixers !    Routines used to print information regarding !    the mixers public :: mixers_init , mixer_init public :: mixers_print , mixers_print_block public :: mixers_history_init public :: mixers_reset ! 2. Public functions for retrieving information !    from external routines public :: mix_method , mix_method_variant ! 3. Actual mixing methods public :: mixing public :: MIX_LINEAR , MIX_FIRE , MIX_PULAY , MIX_BROYDEN interface mixing module procedure mixing_1d , mixing_2d end interface mixing contains !> Initialize a set of mixers by reading in fdf information. !! @param[in] prefix the fdf-label prefixes !! @param[pointer] mixers the mixers that are to be initialized !! @param[in] Comm @opt optional MPI-communicator subroutine mixers_init ( prefix , mixers , Comm ) use parallel , only : IONode , Node use fdf ! FDF-prefix for searching keywords character ( len =* ), intent ( in ) :: prefix ! The array of mixers (has to be nullified upon entry) type ( tMixer ), pointer :: mixers (:) integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf type ( parsed_line ), pointer :: pline ! number of history steps saved integer :: n_hist , n_restart , n_save real ( dp ) :: w integer :: nm , im , im2 character ( len = 10 ) :: lp character ( len = 70 ) :: method , variant ! Default mixing options... if ( fdf_get ( 'Mixer.Debug' ,. false .) ) then debug_mix = IONode debug_msg = 'mix:' end if if ( fdf_get ( 'Mixer.Debug.MPI' ,. false .) ) then debug_mix = . true . write ( debug_msg , '(a,i0,a)' ) 'mix (' , Node , '):' end if lp = trim ( prefix ) // '.Mixer' ! ensure nullification call mixers_reset ( mixers ) ! Return immediately if the user hasn't defined ! an fdf-block for the mixing options... if ( . not . fdf_block ( trim ( lp ) // 's' , bfdf ) ) return ! update mixing weight and kick mixing weight w = fdf_get ( trim ( lp ) // '.Weight' , 0.1_dp ) ! Get history length n_hist = fdf_get ( trim ( lp ) // '.History' , 6 ) ! Restart after this number of iterations n_restart = fdf_get ( trim ( lp ) // '.Restart' , 0 ) n_save = fdf_get ( trim ( lp ) // '.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Read in the options regarding the mixing options nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 end do if ( nm == 0 ) then call die ( 'mixing: No mixing schemes selected. & &Please at least add one mixer.' ) end if ! Allocate all denoted mixers... allocate ( mixers ( nm )) mixers (:)% w = w mixers (:)% n_hist = n_hist mixers (:)% restart = n_restart mixers (:)% restart_save = n_save ! Rewind to grab names. call fdf_brewind ( bfdf ) nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 mixers ( nm )% name = fdf_bnames ( pline , 1 ) end do ! Now read all mixers for this segment and their options do im = 1 , nm call read_block ( mixers ( im ) ) end do ! Create history stack and associate correct ! stack pointers call mixers_history_init ( mixers ) #ifdef MPI if ( present ( Comm ) ) then mixers (:)% Comm = Comm else mixers (:)% Comm = MPI_Comm_World end if #endif contains subroutine read_block ( m ) type ( tMixer ), intent ( inout ), target :: m character ( len = 64 ) :: opt ! create block string opt = trim ( lp ) // '.' // trim ( m % name ) if ( . not . fdf_block ( opt , bfdf ) ) then call die ( 'Block: ' // trim ( opt ) // ' does not exist!' ) end if ! Default to the pulay method... ! This enables NOT writing this in the block method = 'pulay' variant = ' ' ! read method do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'method' ) ) then method = fdf_bnames ( pline , 2 ) else if ( leqi ( opt , 'variant' ) ) then variant = fdf_bnames ( pline , 2 ) end if end do ! Retrieve the method and the variant m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! Define separate defaults which are ! not part of the default input options select case ( m % m ) case ( MIX_LINEAR ) m % n_hist = 0 end select call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'iterations' ) & . or . leqi ( opt , 'itt' ) ) then m % n_itt = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'history' ) ) then m % n_hist = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'weight' ) . or . leqi ( opt , 'w' ) ) then m % w = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'restart' ) ) then m % restart = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'restart.save' ) ) then m % restart_save = fdf_bintegers ( pline , 1 ) m % restart_save = max ( 0 , m % restart_save ) end if end do ! Initialize the mixer by setting the correct ! standard options and allocate space in the mixers... call mixer_init ( m ) ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'next' ) ) then nullify ( m % next ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next ) ) then call die ( 'mixing: Could not find next mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next , target = m ) ) then call die ( 'mixing: Next *must* not be it-self. & &Please change accordingly.' ) end if else if ( leqi ( opt , 'next.conv' ) ) then nullify ( m % next_conv ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next_conv => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next_conv ) ) then call die ( 'mixing: Could not find next convergence mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next_conv , target = m ) ) then call die ( 'mixing: next.conv *must* not be it-self. & &Please change accordingly.' ) end if end if end do ! Ensure that if a next have not been specified ! it will continue indefinitely. if ( . not . associated ( m % next ) ) then m % n_itt = 0 end if ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) ! skip lines without associated content if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) ! Do options so that a pulay option may refer to ! the actual names of the constants if ( m % m == MIX_PULAY ) then ! The linear mixing weight if ( leqi ( opt , 'weight.linear' ) & . or . leqi ( opt , 'w.linear' ) ) then m % rv ( 1 ) = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'svd.cond' ) ) then ! This is only applicable to the Pulay ! mixing scheme... m % rv ( I_SVD_COND ) = fdf_bvalues ( pline , 1 ) end if end if ! Generic options for all advanced methods... if ( leqi ( opt , 'next.p' ) ) then ! Only allow stepping to the next when ! having a next associated if ( associated ( m % next ) ) then m % rv ( I_P_NEXT ) = fdf_bvalues ( pline , 1 ) end if else if ( leqi ( opt , 'restart.p' ) ) then m % rv ( I_P_RESTART ) = fdf_bvalues ( pline , 1 ) end if end do end subroutine read_block end subroutine mixers_init !> Initialize a single mixer depending on the preset !! options. Useful for external correct setup. !! !! @param[inout] mix mixer to be initialized subroutine mixer_init ( mix ) type ( tMixer ), intent ( inout ) :: mix integer :: n ! Correct amount of history in the mixing. if ( 0 < mix % restart . and . & mix % restart < mix % n_hist ) then ! This is if we restart this scheme, ! then it does not make sense to have a history ! greater than the restart count mix % n_hist = mix % restart end if if ( 0 < mix % n_itt . and . & mix % n_itt < mix % n_hist ) then ! If this only runs for n_itt itterations, ! it makes no sense to have a history greater ! than this. mix % n_hist = mix % n_itt end if select case ( mix % m ) case ( MIX_LINEAR ) allocate ( mix % rv ( I_SVD_COND : 0 )) ! Kill any history settings that do not apply to the ! linear mixer. mix % restart = 0 mix % restart_save = 0 case ( MIX_PULAY ) allocate ( mix % rv ( I_SVD_COND : 1 )) mix % rv ( 1 ) = mix % w ! We allocate the double residual (n_hist-1) mix % n_hist = max ( 2 , mix % n_hist ) if ( mix % v == 1 . or . mix % v == 3 ) then ! The GR method requires an even number ! of restart steps ! And then we ensure the history to be aligned ! with a restart (restart has precedence) mix % restart = mix % restart + mod ( mix % restart , 2 ) end if case ( MIX_BROYDEN ) ! allocate temporary array mix % n_hist = max ( 2 , mix % n_hist ) n = 1 + mix % n_hist allocate ( mix % rv ( I_SVD_COND : n )) mix % rv ( 1 : n ) = mix % w end select if ( mix % restart < 0 ) then call die ( 'mixing: restart count must be positive' ) end if mix % restart_save = min ( mix % n_hist - 1 , mix % restart_save ) mix % restart_save = max ( 0 , mix % restart_save ) ! This is the restart parameter ! I.e. if |f_k / f - 1| < rp ! only works for positive rp mix % rv ( I_PREVIOUS_RES ) = huge ( 1._dp ) mix % rv ( I_P_RESTART ) = - 1._dp mix % rv ( I_P_NEXT ) = - 1._dp mix % rv ( I_SVD_COND ) = 1.e-8_dp end subroutine mixer_init !> Initialize all history for the mixers !! !! Routine for clearing all history and setting up the !! arrays so that they may be used subsequently. !! !! @param[inout] mixers the mixers to be initialized subroutine mixers_history_init ( mixers ) type ( tMixer ), intent ( inout ), target :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns logical :: is_GR do im = 1 , size ( mixers ) m => mixers ( im ) if ( debug_mix . and . current_itt ( m ) >= 1 ) then write ( * , '(a,a)' ) trim ( debug_msg ), & ' resetting history of all mixers' exit end if end do ! Clean up all arrays and reference counted ! objects do im = 1 , size ( mixers ) m => mixers ( im ) ! reset history track m % start_itt = 0 m % cur_itt = 0 ! do not try and de-allocate something not ! allocated if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do ! clean-up deallocate ( m % stack ) end if ! Re-populate select case ( m % m ) case ( MIX_LINEAR ) ! do nothing case ( MIX_PULAY ) is_GR = ( m % v == 1 ) . or . ( m % v == 3 ) if ( . not . is_GR ) then allocate ( m % stack ( 3 )) else allocate ( m % stack ( 2 )) end if ! These arrays contains these informations !   s1 = m%stack(1) !   s2 = m%stack(2) !   s3 = m%stack(3) ! Here <> is input function, x[in], and ! <>' is the corresponding output, x[out]. ! First iteration: !   s1 = { 1' - 1 } !   s3 = { 1' } ! Second iteration !   s2 = { 2' - 2 - (1' - 1) } !   s1 = { 2 - 1 , 2' - 2 } !   s3 = { 2' } ! Third iteration !   s2 = { 2' - 2 - (1' - 1) , 3' - 3 - (2' - 2) } !   s1 = { 2 - 1 , 3 - 2, 3' - 3 } !   s3 = { 3' } ! and so on ! allocate x[i+1] - x[i] call new ( m % stack ( 1 ), m % n_hist ) ! allocate F[i+1] - F[i] call new ( m % stack ( 2 ), m % n_hist - 1 ) if ( . not . is_GR ) then call new ( m % stack ( 3 ), 1 ) end if case ( MIX_BROYDEN ) ! Same as original Pulay allocate ( m % stack ( 3 )) call new ( m % stack ( 1 ), m % n_hist ) call new ( m % stack ( 2 ), m % n_hist - 1 ) call new ( m % stack ( 3 ), 1 ) end select end do end subroutine mixers_history_init !> Reset the mixers, i.e. clean _everything_ !! !! Also deallocates (and nullifies) the input array! !! !! @param[inout] mixers array of mixers to be cleaned subroutine mixers_reset ( mixers ) type ( tMixer ), pointer :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns if ( . not . associated ( mixers ) ) return do im = 1 , size ( mixers ) m => mixers ( im ) if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do deallocate ( m % stack ) end if if ( associated ( m % rv ) ) then deallocate ( m % rv ) nullify ( m % rv ) end if if ( associated ( m % iv ) ) then deallocate ( m % iv ) nullify ( m % iv ) end if end do deallocate ( mixers ) nullify ( mixers ) end subroutine mixers_reset !> Print (to std-out) information regarding the mixers !! !! @param[in] prefix the prefix (fdf) for the mixers !! @param[in] mixers array of mixers allocated subroutine mixers_print ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m character ( len = 50 ) :: fmt logical :: bool integer :: i if ( . not . IONode ) return fmt = 'mix.' // trim ( prefix ) // ':' if ( debug_mix ) then write ( * , '(2a,t50,''= '',l)' ) trim ( fmt ), & ' Debug messages' , debug_mix end if ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Linear mixing' , trim ( m % name ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w if ( m % n_hist > 0 . and . (& associated ( m % next ) & . or . associated ( m % next_conv )) ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Carried history steps' , m % n_hist end if case ( MIX_PULAY ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Pulay mixing' , trim ( m % name ) select case ( m % v ) case ( 0 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable' case ( 1 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR' case ( 2 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable-SVD' case ( 3 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR-SVD' end select write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Linear mixing weight' , m % rv ( 1 ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w write ( * , '(2a,t50,''= '',e10.4)' ) trim ( fmt ), & '    SVD condition' , m % rv ( I_SVD_COND ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_BROYDEN ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Broyden mixing' , trim ( m % name ) !write(*,'(2a,t50,''= '',a)') trim(fmt), & !     '    Variant','original' write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Jacobian weight' , m % w write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Weight prime' , m % rv ( 1 ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_FIRE ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Fire mixing' , trim ( m % name ) end select if ( m % n_itt > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Number of mixing iterations' , m % n_itt if ( associated ( m % next ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer' , trim ( m % next % name ) else call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if end if if ( associated ( m % next_conv ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer upon convergence' , trim ( m % next_conv % name ) end if end do end subroutine mixers_print !> Print (to std-out) the fdf-blocks that recreate the mixer settings !! !! @param[in] prefix the fdf-prefix for reading the blocks !! @param[in] mixers array of mixers that should be printed !!    their fdf-blocks subroutine mixers_print_block ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m logical :: bool integer :: i if ( . not . IONode ) return ! Write block of input write ( * , '(/3a)' ) '%block ' , trim ( prefix ), '.Mixers' do i = 1 , size ( mixers ) m => mixers ( i ) write ( * , '(t3,a)' ) trim ( m % name ) end do write ( * , '(3a)' ) '%endblock ' , trim ( prefix ), '.Mixers' ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) ! Write out this block write ( * , '(/4a)' ) '%block ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) write ( * , '(t3,a)' ) '# Mixing method' ! Write out method select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'linear' case ( MIX_PULAY ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'pulay' select case ( m % v ) case ( 0 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable' case ( 1 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR' case ( 2 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable+SVD' case ( 3 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR+SVD' end select case ( MIX_BROYDEN ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'broyden' ! currently no variants exists end select ! remark write ( * , '(/,t3,a)' ) '# Mixing options' ! Weight ! For Broyden this is the inverse Jacobian write ( * , '(t3,a,f6.4)' ) 'weight ' , m % w select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) write ( * , '(t3,a,f6.4)' ) 'weight.linear ' , m % rv ( 1 ) end select if ( m % n_hist > 0 ) then write ( * , '(t3,a,i0)' ) 'history ' , m % n_hist end if bool = . false . if ( m % restart > 0 ) then write ( * , '(t3,a,i0)' ) 'restart ' , m % restart bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(t3,a,e10.5)' ) 'restart.p ' , m % rv ( I_P_RESTART ) bool = . true . end if end select if ( bool ) then write ( * , '(t3,a,i0)' ) 'restart.save ' , m % restart_save end if ! remark bool = . false . if ( m % n_itt > 0 ) then write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,i0)' ) 'iterations ' , m % n_itt bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,f6.4)' ) 'next.p ' , m % rv ( I_P_NEXT ) bool = . true . end if end select if ( bool . and . associated ( m % next ) ) then write ( * , '(t2,2(tr1,a))' ) 'next' , trim ( m % next % name ) else if ( bool ) then call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if if ( associated ( m % next_conv ) ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t2,2(tr1,a))' ) 'next.conv' , trim ( m % next_conv % name ) end if write ( * , '(4a)' ) '%endblock ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) end do write ( * , * ) ! new-line end subroutine mixers_print_block !> Return the integer specification of the mixing type !! !! @param[in] str the character representation of the mixing type !! @return the integer corresponding to the mixing type function mix_method ( str ) result ( m ) use fdf , only : leqi character ( len =* ), intent ( in ) :: str integer :: m if ( leqi ( str , 'linear' ) ) then m = MIX_LINEAR else if ( leqi ( str , 'pulay' ) . or . & leqi ( str , 'diis' ) . or . & leqi ( str , 'anderson' ) ) then m = MIX_PULAY else if ( leqi ( str , 'broyden' ) ) then m = MIX_BROYDEN else if ( leqi ( str , 'fire' ) ) then m = MIX_FIRE call die ( 'mixing: FIRE currently not supported.' ) else call die ( 'mixing: Unknown mixing variant.' ) end if end function mix_method !> Return the variant of the mixing method !! !! @param[in] m the integer type of the mixing method !! @param[in] str the character specification of the mixing method variant !! @return the variant of the mixing method function mix_method_variant ( m , str ) result ( v ) use fdf , only : leqi integer , intent ( in ) :: m character ( len =* ), intent ( in ) :: str integer :: v v = 0 select case ( m ) case ( MIX_LINEAR ) ! no variants case ( MIX_PULAY ) v = 0 ! We do not implement tho non-stable version ! There is no need to have an inferior Pulay mixer... if ( leqi ( str , 'original' ) . or . & leqi ( str , 'kresse' ) . or . leqi ( str , 'stable' ) ) then ! stable version, will nearly always succeed on inversion v = 0 else if ( leqi ( str , 'original+svd' ) . or . & leqi ( str , 'kresse+svd' ) . or . leqi ( str , 'stable+svd' ) ) then ! stable version, will nearly always succeed on inversion v = 2 else if ( leqi ( str , 'gr' ) . or . & leqi ( str , 'guarenteed-reduction' ) . or . & leqi ( str , 'bowler-gillan' ) ) then ! Guarenteed reduction version v = 1 else if ( leqi ( str , 'gr+svd' ) . or . & leqi ( str , 'guarenteed-reduction+svd' ) . or . & leqi ( str , 'bowler-gillan+svd' ) ) then ! Guarenteed reduction version v = 3 end if case ( MIX_BROYDEN ) ! Currently only one variant v = 0 case ( MIX_FIRE ) ! no variants end select end function mix_method_variant ! The basic mixing procedure is this: ! 1. Initialize the mixing algorithm !    This will typically mean that one needs to !    push input and output matrices ! 2. Calculate the mixing coefficients ! 3. Use coefficients to calculate the !    next (optimized) guess ! 4. Finalize the mixing method ! ! Having the routines split up in this manner ! allows one to skip step 2 and use coefficients ! from another set of input/output to retrieve the ! mixing coefficients. ! Say we may retrieve mixing coefficients from ! the Hamiltonian, but use them for the density-matrix !> Initialize the mixing algorithm !! !! @param[pointer] mix the mixing method !! @param[in] n size of the arrays to be used in the algorithm !! @param[in] xin array of the input variables !! @param[in] xout array of the output variables subroutine mixing_init ( mix , n , xin , F ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n ! In/out of the function real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), pointer :: res (:), rres (:) integer :: i , ns real ( dp ) :: dnorm , dtmp logical :: p_next , p_restart ! Initialize action for mixer mix % action = ACTION_MIX ! Step iterator (so first mixing has cur_itt == 1) mix % cur_itt = mix % cur_itt + 1 ! If we are going to skip to next, we signal it ! before entering if ( mix % n_itt > 0 . and . & mix % n_itt <= current_itt ( mix ) ) then mix % action = IOR ( mix % action , ACTION_NEXT ) end if ! Check whether the residual norm is below a certain ! criteria p_next = mix % rv ( I_P_NEXT ) > 0._dp p_restart = mix % rv ( I_P_RESTART ) > 0._dp ! Check whether a parameter next/restart is required if ( p_restart . or . p_next ) then ! Calculate norm: ||f_k|| dnorm = norm ( n , F , F ) #ifdef MPI dtmp = dnorm call MPI_AllReduce ( dtmp , dnorm , 1 , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) #endif ! Calculate the relative difference dtmp = abs ( dnorm / mix % rv ( I_PREVIOUS_RES ) - 1._dp ) ! We first check for next, that has precedence if ( p_next ) then if ( dtmp < mix % rv ( I_P_NEXT ) ) then ! Signal stepping mixer mix % action = IOR ( mix % action , ACTION_NEXT ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < np  :  ' , & dtmp , ' < ' , mix % rv ( I_P_NEXT ) end if if ( p_restart ) then if ( dtmp < mix % rv ( I_P_RESTART ) ) then ! Signal restart mix % action = IOR ( mix % action , ACTION_RESTART ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < rp  :  ' , & dtmp , ' < ' , mix % rv ( I_P_RESTART ) end if ! Store the new residual norm mix % rv ( I_PREVIOUS_RES ) = dnorm end if ! Push information to the stack select case ( mix % m ) case ( MIX_LINEAR ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' linear' call init_linear () case ( MIX_PULAY ) if ( debug_mix ) then select case ( mix % v ) case ( 0 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay' case ( 1 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay, GR' case ( 2 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD' case ( 3 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD, GR' end select end if call init_pulay () case ( MIX_BROYDEN ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' Broyden' call init_broyden () end select contains subroutine init_linear () ! information for this depends on the ! following method call fake_history_from_linear ( mix % next ) call fake_history_from_linear ( mix % next_conv ) end subroutine init_linear subroutine init_pulay () logical :: GR_linear select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do end if case ( 1 , 3 ) ! Whether this is the linear cycle... GR_linear = mod ( current_itt ( mix ), 2 ) == 1 ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F , mix % rv ( 1 )) ns = n_items ( mix % stack ( 1 )) if ( GR_linear . and . current_itt ( mix ) > 1 . and . & ns > 1 ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = rres ( i ) + res ( i ) end do !$OMP end parallel do else if ( ns > 1 . and . . not . GR_linear ) then ! now we can calculate RRes[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) end if end select end subroutine init_pulay subroutine init_broyden () ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do else ! Store F[x_in] (used to create the input residual) call push_stack_data ( mix % stack ( 3 ), n ) res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if end subroutine init_broyden subroutine fake_history_from_linear ( next ) type ( tMixer ), pointer :: next real ( dp ), pointer :: t1 (:), t2 (:) integer :: ns , nh , i , nhl if ( . not . associated ( next ) ) return ! Reduce to # history of linear nhl = mix % n_hist ! if the number of fake-history steps saved is ! zero we immediately return. ! Only if mix%n_hist > 0 will the below ! occur. if ( nhl == 0 ) return ! Check for the type of following method select case ( next % m ) case ( MIX_PULAY ) ! Here it depends on the variant select case ( next % v ) case ( 0 , 2 ) ! stable pulay mixing ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select case ( MIX_BROYDEN ) ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns >= 2 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select end subroutine fake_history_from_linear end subroutine mixing_init !> Function to retrieve the number of coefficients !! calculated in this iteration. !! This is so external routines can query the size !! of the arrays used. !! !! @param[in] mix the used mixer function mixing_ncoeff ( mix ) result ( n ) type ( tMixer ), intent ( in ) :: mix integer :: n n = 0 select case ( mix % m ) case ( MIX_PULAY ) n = n_items ( mix % stack ( 2 )) case ( MIX_BROYDEN ) n = n_items ( mix % stack ( 2 )) end select end function mixing_ncoeff !> Calculate the mixing coefficients for the !! current mixer !! !! @param[in] mix the current mixer !! @param[in] n the number of elements used to calculate !!           the coefficients !! @param[in] xin the input value !! @param[in] F xout - xin, (residual) !! @param[out] coeff the coefficients subroutine mixing_coeff ( mix , n , xin , F , coeff ) use parallel , only : IONode type ( tMixer ), intent ( inout ) :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), intent ( out ) :: coeff (:) integer :: ncoeff ncoeff = size ( coeff ) if ( ncoeff < mixing_ncoeff ( mix ) ) then write ( * , '(a)' ) 'mix: Error in calculating coefficients' ! Do not allow this... return end if select case ( mix % m ) case ( MIX_LINEAR ) call linear_coeff () case ( MIX_PULAY ) call pulay_coeff () case ( MIX_BROYDEN ) call broyden_coeff () end select contains subroutine linear_coeff () integer :: i do i = 1 , ncoeff coeff ( i ) = 0._dp end do end subroutine linear_coeff subroutine pulay_coeff () integer :: ns , nh , nmax integer :: i , j , info logical :: lreturn ! Calculation quantities real ( dp ) :: dnorm , G real ( dp ), pointer :: res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) lreturn = . false . ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) return ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! Allocate arrays for calculating the ! coefficients allocate ( A ( nh , nh ), Ainv ( nh , nh )) ! Calculate A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = norm(RRes[i],RRes[j]) A ( i , j ) = norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Diagonal A ( i , i ) = norm ( n , rres1 , rres1 ) end do #ifdef MPI ! Global operations, but only for the non-extended entries call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) ! copy over reduced arrays A = Ainv #endif ! Get inverse of matrix select case ( mix % v ) case ( 0 , 1 ) call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if case ( 2 , 3 ) ! We forcefully use the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end select ! NOTE, although mix%stack(1) contains ! the x[i] - x[i-1], the tip of the stack ! contains F[i]! ! res == F[i] res => getstackval ( mix , 1 ) ! Initialize the coefficients do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients on all processors do j = 1 , nh ! res  == F[i] ! rres == F[j+1] - F[j] rres => getstackval ( mix , 2 , j ) dnorm = norm ( n , rres , res ) do i = 1 , nh coeff ( i ) = coeff ( i ) - Ainv ( i , j ) * dnorm end do end do #ifdef MPI ! Reduce the coefficients call MPI_AllReduce ( coeff ( 1 ), A ( 1 , 1 ), nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh coeff ( i ) = A ( i , 1 ) end do #endif else info = 0 ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, SVD failed, > linear' end if ! Clean up memory deallocate ( A , Ainv ) end subroutine pulay_coeff subroutine broyden_coeff () integer :: ns , nh , nmax integer :: i , j , k , info ! Calculation quantities real ( dp ) :: dnorm , dtmp real ( dp ), pointer :: w (:), res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: c (:), A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) ! Easy check for initial step... if ( ns == 1 ) then ! reset coeff = 0._dp return end if ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! This is the modified Broyden algorithm... ! Retrieve the previous weights w => mix % rv ( 2 : 1 + nh ) select case ( mix % v ) case ( 2 ) ! Unity Broyden w ( nh ) = 1._dp case ( 1 ) ! RMS Broyden dnorm = norm ( n , F , F ) #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif w (:) = 1._dp / sqrt ( dnorm ) if ( debug_mix ) & write ( * , '(2(a,e10.4))' ) & trim ( debug_msg ) // ' weight = ' , w ( 1 ), & ' , norm = ' , dnorm case ( 0 ) ! Varying weight dnorm = 0._dp !$OMP parallel do default(shared), private(i), & !$OMP& reduction(max:dnorm) do i = 1 , n dnorm = max ( dnorm , abs ( F ( i )) ) end do !$OMP end parallel do #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif ! Problay 0.2 should be changed to user-defined w ( nh ) = exp ( 1._dp / ( dnorm + 0.2_dp ) ) if ( debug_mix ) & write ( * , '(2a,1000(tr1,e10.4))' ) & trim ( debug_msg ), ' weights = ' , w ( 1 : nh ) end select ! Allocate arrays used allocate ( c ( nh )) allocate ( A ( nh , nh ), Ainv ( nh , nh )) !  < RRes[i] | Res[n] > do i = 1 , nh rres => getstackval ( mix , 2 , i ) c ( i ) = norm ( n , rres , F ) end do #ifdef MPI call MPI_AllReduce ( c ( 1 ), A ( 1 , 1 ), nh , & MPI_Double_Precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh c ( i ) = A ( i , 1 ) end do #endif ! Create A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = dot_product(RRes[i],RRes[j]) A ( i , j ) = w ( i ) * w ( j ) * norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Do the diagonal term A ( i , i ) = w ( i ) * w ( i ) * norm ( n , rres1 , rres1 ) end do #ifdef MPI call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) A = Ainv #endif ! Add the diagonal term ! This should also prevent it from being ! singular (unless mix%w == 0) do i = 1 , nh A ( i , i ) = mix % rv ( 1 ) ** 2 + A ( i , i ) end do ! Calculate the inverse call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients... do i = 1 , nh do j = 1 , nh ! Ainv should be symmetric (A is) coeff ( i ) = coeff ( i ) + w ( j ) * c ( j ) * Ainv ( j , i ) end do ! Calculate correct weight... coeff ( i ) = - w ( i ) * coeff ( i ) end do else ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, SVD failed, > linear' end if deallocate ( A , Ainv ) end subroutine broyden_coeff end subroutine mixing_coeff !> Calculate the guess for the next iteration !! !! Note this gets passed the coefficients. Hence, !! they may be calculated from another set of history !! steps. !! This may be useful in certain situations. !! !! @param[in] mix the current mixer !! @param[in] n the number of elements used to calculate !!           the coefficients !! @param[in] xin the input value !! @param[in] F the xin residual !! @param[out] xnext the input for the following iteration !! @param[in] coeff the coefficients subroutine mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ) real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( out ) :: xnext ( n ) real ( dp ), intent ( in ) :: coeff (:) select case ( mix % m ) case ( MIX_LINEAR ) call mixing_linear () case ( MIX_PULAY ) call mixing_pulay () case ( MIX_BROYDEN ) call mixing_broyden () end select contains subroutine mixing_linear () integer :: i real ( dp ) :: w w = mix % w if ( debug_mix ) write ( * , '(2a,e10.4)' ) & trim ( debug_msg ), ' alpha = ' , w !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + w * F ( i ) end do !$OMP end parallel do end subroutine mixing_linear subroutine mixing_pulay () integer :: ns , nh integer :: i , j logical :: lreturn real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Pulay' xnext = 0._dp return end if ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Pulay (initial), alpha = ' , mix % rv ( 1 ) case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Direct mixing, alpha = ' , mix % rv ( 1 ) end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) then ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ),& ' G = ' , G , ', sum(alpha) = ' , sum ( coeff ), & ', alpha = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_pulay subroutine mixing_broyden () integer :: ns , nh integer :: i , j real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Broyden' xnext = 0._dp return end if if ( ns == 1 ) then if ( debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Broyden (initial), alpha = ' , mix % rv ( 1 ) ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' G = ' , G , & ', sum(coeff) = ' , sum ( coeff ), & ', coeff = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_broyden end subroutine mixing_calc_next !> Finalize the mixing algorithm !! !! @param[inout] mix mixer to be finalized !! @param[in] n size of the input arrays !! @param[in] xin the input for this iteration !! @param[in] F the residual for this iteration !! @param[in] xnext the optimized input for the next iteration subroutine mixing_finalize ( mix , n , xin , F , xnext ) use parallel , only : IONode type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ), xnext ( n ) integer :: rsave select case ( mix % m ) case ( MIX_LINEAR ) call fin_linear () case ( MIX_PULAY ) call fin_pulay () case ( MIX_BROYDEN ) call fin_broyden () end select ! Fix the action to finalize it.. if ( mix % restart > 0 . and . & mod ( current_itt ( mix ), mix % restart ) == 0 ) then mix % action = IOR ( mix % action , ACTION_RESTART ) end if ! Check the actual finalization... ! First check whether we should restart history if ( IAND ( mix % action , ACTION_RESTART ) == ACTION_RESTART ) then ! The user has requested to restart the ! mixing scheme now rsave = mix % restart_save select case ( mix % m ) case ( MIX_PULAY ) if ( IONode ) then write ( * , '(a)' ) 'mix: Pulay -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if case ( MIX_BROYDEN ) if ( IONode ) then write ( * , '(a)' ) 'mix: Broyden -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if end select if ( allocated ( mix % stack ) ) then if ( debug_mix ) & write ( * , '(a,a,i0)' ) trim ( debug_msg ), & ' saved hist = ' , n_items ( mix % stack ( 1 )) end if end if ! check whether we should change the mixer if ( IAND ( mix % action , ACTION_NEXT ) == ACTION_NEXT ) then call mixing_step ( mix ) end if contains subroutine fin_linear () ! do nothing... end subroutine fin_linear subroutine fin_pulay () integer :: ns integer :: i logical :: GR_linear real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) select case ( mix % v ) case ( 0 , 2 ) ! stable Pulay if ( n_items ( mix % stack ( 3 )) == 0 ) then call push_stack_data ( mix % stack ( 3 ), n ) end if res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do case ( 1 , 3 ) ! GR Pulay GR_linear = mod ( current_itt ( mix ), 2 ) == 1 if ( n_items ( mix % stack ( 2 )) > 0 . and . & . not . GR_linear ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  rres == F[i] - F[i-1] rres ( i ) = rres ( i ) - res ( i ) ! Output: !  rres == - F[i-1] end do !$OMP end parallel do call pop ( mix % stack ( 1 )) ! Note that this is Res[i-1] = (F&#94;i-1_out - F&#94;i-1_in) res => getstackval ( mix , 1 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - xin ( i ) + xnext ( i ) end do !$OMP end parallel do end if end select end subroutine fin_pulay subroutine fin_broyden () integer :: ns , nh integer :: i real ( dp ), pointer :: res (:), rres (:) ns = current_itt ( mix ) nh = n_items ( mix % stack ( 2 )) if ( ns >= 2 . and . n_items ( mix % stack ( 3 )) > 0 ) then ! Update the residual to reflect the input residual res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do end if ! Update weights (if necessary) if ( nh > 1 ) then do i = 2 , nh mix % rv ( i ) = mix % rv ( i + 1 ) end do end if end subroutine fin_broyden end subroutine mixing_finalize ! Perform the actual mixing... subroutine mixing_1d ( mix , n , xin , F , xnext , nsub ) ! The current mixing method type ( tMixer ), pointer :: mix ! The current step in the SCF and size of arrays integer , intent ( in ) :: n ! x1 == Input function, ! F1 == Residual from x1 real ( dp ), intent ( in ) :: xin ( n ), F ( n ) ! x2 == Next input function real ( dp ), intent ( inout ) :: xnext ( n ) ! Number of elements used for calculating the mixing ! coefficients integer , intent ( in ), optional :: nsub ! Coefficients integer :: ncoeff real ( dp ), allocatable :: coeff (:) call mixing_init ( mix , n , xin , F ) ncoeff = mixing_ncoeff ( mix ) allocate ( coeff ( ncoeff )) ! Calculate coefficients if ( present ( nsub ) ) then call mixing_coeff ( mix , nsub , xin , F , coeff ) else call mixing_coeff ( mix , n , xin , F , coeff ) end if ! Calculate the following output call mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! Coefficients are not needed anymore... deallocate ( coeff ) ! Finalize the mixer call mixing_finalize ( mix , n , xin , F , xnext ) end subroutine mixing_1d subroutine mixing_2d ( mix , n1 , n2 , xin , F , xnext , nsub ) type ( tMixer ), pointer :: mix integer , intent ( in ) :: n1 , n2 real ( dp ), intent ( in ) :: xin ( n1 , n2 ), F ( n1 , n2 ) real ( dp ), intent ( inout ) :: xnext ( n1 , n2 ) integer , intent ( in ), optional :: nsub ! Simple wrapper for 1D if ( present ( nsub ) ) then call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 ) ,& nsub = n1 * nsub ) else call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 )) end if end subroutine mixing_2d ! Step the mixing object and ensure that ! the old history is either copied over or freed subroutine mixing_step ( mix ) use parallel , only : IONode type ( tMixer ), pointer :: mix type ( tMixer ), pointer :: next => null () type ( dData1D ), pointer :: d1D integer :: i , is , n , init_itt logical :: reset_stack , copy_stack ! First try and next => mix % next if ( associated ( next ) ) then ! Whether or not the two methods are allowed ! to share history copy_stack = mix % m == next % m select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) copy_stack = . true . end select end select copy_stack = copy_stack . and . allocated ( mix % stack ) ! If the two methods are similar if ( copy_stack ) then ! They are similar, copy over the history stack do is = 1 , size ( mix % stack ) ! Get maximum size of the current stack, n = n_items ( mix % stack ( is )) ! Note that this will automatically take care of ! wrap-arounds and delete the unneccesry elements do i = 1 , n d1D => get_pointer ( mix % stack ( is ), i ) call push ( next % stack ( is ), d1D ) end do ! nullify nullify ( d1D ) end do end if end if reset_stack = . true . if ( associated ( next ) ) then if ( associated ( next % next , mix ) . and . & next % n_itt > 0 ) then ! if this is a circular mixing routine ! we should not reset the history... reset_stack = . false . end if end if if ( reset_stack ) then select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) n = size ( mix % stack ) do is = 1 , n call reset ( mix % stack ( is )) end do end select end if if ( associated ( next ) ) then init_itt = 0 ! Set-up the next mixer select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) init_itt = n_items ( next % stack ( 1 )) end select next % start_itt = init_itt next % cur_itt = init_itt if ( IONode ) then write ( * , '(3a)' ) trim ( debug_msg ), ' switching mixer --> ' , & trim ( next % name ) end if mix => mix % next end if end subroutine mixing_step ! Calculate the inverse of a matrix subroutine inverse ( n , A , B , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) integer , intent ( out ) :: info integer :: i , j ! Local arrays real ( dp ) :: pm ( n , n ), work ( n * 4 ), err ! Relative tolerance dependent on the magnitude ! For now we retain the old tolerance real ( dp ), parameter :: etol = 1.e-4_dp integer :: ipiv ( n ) ! initialize info info = 0 ! simple check and fast return if ( n == 1 ) then B ( 1 , 1 ) = 1._dp / A ( 1 , 1 ) return end if call lapack_inv () if ( info /= 0 ) call simple_inv () contains subroutine lapack_inv () B = A call dgetrf ( n , n , B , n , ipiv , info ) if ( info /= 0 ) return call dgetri ( n , B , n , ipiv , work , n * 4 , info ) if ( info /= 0 ) return ! This sets info appropriately call check_inv () end subroutine lapack_inv subroutine simple_inv () real ( dp ) :: x integer :: k ! Copy over A B = A do i = 1 , n if ( B ( i , i ) == 0._dp ) then info = - n return end if x = 1._dp / B ( i , i ) B ( i , i ) = 1._dp do j = 1 , n B ( j , i ) = B ( j , i ) * x end do do k = 1 , n if ( ( k - i ) /= 0 ) then x = B ( i , k ) B ( i , k ) = 0._dp do j = 1 , n B ( j , k ) = B ( j , k ) - B ( j , i ) * x end do end if end do end do ! This sets info appropriately call check_inv () end subroutine simple_inv subroutine check_inv () ! Check correcteness pm = matmul ( A , B ) do j = 1 , n do i = 1 , n if ( i == j ) then err = pm ( i , j ) - 1._dp else err = pm ( i , j ) end if ! This is pretty strict tolerance! if ( abs ( err ) > etol ) then ! Signal failure in inversion info = - n - 1 return end if end do end do end subroutine check_inv end subroutine inverse ! Calculate the svd of a matrix ! With   ||(Ax - b)|| << subroutine svd ( n , A , B , cond , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) real ( dp ), intent ( in ) :: cond integer , intent ( out ) :: info ! Local arrays integer :: rank , i character ( len = 50 ) :: fmt real ( dp ) :: AA ( n , n ), S ( n ), work ( n * 5 ) ! Copy A matrix AA = A ! setup pseudo inverse solution for minimizing ! constraints B = 0._dp do i = 1 , n B ( i , i ) = 1._dp end do call dgelss ( n , n , n , AA , n , B , n , S , cond , rank , work , n * 5 , info ) ! if debugging print out the different variables if ( debug_mix ) then ! also mark the rank if ( rank == n ) then ! complete rank write ( * , '(2a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' SVD singular = ' , S else ! this prints the location of the SVD rank, if not full write ( fmt , '(i0,2a)' ) rank , '(tr1,e10.4),'' >'',100(tr1,e10.4)' write ( * , '(2a,' // trim ( fmt ) // ')' ) & trim ( debug_msg ), ' SVD singular = ' , S end if end if end subroutine svd ! ************************************************* ! *                Helper routines                * ! *                    LOCAL                      * ! ************************************************* ! Returns the value array from the stack(:) ! Returns this array: !    mix%stack(sidx)(hidx) ! defaults to the last item function getstackval ( mix , sidx , hidx ) result ( d1 ) type ( tMixer ), intent ( in ) :: mix integer , intent ( in ) :: sidx integer , intent ( in ), optional :: hidx real ( dp ), pointer :: d1 (:) type ( dData1D ), pointer :: dD1 if ( present ( hidx ) ) then dD1 => get_pointer ( mix % stack ( sidx ), hidx ) else dD1 => get_pointer ( mix % stack ( sidx ), & n_items ( mix % stack ( sidx ))) end if d1 => val ( dD1 ) end function getstackval ! Returns true if the following ! \"advanced\" mixer is 'method' function is_next ( mix , method , next ) result ( bool ) type ( tMixer ), intent ( in ), target :: mix integer , intent ( in ) :: method type ( tMixer ), pointer , optional :: next logical :: bool type ( tMixer ), pointer :: m bool = . false . m => mix % next do while ( associated ( m ) ) if ( m % m == MIX_LINEAR ) then m => m % next else if ( m % m == method ) then bool = . true . exit else ! Quit if it does not do anything exit end if ! this will prevent cyclic combinations if ( associated ( m , mix ) ) exit end do if ( present ( next ) ) then next => m end if end function is_next !> Get current iteration count !! !! This is abstracted because the initial iteration !! and the current iteration may be uniquely defined. function current_itt ( mix ) result ( itt ) type ( tMixer ), intent ( in ) :: mix integer :: itt itt = mix % cur_itt - mix % start_itt end function current_itt ! Stack handling routines function stack_check ( stack , n ) result ( check ) type ( Fstack_dData1D ), intent ( inout ) :: stack integer , intent ( in ) :: n logical :: check ! Local arrays type ( dData1D ), pointer :: dD1 if ( n_items ( stack ) == 0 ) then check = . true . else ! Check that the stack stored arrays are ! of same size... dD1 => get_pointer ( stack , 1 ) check = n == size ( dD1 ) end if end function stack_check subroutine push_stack_data ( s_F , n ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n type ( dData1D ) :: dD1 if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if call newdData1D ( dD1 , n , '(F)' ) ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_stack_data subroutine push_F ( s_F , n , F , fact ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( in ), optional :: fact type ( dData1D ) :: dD1 real ( dp ), pointer :: sF (:) integer :: in , ns integer :: i if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) ns = max_size ( s_F ) if ( in == ns ) then ! we have to cycle the storage call get ( s_F , 1 , dD1 ) else call newdData1D ( dD1 , n , '(F)' ) end if sF => val ( dD1 ) if ( present ( fact ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n sF ( i ) = F ( i ) * fact end do !$OMP end parallel do else call dcopy ( n , F , 1 , sF , 1 ) end if ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_F subroutine update_F ( s_F , n , F ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) type ( dData1D ), pointer :: dD1 real ( dp ), pointer :: FF (:) integer :: in if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) if ( in == 0 ) then ! We need to add it as it does not exist call push_F ( s_F , n , F ) else ! we have an entry, update the latest dD1 => get_pointer ( s_F , in ) FF => val ( dD1 ) call dcopy ( n , F , 1 , FF , 1 ) end if end subroutine update_F subroutine push_diff ( s_rres , s_res , alpha ) type ( Fstack_dData1D ), intent ( inout ) :: s_rres type ( Fstack_dData1D ), intent ( in ) :: s_res real ( dp ), intent ( in ), optional :: alpha type ( dData1D ) :: dD1 type ( dData1D ), pointer :: pD1 real ( dp ), pointer :: res1 (:), res2 (:), rres (:) integer :: in , ns , i , n if ( n_items ( s_res ) < 2 ) then call die ( 'mixing: Residual residuals cannot be calculated, & &inferior residual size.' ) end if in = n_items ( s_res ) ! First get the value of in pD1 => get_pointer ( s_res , in - 1 ) res1 => val ( pD1 ) ! get the value of in pD1 => get_pointer ( s_res , in ) res2 => val ( pD1 ) in = n_items ( s_rres ) ns = max_size ( s_rres ) if ( in == ns ) then ! we have to cycle the storage call get ( s_rres , 1 , dD1 ) else call newdData1D ( dD1 , size ( res1 ), '(res)' ) end if ! Get the residual of the residual rres => val ( dD1 ) n = size ( rres ) if ( present ( alpha ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = ( res2 ( i ) - res1 ( i )) * alpha end do !$OMP end parallel do else !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = res2 ( i ) - res1 ( i ) end do !$OMP end parallel do end if ! Push the data to the stack call push ( s_rres , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_diff !> Calculate the norm of two arrays function norm ( n , x1 , x2 ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: x1 ( n ), x2 ( n ) real ( dp ) :: norm ! Currently we use an external routine integer :: i ! Calculate dot product norm = 0._dp !$OMP parallel do default(shared), private(i) & !$OMP& reduction(+:norm) do i = 1 , n norm = norm + x1 ( i ) * x2 ( i ) end do !$OMP end parallel do end function norm end module m_mixing","tags":"","loc":"sourcefile/m_mixing.f90.html","title":"m_mixing.F90 – SIESTA"},{"text":"This file depends on sourcefile~~compute_max_diff.f90~~EfferentGraph sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~precision.f precision.F sourcefile~compute_max_diff.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~compute_max_diff.f90~~AfferentGraph sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_max_diff Source Code compute_max_diff.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_compute_max_diff use precision , only : dp !> Temporary for storing the old maximum change real ( dp ), public , save :: dDmax_current interface compute_max_diff module procedure compute_max_diff_1d module procedure compute_max_diff_2d end interface compute_max_diff public :: compute_max_diff contains subroutine compute_max_diff_2d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:,:), X2 (:,:) real ( dp ), intent ( out ) :: max_diff integer :: n1 , n2 integer :: i1 , i2 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) n2 = size ( X1 , 2 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if if ( size ( X2 , 2 ) /= n2 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (2-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i2,i1), & !$OMP& reduction(max:max_diff), collapse(2) do i2 = 1 , n2 do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 , i2 ) - X2 ( i1 , i2 )) ) end do end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_2d subroutine compute_max_diff_1d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:), X2 (:) real ( dp ), intent ( out ) :: max_diff integer :: n1 integer :: i1 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i1), reduction(max:max_diff) do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 ) - X2 ( i1 )) ) end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_1d end module m_compute_max_diff","tags":"","loc":"sourcefile/compute_max_diff.f90.html","title":"compute_max_diff.F90 – SIESTA"},{"text":"This file depends on sourcefile~~setup_hamiltonian.f~~EfferentGraph sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~sys.f sys.F sourcefile~setup_hamiltonian.f->sourcefile~sys.f sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~setup_hamiltonian.f->sourcefile~siesta_options.f90 sourcefile~atmfuncs.f atmfuncs.f sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~setup_hamiltonian.f->sourcefile~m_rhog.f90 sourcefile~parallel.f parallel.F sourcefile~setup_hamiltonian.f->sourcefile~parallel.f sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~precision.f precision.F sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~atm_types.f atm_types.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~dhscf.f->sourcefile~sys.f sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~precision.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~units.f90 units.f90 sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~mesh.f mesh.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~delk.f90->sourcefile~parallel.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~units.f90->sourcefile~precision.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~parallel.f sourcefile~forhar.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~forhar.f->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~radial.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f var pansourcefilesetup_hamiltonianfEfferentGraph = svgPanZoom('#sourcefilesetup_hamiltonianfEfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~setup_hamiltonian.f~~AfferentGraph sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_setup_hamiltonian Source Code setup_hamiltonian.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_setup_hamiltonian private public :: setup_hamiltonian CONTAINS subroutine setup_hamiltonian ( iscf ) USE siesta_options use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H , S , Hold use sparse_matrices , only : Dscf , Escf , xijo use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , . lastkb , no_s , rmaxv , indxua , iphorb , lasto , . rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use ldau_specs , only : switch_ldau ! This variable determines whether !   the subroutine to compute the !   Hubbard terms should be called !   or not use m_ldau , only : hubbard_term ! Subroutine that compute the !   Hubbard terms use m_dhscf , only : dhscf use m_stress use m_energies use parallel , only : Node use m_steps , only : istp use m_ntm use m_spin , only : spin use m_dipol use alloc , only : re_alloc , de_alloc use m_gamma use m_hsx , only : write_hsx use sys , only : die , bye use m_partial_charges , only : want_partial_charges use files , only : filesOut_t ! derived type for output file names use m_rhog , only : rhog_in , rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none integer , intent ( in ) :: iscf real ( dp ) :: stressl ( 3 , 3 ) real ( dp ), pointer :: fal (:,:) ! Local-node part of atomic F #ifdef MPI real ( dp ) :: buffer1 #endif integer :: io , is , ispin integer :: ifa ! Calc. forces?      0=>no, 1=>yes integer :: istr ! Calc. stress?      0=>no, 1=>yes integer :: ihmat ! Calc. hamiltonian? 0=>no, 1=>yes real ( dp ) :: g2max type ( filesOut_t ) :: filesOut ! blank output file names logical :: use_rhog_in real ( dp ), pointer :: H_vkb (:), H_kin (:), H_ldau (:,:) real ( dp ), pointer :: H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) complex ( dp ) :: Dc integer :: ind , i , j !------------------------------------------------------------------------- BEGIN call timer ( 'setup_H' , 1 ) ! Nullify pointers nullify ( fal ) !$OMP parallel default(shared), private(ispin,io) !     Save present H matrix !$OMP do collapse(2) do ispin = 1 , spin % H do io = 1 , maxnh Hold ( io , ispin ) = H ( io , ispin ) end do end do !$OMP end do !$OMP single H_kin => val ( H_kin_1D ) H_vkb => val ( H_vkb_1D ) if ( spin % SO_onsite ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so_on => val ( H_so_on_2D ) else if ( spin % SO_offsite ) then H_so_off => val ( H_so_off_2D ) end if !$OMP end single ! keep wait ! Initialize diagonal Hamiltonian do ispin = 1 , spin % spinor !$OMP do do io = 1 , maxnh H ( io , ispin ) = H_kin ( io ) + H_vkb ( io ) end do !$OMP end do nowait end do if ( spin % SO_onsite ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = H_so_on ( io , ispin - 2 ) end do end do !$OMP end do nowait else !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = 0._dp end do end do !$OMP end do nowait end if ! .................. ! Non-SCF part of total energy ....................................... ! Note that these will be \"impure\" for a mixed Dscf ! If mixing the charge, Dscf is the previous step's DM_out. Since ! the \"scf\" components of the energy are computed with the (mixed) ! charge, this introduces an inconsistency. In this case the energies ! coming out of this routine need to be corrected. ! !$OMP single Ekin = 0.0_dp Enl = 0.0_dp Eso = 0.0_dp !$OMP end single ! keep wait !$OMP do collapse(2), reduction(+:Ekin,Enl) do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) end do end do !$OMP end do nowait ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! if ( spin % SO_offsite ) then do io = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ), dp ) Eso = Eso + real ( H_so_off ( io , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ), dp ) Eso = Eso + real ( H_so_off ( io , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( io , 3 ), Dscf ( io , 4 ), dp ) Eso = Eso + real ( H_so_off ( io , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( io , 7 ), - Dscf ( io , 8 ), dp ) Eso = Eso + real ( H_so_off ( io , 3 ) * Dc , dp ) end do else if ( spin % SO_onsite ) then !$OMP do reduction(+:Eso) do io = 1 , maxnh Eso = Eso + H_so_on ( io , 1 ) * Dscf ( io , 7 ) + & H_so_on ( io , 2 ) * Dscf ( io , 8 ) + H_so_on ( io , 5 ) * Dscf ( io , 3 ) + & H_so_on ( io , 6 ) * Dscf ( io , 4 ) - H_so_on ( io , 3 ) * Dscf ( io , 5 ) - & H_so_on ( io , 4 ) * Dscf ( io , 6 ) end do !$OMP end do nowait end if !$OMP end parallel #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 end if #endif !     Non-SCF part of total energy call update_E0 () ! Hubbard term for LDA+U: energy, forces, stress and matrix elements .... if ( switch_ldau ) then if ( spin % NCol ) then call die ( 'LDA+U cannot be used with non-collinear spin.' ) end if if ( spin % SO ) then call die ( 'LDA+U cannot be used with spin-orbit coupling.' ) end if call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) H_ldau => val ( H_ldau_2D ) call hubbard_term ( scell , na_u , na_s , isa , xa , indxua , . maxnh , maxnh , lasto , iphorb , no_u , no_l , . numh , listhptr , listh , numh , listhptr , listh , . spin % spinor , Dscf , Eldau , DEldau , H_ldau , . fal , stressl , H , iscf , . matrix_elements_only = . true .) #ifdef MPI ! Global reduction of energy terms call globalize_sum ( Eldau , buffer1 ) Eldau = buffer1 ! DEldau should not be globalized ! as it is based on globalized occupations #endif Eldau = Eldau + DEldau call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) endif ! .................. ! Add SCF contribution to energy and matrix elements .................. g2max = g2cut call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) ifa = 0 istr = 0 ihmat = 1 if (( hirshpop . or . voropop ) $ . and . partial_charges_at_every_scf_step ) then want_partial_charges = . true . endif use_rhog_in = ( mix_charge . and . iscf > 1 ) call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa , indxua , . ntm , ifa , istr , ihmat , filesOut , . maxnh , numh , listhptr , listh , Dscf , Datm , . maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , . Exc , Dxc , dipol , stress , fal , stressl , . use_rhog_in ) if ( spin % SO_offsite ) then ! H(:, [5, 6]) are not updated in dhscf, see vmat for details. !------- H(u,u) H (:, 1 ) = H (:, 1 ) + real ( H_so_off (:, 1 ), dp ) H (:, 5 ) = dimag ( H_so_off (:, 1 )) !------- H(d,d) H (:, 2 ) = H (:, 2 ) + real ( H_so_off (:, 2 ), dp ) H (:, 6 ) = dimag ( H_so_off (:, 2 )) !------- H(u,d) H (:, 3 ) = H (:, 3 ) + real ( H_so_off (:, 3 ), dp ) H (:, 4 ) = H (:, 4 ) + dimag ( H_so_off (:, 3 )) !------- H(d,u) H (:, 7 ) = H (:, 7 ) + real ( H_so_off (:, 4 ), dp ) H (:, 8 ) = H (:, 8 ) - dimag ( H_so_off (:, 4 )) endif ! This statement will apply to iscf = 1, for example, when ! we do not use rhog_in. Rhog here is always the charge used to ! build H, that is, rhog_in. if ( mix_charge ) rhog_in = rhog want_partial_charges = . false . call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) !  It is wasteful to write over and over H and S, as there are !  no different files. ! Save Hamiltonian and overlap matrices ............................ ! Only in HSX format now.  Use Util/HSX/hsx2hs to generate an HS file if ( savehs . or . write_coop ) then call write_hsx ( gamma , no_u , no_s , spin % H , indxuo , & maxnh , numh , listhptr , listh , H , S , qtot , & temp , xijo ) endif call timer ( 'setup_H' , 2 ) #ifdef SIESTA__PEXSI if ( node == 0 ) call memory_snapshot ( \"after setup_H\" ) #endif if ( h_setup_only ) then call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call bye ( \"H-Setup-Only requested\" ) STOP endif !------------------------------------------------------------------------- END END subroutine setup_hamiltonian END module m_setup_hamiltonian","tags":"","loc":"sourcefile/setup_hamiltonian.f.html","title":"setup_hamiltonian.F – SIESTA"},{"text":"This file depends on sourcefile~~compute_dm.f~~EfferentGraph sourcefile~compute_dm.f compute_dm.F sourcefile~sys.f sys.F sourcefile~compute_dm.f->sourcefile~sys.f sourcefile~units.f90 units.f90 sourcefile~compute_dm.f->sourcefile~units.f90 sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~compute_dm.f->sourcefile~siesta_options.f90 sourcefile~precision.f precision.F sourcefile~compute_dm.f->sourcefile~precision.f sourcefile~parallel.f parallel.F sourcefile~compute_dm.f->sourcefile~parallel.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~units.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~compute_dm.f~~AfferentGraph sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_dm.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_dm Source Code compute_dm.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_compute_dm private public :: compute_dm logical , public , save :: PreviousCallDiagon = . false . CONTAINS subroutine compute_dm ( iscf ) use precision use units , only : eV USE siesta_options use class_dSpData1D , only : val use sparse_matrices use siesta_geom use atomlist , only : qa , lasto , iphorb , iaorb , no_u , no_s , indxuo , & qtot , Qtots , no_l use sys , only : die , bye use kpoint_scf_m , only : kpoints_scf use m_energies , only : Ebs , Ecorrec , Entropy , DE_NEGF use m_energies , only : Ef , Efs use m_rmaxh use m_eo use m_spin , only : spin use m_diagon , only : diagon use m_gamma use parallel , only : IONode use parallel , only : SIESTA_worker use m_compute_ebs_shift , only : compute_ebs_shift #ifdef SIESTA__PEXSI use m_pexsi_solver , only : pexsi_solver #endif use m_hsx , only : write_hs_formatted #ifdef MPI use mpi_siesta #endif #ifdef CDF use iodmhs_netcdf , only : write_dmh_netcdf #endif use m_dminim , only : dminim use m_zminim , only : zminim use m_ordern , only : ordern use m_steps , only : istp use m_normalize_dm , only : normalize_dm #ifdef SIESTA__CHESS use m_chess , only : CheSS_wrapper #endif use m_energies , only : DE_NEGF use m_ts_global_vars , only : TSmode , TSinit , TSrun use m_transiesta , only : transiesta implicit none !     Input variables integer , intent ( in ) :: iscf real ( dp ) :: delta_Ebs , delta_Ef logical :: CallDiagon integer :: nnz real ( dp ), pointer :: H_kin (:) ! e1>e2 to signal that we do not want DOS weights real ( dp ), parameter :: e1 = 1.0_dp , e2 = - 1.0_dp real ( dp ) :: buffer1 integer :: mpierr !       character(15)            :: filename, indexstr !       character(15), parameter :: fnameform = '(A,A,A)' !-------------------------------------------------------------------- BEGIN if ( SIESTA_worker ) call timer ( 'compute_dm' , 1 ) #ifdef MPI call MPI_Bcast ( isolve , 1 , MPI_integer , 0 , true_MPI_Comm_World , mpierr ) #endif if ( SIESTA_worker ) then ! Save present density matrix !$OMP parallel default(shared) if ( converge_EDM ) then !$OMP workshare Eold (:,:) = Escf (:,:) Dold (:,:) = Dscf (:,:) !$OMP end workshare else !$OMP workshare Dold (:,:) = Dscf (:,:) !$OMP end workshare end if !$OMP end parallel end if ! Compute shift in Tr(H*DM) for fermi-level bracketting ! Use the current H, the previous iteration H, and the ! previous iteration DM if ( SIESTA_worker ) then if ( iscf > 1 ) then call compute_Ebs_shift ( Dscf , H , Hold , delta_Ebs ) delta_Ef = delta_Ebs / qtot if ( ionode . and . isolve . eq . SOLVE_PEXSI ) then write ( 6 , \"(a,f16.5)\" ) $ \"Estimated change in band-structure energy:\" , $ delta_Ebs / eV , \"Estimated shift in E_fermi: \" , $ delta_Ef / eV endif else delta_Ebs = 0.0_dp delta_Ef = 0.0_dp endif endif #ifdef SIESTA__PEXSI if ( isolve . eq . SOLVE_PEXSI ) then ! This test done in node 0 since NonCol and SpOrb ! are not set for PEXSI-solver-only processes if ( ionode ) then if ( spin % NCol . or . spin % SO ) call die ( $ \"The PEXSI solver does not implement \" // $ \"non-coll spins or Spin-orbit yet\" ) endif call pexsi_solver ( iscf , no_u , no_l , spin % spinor , $ maxnh , numh , listhptr , listh , $ H , S , qtot , Dscf , Escf , $ ef , Entropy , temp , delta_Ef ) endif if (. not . SIESTA_worker ) RETURN #endif ! Here we decide if we want to calculate one or more SCF steps by ! diagonalization before proceeding with the OMM routine CallDiagon = . false . if ( isolve . eq . SOLVE_MINIM ) then if ( istp . eq . 1 ) then if (( iscf . le . call_diagon_first_step ) . or . & ( call_diagon_first_step < 0 )) CallDiagon = . true . else if (( iscf . le . call_diagon_default ) . or . & ( call_diagon_default < 0 )) CallDiagon = . true . endif endif if ( isolve . eq . MATRIX_WRITE ) then !             write(indexstr,'(I15)') iscf !             write(filename,fnameform) 'H_', trim(adjustl(indexstr)), !      &                                '.matrix' !             call write_global_matrix( no_s, no_l, maxnh, numh, listh, !      &           H(1:maxnh,1), filename ) ! !             write(filename,fnameform) 'S_', trim(adjustl(indexstr)), !      &                                '.matrix' !        Note: only one-shot for now call write_hs_formatted ( no_u , spin % H , $ maxnh , numh , listhptr , listh , H , S ) call bye ( \"End of run after writing H.matrix and S.matrix\" ) c$        call write_global_matrix_singlenodewrite( c$     &           no_u, no_s, maxnh, numh, listhptr, listh, c$     &           H(:,1), 'H.matrix') c$ c$        call write_global_matrix_singlenodewrite( c$     &           no_u, no_s, maxnh, numh, listhptr, listh, c$     &           S, 'S.matrix') elseif (( isolve . eq . SOLVE_DIAGON ) . or . ( CallDiagon )) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0.0_dp PreviousCallDiagon = . true . elseif ( isolve . eq . SOLVE_ORDERN ) then if (. not . gamma ) call die ( \"Cannot do O(N) with k-points.\" ) if ( spin % NCol . or . spin % SO ) . call die ( \"Cannot do O(N) with non-coll spins or Spin-orbit\" ) call ordern ( usesavelwf , ioptlwf , na_u , no_u , no_l , lasto , & isa , qa , rcoor , rmaxh , ucell , xa , iscf , & istp , ncgmax , etol , eta , qtot , maxnh , numh , & listhptr , listh , H , S , chebef , noeta , rcoorcp , & beta , pmax , Dscf , Escf , Ecorrec , spin % H , qtots ) Entropy = 0.0_dp elseif ( isolve . eq . SOLVE_MINIM ) then if ( spin % NCol . or . spin % SO ) & call die ( ' ERROR : Non - collinear spin calculations & not yet implemented with OMM !') H_kin => val ( H_kin_1D ) if ( gamma ) then call dminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , H , S , H_kin ) else call zminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , no_s , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & H , S , H_kin ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #ifdef SIESTA__CHESS elseif ( isolve . eq . SOLVE_CHESS ) then ! FOE solver from the CheSS library if ( gamma ) then call CheSS_wrapper (. false ., PreviousCallDiagon , & iscf , istp , no_l , & spin % spinor , no_u , maxnh , numh , listhptr , listh , & qs , h , s , & Dscf , Escf , Ef ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #endif elseif ( TSmode . and . TSinit ) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0._dp else if ( TSrun ) then call transiesta ( iscf , spin % H , block_dist , sparse_pattern , & Gamma , ucell , nsc , isc_off , no_u , na_u , lasto , xa , maxnh , & H , S , Dscf , Escf , Ef , Qtot , . false ., DE_NEGF ) Ecorrec = 0._dp Entropy = 0.0_dp else !call die('siesta: ERROR: wrong solution method') endif #ifdef CDF if ( writedmhs_cdf_history ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf ) else if ( writedmhs_cdf ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf , & overwrite = . true . ) endif #endif ! Write orbital indexes. JMS Dec.2009 if ( IOnode . and . iscf == 1 ) then call write_orb_indx ( na_u , na_s , no_u , no_s , isa , xa , . iaorb , iphorb , indxuo , nsc , ucell ) endif !     Normalize density matrix to exact charge !     Placed here for now to avoid disturbing EHarris if ( . not . TSrun ) then call normalize_dm ( first = . false . ) end if call timer ( 'compute_dm' , 2 ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after compute_DM\" ) #endif !-----------------------------------------------------------------------END END subroutine compute_dm END MODULE m_compute_dm","tags":"","loc":"sourcefile/compute_dm.f.html","title":"compute_dm.F – SIESTA"},{"text":"This file depends on sourcefile~~m_mixing_scf.f90~~EfferentGraph sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~parallel.f parallel.F sourcefile~m_mixing_scf.f90->sourcefile~parallel.f sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~precision.f precision.F sourcefile~m_mixing_scf.f90->sourcefile~precision.f sourcefile~m_mixing.f90->sourcefile~parallel.f sourcefile~m_mixing.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_mixing_scf.f90~~AfferentGraph sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_mixing_scf Source Code m_mixing_scf.F90 Source Code ! Also the mixing container module m_mixing_scf use class_Fstack_dData1D use m_mixing , only : tMixer implicit none private save type ( tMixer ), pointer :: scf_mixs (:) => null () type ( tMixer ), pointer :: scf_mix => null () ! Default mixing, no discrepancy between spin-components integer , parameter :: MIX_SPIN_ALL = 1 ! Only use spinor components for mixing integer , parameter :: MIX_SPIN_SPINOR = 2 ! Only use spin-sum for mixing (implicit on spinor) integer , parameter :: MIX_SPIN_SUM = 3 ! Use both spin-sum and spin-difference density for mixing (implicit on spinor) integer , parameter :: MIX_SPIN_SUM_DIFF = 4 ! It makes little sense to only mix difference as for spin-polarised ! calculations with no difference it will converge immediately ! How the spin mixing algorthim is chosen integer :: mix_spin = MIX_SPIN_ALL public :: scf_mixs , scf_mix public :: mix_spin public :: MIX_SPIN_ALL , MIX_SPIN_SPINOR , MIX_SPIN_SUM , MIX_SPIN_SUM_DIFF public :: mixers_scf_init public :: mixers_scf_print , mixers_scf_print_block public :: mixers_scf_history_init public :: mixers_scf_reset public :: mixing_scf_converged contains subroutine mixers_scf_init ( nspin , Comm ) use fdf use precision , only : dp #ifdef MPI use mpi_siesta , only : MPI_Comm_World #endif use m_mixing , only : mixers_reset , mixers_init use m_mixing , only : mix_method , mix_method_variant use m_mixing , only : mixer_init use m_mixing , only : mixers_history_init ! The number of spin-components integer , intent ( in ) :: nspin ! The communicator used for the mixer integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf ! Get number of history steps integer :: n_hist , n_kick , n_restart , n_save real ( dp ) :: w , w_kick integer :: n_lin_after real ( dp ) :: w_lin_after logical :: lin_after ! number of history steps saved type ( tMixer ), pointer :: m integer :: nm , im , im2 , tmp logical :: is_broyden character ( len = 70 ) :: method , variant , opt ! If the mixers are denoted by a block, then ! the entire logic *MUST* be defined in the blocks opt = fdf_get ( 'SCF.Mix.Spin' , 'all' ) if ( leqi ( opt , 'all' ) ) then mix_spin = MIX_SPIN_ALL else if ( leqi ( opt , 'spinor' ) ) then mix_spin = MIX_SPIN_SPINOR else if ( leqi ( opt , 'sum' ) ) then mix_spin = MIX_SPIN_SUM else if ( leqi ( opt , 'sum+diff' ) ) then mix_spin = MIX_SPIN_SUM_DIFF else call die ( \"Unknown option given for SCF.Mix.Spin & &all|spinor|sum|sum+diff\" ) end if ! If there is only one spinor we should mix all... if ( nspin == 1 ) mix_spin = MIX_SPIN_ALL ! Initialize to ensure debug stuff read call mixers_init ( 'SCF' , scf_mixs , Comm = Comm ) ! Check for existance of the SCF.Mix block if ( associated ( scf_mixs ) ) then if ( size ( scf_mixs ) > 0 ) then return end if ! Something has gone wrong... ! The user has supplied a block, but ! haven't added any content to the block... ! However, we fall-back to the default mechanism end if ! ensure nullification call mixers_reset ( scf_mixs ) ! >>>*** FIRST ***<<< ! Read in compatibility options ! Figure out if we are dealing with ! Broyden or Pulay n_hist = fdf_get ( 'DM.NumberPulay' , 2 ) tmp = fdf_get ( 'DM.NumberBroyden' , 0 ) is_broyden = tmp > 0 if ( is_broyden ) then n_hist = tmp end if ! Define default mixing weight (used for ! Pulay, Broyden and linear mixing) w = fdf_get ( 'DM.MixingWeight' , 0.25_dp ) ! Default kick-options n_kick = fdf_get ( 'DM.NumberKick' , 0 ) w_kick = fdf_get ( 'DM.KickMixingWeight' , 0.5_dp ) lin_after = fdf_get ( 'SCF.LinearMixingAfterPulay' , . false .) w_lin_after = fdf_get ( 'SCF.MixingWeightAfterPulay' , w ) ! >>>*** END ***<<< ! Read options in new format ! Get history length n_hist = fdf_get ( 'SCF.Mixer.History' , n_hist ) ! update mixing weight and kick mixing weight w = fdf_get ( 'SCF.Mixer.Weight' , w ) n_kick = fdf_get ( 'SCF.Mixer.Kick' , n_kick ) w_kick = fdf_get ( 'SCF.Mixer.Kick.Weight' , w_kick ) ! Restart after this number of iterations n_restart = fdf_get ( 'SCF.Mixer.Restart' , 0 ) n_save = fdf_get ( 'SCF.Mixer.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Get the variant of the mixing method if ( is_broyden ) then method = 'Broyden' else if ( n_hist > 0 ) then method = 'Pulay' else method = 'Linear' end if method = fdf_get ( 'SCF.Mixer.Method' , trim ( method )) variant = fdf_get ( 'SCF.Mixer.Variant' , 'original' ) ! Determine whether linear mixing should be ! performed after the \"advanced\" mixing n_lin_after = fdf_get ( 'SCF.Mixer.Linear.After' , - 1 ) w_lin_after = fdf_get ( 'SCF.Mixer.Linear.After.Weight' , w_lin_after ) ! Determine total number of mixers nm = 1 if ( n_lin_after >= 0 . or . lin_after ) nm = nm + 1 if ( n_kick > 0 ) nm = nm + 1 ! Initiailaze all mixers allocate ( scf_mixs ( nm )) scf_mixs (:)% w = w scf_mixs (:)% n_hist = n_hist scf_mixs (:)% restart = n_restart scf_mixs (:)% restart_save = n_save ! 1. Current mixing index im = 1 ! Store the advanced mixer index (for references to ! later mixers) im2 = im m => scf_mixs ( im ) m % name = method m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! 2. Setup the linear mixing after the actual mixing if ( n_lin_after > 0 . or . lin_after ) then im = im + 1 m => scf_mixs ( im ) ! Signal to switch to this mixer after ! convergence scf_mixs ( im2 )% next_conv => m m % name = 'Linear-After' m % m = mix_method ( 'linear' ) m % w = w_lin_after m % n_itt = n_lin_after ! jump back to previous after having run a ! few iterations m % next => scf_mixs ( im2 ) end if ! In case we have a kick, apply the kick here ! This overrides the \"linear.after\" option if ( n_kick > 0 ) then im = im + 1 m => scf_mixs ( im ) m % name = 'Linear-Kick' m % n_itt = 1 m % n_hist = 0 m % m = mix_method ( 'linear' ) m % w = w_kick m % next => scf_mixs ( im2 ) ! set the default mixer to kick scf_mixs ( im2 )% n_itt = n_kick - 1 scf_mixs ( im2 )% next => m scf_mixs ( im2 )% restart = n_kick - 1 end if ! Correct the input do im = 1 , nm call mixer_init ( scf_mixs ( im ) ) end do ! Initialize the allocation of each mixer call mixers_history_init ( scf_mixs ) #ifdef MPI if ( present ( Comm ) ) then scf_mixs (:)% Comm = Comm else scf_mixs (:)% Comm = MPI_Comm_World end if #endif end subroutine mixers_scf_init subroutine mixers_scf_print ( nspin ) use parallel , only : IONode use m_mixing , only : mixers_print integer , intent ( in ) :: nspin ! Print mixing options call mixers_print ( 'SCF' , scf_mixs ) if ( IONode . and . nspin > 1 ) then select case ( mix_spin ) case ( MIX_SPIN_ALL ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'all' case ( MIX_SPIN_SPINOR ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'spinor' if ( nspin <= 2 ) then call die ( \"SCF.Mixer.Spin spinor option only valid for & &non-collinear and spin-orbit calculations\" ) end if case ( MIX_SPIN_SUM ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum' case ( MIX_SPIN_SUM_DIFF ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum and diff' end select end if end subroutine mixers_scf_print subroutine mixers_scf_print_block ( ) use m_mixing , only : mixers_print_block ! Print mixing options call mixers_print_block ( 'SCF' , scf_mixs ) end subroutine mixers_scf_print_block subroutine mixing_scf_converged ( SCFconverged ) use parallel , only : IONode logical , intent ( inout ) :: SCFconverged integer :: i ! Return if no convergence if ( . not . SCFconverged ) return if ( associated ( scf_mix % next_conv ) ) then ! this means that we skip to the ! following algorithm scf_mix => scf_mix % next_conv SCFconverged = . false . if ( allocated ( scf_mix % stack ) ) then do i = 1 , size ( scf_mix % stack ) ! delete all but one history ! This should be fine call reset ( scf_mix % stack ( i ), - 1 ) end do end if if ( IONode ) then write ( * , '(/,2a)' ) ':!: SCF cycle continuation mixer: ' , & trim ( scf_mix % name ) end if end if end subroutine mixing_scf_converged subroutine mixers_scf_reset () use m_mixing , only : mixers_reset nullify ( scf_mix ) call mixers_reset ( scf_mixs ) end subroutine mixers_scf_reset subroutine mixers_scf_history_init ( ) use m_mixing , only : mixers_history_init call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) end subroutine mixers_scf_history_init end module m_mixing_scf","tags":"","loc":"sourcefile/m_mixing_scf.f90.html","title":"m_mixing_scf.F90 – SIESTA"},{"text":"This file depends on sourcefile~~vmat.f90~~EfferentGraph sourcefile~vmat.f90 vmat.F90 sourcefile~parallel.f parallel.F sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~mesh.f mesh.F sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~precision.f precision.F sourcefile~vmat.f90->sourcefile~precision.f sourcefile~atm_types.f atm_types.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~radial.f radial.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~sys.f sys.F sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~radial.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~vmat.f90~~AfferentGraph sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_vmat Source Code vmat.F90 Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module m_vmat implicit none private public :: vmat contains subroutine vmat ( no , np , dvol , spin , V , nvmax , & numVs , listVsptr , listVs , Vs , & nuo , nuotot , iaorb , iphorb , isa ) !! author: P.Ordejon !! !! Finds the matrix elements of the potential. !! !! First version written by P. !! Name and interface modified by J.M.Soler. May'95. !! !! Re-ordered so that mesh is the outer loop and the orbitals are !! handled as lower-half triangular. J.D.Gale and J.M.Soler, Feb'99 !! !! Version of vmat that uses a direct algorithm to save memory. !! Modified by J.D.Gale, November'99 !  Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo use t_spin , only : tSpin use listsc_module , only : LISTSC use mesh , only : dxa , nsp , xdop , xdsp , meshLim use meshdscf , only : matrixMtoO use meshdscf , only : needdscfl , listdl , numdl , nrowsdscfl , listdlptr use meshphi , only : directphi , endpht , lstpht , listp2 , phi use parallel , only : Nodes , Node use alloc , only : re_alloc , de_alloc use parallelsubs , only : GlobalToLocalOrb #ifdef MPI use mpi_siesta #endif #ifdef _OPENMP use omp_lib #endif ! Argument types and dimensions integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of columns in C (local) integer , intent ( in ) :: nvmax !! First dimension of `listV` and `Vs`, and maximum !! number of nonzero elements in any row of `Vs` integer , intent ( in ) :: nuo integer , intent ( in ) :: nuotot integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs type ( tSpin ), intent ( in ) :: spin !! Spin configuration integer , intent ( in ) :: numVs ( nuo ) !! Number of non-zero elements in a row of `Vs` integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms integer , intent ( in ) :: listVsptr ( nuo ) !! Pointer to the start of rows in `listVs` integer , intent ( in ) :: listVs ( nvmax ) !! List of non-zero elements of `Vs` real ( grid_p ), intent ( in ) :: V ( nsp , np , spin % Grid ) !! Value of the potential at the mesh points real ( dp ), intent ( in ) :: dvol !! Volume per mesh point real ( dp ), target :: Vs ( nvmax , spin % H ) !! Value of nonzero elements in each row !! of `Vs` to which the potential matrix !! elements are summed up ! Internal variables and arrays integer , parameter :: minloc = 1000 ! Min buffer size integer , parameter :: maxoa = 100 ! Max # of orb/atom integer :: i , ia , ic , ii , ijl , il , imp , ind , iop integer :: ip , iphi , io , is , isp , ispin , iu , iul integer :: j , jc , jl , last , lasta , lastop integer :: maxloc , maxloc2 , nc , nlocal , nphiloc integer :: nvmaxl , triang , lenx , leny , lenz , lenxy ! Size of Hamiltonian logical :: ParallelLocal real ( dp ) :: Vij , r2sp , dxsp ( 3 ), VClocal ( nsp ) integer , pointer :: ilc (:), ilocal (:), iorb (:) real ( dp ), pointer :: DscfL (:,:), t_DscfL (:,:,:) real ( dp ), pointer :: Vss (:,:), t_Vss (:,:,:), Clocal (:,:) real ( dp ), pointer :: Vlocal (:,:), phia (:,:), r2cut (:) integer :: NTH , TID #ifdef _TRACE_ integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE vmat' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 4 ) #endif !   Start time counter call timer ( 'vmat' , 1 ) !   Find atomic cutoff radii nullify ( r2cut ) call re_alloc ( r2cut , 1 , nsmax , 'r2cut' , 'vmat' ) r2cut = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) end do !   Set algorithm logical ParallelLocal = ( Nodes > 1 ) lenx = meshLim ( 2 , 1 ) - meshLim ( 1 , 1 ) + 1 leny = meshLim ( 2 , 2 ) - meshLim ( 1 , 2 ) + 1 lenz = meshLim ( 2 , 3 ) - meshLim ( 1 , 3 ) + 1 lenxy = lenx * leny !   Find value of maxloc maxloc2 = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxloc = maxloc2 + minloc maxloc = min ( maxloc , no ) triang = ( maxloc + 1 ) * ( maxloc + 2 ) / 2 if ( ParallelLocal ) then if ( nrowsDscfL > 0 ) then nvmaxl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else nvmaxl = 1 end if end if !   Allocate local memory !$OMP parallel default(shared), & !$OMP&shared(NTH,t_DscfL,t_Vss,spin), & !$OMP&private(TID,last), & !$OMP&private(ip,nc,nlocal,ic,imp,i,il,iu,iul,ii,ind,j,ijl,ispin), & !$OMP&private(lasta,lastop,ia,is,iop,isp,dxsp,r2sp,nphiloc,iphi,jc,jl), & !$OMP&private(Vij,VClocal,DscfL,Vss,ilocal,ilc,iorb,Vlocal,Clocal,phia) !$OMP single #ifdef _OPENMP NTH = omp_get_num_threads ( ) #else NTH = 1 #endif !$OMP end single ! implicit barrier, IMPORTANT #ifdef _OPENMP TID = omp_get_thread_num ( ) + 1 #else TID = 1 #endif nullify ( Clocal , phia , ilocal , ilc , iorb , Vlocal ) !$OMP critical ! Perhaps the critical section is not needed, ! however it \"tells\" the OS to allocate per ! thread, possibly waiting for each thread to ! place the memory in the best position. allocate ( Clocal ( nsp , maxloc2 ) ) allocate ( ilocal ( no ) , ilc ( maxloc2 ) , iorb ( maxloc ) ) allocate ( Vlocal ( triang , spin % Grid ) ) if ( DirectPhi ) allocate ( phia ( maxoa , nsp ) ) !$OMP end critical !$OMP single if ( ParallelLocal ) then nullify ( t_DscfL ) call re_alloc ( t_DscfL , 1 , nvmaxl , 1 , spin % H , 1 , NTH , & 'DscfL' , 'vmat' ) else if ( NTH > 1 ) then nullify ( t_Vss ) call re_alloc ( t_Vss , 1 , nvmax , 1 , spin % H , 2 , NTH , & 'Vss' , 'vmat' ) end if end if !$OMP end single ! implicit barrier if ( ParallelLocal ) then DscfL => t_DscfL ( 1 : nvmaxl ,:, TID ) DscfL ( 1 : nvmaxl ,:) = 0._dp else if ( NTH > 1 ) then if ( TID == 1 ) then Vss => Vs else Vss => t_Vss ( 1 : nvmax ,:, TID ) Vss ( 1 : nvmax ,:) = 0._dp end if else Vss => Vs end if end if !   Full initializations done only once ilocal ( 1 : no ) = 0 iorb ( 1 : maxloc ) = 0 Vlocal ( 1 : triang ,:) = 0._dp last = 0 !   Loop over grid points !$OMP do do ip = 1 , np !      Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !      Find new required size of Vlocal nlocal = last do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) nlocal = nlocal + 1 end do !      If overflooded, add Vlocal to Vs and reinitialize it if ( nlocal > maxloc . and . last > 0 ) then if ( ParallelLocal ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + & Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = LISTSC ( i , iu , listdl ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + & Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do else do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) call GlobalToLocalOrb ( iu , Node , Nodes , iul ) if ( i == iu ) then do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = listVs ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = LISTSC ( i , iu , listVs ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do end if !         Reset local arrays do i = 1 , last ilocal ( iorb ( i )) = 0 end do iorb ( 1 : last ) = 0 ijl = ( last + 1 ) * ( last + 2 ) / 2 do ispin = 1 , spin % Grid do i = 1 , ijl Vlocal ( i , ispin ) = 0._dp end do end do last = 0 end if !      Look for required orbitals not yet in Vlocal if ( nlocal > last ) then do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) then last = last + 1 ilocal ( i ) = last iorb ( last ) = i end if end do end if !      Check algorithm if ( DirectPhi ) then lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !            Generate or retrieve phi values if ( ia /= lasta . or . iop /= lastop ) then lasta = ia lastop = iop is = isa ( ia ) do isp = 1 , nsp dxsp (:) = xdsp (:, isp ) + xdop (:, iop ) - dxa (:, ia ) r2sp = sum ( dxsp ** 2 ) if ( r2sp < r2cut ( is ) ) then !$OMP critical call all_phi ( is , + 1 , dxsp , nphiloc , phia (:, isp ) ) !$OMP end critical else phia (:, isp ) = 0.0_dp end if end do end if iphi = iphorb ( i ) Clocal (:, ic ) = phia ( iphi ,:) do ispin = 1 , spin % Grid !               Create VClocal for the first orbital of mesh point Vij = 0._dp do isp = 1 , nsp VClocal ( isp ) = V ( isp , ip , ispin ) * Clocal ( isp , ic ) !                  This is the jc == ic value Vij = Vij + VClocal ( isp ) * Clocal ( isp , ic ) end do !               ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij !               Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !                  Calculate ic * jc Vij = 0._dp do isp = 1 , nsp Vij = Vij + VClocal ( isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij * 2._dp else Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij end if end do end do end do else do ic = 1 , nc imp = endpht ( ip - 1 ) + ic il = ilocal ( lstpht ( imp )) ilc ( ic ) = il Clocal (:, ic ) = phi (:, imp ) do ispin = 1 , spin % Grid !               Create VClocal for the first orbital of mesh point Vij = 0._dp do isp = 1 , nsp VClocal ( isp ) = V ( isp , ip , ispin ) * Clocal ( isp , ic ) !                  This is the jc == ic value Vij = Vij + VClocal ( isp ) * Clocal ( isp , ic ) end do !               ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij !               Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !                  Calculate ic * jc Vij = 0._dp do isp = 1 , nsp Vij = Vij + VClocal ( isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij * 2._dp else Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij end if end do end do end do end if end do !$OMP end do nowait ! Note that this is already performed in parallel! !   Add final Vlocal to Vs if ( ParallelLocal . and . last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = LISTSC ( i , iu , listdl ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do else if ( last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) if ( i == iu ) then do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = listVs ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = LISTSC ( i , iu , listVs ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do end if !$OMP barrier if ( ParallelLocal . and . NTH > 1 ) then !$OMP do collapse(2) do ispin = 1 , spin % H do ind = 1 , nvmaxl do ii = 2 , NTH t_DscfL ( ind , ispin , 1 ) = t_DscfL ( ind , ispin , 1 ) + & t_DscfL ( ind , ispin , ii ) end do end do end do !$OMP end do else if ( NTH > 1 ) then !$OMP do collapse(2) do ispin = 1 , spin % H do ind = 1 , nvmax do ii = 2 , NTH Vs ( ind , ispin ) = Vs ( ind , ispin ) + t_Vss ( ind , ispin , ii ) end do end do end do !$OMP end do end if !   Free memory deallocate ( Clocal , ilocal , ilc , iorb , Vlocal ) if ( DirectPhi ) deallocate ( phia ) !$OMP master if ( ParallelLocal ) then !      Redistribute Hamiltonian from mesh to orbital based distribution DscfL => t_DscfL ( 1 : nvmaxl , 1 : spin % H , 1 ) call matrixMtoO ( nvmaxl , nvmax , numVs , listVsptr , nuo , & spin % H , DscfL , Vs ) call de_alloc ( t_DscfL , 'DscfL' , 'vmat' ) else if ( NTH > 1 ) then call de_alloc ( t_Vss , 'Vss' , 'vmat' ) end if !$OMP end master !$OMP end parallel call de_alloc ( r2cut , 'r2cut' , 'vmat' ) #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'vmat' , 2 ) #ifdef DEBUG call write_debug ( '    POS vmat' ) #endif contains ! In any case will the compiler most likely inline this ! small routine. So it should not pose any problem. pure function idx_ijl ( i , j ) result ( ij ) integer , intent ( in ) :: i , j integer :: ij if ( i > j ) then ij = i * ( i + 1 ) / 2 + j + 1 else ij = j * ( j + 1 ) / 2 + i + 1 end if end function idx_ijl end subroutine vmat end module m_vmat","tags":"","loc":"sourcefile/vmat.f90.html","title":"vmat.F90 – SIESTA"},{"text":"Files dependent on this one sourcefile~~precision.f~~AfferentGraph sourcefile~precision.f precision.F sourcefile~compute_max_diff.f90 compute_max_diff.F90 sourcefile~compute_max_diff.f90->sourcefile~precision.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~precision.f sourcefile~siesta_forces.f90->sourcefile~compute_max_diff.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing_scf.f90 sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~units.f90 units.f90 sourcefile~siesta_forces.f90->sourcefile~units.f90 sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~siesta_forces.f90->sourcefile~m_rhog.f90 sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~siesta_forces.f90->sourcefile~m_mixing.f90 sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~m_mixing_scf.f90->sourcefile~precision.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~forhar.f forhar.F sourcefile~forhar.f->sourcefile~precision.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~mesh.f mesh.F sourcefile~forhar.f->sourcefile~mesh.f sourcefile~m_iorho.f m_iorho.F sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~compute_dm.f->sourcefile~precision.f sourcefile~compute_dm.f->sourcefile~units.f90 sourcefile~compute_energies.f90->sourcefile~precision.f sourcefile~compute_energies.f90->sourcefile~m_rhog.f90 sourcefile~dhscf.f dhscf.F sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~dfscf.f dfscf.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~atm_types.f atm_types.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~radial.f radial.f sourcefile~radial.f->sourcefile~precision.f sourcefile~reord.f reord.f sourcefile~reord.f->sourcefile~precision.f sourcefile~units.f90->sourcefile~precision.f sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~poison.f poison.F sourcefile~poison.f->sourcefile~precision.f sourcefile~dhscf.f->sourcefile~precision.f sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~m_mixing.f90->sourcefile~precision.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~precision.f sourcefile~rhooda.f->sourcefile~mesh.f sourcefile~rhooda.f->sourcefile~atmfuncs.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~state_analysis.f->sourcefile~units.f90 sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~units.f90 sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~state_init.f->sourcefile~units.f90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~setup_hamiltonian.f->sourcefile~m_rhog.f90 sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f var pansourcefileprecisionfAfferentGraph = svgPanZoom('#sourcefileprecisionfAfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules precision Source Code precision.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module precision !! Precision handling !! !! Select precision of certain parts of the program !! These are set through preprocessor directives. The !! default behavior is to use single-precision variables !! for the values of the orbitals on the grid, the Broyden !! mixing auxiliary arrays, and the O(N) arrays, and !! double precision for the grid function arrays. implicit none integer , parameter :: i8b = selected_int_kind ( 18 ) integer , parameter :: sp = selected_real_kind ( 6 , 30 ) integer , parameter :: dp = selected_real_kind ( 14 , 100 ) #ifdef BROYDEN_DP integer , parameter :: broyden_p = dp #else integer , parameter :: broyden_p = sp #endif #ifdef GRID_SP integer , parameter :: grid_p = sp integer , parameter :: phi_grid_p = sp #elif defined(GRID_DP) integer , parameter :: grid_p = dp integer , parameter :: phi_grid_p = dp #else integer , parameter :: grid_p = dp integer , parameter :: phi_grid_p = sp #endif #ifdef ON_DP integer , parameter :: on_p = dp #else integer , parameter :: on_p = sp #endif ! For future use in trying to limit the memory usage by ! reducing precision in Transiesta ! Preprocessor flag could be: ! dp = TRANSIESTA_DP ! sp = TRANSIESTA_SP integer , parameter :: ts_p = dp public end module precision","tags":"","loc":"sourcefile/precision.f.html","title":"precision.F – SIESTA"},{"text":"This file depends on sourcefile~~units.f90~~EfferentGraph sourcefile~units.f90 units.f90 sourcefile~precision.f precision.F sourcefile~units.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~units.f90~~AfferentGraph sourcefile~units.f90 units.f90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~units.f90 sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~state_analysis.f->sourcefile~units.f90 sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~compute_dm.f->sourcefile~units.f90 sourcefile~state_init.f->sourcefile~units.f90 sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~units.f90 sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~compute_energies.f90->sourcefile~dhscf.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules units Source Code units.f90 Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module units !! Define various unit conversion factors from internal units. !! !! Internally, siesta works with: !! !! * length: Bohr !! * energy: Rydberg. !! * time: femtosecond !! !! The easy way to make sense of units conversion: !! !! * `real(dp), parameter :: Bohr   = 1.0_dp` !! * `real(dp), parameter :: Rydberg = 1.0_dp` !! * `real(dp), parameter :: Femtosecond = 1.0_dp` !! !! * Ang = Bohr / 0.529177 !! * eV = Rydberg / 13.60580 !! * Joule = eV / 1.6e-19_dp !! * Meter = Ang / 1.0e-10_dp !! * Pascal = Joule/Meter**2 !! * kBar  = Pascal * 1.0e4 !! * Ryd&#94;-1 (time) = fs/0.04837769 !! * .... and so on. use precision , only : dp implicit none real ( dp ), parameter :: Ang = 1._dp / 0.529177_dp real ( dp ), parameter :: eV = 1._dp / 1 3.60580_dp real ( dp ), parameter :: kBar = 1._dp / 1.47108e5_dp real ( dp ), parameter :: GPa = kBar * 10 real ( dp ), parameter :: Kelvin = eV / 1160 4.45_dp real ( dp ), parameter :: Debye = 0.393430_dp real ( dp ), parameter :: amu = 2.133107_dp real ( dp ), parameter :: Ryd_time = 1._dp / 0.04837769_dp real ( dp ), parameter :: pi = 3.14159265358979323846264338327950288419716939937510_dp !!  \\pi  to 50 digits real ( dp ), parameter :: deg = pi / 18 0.0_dp end module units","tags":"","loc":"sourcefile/units.f90.html","title":"units.f90 – SIESTA"},{"text":"Contents Subroutines CROSS Source Code cross.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! ! $Id: cross.f,v 1.2 1999/01/31 10:53:49 emilio Exp $ SUBROUTINE CROSS ( A , B , AXB ) !! author: J.M.Soler !! !! Finds the cross product AxB of vectors A and B IMPLICIT NONE DOUBLE PRECISION A ( 3 ), B ( 3 ), AXB ( 3 ) AXB ( 1 ) = A ( 2 ) * B ( 3 ) - A ( 3 ) * B ( 2 ) AXB ( 2 ) = A ( 3 ) * B ( 1 ) - A ( 1 ) * B ( 3 ) AXB ( 3 ) = A ( 1 ) * B ( 2 ) - A ( 2 ) * B ( 1 ) END","tags":"","loc":"sourcefile/cross.f.html","title":"cross.f – SIESTA"},{"text":"This file depends on sourcefile~~m_iorho.f~~EfferentGraph sourcefile~m_iorho.f m_iorho.F sourcefile~sys.f sys.F sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~parallel.f parallel.F sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~precision.f precision.F sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~sys.f->sourcefile~parallel.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_iorho.f~~AfferentGraph sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_iorho Source Code m_iorho.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module m_iorho !! To have both reading and writing, allowing a change in the !! values of variables used as array dimensions, is not very !! safe. Thus, the old iorho has been split in three routines: !! !! 1. [[write_rho(proc)]]: Writes grid magnitudes to file !! 2. [[read_rho(proc)]]: Reads grid magnitudes from file !! 3. [[check_rho(proc)]]: Checks the appropriate dimensions for reading. !! !! The magnitudes are saved in `SINGLE PRECISION` (sp), regardless !! of the internal precision used in the program. Historically, !! the internal precision has been \"single\". use precision , only : dp , grid_p , sp use parallel , only : Node , ProcessorY use sys , only : die use alloc , only : re_alloc , de_alloc #ifdef MPI use mpi_siesta use parallel , only : Nodes use parallelsubs , only : HowManyMeshPerNode #endif implicit          none character ( len =* ), parameter :: fform = \"unformatted\" public :: write_rho , read_rho , check_rho private CONTAINS subroutine write_rho ( fname , cell , mesh , nsm , maxp , nspin , rho ) !! author: J.Soler !! date: July 1997 !! !! Writes the electron density or potential at the mesh points. !! !! Parallel modifications added, while maintaining independence !! of density matrix on disk from parallel distribution. Uses a !! block distribution for density matrix. It is important to !! remember that the density matrix is divided so that all !! sub-points reside on the same node. !! !! Modified by J.D.Gale March 1999. !! !!@note !! In order to achieve a consistent format of the disk file !! each record in the unformatted file corresponds to one pair of !! Y and Z values. Hence there will be a total of mesh(2) x mesh(3) !! records. !!@endnote character ( len =* ), intent ( in ) :: fname !! File name integer , intent ( in ) :: nsm !! Number of sub-mesh points per mesh point !! (not used in this version) integer , intent ( in ) :: mesh ( 3 ) !! Number of mesh divisions of each lattice vector integer , intent ( in ) :: maxp integer , intent ( in ) :: nspin real ( grid_p ), intent ( in ) :: rho ( maxp , nspin ) real ( dp ), intent ( in ) :: cell ( 3 , 3 ) external io_assign , io_close , memory ! Internal variables and arrays integer i , ip , iu , is , j , np , . BlockSizeY , BlockSizeZ , ProcessorZ , . meshnsm ( 3 ), NRemY , NRemZ , . iy , iz , izm , Ind , ir #ifdef MPI integer Ind2 , MPIerror , Request , . Status ( MPI_Status_Size ), BNode , NBlock real ( grid_p ), pointer :: bdens (:) => null () #endif real ( sp ), pointer :: temp (:) => null () #ifdef MPI #ifdef DEBUG call write_debug ( 'ERROR: write_rho not ready yet' ) call write_debug ( fname ) #endif ! Work out density block dimensions if ( mod ( Nodes , ProcessorY ). gt . 0 ) $ call die ( 'ERROR: ProcessorY must be a factor of the' // $ ' number of processors!' ) ProcessorZ = Nodes / ProcessorY BlockSizeY = (((( mesh ( 2 ) / nsm ) - 1 ) / ProcessorY ) + 1 ) * nsm call re_alloc ( bdens , 1 , BlockSizeY * mesh ( 1 ), 'bdens' , 'write_rho' ) #else ProcessorZ = 1 #endif call re_alloc ( temp , 1 , mesh ( 1 ), 'temp' , 'write_rho' ) ! Open file if ( Node . eq . 0 ) then call io_assign ( iu ) open ( iu , file = fname , form = fform , status = 'unknown' ) endif np = mesh ( 1 ) * mesh ( 2 ) * mesh ( 3 ) meshnsm ( 1 ) = mesh ( 1 ) / nsm meshnsm ( 2 ) = mesh ( 2 ) / nsm meshnsm ( 3 ) = mesh ( 3 ) / nsm ! Write data if ( Node . eq . 0 ) then if ( fform . eq . 'formatted' ) then do i = 1 , 3 write ( iu , * ) ( cell ( j , i ), j = 1 , 3 ) enddo write ( iu , * ) mesh , nspin else write ( iu ) cell write ( iu ) mesh , nspin endif endif ! Outer loop over spins do is = 1 , nspin Ind = 0 ! Loop over Z dimension of processor grid do iz = 1 , ProcessorZ BlockSizeZ = ( meshnsm ( 3 ) / ProcessorZ ) NRemZ = meshnsm ( 3 ) - BlockSizeZ * ProcessorZ if ( iz - 1. lt . NRemZ ) BlockSizeZ = BlockSizeZ + 1 BlockSizeZ = BlockSizeZ * nsm ! Loop over local Z mesh points do izm = 1 , BlockSizeZ ! Loop over blocks in Y mesh direction do iy = 1 , ProcessorY ! Work out size of density sub-matrix to be transfered BlockSizeY = ( meshnsm ( 2 ) / ProcessorY ) NRemY = meshnsm ( 2 ) - BlockSizeY * ProcessorY if ( iy - 1. lt . NRemY ) BlockSizeY = BlockSizeY + 1 BlockSizeY = BlockSizeY * nsm #ifdef MPI NBlock = BlockSizeY * mesh ( 1 ) ! Work out which node block is stored on BNode = ( iy - 1 ) * ProcessorZ + iz - 1 if ( BNode . eq . 0. and . Node . eq . BNode ) then #endif ! If density sub-matrix is local Node 0 then just write it out if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY write ( iu , '(e15.6)' ) ( rho ( Ind + ip , is ), . ip = 1 , mesh ( 1 )) Ind = Ind + mesh ( 1 ) enddo else !AG** do ir = 1 , BlockSizeY temp ( 1 : mesh ( 1 )) = $ real ( rho ( Ind + 1 : Ind + mesh ( 1 ), is ), kind = sp ) write ( iu ) temp ( 1 : mesh ( 1 )) Ind = Ind + mesh ( 1 ) enddo endif #ifdef MPI elseif ( Node . eq . 0 ) then ! If this is Node 0 then recv and write density sub-matrix call MPI_IRecv ( bdens , NBlock , MPI_grid_real , BNode , 1 , . MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) elseif ( Node . eq . BNode ) then ! If this is the Node where the density sub-matrix is, then send call MPI_ISend ( rho ( Ind + 1 , is ), NBlock , MPI_grid_real , 0 , 1 , . MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) Ind = Ind + NBlock endif if ( BNode . ne . 0 ) then call MPI_Barrier ( MPI_Comm_World , MPIerror ) if ( Node . eq . 0 ) then Ind2 = 0 if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY write ( iu , '(e15.6)' ) ( bdens ( Ind2 + ip ), ip = 1 , . mesh ( 1 )) Ind2 = Ind2 + mesh ( 1 ) enddo else !AG** do ir = 1 , BlockSizeY temp ( 1 : mesh ( 1 )) = $ real ( bdens ( Ind2 + 1 : Ind2 + mesh ( 1 )), kind = sp ) write ( iu ) temp ( 1 : mesh ( 1 )) Ind2 = Ind2 + mesh ( 1 ) enddo endif endif endif #endif enddo enddo enddo enddo if ( Node . eq . 0 ) then ! Close file call io_close ( iu ) endif #ifdef MPI C Deallocate density buffer memory call de_alloc ( bdens , 'bdens' , 'write_rho' ) #endif call de_alloc ( temp , 'temp' , 'write_rho' ) end subroutine write_rho subroutine read_rho ( fname , cell , mesh , nsm , maxp , nspin , rho ) !! author: J.Soler !! date: July 1997 !! !! Reads the electron density or potential at the mesh points. !! !! If the values of maxp or nspin on input are less than !! those required to copy the array rho from the file, the subroutine !! stops. Use subroutine [[check_rho(proc)]] !! to find the right `maxp` and `nspin`. !! !!@note !! AG: The magnitudes are saved in SINGLE PRECISION (sp), regardless !!     of the internal precision used in the program. Historically, !!     the internal precision has been \"single\". !!@endnote !! !! Parallel modifications added, while maintaining independence !! of density matrix on disk from parallel distribution. Uses a !! block distribution for density matrix. It is important to !! remember that the density matrix is divided so that all !! sub-points reside on the same node. !! !! Modified by J.D.Gale March 1999. !! !!@note !! In order to achieve a consistent format of the disk file !! each record in the unformatted file corresponds to one pair of !! Y and Z values. Hence there will be a total of mesh(2) x mesh(3) !! records. !!@endnote character ( len =* ), intent ( in ) :: fname !! File name for input or output integer , intent ( in ) :: nsm !! Number of sub-mesh points per mesh point !! (not used in this version) integer , intent ( in ) :: maxp !! First dimension of array rho integer , intent ( in ) :: nspin !! Second dimension of array rho integer , intent ( out ) :: mesh ( 3 ) !! Number of mesh divisions of each lattice vector real ( grid_p ), intent ( out ) :: rho ( maxp , nspin ) !! Electron density real ( dp ), intent ( out ) :: cell ( 3 , 3 ) !! Lattice vectors external io_assign , io_close , memory ! Internal variables and arrays integer ip , iu , is , np , ns , . BlockSizeY , BlockSizeZ , ProcessorZ , . meshnsm ( 3 ), npl , NRemY , NRemZ , . iy , iz , izm , Ind , ir #ifdef MPI integer Ind2 , MPIerror , Request , meshl ( 3 ), . Status ( MPI_Status_Size ), BNode , NBlock logical ltmp real ( grid_p ), pointer :: bdens (:) => null () #endif real ( sp ), pointer :: temp (:) => null () logical baddim , found #ifdef MPI ! Work out density block dimensions if ( mod ( Nodes , ProcessorY ). gt . 0 ) $ call die ( 'ERROR: ProcessorY must be a factor of the' // $ ' number of processors!' ) ProcessorZ = Nodes / ProcessorY BlockSizeY = (((( mesh ( 2 ) / nsm ) - 1 ) / ProcessorY ) + 1 ) * nsm call re_alloc ( bdens , 1 , BlockSizeY * mesh ( 1 ), 'bdens' , 'read_rho' ) #else ProcessorZ = 1 #endif ! Check if input file exists if ( Node . eq . 0 ) then inquire ( file = fname , exist = found ) if (. not . found ) call die ( \"Cannot find file \" // trim ( fname )) endif ! Open file if ( Node . eq . 0 ) then call io_assign ( iu ) open ( iu , file = fname , form = fform , status = 'old' ) ! Read cell vectors and number of points if ( fform . eq . 'formatted' ) then read ( iu , * ) cell read ( iu , * ) mesh , ns else read ( iu ) cell read ( iu ) mesh , ns endif endif call re_alloc ( temp , 1 , mesh ( 1 ), 'temp' , 'read_rho' ) #ifdef MPI call MPI_Bcast ( cell ( 1 , 1 ), 9 , MPI_double_precision , 0 , . MPI_Comm_World , MPIerror ) call MPI_Bcast ( mesh , 3 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) call MPI_Bcast ( ns , 1 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) #endif np = mesh ( 1 ) * mesh ( 2 ) * mesh ( 3 ) ! Get local dimensions meshnsm ( 1 ) = mesh ( 1 ) / nsm meshnsm ( 2 ) = mesh ( 2 ) / nsm meshnsm ( 3 ) = mesh ( 3 ) / nsm #ifdef MPI call HowManyMeshPerNode ( meshnsm , Node , Nodes , npl , meshl ) #else npl = np #endif ! Check dimensions baddim = . false . if ( ns . gt . nspin ) baddim = . true . if ( npl . gt . maxp ) baddim = . true . #ifdef MPI ! Globalise dimension check call MPI_AllReduce ( baddim , ltmp , 1 , MPI_logical , MPI_Lor , . MPI_Comm_World , MPIerror ) baddim = ltmp #endif if ( baddim ) then call die ( \"Dimensions of array rho too small in read_rho\" ) endif ! Outer loop over spins do is = 1 , ns Ind = 0 ! Loop over Z mesh direction do iz = 1 , ProcessorZ ! Work out number of mesh points in Z direction BlockSizeZ = ( meshnsm ( 3 ) / ProcessorZ ) NRemZ = meshnsm ( 3 ) - BlockSizeZ * ProcessorZ if ( iz - 1. lt . NRemZ ) BlockSizeZ = BlockSizeZ + 1 BlockSizeZ = BlockSizeZ * nsm ! Loop over local Z mesh points do izm = 1 , BlockSizeZ ! Loop over blocks in Y mesh direction do iy = 1 , ProcessorY ! Work out size of density sub-matrix to be transfered BlockSizeY = ( meshnsm ( 2 ) / ProcessorY ) NRemY = meshnsm ( 2 ) - BlockSizeY * ProcessorY if ( iy - 1. lt . NRemY ) BlockSizeY = BlockSizeY + 1 BlockSizeY = BlockSizeY * nsm #ifdef MPI NBlock = BlockSizeY * mesh ( 1 ) ! Work out which node block is stored on BNode = ( iy - 1 ) * ProcessorZ + iz - 1 if ( BNode . eq . 0. and . Node . eq . BNode ) then #endif ! If density sub-matrix is local Node 0 then just read it in if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY read ( iu , * ) ( rho ( Ind + ip , is ), ip = 1 , mesh ( 1 )) Ind = Ind + mesh ( 1 ) enddo else ! ! AG**: Check sizes of records here -- grid-precision issues !       Either agree on a \"single\" format for file storage, !       or put in some intelligence in all post-processors do ir = 1 , BlockSizeY read ( iu ) temp ( 1 : mesh ( 1 )) rho ( Ind + 1 : Ind + mesh ( 1 ), is ) = $ real ( temp ( 1 : mesh ( 1 )), kind = grid_p ) Ind = Ind + mesh ( 1 ) enddo endif #ifdef MPI elseif ( Node . eq . 0 ) then C If this is Node 0 then read and send density sub-matrix Ind2 = 0 if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY read ( iu , * ) ( bdens ( Ind2 + ip ), ip = 1 , mesh ( 1 )) Ind2 = Ind2 + mesh ( 1 ) enddo else !AG** do ir = 1 , BlockSizeY read ( iu ) temp ( 1 : mesh ( 1 )) bdens ( Ind2 + 1 : Ind2 + mesh ( 1 )) = $ real ( temp ( 1 : mesh ( 1 )), kind = grid_p ) Ind2 = Ind2 + mesh ( 1 ) enddo endif call MPI_ISend ( bdens , NBlock , MPI_grid_real , BNode , 1 , . MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) elseif ( Node . eq . BNode ) then ! If this is the Node where the density sub-matrix is, then receive call MPI_IRecv ( rho ( Ind + 1 , is ), NBlock , MPI_grid_real , . 0 , 1 , MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) Ind = Ind + NBlock endif if ( BNode . ne . 0 ) then call MPI_Barrier ( MPI_Comm_World , MPIerror ) endif #endif enddo enddo enddo enddo ! Close file if ( Node . eq . 0 ) then call io_close ( iu ) endif #ifdef MPI ! Deallocate density buffer memory call de_alloc ( bdens , 'bdens' , 'read_rho' ) #endif call de_alloc ( temp , 'temp' , 'read_rho' ) end subroutine read_rho subroutine check_rho ( fname , maxp , nspin , nsm , found , overflow ) character ( len =* ), intent ( in ) :: fname integer , intent ( in ) :: nsm !! Number of sub-mesh points per mesh point !! (not used in this version) integer , intent ( inout ) :: maxp !! Required first dimension of array rho, !! equal to `mesh(1)*mesh(2)*mesh(3)` integer , intent ( inout ) :: nspin !! Number of spin polarizations (1 or 2) logical , intent ( out ) :: found !! Were data found? logical , intent ( out ) :: overflow !! True if `maxp` or `nspin` were changed real ( dp ) :: cell ( 3 , 3 ) integer :: mesh ( 3 ) integer :: meshnsm ( 3 ), npl , ns , iu , np , npmax #ifdef MPI integer :: meshl ( 3 ) logical :: ltmp integer :: MPIerror #endif ! Check if input file exists if ( Node . eq . 0 ) then inquire ( file = fname , exist = found ) endif #ifdef MPI call MPI_Bcast ( found , 1 , MPI_logical , 0 , MPI_Comm_World , MPIerror ) #endif if (. not . found ) return ! Open file if ( Node . eq . 0 ) then call io_assign ( iu ) open ( iu , file = fname , form = fform , status = 'old' ) ! Read cell vectors and number of points if ( fform . eq . 'formatted' ) then read ( iu , * ) cell read ( iu , * ) mesh , ns else read ( iu ) cell read ( iu ) mesh , ns endif endif #ifdef MPI call MPI_Bcast ( cell ( 1 , 1 ), 9 , MPI_double_precision , 0 , . MPI_Comm_World , MPIerror ) call MPI_Bcast ( mesh , 3 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) call MPI_Bcast ( ns , 1 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) #endif np = mesh ( 1 ) * mesh ( 2 ) * mesh ( 3 ) ! Get local dimensions meshnsm ( 1 ) = mesh ( 1 ) / nsm meshnsm ( 2 ) = mesh ( 2 ) / nsm meshnsm ( 3 ) = mesh ( 3 ) / nsm #ifdef MPI call HowManyMeshPerNode ( meshnsm , Node , Nodes , npl , meshl ) #else npl = np #endif ! Check dimensions overflow = . false . if ( ns . gt . nspin ) overflow = . true . if ( npl . gt . maxp ) overflow = . true . #ifdef MPI ! Globalise dimension check call MPI_AllReduce ( overflow , ltmp , 1 , MPI_logical , MPI_Lor , . MPI_Comm_World , MPIerror ) overflow = ltmp #endif if ( overflow ) then ! Some processor has npl > maxp ... #ifdef MPI ! Find largest value of npl call MPI_AllReduce ( npl , npmax , 1 , MPI_integer , MPI_Max , . MPI_Comm_World , MPIerror ) #else npmax = np #endif maxp = npmax nspin = ns endif if ( Node . eq . 0 ) call io_close ( iu ) end subroutine check_rho end module m_iorho","tags":"","loc":"sourcefile/m_iorho.f.html","title":"m_iorho.F – SIESTA"},{"text":"This file depends on sourcefile~~siesta_analysis.f~~EfferentGraph sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~siesta_analysis.f->sourcefile~siesta_options.f90 sourcefile~units.f90 units.f90 sourcefile~siesta_analysis.f->sourcefile~units.f90 sourcefile~dhscf.f dhscf.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~parallel.f parallel.F sourcefile~siesta_analysis.f->sourcefile~parallel.f sourcefile~precision.f precision.F sourcefile~units.f90->sourcefile~precision.f sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~precision.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~sys.f sys.F sourcefile~dhscf.f->sourcefile~sys.f sourcefile~mesh.f mesh.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~delk.f90->sourcefile~parallel.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~atm_types.f atm_types.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~parallel.f sourcefile~forhar.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~forhar.f->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~radial.f->sourcefile~precision.f var pansourcefilesiesta_analysisfEfferentGraph = svgPanZoom('#sourcefilesiesta_analysisfEfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_siesta_analysis Source Code siesta_analysis.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_siesta_analysis use write_subs private public :: siesta_analysis CONTAINS subroutine siesta_analysis ( relaxd ) USE band , only : nbk , bk , maxbk , bands USE writewave , only : nwk , wfk , wwave USE writewave , only : setup_wfs_list , wfs_filename USE m_ksvinit , only : nkpol , kpol , wgthpol use m_ksv USE m_projected_DOS , only : projected_DOS USE m_local_DOS , only : local_DOS #ifdef SIESTA__PEXSI USE m_pexsi_local_DOS , only : pexsi_local_DOS USE m_pexsi_dos , only : pexsi_dos #endif USE siesta_options use units , only : Debye , eV use sparse_matrices , only : maxnh , listh , listhptr , numh use sparse_matrices , only : H , S , Dscf , xijo use siesta_geom use m_dhscf , only : dhscf use atomlist , only : indxuo , iaorb , lastkb , lasto , datm , no_l , & iphkb , no_u , no_s , iza , iphorb , rmaxo , indxua use atomlist , only : qtot use fdf use writewave , only : wwave use siesta_cml use files , only : slabel use files , only : filesOut_t ! derived type for output file names use zmatrix , only : lUseZmatrix , write_zmatrix use kpoint_scf_m , only : kpoints_scf use parallel , only : IOnode use parallel , only : SIESTA_worker use files , only : label_length use m_energies use m_steps , only : final use m_ntm use m_spin , only : nspin , spinor_dim , h_spin_dim use m_spin , only : SpOrb , NonCol , SPpol , NoMagn use m_dipol use m_eo use m_forces , only : fa use m_gamma use alloc , only : re_alloc , de_alloc use basis_enthalpy , only : write_basis_enthalpy use m_partial_charges , only : want_partial_charges use m_iodm_old , only : read_spmatrix use m_siesta2wannier90 , only : siesta2wannier90 use m_mpi_utils , only : barrier #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_ANALYSIS #endif implicit none logical :: relaxd , getPSI , quenched_MD , found real ( dp ) :: dummy_str ( 3 , 3 ) real ( dp ) :: dummy_strl ( 3 , 3 ) real ( dp ) :: qspin ( 4 ) ! Local real ( dp ) :: polxyz ( 3 , nspin ) ! Autom., small real ( dp ) :: polR ( 3 , nspin ) ! Autom., small real ( dp ) :: qaux real ( dp ), pointer :: ebk (:,:,:) ! Band energies integer :: j , ix , ind , ik , io , ispin integer :: wfs_band_min , wfs_band_max real ( dp ) :: g2max , current_ef type ( filesOut_t ) :: filesOut ! blank output file names !-----------------------------------------------------------------------BEGIN if ( SIESTA_worker ) call timer ( \"Analysis\" , 1 ) !! Check that we are converged in geometry, !! if strictly required, !! before carrying out any analysis. !!@code quenched_MD = ( ( iquench > 0 ) . and . $ (( idyn . eq . 1 ) . or . ( idyn . eq . 3 )) ) if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( SIESTA_worker ) then ! For timing ops and associated barrier if ( GeometryMustConverge . and . (. not . relaxd )) then call message ( \"FATAL\" , $ \"GEOM_NOT_CONV: Geometry relaxation not converged\" ) call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call barrier () call die ( \"ABNORMAL_TERMINATION\" ) endif endif endif !!@endcode !     All the comments below assume that this compatibility option !     is not used. !     Note also that full compatibility cannot be guaranteed if (. not . compat_pre_v4_dynamics ) then !     This is a sanity safeguard: we reset the geometry (which might !     have been moved by the relaxation or MD routines) to the one used !     in the last computation of the electronic structure. !     See the comments below for explanation !$OMP parallel workshare default(shared) xa ( 1 : 3 , 1 : na_s ) = xa_last ( 1 : 3 , 1 : na_s ) ucell ( 1 : 3 , 1 : 3 ) = ucell_last ( 1 : 3 , 1 : 3 ) scell ( 1 : 3 , 1 : 3 ) = scell_last ( 1 : 3 , 1 : 3 ) !$OMP end parallel workshare endif ! zmatrix info reset?? if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then if ( SIESTA_worker ) then call read_spmatrix ( maxnh , no_l , h_spin_dim , numh , . listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) current_ef = ef ef = fdf_get ( \"Manual-Fermi-Level\" , current_ef , \"Ry\" ) endif endif #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.DOS\" ,. false .)) then call pexsi_dos ( no_u , no_l , spinor_dim , $ maxnh , numh , listhptr , listh , H , S , qtot , ef ) endif #endif ! section done by Siesta subset of nodes if ( SIESTA_worker ) then final = . true . if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'Finalization' ) endif #ifdef SIESTA__FLOOK ! Call lua right before doing the analysis, ! possibly changing some of the variables call slua_call ( LUA , LUA_ANALYSIS ) #endif ! !     NOTE that the geometry output by the following sections !     used to be that \"predicted\" for the next MD or relaxation step. !     This is now changed ! if ( IOnode ) then ! Print atomic coordinates ! This covers CG and MD-quench (verlet, pr), instances in which ! \"relaxd\" is meaningful if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( relaxd ) then ! xa = xa_last ! The \"relaxation\" routines do not update ! the coordinates if relaxed, so this behavior is unchanged call outcoor ( ucell , xa , na_u , 'Relaxed' , . true . ) else ! Since xa = xa_last now, this will just repeat the ! last set of coordinates used, not the predicted ones. call outcoor ( ucell , xa , na_u , 'Final (unrelaxed)' , . true . ) endif endif ! This call will write xa_last to the .STRUCT_OUT file ! (again, since it has already been written by state_init), ! CML records of the latest processed structure, and ! possibly zmatrix info.  *** unmoved?? how? ! Note that the .STRUCT_NEXT_ITER file is produced ! in siesta_move for checkpointing of relaxations and MD runs. ! If all we want are the CML records (to satisfy some expectation ! of appearance in the \"finalization\" section, we might put the ! cml call explicitly and forget about the rest. if ( compat_pre_v4_dynamics ) then call siesta_write_positions ( moved = . true .) else call siesta_write_positions ( moved = . false .) endif ! ??  Clarify Zmatrix behavior **** if ( lUseZmatrix ) call write_Zmatrix ! Print unit cell (cell_last) for variable cell and server operation if ( varcel . or . ( idyn . eq . 8 )) call outcell ( ucell ) !------------------------------------------------------------------ ! It can be argued that these needed the xa_last coordinates ! all along !       Print coordinates in xmol format in a separate file if ( fdf_boolean ( 'WriteCoorXmol' ,. false .)) & call coxmol ( iza , xa , na_u ) !       Print coordinates in cerius format in a separate file if ( fdf_boolean ( 'WriteCoorCerius' ,. false .)) & call coceri ( iza , xa , ucell , na_u , sname ) !       Find interatomic distances (output in file BONDS_FINAL) call bonds ( ucell , na_u , isa , xa , & rmax_bonds , trim ( slabel ) // \".BONDS_FINAL\" ) endif ! IONode !--- end output of geometry information ! ! ! NOTE: In the following sections, wavefunction generation, computation !       of band structure, etc, are carried out using the last Hamiltonian !       generated in the SCF run for the last geometry considered. !   But, if xa /= xa_last, the computation of, say, bands, will use !      H phases which are not the same as those producing the final !      ground-state electronic structure. ! !    Also, since we have removed the replication (superx call) !      of \"moved\" coordinates !      into the auxiliary supercell from 'siesta_move' (recall that it is !      done always in state_init for every new geometry), the \"moved unit !      cell coordinates\" could coexist here with \"unmoved non-unit cell SC coords\", !      which is wrong. !      For all of the above, we should put here a sanity safeguard !        (if we have not done so already at the top of this routine) !        xa(1:3,1:na_s) = xa_last(1:3,1:na_s) !        ucell(1:3,1:3) = ucell_last(1:3,1:3) !        scell(1:3,1:3) = scell_last(1:3,1:3) !        DM and H issues ! !        Some of the routines that follow use H and S, and some use the DM. !        Those which use the DM should work with the final \"out\" DM for !        consistency. !        Those which use H,S should work with the latest diagonalized H,S pair. ! !      If mixing the DM during the scf loop we should avoid mixing it one more time !        after convergence (or restoring Dold) !        If mixing H, we should avoid mixing it one more time !        after convergence (and restoring Hold to have the exact H that generated the !        latest DM, although this is probably too much). !        See the logic in siesta_forces. !     Find and print wavefunctions at selected k-points !   This uses H,S, and xa if ( nwk . gt . 0 ) then wfs_filename = trim ( slabel ) // \".selected.WFSX\" if ( IONode ) print \"(a)\" , $ \"Writing WFSX for selected k-points in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , & nwk , & numh , listhptr , listh , H , S , Ef , xijo , indxuo , & nwk , wfk , no_u , gamma , occtol ) endif !   This uses H,S, and xa if ( write_coop ) then ! Output the wavefunctions for the kpoints in the SCF set ! Note that we allow both a band number and an energy range ! The user is responsible for using appropriate values. wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( kpoints_scf % N , no_u , & wfs_band_min , wfs_band_max , $ use_scf_weights = . true ., $ use_energy_window = . true .) wfs_filename = trim ( slabel ) // \".fullBZ.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for COOP/COHP in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . kpoints_scf % N , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . kpoints_scf % N , kpoints_scf % k , no_u , gamma , occtol ) endif !     Compute bands !   This uses H,S, and xa nullify ( ebk ) call re_alloc ( ebk , 1 , no_u , 1 , spinor_dim , 1 , maxbk , & 'ebk' , 'siesta_analysis' ) if ( nbk . gt . 0 ) then if ( IONode ) print \"(a)\" , \"Computing bands...\" getPSI = fdf_get ( 'WFS.Write.For.Bands' , . false .) if ( getPSI ) then wfs_filename = trim ( slabel ) // \".bands.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for bands in \" $ // trim ( wfs_filename ) wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nbk , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . false ., $ use_energy_window = . false .) endif call bands ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . maxbk , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . . true ., nbk , bk , ebk , occtol , getPSI ) if ( IOnode ) then if ( writbk ) then write ( 6 , '(/,a,/,a4,a12)' ) & 'siesta: Band k vectors (Bohr**-1):' , 'ik' , 'k' do ik = 1 , nbk write ( 6 , '(i4,3f12.6)' ) ik , ( bk ( ix , ik ), ix = 1 , 3 ) enddo endif if ( writb ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Band energies (eV):' , 'ik' , 'is' , 'eps' do ispin = 1 , spinor_dim do ik = 1 , nbk write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin , ( ebk ( io , ispin , ik ) / eV , io = 1 , min ( 10 , no_u )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( ebk ( io , ispin , ik ) / eV , io = 11 , no_u ) enddo enddo endif endif endif !     Print eigenvalues if ( IOnode . and . writeig ) then if (( isolve . eq . SOLVE_DIAGON . or . & (( isolve . eq . SOLVE_MINIM ) . and . minim_calc_eigenvalues )) & . and . no_l . lt . 1000 ) then if ( h_spin_dim <= 2 ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Eigenvalues (eV):' , 'ik' , 'is' , 'eps' do ik = 1 , kpoints_scf % N do ispin = 1 , spinor_dim write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin ,( eo ( io , ispin , ik ) / eV , io = 1 , min ( 10 , neigwanted )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( eo ( io , ispin , ik ) / eV , io = 11 , neigwanted ) enddo enddo else write ( 6 , '(/,a)' ) 'siesta: Eigenvalues (eV):' do ik = 1 , kpoints_scf % N write ( 6 , '(a,i6)' ) 'ik =' , ik write ( 6 , '(10f7.2)' ) & (( eo ( io , ispin , ik ) / eV , io = 1 , neigwanted ), ispin = 1 , 2 ) enddo endif write ( 6 , '(a,f15.6,a)' ) 'siesta: Fermi energy =' , ef / eV , ' eV' endif endif if ((( isolve . eq . SOLVE_DIAGON ). or . & (( isolve . eq . SOLVE_MINIM ). and . minim_calc_eigenvalues )) & . and . IOnode ) & call ioeig ( eo , ef , neigwanted , nspin , kpoints_scf % N , & no_u , spinor_dim , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w ) !   This uses H,S, and xa, as it diagonalizes them again call projected_DOS () !     Print program's energy decomposition and final forces if ( IOnode ) then call siesta_write_energies ( iscf = 0 , dDmax = 0._dp , dHmax = 0._dp ) ! final == .true. which makes the step counter irrelevant call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () call write_basis_enthalpy ( FreeE , FreeEHarris ) endif ! NOTE: Here, the spin polarization is computed using Dscf, which is !       a density matrix obtained after mixing the \"in\" and \"out\" !       DMs of the SCF run for the last geometry considered. !       This can be considered a feature or a bug. call print_spin ( qspin ) ! qspin returned for use below !     This uses the last computed dipole in dhscf during the scf cycle, !     which is in fact derived from the \"in\" DM. !     Perhaps this section should be moved after the call to dhscf below !     AND use the DM_out of the last step (but there might not be a call !     to dhscf if there are no files to output, and the computation of the !     charge density is expensive... !     Print electric dipole if ( shape . ne . 'bulk' ) then if ( IOnode ) then write ( 6 , '(/,a,3f12.6)' ) & 'siesta: Electric dipole (a.u.)  =' , dipol write ( 6 , '(a,3f12.6)' ) & 'siesta: Electric dipole (Debye) =' , & ( dipol ( ix ) / Debye , ix = 1 , 3 ) endif if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = dipol / Debye , & title = 'Electric dipole' , dictref = 'siesta:dipol' , . units = 'siestaUnits:Debye' ) endif !cml_p endif ! NOTE: The use of *_last geometries in the following sections !       guarantees that the analysis of the electronic structure !       is done for the geometry for which it was computed. !  BUT these routines need H,S, so H should not be mixed after !       convergence. !     Calculation of the bulk polarization using the Berry phase !     formulas by King-Smith and Vanderbilt if ( nkpol . gt . 0 . and . . not . bornz ) then if ( NonCol . or . SpOrb ) then if ( IOnode ) then write ( 6 , '(/a)' ) . 'siesta_analysis: bulk polarization implemented only for' write ( 6 , '(/a)' ) . 'siesta_analysis: paramagnetic or collinear spin runs' endif else call KSV_pol ( na_u , na_s , xa_last , rmaxo , scell_last , & ucell_last , no_u , no_l , no_s , nspin , qspin , & maxnh , nkpol , numh , listhptr , listh , & H , S , xijo , indxuo , isa , iphorb , & iaorb , lasto , shape , & nkpol , kpol , wgthpol , polR , polxyz ) endif endif !     Calculation of the optical conductivity call optical ( na_u , na_s , xa_last , scell_last , ucell_last , & no_u , no_l , no_s , nspin , qspin , & maxnh , numh , listhptr , listh , H , S , xijo , $ indxuo , ebk , ef , temp , & isa , iphorb , iphKB , lasto , lastkb , shape ) call de_alloc ( ebk , 'ebk' , 'siesta_analysis' ) !................................... ! !  NOTE: Dscf here might be the mixed one (see above). ! want_partial_charges = ( hirshpop . or . voropop ) . AND . $ (. not . partial_charges_at_every_geometry ) !     Save electron density and potential if ( saverho . or . savedrho . or . saverhoxc . or . & savevh . or . savevt . or . savevna . or . & savepsch . or . savetoch . or . & save_ebs_dens . or . & want_partial_charges ) then if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( save_ebs_dens ) filesOut % ebs_dens = trim ( slabel ) // '.EBS' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' g2max = g2cut dummy_str = 0.0 dummy_strl = 0.0 call dhscf ( nspin , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa_last , indxua , & ntm , 0 , 0 , 0 , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_str , fa , dummy_strl ) ! next to last argument is dummy here, ! as no forces are calculated ! todo: make all these optional endif C C     Call the wannier90 interface here, as local_DOS destroys the DM... C if ( w90_processing ) call siesta2wannier90 () C     Find local density of states !  It needs H,S, and xa, as it diagonalizes them again !  NOTE: This call will obliterate Dscf !  It is better to put a explicit out argument for the partial DM computed. call local_DOS () ! In summary, it is better to: ! !   -- Avoid (or warn the user about) doing any analysis if the calculation is not converged !   -- Avoid mixing DM or H after scf convergence !   -- Set xa to xa_last at the top of this file. Write any \"next iter\" coordinate file !      in 'siesta_move' endif ! SIESTA_worker #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.LDOS\" ,. false .)) then call pexsi_local_DOS () endif #endif if ( SIESTA_worker ) call timer ( \"Analysis\" , 2 ) !------------------------------------------------------------------------- END END subroutine siesta_analysis END module m_siesta_analysis","tags":"","loc":"sourcefile/siesta_analysis.f.html","title":"siesta_analysis.F – SIESTA"},{"text":"This file depends on sourcefile~~m_planewavematrixvar.f90~~EfferentGraph sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~precision.f precision.F sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~m_planewavematrixvar.f90~~AfferentGraph sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90 delk.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_planewavematrixvar Source Code m_planewavematrixvar.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_planewavematrixvar !! author: J. Junquera !! date: June-July 2013 !! !! In this module we define all the variables required !! to compute the matrix elements of a plane wave in a basis of !! numerical atomic orbitals. use precision , only : dp implicit none complex ( dp ), pointer , save :: delkmatgen (:,:) !! The matrix elements of a planewave are not !! self-consistent. They can be computed !! once for a given k-point and stored in !! delkmatgen. !! !! The first index refers to the number of !! different k-points for which the planewave !! will be computed. !! !! The second index is the index of the !! sparse matrix. !! !! This pointer has to be allocated in !! the calling routine. complex ( dp ), pointer , save :: delkmat (:) !! Matrix elements of a plane wave !! <\\phi_{\\mu}|e&#94;{( \\boldsymbol{isigneikr} \\: \\cdot \\: !! i \\: \\cdot \\: \\vec{kptpw} \\: \\cdot \\: \\vec{r} )}|\\phi_{\\nu}> !! !! This pointer has to be allocated in the calling routine. !! !! This is a sparse matrix, whose structure !! follows the same scheme as the !! hamiltonian or overlap matrix elements. real ( dp ) :: wavevector ( 3 ) !! Wave vector of the plane wave. endmodule m_planewavematrixvar","tags":"","loc":"sourcefile/m_planewavematrixvar.f90.html","title":"m_planewavematrixvar.F90 – SIESTA"},{"text":"This file depends on sourcefile~~compute_energies.f90~~EfferentGraph sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~compute_energies.f90->sourcefile~siesta_options.f90 sourcefile~precision.f precision.F sourcefile~compute_energies.f90->sourcefile~precision.f sourcefile~parallel.f parallel.F sourcefile~compute_energies.f90->sourcefile~parallel.f sourcefile~dhscf.f dhscf.F sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~compute_energies.f90->sourcefile~m_rhog.f90 sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~dhscf.f->sourcefile~precision.f sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~sys.f sys.F sourcefile~dhscf.f->sourcefile~sys.f sourcefile~units.f90 units.f90 sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~mesh.f mesh.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~parallel.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~atm_types.f atm_types.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~units.f90->sourcefile~precision.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~parallel.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~forhar.f->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~radial.f->sourcefile~precision.f var pansourcefilecompute_energiesf90EfferentGraph = svgPanZoom('#sourcefilecompute_energiesf90EfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~compute_energies.f90~~AfferentGraph sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_compute_energies Source Code compute_energies.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module m_compute_energies implicit none public :: compute_energies CONTAINS subroutine compute_energies ( iscf ) !! This routine computes the Harris energy from E_KS(DM_in): !! !!@todo !! Edit formulas !!@endtodo !! !!    E_Harris = E_KS(DM_in) + Tr[H_in*(DM_out-DM_in)] !! !! and possibly E_KS(DM_out) as !! !!    E_KS(DM_out) = Tr[H_in*DM_out] + E_HXC(DM_out) !! !! Note that E_KS(DM_in), as computed in setup_hamiltonian, is not !! variational if mixing the DM, as the kinetic energy term is !! using a DM (DM_in) which is not derived from wave-functions. It !! is also not consistent if we are using the charge density as !! primary variable for mixing. In this case the Enl and Ekin parts !! are computed with the previous step's DM, whereas the \"scf\" part !! comes from the (mixed) rho. !! !! E_KS(DM_out) computed in setup_hamiltonian with DM_out is !! variational. Rather than calling again setup_hamiltonian, it is !! enough to compute the \"scf\" part of the energy by calling dhscf !! with DM_out. !! !! The routine checks the mixing type and proceeds accordingly use precision , only : dp use fdf , only : fdf_get use siesta_options , only : g2cut , Temp use siesta_options , only : mix_charge , mixH use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H use sparse_matrices , only : Dscf , Dold use m_dhscf , only : dhscf use m_energies use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use m_ntm , only : ntm use m_spin , only : spin use parallel , only : IONode use m_dipol , only : dipol use siesta_geom , only : na_u , na_s , xa , isa use m_rhog , only : rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif integer , intent ( in ) :: iscf integer :: ispin , io #ifdef MPI real ( dp ) :: buffer1 #endif logical :: mixDM mixDM = (. not . ( mixH . or . mix_charge )) !     Compute the band-structure energy call compute_EBS () ! These energies were calculated in the latest call to ! setup_hamiltonian, using as ingredient D_in ! Ecorrec comes from O(N)... ! DUext is the energy of the charge from DM_in in a field. ! Emad, Emm, Emeta are extra terms that are added for ! consistency of the total energy. ! E0 = Ena + Ekin + Enl + Eso - Eions call update_DEna () call update_Etot () ! Harris energy ! It does not make sense if mixing the charge density, as there is no DM_in ! If mixing the Hamiltonian its usefulness is questionable also. if ( mix_charge ) then ! possibly add mixH here EHarrs = 0.0_dp else call compute_DEharr () Eharrs = Etot + DEharr endif ! Possible correction to Etot if mixing the DM. This is purely ! cosmetic, to show uniformly converged values during the scf ! cycle. The final energy will be completely correct if the DM is not ! mixed after scf convergence. ! The correction is mandatory if mixing the charge density. In this ! case the call to dhscf is needed to generate rho(G)_out ! If mixing H the KS energy is already variational, as it is ! computed with DM_out if ( mixDM ) then if ( fdf_get ( \"SCF.Want.Variational.EKS\" ,. false .)) then call compute_correct_EKS () endif else if ( mixH ) then ! not needed else if ( mix_charge ) then call compute_correct_EKS () endif call update_FreeE ( Temp ) CONTAINS subroutine compute_EBS () real ( dp ) :: Ebs_SO ( 4 ) complex ( dp ) :: Ebs_Daux ( 2 , 2 ), Ebs_Haux ( 2 , 2 ) integer :: i , j , ind Ebs = 0.0_dp ! Modifed for Off-Site Spin-orbit coupling by R. Cuadrado, Feb. 2018 ! !***************************************************************************** !  Note about Ebs and E_Harris calculation for the Spin-Orbit: !***************************************************************************** ! !  E_bs and E_Harris are calculated by means of the following ! complex matrix multiplication: ! !                 E_bs = Re { Tr[ H * DM ] } !             E_Harris = Re { Tr[ H * (DM-DM_old) ] } ! !  In the following: DM/H(1,1) --> up/up     <--> uu !                    DM/H(2,2) --> down/down <--> dd !                    DM/H(1,2) --> up/down   <--> ud !                    DM/H(2,1) --> down/up   <--> du ! !  Using DM/H components, E_bs would be sum(E_bs(1:4)), where ! !   E_bs(1)=Re{sum_ij(H_ij(1,1)*D_ji(1,1))}=Re{sum_ij(H_ij&#94;uu*(DM_ij&#94;uu)&#94;*)} !   E_bs(2)=Re{sum_ij(H_ij(2,2)*D_ji(2,2))}=Re{sum_ij(H_ij&#94;dd*(DM_ij&#94;dd)&#94;*)} !   E_bs(3)=Re{sum_ij(H_ij(1,2)*D_ji(2,1))}=Re{sum_ij(H_ij&#94;ud*(DM_ij&#94;ud)&#94;*)} !   E_bs(4)=Re{sum_ij(H_ij(2,1)*D_ji(1,2))}=Re{sum_ij(H_ij&#94;du*(DM_ij&#94;du)&#94;*)} ! !         since, due to overall hermiticity,  DM_ij&#94;ab = (DM_ji&#94;ba)&#94;* ! !   The trace operation is then an extended dot product over the \"ij\" !   sparse index, which can also be conveniently done in parallel, as !   each processor handles the same indexes in H and the DM. Only a !   global reduction is needed at the end. !  Same comments are valid for the E_Harris calculation. ! !***************************************************************************** ! if ( spin % SO ) then Ebs_SO = 0.0_dp Ebs_Daux = dcmplx ( 0.0_dp , 0.0_dp ) Ebs_Haux = dcmplx ( 0.0_dp , 0.0_dp ) do io = 1 , maxnh Ebs_Haux ( 1 , 1 ) = dcmplx ( H ( io , 1 ), H ( io , 5 )) Ebs_Haux ( 2 , 2 ) = dcmplx ( H ( io , 2 ), H ( io , 6 )) Ebs_Haux ( 1 , 2 ) = dcmplx ( H ( io , 3 ), - H ( io , 4 )) Ebs_Haux ( 2 , 1 ) = dcmplx ( H ( io , 7 ), H ( io , 8 )) Ebs_Daux ( 1 , 1 ) = dcmplx ( Dscf ( io , 1 ), Dscf ( io , 5 )) Ebs_Daux ( 2 , 2 ) = dcmplx ( Dscf ( io , 2 ), Dscf ( io , 6 )) Ebs_Daux ( 1 , 2 ) = dcmplx ( Dscf ( io , 3 ), - Dscf ( io , 4 )) Ebs_Daux ( 2 , 1 ) = dcmplx ( Dscf ( io , 7 ), Dscf ( io , 8 )) Ebs_SO ( 1 ) = Ebs_SO ( 1 ) + real ( Ebs_Haux ( 1 , 1 ) * dconjg ( Ebs_Daux ( 1 , 1 )) ) Ebs_SO ( 2 ) = Ebs_SO ( 2 ) + real ( Ebs_Haux ( 2 , 2 ) * dconjg ( Ebs_Daux ( 2 , 2 )) ) Ebs_SO ( 3 ) = Ebs_SO ( 3 ) + real ( Ebs_Haux ( 1 , 2 ) * dconjg ( Ebs_Daux ( 1 , 2 )) ) Ebs_SO ( 4 ) = Ebs_SO ( 4 ) + real ( Ebs_Haux ( 2 , 1 ) * dconjg ( Ebs_Daux ( 2 , 1 )) ) enddo Ebs = sum ( Ebs_SO ) else if ( spin % NCol ) then do io = 1 , maxnh Ebs = Ebs + H ( io , 1 ) * ( Dscf ( io , 1 ) ) & + H ( io , 2 ) * ( Dscf ( io , 2 ) ) & + 2.0_dp * H ( io , 3 ) * ( Dscf ( io , 3 ) ) & + 2.0_dp * H ( io , 4 ) * ( Dscf ( io , 4 ) ) enddo else if ( spin % Col ) then do io = 1 , maxnh Ebs = Ebs + H ( io , 1 ) * Dscf ( io , 1 ) & + H ( io , 2 ) * Dscf ( io , 2 ) enddo else if ( spin % none ) then do io = 1 , maxnh Ebs = Ebs + H ( io , 1 ) * Dscf ( io , 1 ) enddo endif #ifdef MPI !     Global reduction call globalize_sum ( Ebs , buffer1 ) Ebs = buffer1 #endif end subroutine compute_EBS subroutine compute_DEharr () real ( dp ) :: DEharr_SO ( 4 ) complex ( dp ) :: DEharr_Daux ( 2 , 2 ), DEharr_Haux ( 2 , 2 ), DEharr_Daux_old ( 2 , 2 ) DEharr = 0.0_dp if ( spin % SO ) then DEharr_SO = 0.0_dp DEharr_Daux = dcmplx ( 0.0_dp , 0.0_dp ) DEharr_Daux_old = dcmplx ( 0.0_dp , 0.0_dp ) DEharr_Haux = dcmplx ( 0.0_dp , 0.0_dp ) do io = 1 , maxnh DEharr_Haux ( 1 , 1 ) = dcmplx ( H ( io , 1 ), H ( io , 5 ) ) DEharr_Haux ( 2 , 2 ) = dcmplx ( H ( io , 2 ), H ( io , 6 ) ) DEharr_Haux ( 1 , 2 ) = dcmplx ( H ( io , 3 ), - H ( io , 4 ) ) DEharr_Haux ( 2 , 1 ) = dcmplx ( H ( io , 7 ), H ( io , 8 ) ) DEharr_Daux ( 1 , 1 ) = dcmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ) ) DEharr_Daux ( 2 , 2 ) = dcmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ) ) DEharr_Daux ( 1 , 2 ) = dcmplx ( Dscf ( io , 3 ), - Dscf ( io , 4 ) ) DEharr_Daux ( 2 , 1 ) = dcmplx ( Dscf ( io , 7 ), Dscf ( io , 8 ) ) DEharr_Daux_old ( 1 , 1 ) = dcmplx ( Dold ( io , 1 ), Dold ( io , 5 ) ) DEharr_Daux_old ( 2 , 2 ) = dcmplx ( Dold ( io , 2 ), Dold ( io , 6 ) ) DEharr_Daux_old ( 1 , 2 ) = dcmplx ( Dold ( io , 3 ), - Dold ( io , 4 ) ) DEharr_Daux_old ( 2 , 1 ) = dcmplx ( Dold ( io , 7 ), Dold ( io , 8 ) ) DEharr_SO ( 1 ) = DEharr_SO ( 1 ) & + real ( DEharr_Haux ( 1 , 1 ) * dconjg ( DEharr_Daux ( 1 , 1 ) - DEharr_Daux_old ( 1 , 1 )) ) DEharr_SO ( 2 ) = DEharr_SO ( 2 ) & + real ( DEharr_Haux ( 2 , 2 ) * dconjg ( DEharr_Daux ( 2 , 2 ) - DEharr_Daux_old ( 2 , 2 )) ) DEharr_SO ( 3 ) = DEharr_SO ( 3 ) & + real ( DEharr_Haux ( 1 , 2 ) * dconjg ( DEharr_Daux ( 1 , 2 ) - DEharr_Daux_old ( 1 , 2 )) ) DEharr_SO ( 4 ) = DEharr_SO ( 4 ) & + real ( DEharr_Haux ( 2 , 1 ) * dconjg ( DEharr_Daux ( 2 , 1 ) - DEharr_Daux_old ( 2 , 1 )) ) enddo DEharr = sum ( DEharr_SO ) else if ( spin % NCol ) then do io = 1 , maxnh DEharr = DEharr + H ( io , 1 ) * ( Dscf ( io , 1 ) - Dold ( io , 1 ) ) & + H ( io , 2 ) * ( Dscf ( io , 2 ) - Dold ( io , 2 ) ) & + 2.0_dp * H ( io , 3 ) * ( Dscf ( io , 3 ) - Dold ( io , 3 ) ) & + 2.0_dp * H ( io , 4 ) * ( Dscf ( io , 4 ) - Dold ( io , 4 ) ) enddo elseif ( spin % Col ) then do io = 1 , maxnh DEharr = DEharr + H ( io , 1 ) * ( Dscf ( io , 1 ) - Dold ( io , 1 ) ) & + H ( io , 2 ) * ( Dscf ( io , 2 ) - Dold ( io , 2 ) ) enddo elseif ( spin % none ) then do io = 1 , maxnh DEharr = DEharr + H ( io , 1 ) * ( Dscf ( io , 1 ) - Dold ( io , 1 ) ) enddo endif #ifdef MPI !     Global reduction of DEharr call globalize_sum ( DEharr , buffer1 ) DEharr = buffer1 #endif end subroutine compute_DEharr subroutine compute_correct_EKS () use files , only : filesOut_t ! derived type for output file names use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_on_2D , H_so_off_2D type ( filesOut_t ) :: filesOut ! blank output file names real ( dp ), pointer :: H_vkb (:), H_kin (:), H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) complex ( dp ) :: Hc , Dc real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ) real ( dp ) :: dummy_E , g2max , dummy_H ( 1 , 1 ) integer :: ihmat , ifa , istr ! Compute E_KS(DM_out) g2max = g2cut ifa = 0 istr = 0 ihmat = 0 ! Pass DM_out to compute E_HXC(out) ! Remove unwanted arguments... call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , & no_u , na_u , na_s , isa , xa , indxua , & ntm , ifa , istr , ihmat , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , dummy_H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_stress , dummy_fa , dummy_stress ) ! (when mix_charge is true, rhog will contain the output rho(G)) call update_DEna () !     Compute Tr[H_0*DM_out] = Ekin + Enl + Eso with DM_out H_vkb => val ( H_vkb_1D ) H_kin => val ( H_kin_1D ) Ekin = 0.0_dp Enl = 0.0_dp do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) enddo enddo #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 #endif Eso = 0._dp if ( spin % SO_offsite ) then H_so_off => val ( H_so_off_2D ) ! The computation of the trace is different here, as H_so_off has ! a different structure from H and the DM. do io = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ), dp ) Eso = Eso + real ( H_so_off ( io , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ), dp ) Eso = Eso + real ( H_so_off ( io , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( io , 3 ), Dscf ( io , 4 ), dp ) Eso = Eso + real ( H_so_off ( io , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( io , 7 ), - Dscf ( io , 8 ), dp ) Eso = Eso + real ( H_so_off ( io , 3 ) * Dc , dp ) end do else if ( spin % SO_onsite ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so_on => val ( H_so_on_2D ) do io = 1 , maxnh Eso = Eso + H_so_on ( io , 1 ) * Dscf ( io , 7 ) + H_so_on ( io , 2 ) * Dscf ( io , 8 ) & + H_so_on ( io , 5 ) * Dscf ( io , 3 ) + H_so_on ( io , 6 ) * Dscf ( io , 4 ) & - H_so_on ( io , 3 ) * Dscf ( io , 5 ) - H_so_on ( io , 4 ) * Dscf ( io , 6 ) end do end if #ifdef MPI if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 endif #endif ! E0 = Ena + Ekin + Enl + Eso - Eions ! Clarify: Ecorrec (from O(N)) call update_Etot () end subroutine compute_correct_EKS end subroutine compute_energies end module m_compute_energies","tags":"","loc":"sourcefile/compute_energies.f90.html","title":"compute_energies.F90 – SIESTA"},{"text":"This file depends on sourcefile~~rhooda.f~~EfferentGraph sourcefile~rhooda.f rhooda.F sourcefile~mesh.f mesh.F sourcefile~rhooda.f->sourcefile~mesh.f sourcefile~precision.f precision.F sourcefile~rhooda.f->sourcefile~precision.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~rhooda.f->sourcefile~atmfuncs.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~sys.f sys.F sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~atm_types.f atm_types.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~parallel.f parallel.F sourcefile~sys.f->sourcefile~parallel.f sourcefile~radial.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Subroutines rhooda Source Code rhooda.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! subroutine rhooda ( no , np , Datm , rhoatm , iaorb , iphorb , isa ) !! author: P.Ordejon and J.M.Soler !! date: May 1995 !! !! Finds the Harris density at the mesh points from the atomic occupations. !! !! Inverted so that grid points are the outer loop, J.D. Gale, Jan'99 ! Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , phiatm use atomlist , only : indxuo use mesh , only : nsp , dxa , xdop , xdsp use meshphi implicit none integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of mesh points integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms real ( grid_p ), intent ( out ) :: rhoatm ( nsp , np ) !! Harris (sum of atoms) density at mesh points real ( dp ), intent ( in ) :: Datm ( no ) !! Occupations of basis orbitals in free atom real ( dp ) :: phip integer :: i , ip , isp , iu , kn , iop , is , iphi , ia , ix real ( dp ) :: Ci , gradCi ( 3 ), r2o , r2sp , dxsp ( 3 ) #ifdef DEBUG call write_debug ( '      Pre rhooda' ) #endif !  Loop on mesh points do ip = 1 , np !  Initialise rhoatm do isp = 1 , nsp rhoatm ( isp , ip ) = 0.0_grid_p enddo !  Loop on orbitals of mesh point do kn = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( kn ) iu = indxuo ( i ) if ( DirectPhi ) then !  Generate phi value and loop on subpoints iphi = iphorb ( i ) ia = iaorb ( i ) is = isa ( ia ) r2o = rcut ( is , iphi ) ** 2 iop = listp2 ( kn ) do isp = 1 , nsp do ix = 1 , 3 dxsp ( ix ) = xdop ( ix , iop ) + xdsp ( ix , isp ) - dxa ( ix , ia ) enddo r2sp = dxsp ( 1 ) ** 2 + dxsp ( 2 ) ** 2 + dxsp ( 3 ) ** 2 if ( r2sp . lt . r2o ) then call phiatm ( is , iphi , dxsp , phip , gradCi ) Ci = phip rhoatm ( isp , ip ) = rhoatm ( isp , ip ) + Datm ( iu ) * Ci * Ci endif enddo else !  Loop on sub-points do isp = 1 , nsp Ci = phi ( isp , kn ) rhoatm ( isp , ip ) = rhoatm ( isp , ip ) + Datm ( iu ) * Ci * Ci enddo endif enddo enddo #ifdef DEBUG call write_debug ( '      POS rhooda' ) #endif end subroutine rhooda","tags":"","loc":"sourcefile/rhooda.f.html","title":"rhooda.F – SIESTA"},{"text":"Contents Modules atmparams Source Code atmparams.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module atmparams !! Hard-wired parameters (to be eliminated in the future) implicit none integer , parameter , public :: nzetmx = 200 !! Maximum number of PAOs or polarization orbitals !! with the same angular  momentum and !! for the same species. integer , parameter , public :: nkbmx = 4 !! Maximum number of Kleinman-Bylander projectors !! for each angular momentum !! !! For the off-site SO calculation plus semicore states !! there will be at least 4 KBs for each l angular momentum !! (for each l shell we have `J = l +/- 1/2` ) integer , parameter , public :: nsmx = 2 !! Maximum number of semicore shells for each angular !! momentum present in the atom ( for normal atom `nsmx=0` ) integer , parameter , public :: nsemx = 1 + nsmx integer , parameter , public :: ntbmax = 500 !! Maximum number of points in the tables defining !! orbitals, projectors and local neutral-atom !! pseudopotential. integer , parameter , public :: lmaxd = 4 !! Maximum angular momentum for both orbitals and projectors. integer , parameter , public :: lmx2 = ( lmaxd + 1 ) * ( lmaxd + 1 ) integer , parameter , public :: nrmax = 20000 !! Maximum number of points in the functions read !! from file '.vps' or '.psf' (this number is !! determined by the parameter nrmax in the !! program ATOM, which generates the files with !! the pseudopotential information). The number !! of points in the grid can be redefined if the !! pseudopotential is reparametrized. !! `nrmax = 20000` is a typical safe value in this case. integer , parameter , public :: maxos = 2 * nzetmx * lmx2 * nsemx private end module atmparams","tags":"","loc":"sourcefile/atmparams.f.html","title":"atmparams.f – SIESTA"},{"text":"This file depends on sourcefile~~sys.f~~EfferentGraph sourcefile~sys.f sys.F sourcefile~parallel.f parallel.F sourcefile~sys.f->sourcefile~parallel.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~sys.f~~AfferentGraph sourcefile~sys.f sys.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~sys.f sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~atmfuncs.f atmfuncs.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~m_iorho.f m_iorho.F sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~compute_dm.f->sourcefile~sys.f sourcefile~state_init.f->sourcefile~sys.f sourcefile~poison.f poison.F sourcefile~poison.f->sourcefile~sys.f sourcefile~setup_hamiltonian.f->sourcefile~sys.f sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~dhscf.f->sourcefile~sys.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~atmfuncs.f sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f var pansourcefilesysfAfferentGraph = svgPanZoom('#sourcefilesysfAfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules sys Source Code sys.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module sys !! Termination and messaging routines, MPI aware implicit none public :: die ! Prints an error message and calls MPI_Abort public :: bye ! Prints an error message and calls MPI_Finalize public :: message ! Prints a message string if node==0 private CONTAINS subroutine message ( level , str ) !! Prints a message string if `node==0` use parallel , only : Node character ( len =* ), intent ( in ) :: level !! One of INFO, WARNING, FATAL character ( len =* ), intent ( in ) :: str external :: io_assign , io_close integer :: lun if ( Node . eq . 0 ) then write ( 6 , '(a)' ) trim ( str ) write ( 0 , '(a)' ) trim ( str ) call io_assign ( lun ) open ( lun , file = \"MESSAGES\" , status = \"unknown\" , $ position = \"append\" , action = \"write\" ) write ( lun , \"(a)\" ) trim ( level ) // \": \" // trim ( str ) call io_close ( lun ) call pxfflush ( 6 ) call pxfflush ( 0 ) endif end subroutine message subroutine die ( str ) !! Prints an error message and calls MPI_Abort use parallel , only : Node use siesta_cml #ifdef MPI use mpi_siesta #endif character ( len =* ), intent ( in ), optional :: str external :: io_assign , io_close integer :: lun #ifdef MPI integer MPIerror #endif ! Even though formally (in MPI 1.X), only the master node ! can do I/O, in those systems that allow it having each ! node state its complaint can be useful. !                                        if (Node.eq.0) then if ( present ( str )) then write ( 6 , '(a)' ) trim ( str ) write ( 0 , '(a)' ) trim ( str ) endif write ( 6 , '(a,i4)' ) 'Stopping Program from Node: ' , Node write ( 0 , '(a,i4)' ) 'Stopping Program from Node: ' , Node !                                        endif if ( Node . eq . 0 ) then call io_assign ( lun ) open ( lun , file = \"MESSAGES\" , status = \"unknown\" , $ position = \"append\" , action = \"write\" ) write ( lun , \"(a)\" ) 'FATAL: ' // trim ( str ) call io_close ( lun ) call pxfflush ( 6 ) call pxfflush ( 0 ) If ( cml_p ) Then Call cmlFinishFile ( mainXML ) Endif !cml_p endif #ifdef MPI call MPI_Abort ( MPI_Comm_World , 1 , MPIerror ) stop #else call pxfabort () #endif end subroutine die subroutine bye ( str ) !! Prints an error message and calls MPI_Finalize use parallel , only : Node use siesta_cml #ifdef MPI use mpi_siesta #endif character ( len =* ), intent ( in ), optional :: str #ifdef MPI integer rc #endif if ( Node . eq . 0 ) then if ( present ( str )) then write ( 6 , '(a)' ) trim ( str ) endif write ( 6 , '(a)' ) 'Requested End of Run. Bye!!' call pxfflush ( 6 ) If ( cml_p ) Then Call cmlFinishFile ( mainXML ) Endif !cml_p endif #ifdef MPI call MPI_Finalize ( rc ) stop #else stop #endif end subroutine bye end module sys","tags":"","loc":"sourcefile/sys.f.html","title":"sys.F – SIESTA"},{"text":"This file depends on sourcefile~~radial.f~~EfferentGraph sourcefile~radial.f radial.f sourcefile~precision.f precision.F sourcefile~radial.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~radial.f~~AfferentGraph sourcefile~radial.f radial.f sourcefile~atm_types.f atm_types.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~delk.f90 delk.F90 sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~vmat.f90 vmat.F90 sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90->sourcefile~dhscf.f var pansourcefileradialfAfferentGraph = svgPanZoom('#sourcefileradialfAfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules radial Source Code radial.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module radial use precision use xml use interpolation , only : spline ! set spline interpolation use interpolation , only : splint ! spline interpolation implicit none public :: rad_alloc , rad_get , rad_setup_d2 , rad_zero public :: radial_read_ascii , radial_dump_ascii public :: radial_dump_xml , reset_rad_func public :: rad_func type :: rad_func integer n real ( dp ) cutoff real ( dp ) delta real ( dp ), pointer :: f (:) => null () !! Actual data real ( dp ), pointer :: d2 (:) => null () !! 2nd derivative end type rad_func private CONTAINS subroutine reset_rad_func ( func ) implicit none type ( rad_func ) :: func func % n = 0 nullify ( func % f ) nullify ( func % d2 ) end subroutine reset_rad_func subroutine rad_alloc ( func , n ) !! Sets the 'size' n of the arrays and allocates f and d2. use alloc , only : re_alloc implicit none type ( rad_func ), intent ( inout ) :: func integer , intent ( in ) :: n func % n = n nullify ( func % f , func % d2 ) call re_alloc ( func % f , 1 , n , 'func%f' , 'rad_alloc' ) call re_alloc ( func % d2 , 1 , n , 'func%d2' , 'rad_alloc' ) !      allocate(func%f(n),func%d2(n)) end subroutine rad_alloc subroutine rad_get ( func , r , fr , dfdr ) type ( rad_func ), intent ( in ) :: func real ( dp ), intent ( in ) :: r real ( dp ), intent ( out ) :: fr real ( dp ), intent ( out ) :: dfdr if ( func % n . eq . 0 ) then fr = 0._dp dfdr = 0._dp else call splint ( func % delta , func % f , func % d2 , func % n , r , fr , dfdr ) endif end subroutine rad_get subroutine rad_setup_d2 ( func , yp1 , ypn ) !! Set up second derivative in a radial function type ( rad_func ), intent ( inout ) :: func real ( dp ), intent ( in ) :: yp1 , ypn if ( func % n . eq . 0 ) return call spline ( func % delta , func % f , func % n , yp1 , ypn , func % d2 ) end subroutine rad_setup_d2 subroutine rad_zero ( func ) type ( rad_func ), intent ( inout ) :: func func % n = 0 end subroutine rad_zero ! !     Do not use yet... interface in need of fuller specification ! function rad_rvals ( func ) result ( r ) use alloc , only : re_alloc implicit none real ( dp ), dimension (:), pointer :: r type ( rad_func ), intent ( in ) :: func integer i nullify ( r ) if ( func % n . eq . 0 ) return !      allocate(r(func%n)) call re_alloc ( r , 1 , func % n , 'r' , 'rad_rvals' ) do i = 1 , func % n r ( i ) = func % delta * ( i - 1 ) enddo end function rad_rvals subroutine radial_read_ascii ( op , lun , yp1 , ypn ) type ( rad_func ) :: op real ( dp ), intent ( in ) :: yp1 , ypn integer lun integer j , npts real ( dp ) dummy read ( lun , * ) npts , op % delta , op % cutoff call rad_alloc ( op , npts ) do j = 1 , npts read ( lun , * ) dummy , op % f ( j ) enddo call rad_setup_d2 ( op , yp1 , ypn ) end subroutine radial_read_ascii subroutine radial_dump_ascii ( op , lun , header ) type ( rad_func ) :: op integer :: lun logical , intent ( in ), optional :: header integer :: j logical :: print_header ! !     The standard dump is to unit \"lun\" !     and includes a header with npts, delta, and cutoff ! print_header = . true . if ( present ( header )) then print_header = header endif ! if ( print_header ) then write ( lun , '(i4,2g22.12,a)' ) op % n , $ op % delta , op % cutoff , \" # npts, delta, cutoff\" endif do j = 1 , op % n write ( lun , '(2g22.12)' ) ( j - 1 ) * op % delta , op % f ( j ) enddo end subroutine radial_dump_ascii subroutine radial_dump_xml ( op , lun ) type ( rad_func ) :: op integer lun integer j write ( lun , '(a)' ) '<radfunc>' call xml_dump_element ( lun , 'npts' , str ( op % n )) call xml_dump_element ( lun , 'delta' , str ( op % delta )) call xml_dump_element ( lun , 'cutoff' , str ( op % cutoff )) write ( lun , '(a)' ) '<data>' do j = 1 , op % n write ( lun , '(2g22.12)' ) ( j - 1 ) * op % delta , op % f ( j ) enddo write ( lun , '(a)' ) '</data>' write ( lun , '(a)' ) '</radfunc>' end subroutine radial_dump_xml end module radial","tags":"","loc":"sourcefile/radial.f.html","title":"radial.f – SIESTA"},{"text":"Files dependent on this one sourcefile~~siesta_options.f90~~AfferentGraph sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~siesta_options.f90 sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90->sourcefile~state_analysis.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~setup_h0.f setup_H0.F sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~compute_dm.f compute_dm.F sourcefile~siesta_forces.f90->sourcefile~compute_dm.f sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90->sourcefile~state_init.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~siesta_forces.f90->sourcefile~m_rhog.f90 sourcefile~state_analysis.f->sourcefile~siesta_options.f90 sourcefile~setup_hamiltonian.f->sourcefile~siesta_options.f90 sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f->sourcefile~m_rhog.f90 sourcefile~setup_h0.f->sourcefile~siesta_options.f90 sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~compute_dm.f->sourcefile~siesta_options.f90 sourcefile~state_init.f->sourcefile~siesta_options.f90 sourcefile~compute_energies.f90->sourcefile~siesta_options.f90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~compute_energies.f90->sourcefile~m_rhog.f90 sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~siesta_options.f90 sourcefile~siesta_analysis.f->sourcefile~dhscf.f var pansourcefilesiesta_optionsf90AfferentGraph = svgPanZoom('#sourcefilesiesta_optionsf90AfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules siesta_options Source Code siesta_options.F90 Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE siesta_options #ifdef SIESTA__FLOOK use flook , only : luaState #endif implicit none integer , parameter , private :: dp = selected_real_kind ( 10 , 100 ) PUBLIC save ! Compatibility options ! -- pre 4.0 DM and H flow logic logical :: compat_pre_v4_DM_H !! General switch logical :: mix_after_convergence !! Mix DM or H even after convergence logical :: recompute_H_after_scf !! Update H while computing forces ! -- pre 4.0 coordinate output logic -- to be implemented logical :: compat_pre_v4_dynamics ! General switch logical :: mix_scf_first !! Mix first SCF step? logical :: mix_charge !! New: mix fourier components of rho logical :: mixH !! Mix H instead of DM logical :: h_setup_only !! H Setup only logical :: chebef !! Compute the chemical potential in ordern? logical :: dumpcharge !! Write electron density? logical :: fire_mix !! SCF mixing with FIRE method logical :: fixspin !! Keep the total spin fixed? logical :: init_anti_ferro !! Antiferro spin ordering in initdm? logical :: initdmaux !! Re-initialize DM when auxiliary supercell changes? logical :: allow_dm_reuse !! Allow re-use of the previous geometry DM ? (with possible extrapolation) logical :: allow_dm_extrapolation !! Allow the extrapolation of previous geometries' DM ? logical :: change_kgrid_in_md !! Allow k-point grid to change in MD calculations logical :: negl !! Neglect hamiltonian matrix elements without overlap? logical :: noeta !! Use computed chemical potential instead of eta in ordern? logical :: new_diagk !! Use new diagk routine with file storage of eigenvectors? logical :: outlng !! Long output in the output file? logical :: pulfile !! Use file to store Pulay info in pulayx? (Obsolete) logical :: RelaxCellOnly !! Relax only lattice vectors, not atomic coordinates logical :: RemoveIntraMolecularPressure !! Remove molecular virial contribution to p logical :: savehs !! Write file with Hamiltonian electrostatic potential? logical :: savevh !! Write file with Hartree electrostatic potential? logical :: savevna !! Write file with neutral-atom potential? logical :: savevt !! Write file with total effective potential? logical :: savedrho !! Write file with diff. between SCF and atomic density? logical :: saverho !! Write file with electron density? logical :: saverhoxc !! Write file with electron density including nonlinear core correction? logical :: save_ebs_dens !! Write file with band structure energy density? logical :: savepsch !! Write file with ionic (local pseudopotential) charge? logical :: savetoch !! Write file with total charge? logical :: savebader !! Write file with charge for Bader analysis? logical :: usesaveddata !! Default for UseSavedData flag logical :: usesavecg !! Use continuation file for CG geometry relaxation? logical :: usesavelwf !! Use continuation file for Wannier functions? logical :: usesavedm !! Use cont. file for density matrix? logical :: usesavedmloc !! Temporary to keep usesavedm value logical :: usesavexv !! Use cont. file for atomic positions and velocities? logical :: usesavezm !! Use cont. file for Z-matrix? logical :: writeig !! Write eigenvalues? logical :: writbk !! Write k vectors of bands? logical :: writmd logical :: writpx !! Write atomic coordinates at every geometry step? logical :: writb !! Write band eigenvalues? logical :: writec !! Write atomic coordinates at every geometry step? logical :: write_coop !! Write information for COOP/COHP analysis ? ! Create graphviz information to visualize connectivity graph integer :: write_GRAPHVIZ !---------------------------------------------------- ! Wannier90 interface ! logical :: w90_processing !! Will we call the interface with Wannier90 logical :: w90_write_mmn !! Write the Mmn matrix for the interface with Wannier logical :: w90_write_amn !! Write the Amn matrix for the interface with Wannier logical :: w90_write_eig !! Write the eigenvalues or the interface with Wannier logical :: w90_write_unk !! Write the unks for the interface with Wannier logical :: hasnobup !! Is the number of bands with spin up for !!   wannierization defined? logical :: hasnobdown !! Is the number of bands with spin down for !!   wannierization defined? logical :: hasnob !! Is the number of bands for wannierization defined? !!   (for non spin-polarized calculations). integer :: nobup !! Number of bands with spin up for wannierization integer :: nobdown !! Number of bands with spin down for wannierization integer :: nob !! Number of bands for wannierization !!   (for non spin-polarized calculations). !---------------------------------------------------- logical :: writef !! Write atomic forces at every geometry step? logical :: writek !! Write the k vectors of the BZ integration mesh? logical :: writic !! Write the initial atomic ccordinates? logical :: varcel !! Change unit cell during relaxation or dynamics? logical :: do_pdos !! Compute the projected density of states? logical :: write_tshs_history !! Write the MD track of Hamiltonian and overlap matrices in transiesta format logical :: write_hs_history !! Write the MD track of Hamiltonian and overlap matrices logical :: writedm !! Write file with density matrix? logical :: write_dm_at_end_of_cycle !! Write DM at end of SCF cycle? (converged or not) logical :: writeH !! Write file with Hamiltonian? (in \"DM\" format) logical :: write_H_at_end_of_cycle ! Write H at end of SCF cycle? logical :: writedm_cdf !! Write file with density matrix in netCDF form? #ifdef NCDF_4 logical :: write_cdf !! Write file with all information attached integer :: cdf_comp_lvl !! The compression level of the Netcdf-4 file logical :: cdf_w_parallel !! Allows writing NetCDF files in parallel logical :: cdf_r_parallel !! Allows reading NetCDF files in parallel, parallel read does not impose the same requirements as w_parallel #endif logical :: writedm_cdf_history !! Write file with SCF history of DM in netCDF form? logical :: writedmhs_cdf !! Write file with DM_in, H, DM_out, and S in netCDF form? logical :: writedmhs_cdf_history !! Write file with SCF history in netCDF form? logical :: read_charge_cdf !! Read charge density from file in netCDF form? logical :: read_deformation_charge_cdf !! Read deformation charge density from file in netCDF form? ! logical :: save_initial_charge_density !! Just save the initial charge density used logical :: analyze_charge_density_only !! Exit dhscf after processing charge logical :: atmonly !! Set up pseudoatom information only? logical :: harrisfun !! Use Harris functional? logical :: muldeb !! Write Mulliken populations at every SCF step? logical :: spndeb !! Write spin-polarization information at every SCF step? logical :: orbmoms !! Write orbital moments? logical :: split_sr_so !! Cosmetic: split full lj NL energies into SR and SO parts ! Convergence options logical :: converge_FreeE !! free Energy conv. to finish SCF iteration? real ( dp ) :: tolerance_FreeE !! Free-energy tolerance logical :: converge_Eharr !! to finish SCF iteration? real ( dp ) :: tolerance_Eharr !! Harris tolerance logical :: converge_EDM !! to finish SCF iteration? real ( dp ) :: tolerance_EDM !! Tolerance in change of EDM elements to finish SCF iteration logical :: converge_DM !! to finish SCF iteration? real ( dp ) :: dDtol !! Tolerance in change of DM elements to finish SCF iteration logical :: converge_H !! to finish SCF iteration? real ( dp ) :: dHtol !! Tolerance in change of H elements to finish SCF iteration logical :: broyden_optim !! Use Broyden method to optimize geometry? logical :: fire_optim !! Use FIRE method to optimize geometry? logical :: struct_only !! Output initial structure only? logical :: use_struct_file !! Read structural information from a special file? logical :: bornz !! Calculate Born polarization charges? logical :: SCFMustConverge !! Do we have to converge for each SCF calculation? logical :: GeometryMustConverge !! Do we *have to* converge the relaxation? logical :: want_domain_decomposition !! Use domain decomposition for orbitals logical :: want_spatial_decomposition !! Use spatial decomposition for orbitals logical :: hirshpop !! Perform Hirshfeld population analysis? logical :: voropop !! Perform Voronoi population analysis? logical :: partial_charges_at_every_geometry logical :: partial_charges_at_every_scf_step logical :: monitor_forces_in_scf !! Compute forces and stresses at every step logical :: minim_calc_eigenvalues !! Use diagonalization at the end of each MD step to find eigenvalues for OMM !TDDFT Feb 17, 2014 logical :: writetdwf !! To write the wavefuctions at the end of SCF. These !! would serve as the initial states for time evolution !! of KS states in TD-DFT. logical :: extrapol_H_tdks !! Extrapolate Hamiltonian within Crank-Nicolson integration? logical :: td_elec_dyn !! To do TDDFT calculation on second run logical :: etot_time !! Write Etot vs time during TDDFT logical :: eigen_time !! Write instataneous energy of the electronic states in TDDFT logical :: dip_time !! Write dipol moment againstan time in TDDFT logical :: tdsavewf !! To save the wavefunctions at the end of a calculation for restart./ logical :: tdsaverho !! To save TD-Rho after a given number of time steps logical :: td_inverse_linear !! Matrix inversion option? integer :: ntdsaverho !! Each number of steps TD-Rho is saved. integer :: itded !! a TDDFT counterpart of iscf integer :: ntded !! Number of TDED steps in each MD iteration. !! Or total number of TDED steps in an only electron calcuation !! (MD.FinalTimeStep = 1) integer :: ntded_sub !! Number of TDED sub-steps extrapolate H is applied to TDKS states. real ( dp ) :: td_dt !! Time step in electron dynamics. In case of doing electron dyanmics !! with MD, the dt in MD would be dt = td_dt x ntded real ( dp ) :: rstart_time !! Restart time real ( dp ) :: totime !! Total time including the restart time mainly for plotting integer :: ia1 !! Atom index integer :: ia2 !! Atom index integer :: ianneal !! Annealing option read in redata and passed to anneal integer :: idyn !! Geommetry relaxation/dynamics option integer :: ifinal !! Last geommetry iteration step for some types of dynamics integer :: ioptlwf !! Order-N functional option read in redata used in ordern integer :: iquench !! Quenching option, read in redata, used in dynamics routines integer :: isolve !! Option to find density matrix: 0=>diag, 1=>order-N integer :: istart !! First geommetry iteration step for certain types of dynamics integer :: DM_history_depth !! Number of previous density matrices used in extrapolation and reuse integer :: maxsav !! Number of previous density matrices used in Pulay mixing integer :: broyden_maxit !! Max. iterations in Broyden geometry relaxation integer :: mullipop !! Option for Mulliken population level of detail integer :: ncgmax !! Max. number of conjugate gradient steps in order-N minim. integer :: nkick !! Period between 'kick' steps in SCF iteration integer :: nmove !! Number of geometry iterations integer :: nscf !! Number of SCF iteration steps integer :: min_nscf !! Minimum number of SCF iteration steps integer :: pmax integer :: neigwanted !! Wanted number of eigenstates (per k point) integer :: level !! Option for allocation report level of detail integer :: call_diagon_default !! Default number of SCF steps for which to use diagonalization before OMM integer :: call_diagon_first_step !! Number of SCF steps for which to use diagonalization before OMM (first MD step) real ( dp ) :: beta !! Inverse temperature for Chebishev expansion. real ( dp ) :: bulkm !! Bulk modulus real ( dp ) :: charnet !! Net electric charge real ( dp ) :: rijmin !! Min. permited interatomic distance without warning real ( dp ) :: dm_normalization_tol !! Threshold for DM normalization mismatch error logical :: normalize_dm_during_scf !! Whether we normalize the DM real ( dp ) :: dt !! Time step in dynamics real ( dp ) :: dx !! Atomic displacement used to calculate Hessian matrix real ( dp ) :: dxmax !! Max. atomic displacement allowed during geom. relaxation real ( dp ) :: eta ( 2 ) !! Chemical-potential param. Read by redata used in ordern real ( dp ) :: etol !! Relative tol. in CG minim, read by redata, used in ordern real ( dp ) :: ftol !! Force tolerance to stop geometry relaxation real ( dp ) :: g2cut !! Required planewave cutoff of real-space integration mesh real ( dp ) :: mn !! Mass of Nose thermostat real ( dp ) :: mpr !! Mass of Parrinello-Rahman variables real ( dp ) :: occtol !! Occupancy threshold to build DM real ( dp ) :: rcoor !! Cutoff radius of Localized Wave Functions in ordern real ( dp ) :: rcoorcp !! Cutoff radius to find Fermi level by projection in ordern real ( dp ) :: rmax_bonds !! Cutoff length for bond definition real ( dp ) :: strtol !! Stress tolerance in relaxing the unit cell real ( dp ) :: taurelax !! Relaxation time to reach desired T and P in anneal real ( dp ) :: temp real ( dp ) :: tempinit !! Initial ionic temperature read in redata real ( dp ) :: threshold !! Min. size of arrays printed by alloc_report real ( dp ) :: tp !! Target pressure. Read in redata. Used in dynamics routines real ( dp ) :: total_spin !! Total spin used in spin-polarized calculations real ( dp ) :: tt !! Target temperature. Read in redata. Used in dynamics rout. real ( dp ) :: wmix !! Mixing weight for DM in SCF iteration real ( dp ) :: wmixkick !! Mixing weight for DM in special 'kick' SCF steps character ( len = 164 ) :: sname !! System name, used to initialise read integer , parameter :: SOLVE_DIAGON = 0 integer , parameter :: SOLVE_ORDERN = 1 integer , parameter :: SOLVE_TRANSI = 2 integer , parameter :: SOLVE_MINIM = 3 integer , parameter :: SOLVE_PEXSI = 4 integer , parameter :: MATRIX_WRITE = 5 integer , parameter :: SOLVE_CHESS = 6 #ifdef SIESTA__FLOOK ! LUA-handle type ( luaState ) :: LUA #endif END MODULE siesta_options","tags":"","loc":"sourcefile/siesta_options.f90.html","title":"siesta_options.F90 – SIESTA"},{"text":"This file depends on sourcefile~~dfscf.f~~EfferentGraph sourcefile~dfscf.f dfscf.f sourcefile~atm_types.f atm_types.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~precision.f precision.F sourcefile~dfscf.f->sourcefile~precision.f sourcefile~sys.f sys.F sourcefile~dfscf.f->sourcefile~sys.f sourcefile~mesh.f mesh.F sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~parallel.f parallel.F sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~radial.f radial.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~radial.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~dfscf.f~~AfferentGraph sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_dfscf Source Code dfscf.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module m_dfscf public :: dfscf contains subroutine dfscf ( ifa , istr , na , no , nuo , nuotot , np , nspin , . indxua , isa , iaorb , iphorb , . maxnd , numd , listdptr , listd , Dscf , Datm , . Vscf , Vatm , dvol , VolCel , Fal , Stressl ) !! author: P.Ordejon, J.M.Soler, and J.Gale !! !! Adds the SCF contribution to atomic forces and stress. !! !! Written by P.Ordejon, J.M.Soler, and J.Gale. !! Last modification by J.M.Soler, October 2000. !! Modifed for Off-Site Spin-orbit coupling by R. Cuadrado, Feb. 2018 !  Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo use listsc_module , only : listsc use mesh , only : dxa , nsp , xdop , xdsp use meshphi , only : endpht , lstpht , listp2 use meshdscf , only : matrixOtoM use meshdscf , only : DscfL , nrowsDscfL , needDscfL use meshdscf , only : listDl , listDlPtr , numdL use alloc , only : re_alloc , de_alloc , alloc_default , $ allocDefaults use parallel , only : Nodes , Node use sys , only : die use parallelsubs , only : GlobalToLocalOrb implicit none !  Passed arguments integer , intent ( in ) :: ifa !! Are forces required? (`1`=yes,`0`=no) integer , intent ( in ) :: istr !! Is stress required? (`1`=yes, `0`=no) integer , intent ( in ) :: na !! Number of atoms integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: nuo !! Number of orbitals in unit cell (local) integer , intent ( in ) :: nuotot !! Number of orbitals in unit cell (global) integer , intent ( in ) :: np !! Number of mesh points (total is nsp*np) integer , intent ( in ) :: nspin !! spin%Grid (i.e., `min(nspin,4)`) !! `nspin=1` => Unpolarized, !! `nspin=2` => polarized !! `nspin=4` => Noncollinear spin/SOC integer , intent ( in ) :: indxua ( na ) !! Index of equivalent atom in unit cell integer , intent ( in ) :: isa ( na ) !! Species index of each atom integer , intent ( in ) :: iaorb ( no ) !! Atom to which orbitals belong integer , intent ( in ) :: iphorb ( no ) !! Index of orbital within its atom integer , intent ( in ) :: maxnd !! First dimension of Dscf integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero elemts in each row of Dscf integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of row in listd integer , intent ( in ) :: listd ( maxnd ) !! List of nonzero elements of Dscf real ( grid_p ), intent ( in ) :: Vscf ( nsp , np , nspin ) !! Value of SCF potential at the mesh points real ( grid_p ), intent ( in ) :: Vatm ( nsp , np ) !! Value of Harris potential (Hartree potential !! of sum of atomic desities) at mesh points. !! Notice single precision of Vscf and Vatm. real ( dp ), intent ( in ) :: dvol !! Volume per mesh point real ( dp ), intent ( in ) :: VolCel !! Unit cell volume real ( dp ), intent ( in ) :: Datm ( nuotot ) real ( dp ), intent ( in ), target :: Dscf (:,:) !! `Dscf(maxnd,*)`: Value of nonzero elemens of density matrix. !! It is used in \"hermitified\" here. real ( dp ), intent ( inout ) :: Fal ( 3 , * ) !! Atomic forces (contribution added on output) real ( dp ), intent ( inout ) :: Stressl ( 9 ) !! Stress tensor ! Internal variables integer , parameter :: . minb = 100 , ! Min buffer size for local copy of Dscf . maxoa = 100 ! Max # of orbitals per atom integer . i , ia , ib , ibuff ( no ), ic , ii , imp , ind , iop , ip , iphi , io , . is , isp , ispin , iu , iua , iul , ix , ix1 , ix2 , iy , . j , jb , jc , last , lasta , lastop , maxb , maxc , maxndl , . nc , nphiloc real ( dp ) . CD ( nsp ), CDV ( nsp ), DF ( 12 ), Dji , dxsp ( 3 , nsp ), . gCi ( 12 , nsp ), grada ( 3 , maxoa , nsp ), . phia ( maxoa , nsp ), rvol , r2sp , r2cut ( nsmax ) ! Allocate real ( dp ), pointer :: V (:,:) => null () real ( dp ), pointer :: DM_spbherm (:,:) => null () ! integer , pointer , save :: ibc (:), iob (:) real ( dp ), pointer , save :: C (:,:), D (:,:,:), . gC (:,:,:), xgC (:,:,:) logical :: Parallel_Run , nullified = . false . type ( allocDefaults ) oldDefaults !  Start time counter call timer ( 'dfscf' , 1 ) !  Nullify pointers if (. not . nullified ) then nullify ( C , D , gC , ibc , iob , xgC ) nullified = . true . end if !  Get old allocation defaults and set new ones call alloc_default ( old = oldDefaults , . copy = . false ., shrink = . false ., . imin = 1 , routine = 'dfscf' ) if ( size ( Dscf , 2 ) == 8 ) then if ( nspin /= 4 ) call die ( \"Spin size inconsistency in dfscf\" ) !     Prepare \"spin-box hermitian\" form of DM for work below !     We could re-use similar work in rhoofd !     This is the relevant part of the DM in view of the structure !     of the potential Vscf. call re_alloc ( DM_spbherm , 1 , size ( Dscf , 1 ), 1 , 4 , \"DM_spbherm\" ) DM_spbherm (:, 1 ) = Dscf (:, 1 ) DM_spbherm (:, 2 ) = Dscf (:, 2 ) DM_spbherm (:, 3 ) = 0.5_dp * ( Dscf (:, 3 ) + Dscf (:, 7 )) DM_spbherm (:, 4 ) = 0.5_dp * ( Dscf (:, 4 ) + Dscf (:, 8 )) else DM_spbherm => Dscf ! Just use passed Dscf end if !  Allocate buffers to store partial copies of Dscf and C maxc = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxb = maxc + minb maxb = min ( maxb , no ) call re_alloc ( C , 1 , nsp , 1 , maxc , 'C' , 'dfscf' ) call re_alloc ( D , 0 , maxb , 0 , maxb , 1 , nspin , 'D' , 'dfscf' ) call re_alloc ( gC , 1 , 3 , 1 , nsp , 1 , maxc , 'gC' , 'dfscf' ) call re_alloc ( ibc , 1 , maxc , 'ibc' , 'dfscf' ) call re_alloc ( iob , 0 , maxb , 'iob' , 'dfscf' ) call re_alloc ( xgC , 1 , 9 , 1 , nsp , 1 , maxc , 'xgC' , 'dfscf' ) call re_alloc ( V , 1 , nsp , 1 , nspin , 'V' , 'dfscf' ) V = 0._dp !  Set logical that determines whether we need to use parallel or serial mode Parallel_Run = ( Nodes . gt . 1 ) !  If parallel, allocate temporary storage for Local Dscf if ( Parallel_Run ) then if ( nrowsDscfL . gt . 0 ) then maxndl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else maxndl = 1 endif call re_alloc ( DscfL , 1 , maxndl , 1 , nspin , . 'DscfL' , 'dfscf' ) ! Redistribute Dscf to DscfL form call matrixOtoM ( maxnd , numd , listdptr , maxndl , nuo , . nspin , DM_spbherm , DscfL ) endif !     Note: Since this routine is only called to get forces AND stresses, !     ifa = 1 and istr = 1 always. We could get rid of ix1 and ix2 and !     unroll the relevant loops below for more efficiency. !     Also, it might be worth unrolling some of the nsp loops below !     if nsp is always 8. !  Find range of a single array to hold force and stress derivatives !  Range 1-3 for forces if ( ifa . eq . 1 ) then ix1 = 1 else ix1 = 4 end if !  Range 4-12 for stress if ( istr . eq . 1 ) then ix2 = 12 else ix2 = 3 end if !  Initialise variables D (:,:,:) = 0.0_dp ibuff (:) = 0 iob (:) = 0 last = 0 !  Find atomic cutoff radii r2cut (:) = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) enddo !  Evaluate constants rvol = 1.0_dp / VolCel !  Loop over grid points do ip = 1 , np !  Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !  iob(ib)>0 means that row ib of D must not be overwritten !  iob(ib)=0 means that row ib of D is empty !  iob(ib)<0 means that row ib of D contains a valid row of !             Dscf, but which is not required at this point do imp = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( imp ) ib = ibuff ( i ) if ( ib . gt . 0 ) iob ( ib ) = i enddo !  Look for required rows of Dscf not yet stored in D do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ibuff ( i ) . eq . 0 ) then !  Look for an available row in D do ib = 1 , maxb !  last runs circularly over rows of D last = last + 1 if ( last . gt . maxb ) last = 1 if ( iob ( last ) . le . 0 ) goto 10 enddo call die ( 'dfscf: no slot available in D' ) 10 continue !  Copy row i of Dscf into row last of D j = abs ( iob ( last )) if ( j . ne . 0 ) ibuff ( j ) = 0 ibuff ( i ) = last iob ( last ) = i ib = last iu = indxuo ( i ) if ( Parallel_Run ) then iul = NeedDscfL ( iu ) do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) if ( i . ne . iu ) j = listsc ( i , iu , j ) jb = ibuff ( j ) ! i-j symmetry due to the spin-box hermiticity D ( ib , jb ,:) = DscfL ( ind ,:) D ( jb , ib ,:) = DscfL ( ind ,:) enddo else call GlobalToLocalOrb ( iu , Node , Nodes , iul ) do ii = 1 , numd ( iul ) ind = listdptr ( iul ) + ii j = listd ( ind ) if ( i . ne . iu ) j = listsc ( i , iu , j ) jb = ibuff ( j ) D ( ib , jb ,:) = DM_spbherm ( ind ,:) D ( jb , ib ,:) = DM_spbherm ( ind ,:) enddo endif endif ibc ( ic ) = ibuff ( i ) enddo !  Restore iob for next point do imp = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( imp ) ib = ibuff ( i ) iob ( ib ) = - i enddo !  Calculate all phi values and derivatives at all subpoints lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) iu = indxuo ( i ) ia = iaorb ( i ) iphi = iphorb ( i ) is = isa ( ia ) iua = indxua ( ia ) iop = listp2 ( imp ) if ( ia . ne . lasta . or . iop . ne . lastop ) then lasta = ia lastop = iop do isp = 1 , nsp dxsp ( 1 : 3 , isp ) = xdop ( 1 : 3 , iop ) + xdsp ( 1 : 3 , isp ) - dxa ( 1 : 3 , ia ) r2sp = dxsp ( 1 , isp ) ** 2 + dxsp ( 2 , isp ) ** 2 + dxsp ( 3 , isp ) ** 2 if ( r2sp . lt . r2cut ( is )) then call all_phi ( is , + 1 , dxsp (:, isp ), nphiloc , . phia (:, isp ), grada (:,:, isp )) else phia (:, isp ) = 0.0_dp grada ( 1 : 3 ,:, isp ) = 0.0_dp endif enddo endif C ( 1 : nsp , ic ) = phia ( iphi , 1 : nsp ) gC ( 1 : 3 , 1 : nsp , ic ) = grada ( 1 : 3 , iphi , 1 : nsp ) !  If stress required. Generate stress derivatives if ( istr . eq . 1 ) then do isp = 1 , nsp ii = 0 do ix = 1 , 3 do iy = 1 , 3 ii = ii + 1 xgC ( ii , isp , ic ) = dxsp ( iy , isp ) * gC ( ix , isp , ic ) * rvol enddo enddo enddo endif enddo !     Copy potential to a double precision array V ( 1 : nsp , 1 : nspin ) = Vscf ( 1 : nsp , ip , 1 : nspin ) !     Factor two for nondiagonal elements for non-collinear spin (and SO) if ( nspin == 4 ) then V ( 1 : nsp , 3 : 4 ) = 2.0_dp * V ( 1 : nsp , 3 : 4 ) end if !     Loop on first orbital of mesh point do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) iu = indxuo ( i ) ia = iaorb ( i ) iua = indxua ( ia ) ib = ibc ( ic ) !  Copy force and stress gradients to a single array gCi ( 1 : 3 , 1 : nsp ) = gC ( 1 : 3 , 1 : nsp , ic ) gCi ( 4 : 12 , 1 : nsp ) = xgC ( 1 : 9 , 1 : nsp , ic ) !  Find Sum_{j,spin}(Vscf*Dij*Cj) for every subpoint !  Some loops are not done using f90 form as this !  leads to much slower execution on machines with stupid f90 !  compilers at the moment CDV ( 1 : nsp ) = 0.0_dp do ispin = 1 , nspin !  Loop on second orbital of mesh point CD ( 1 : nsp ) = 0.0_dp do jc = 1 , nc jb = ibc ( jc ) Dji = D ( jb , ib , ispin ) do isp = 1 , nsp CD ( isp ) = CD ( isp ) + C ( isp , jc ) * Dji end do end do do isp = 1 , nsp CDV ( isp ) = CDV ( isp ) + CD ( isp ) * V ( isp , ispin ) end do enddo do isp = 1 , nsp CDV ( isp ) = CDV ( isp ) - . C ( isp , ic ) * Datm ( iu ) * Vatm ( isp , ip ) CDV ( isp ) = 2.0_dp * dVol * CDV ( isp ) end do !  Add 2*Dscf_ij*<Cj|Vscf|gCi> to forces do ix = ix1 , ix2 DF ( ix ) = 0.0_dp do isp = 1 , nsp DF ( ix ) = DF ( ix ) + gCi ( ix , isp ) * CDV ( isp ) enddo enddo !  Add force and stress to output arrays if ( ifa . eq . 1 ) then Fal ( 1 : 3 , iua ) = Fal ( 1 : 3 , iua ) + DF ( 1 : 3 ) endif if ( istr . eq . 1 ) then Stressl ( 1 : 9 ) = Stressl ( 1 : 9 ) + DF ( 4 : 12 ) endif !  End of first orbital loop enddo !  End of mesh point loop enddo !  Deallocate local memory call de_alloc ( xgC , 'xgC' , 'dfscf' ) call de_alloc ( iob , 'iob' , 'dfscf' ) call de_alloc ( ibc , 'ibc' , 'dfscf' ) call de_alloc ( gC , 'gC' , 'dfscf' ) call de_alloc ( D , 'D' , 'dfscf' ) call de_alloc ( C , 'C' , 'dfscf' ) call de_alloc ( V , 'V' , 'dfscf' ) if ( Parallel_Run ) then call de_alloc ( DscfL , 'DscfL' , 'dfscf' ) endif if ( size ( Dscf , 2 ) == 8 ) then call de_alloc ( DM_spbherm , 'DM_spbherm' , 'dfscf' ) endif !  Restore old allocation defaults call alloc_default ( restore = oldDefaults ) call timer ( 'dfscf' , 2 ) end subroutine dfscf end module m_dfscf","tags":"","loc":"sourcefile/dfscf.f.html","title":"dfscf.f – SIESTA"},{"text":"This file depends on sourcefile~~atm_types.f~~EfferentGraph sourcefile~atm_types.f atm_types.f sourcefile~radial.f radial.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~precision.f precision.F sourcefile~atm_types.f->sourcefile~precision.f sourcefile~radial.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~atm_types.f~~AfferentGraph sourcefile~atm_types.f atm_types.f sourcefile~vmat.f90 vmat.F90 sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~delk.f90 delk.F90 sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~dfscf.f dfscf.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~dhscf.f dhscf.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90->sourcefile~dhscf.f var pansourcefileatm_typesfAfferentGraph = svgPanZoom('#sourcefileatm_typesfAfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules atm_types Source Code atm_types.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module atm_types !! Derived types for orbitals, KB projectors, and LDA+U projectors !! !! Storage of orbital and projector real-space tables and other !! characteristics !! !! These parameters are over-dimensioned, but there is no storage !! penalty, as the real information is packed and indexed. use precision , only : dp use radial , only : rad_func implicit none integer , parameter , public :: maxnorbs = 100 !! Maximum number of nlm orbitals integer , parameter , public :: maxn_pjnl = 20 !! Maximum number of projectors (not counting different \"m\" copies) integer , parameter , public :: maxn_orbnl = 200 !! Maximum number of nl orbitals (not counting different \"m\" copies) !! Now very large to accommodate filteret basis sets integer , parameter , public :: maxnprojs = 100 !! Maximum number of nlm projectors type , public :: species_info !! Species_info: Consolidate all the pieces of information in one place character ( len = 2 ) :: symbol character ( len = 20 ) :: label integer :: z !! Atomic number real ( dp ) :: mass real ( dp ) :: zval !! Valence charge real ( dp ) :: self_energy !! Electrostatic self-energy !        Orbitals !             We keep track of just one orbital for each !             \"nl\" family integer :: n_orbnl = 0 !! num of nl orbs integer :: lmax_basis = 0 !! basis l cutoff integer , dimension ( maxn_orbnl ) :: orbnl_l !! l of each nl orb integer , dimension ( maxn_orbnl ) :: orbnl_n !! n of each nl orb integer , dimension ( maxn_orbnl ) :: orbnl_z !! z of each nl orb logical , dimension ( maxn_orbnl ) :: orbnl_ispol !! is it a pol. orb? real ( dp ), dimension ( maxn_orbnl ) :: orbnl_pop !! population of nl orb (total of 2l+1 components) !        KB Projectors !             For each l, there can be several projectors. Formally, we !             can can use the \"nl\" terminology for them. n will run from !             1 to the total number of projectors at that l. logical :: lj_projs = . false . integer :: n_pjnl = 0 !! num of \"nl\" projs integer :: lmax_projs = 0 !! l cutoff for projs integer , dimension ( maxn_pjnl ) :: pjnl_l !! l of each nl proj real ( dp ), dimension ( maxn_pjnl ) :: pjnl_j !! j of each nl proj integer , dimension ( maxn_pjnl ) :: pjnl_n !! n of each nl proj real ( dp ), dimension ( maxn_pjnl ) :: pjnl_ekb !! energy of !        Aggregate numbers of orbitals and projectors (including 2l+1 !        copies for each \"nl\"), and index arrays keeping track of !        which \"nl\" family they belong to, and their n, l, and m (to avoid !        a further dereference) integer :: norbs = 0 integer , dimension ( maxnorbs ) :: orb_index integer , dimension ( maxnorbs ) :: orb_n integer , dimension ( maxnorbs ) :: orb_l integer , dimension ( maxnorbs ) :: orb_m integer , dimension ( maxnorbs ) :: orb_gindex real ( dp ), dimension ( maxnorbs ) :: orb_pop !! population of nl orb integer :: nprojs = 0 integer , dimension ( maxnprojs ) :: pj_index integer , dimension ( maxnprojs ) :: pj_n integer , dimension ( maxnprojs ) :: pj_l real ( dp ), dimension ( maxnprojs ) :: pj_j integer , dimension ( maxnprojs ) :: pj_m integer , dimension ( maxnprojs ) :: pj_gindex !        LDA+U Projectors !        Here we follow the scheme used for the KB projectors integer :: n_pjldaunl = 0 !! num of \"nl\" projs. Not counting the \"m copies\" integer :: lmax_ldau_projs = 0 !! l cutoff for LDA+U proj integer , dimension ( maxn_pjnl ) :: pjldaunl_l !! l of each nl proj integer , dimension ( maxn_pjnl ) :: pjldaunl_n !! n of each nl proj !! Here, n is not the principal !! quantum number, but a sequential !! index from 1 to the total !! number of projectors for that l. !! In the case of LDA+U projectors, !! It is always equal to 1. real ( dp ), dimension ( maxn_pjnl ) :: pjldaunl_U !! U of each nl projector real ( dp ), dimension ( maxn_pjnl ) :: pjldaunl_J !! J of each nl projector integer :: nprojsldau = 0 !! Total number of LDA+U proj. !! Counting the \"m copies\" (including the `(2l + 1)` factor). integer , dimension ( maxnprojs ) :: pjldau_index integer , dimension ( maxnprojs ) :: pjldau_n integer , dimension ( maxnprojs ) :: pjldau_l integer , dimension ( maxnprojs ) :: pjldau_m integer , dimension ( maxnprojs ) :: pjldau_gindex type ( rad_func ), dimension (:), pointer :: orbnl type ( rad_func ), dimension (:), pointer :: pjnl type ( rad_func ), dimension (:), pointer :: pjldau type ( rad_func ) :: vna integer :: vna_gindex = 0 type ( rad_func ) :: chlocal type ( rad_func ) :: reduced_vlocal logical :: there_is_core type ( rad_func ) :: core logical :: read_from_file end type species_info integer , save , public :: nspecies integer , save , public :: npairs type ( species_info ), target , allocatable , $ save , public :: species (:) type ( rad_func ), allocatable , target , $ save , public :: elec_corr (:) private end module atm_types","tags":"","loc":"sourcefile/atm_types.f.html","title":"atm_types.f – SIESTA"},{"text":"This file depends on sourcefile~~poison.f~~EfferentGraph sourcefile~poison.f poison.F sourcefile~sys.f sys.F sourcefile~poison.f->sourcefile~sys.f sourcefile~parallel.f parallel.F sourcefile~poison.f->sourcefile~parallel.f sourcefile~precision.f precision.F sourcefile~poison.f->sourcefile~precision.f sourcefile~sys.f->sourcefile~parallel.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Subroutines poison Source Code poison.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! subroutine poison ( CELL , N1 , N2 , N3 , Mesh , RHO , U , V , STRESS , & NSM ) !! author: J.M.Soler !! date: June 1995 !! !! Solves Poisson's Equation. Energy and potential returned in Rydberg units. !     Modules use precision , only : dp , grid_p use parallel , only : Node , Nodes , ProcessorY use sys , only : die use alloc , only : re_alloc , de_alloc use m_fft , only : fft ! 3-D fast Fourier transform use cellsubs , only : reclat ! Finds reciprocal lattice vectors use cellsubs , only : volcel ! Finds unit cell volume use m_chkgmx , only : chkgmx ! Checks planewave cutoff of a mesh #ifdef MPI use mpi_siesta #endif implicit          none !     Input/output variables integer , intent ( in ) :: N1 , N2 , N3 !! Number of mesh divisions in each cell vector integer , intent ( in ) :: Mesh ( 3 ) !! Number of global mesh divisions real ( grid_p ), intent ( in ) :: RHO ( N1 * N2 * N3 ) !! Densitiy at mesh points real ( dp ), intent ( in ) :: CELL ( 3 , 3 ) !! Unit cell vectors integer , intent ( out ) :: NSM !! Number of sub-mesh points per mesh point along each axis real ( grid_p ) :: V ( N1 * N2 * N3 ) !! Electrostatic potential (in Ry) !! V and Rho may be the same physical array real ( dp ) :: U !! Electrostatic energy (in Ry) real ( dp ) :: STRESS ( 3 , 3 ) !! Electrostatic-energy contribution to stress !! tensor (in Ry/Bohr**3) assuming constant density !! (not charge), i.e. r->r' => rho'(r') = rho(r) !! For plane-wave and grid (finite difference) !! basis sets, density rescaling gives an extra !! term (not included) equal to -2*U/cell\\_volume !! for the diagonal elements of stress. For other !! basis sets, the extra term is, in general: !! IntegralOf( V * d\\_rho/d\\_strain ) / cell\\_volume !     Local variables integer :: I , I1 , I2 , I3 , IX , J , J1 , J2 , J3 , JX , & NP , NG , NG2 , NG3 , & ProcessorZ , Py , Pz , J2min , J2max , & J3min , J3max , J2L , J3L , NRemY , NRemZ , & BlockSizeY , BlockSizeZ real ( dp ) :: C , B ( 3 , 3 ), DU , G ( 3 ), G2 , G2MAX , & PI , VG , VOLUME , PI8 real ( grid_p ), pointer :: CG (:,:) real ( dp ), parameter :: K0 ( 3 ) = ( / 0.0 , 0.0 , 0.0 / ), TINY = 1.0e-15 #ifdef MPI integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE POISON' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 2 ) #endif !     Start time counter call timer ( 'POISON' , 1 ) !     Allocate local memory nullify ( CG ) call re_alloc ( CG , 1 , 2 , 1 , n1 * n2 * n3 , 'CG' , 'poison' ) !     Find unit cell volume VOLUME = VOLCEL ( CELL ) !     Find reciprocal lattice vectors call reclat ( CELL , B , 1 ) !     Find maximun planewave cutoff NP = N1 * N2 * N3 G2MAX = 1.0e30_dp call CHKGMX ( K0 , B , Mesh , G2MAX ) !     Copy density to complex array !$OMP parallel do default(shared), private(I) do I = 1 , NP CG ( 1 , I ) = RHO ( I ) CG ( 2 , I ) = 0.0_grid_p enddo !$OMP end parallel do !     Find fourier transform of density call fft ( CG , Mesh , - 1 ) !     Initialize stress contribution do IX = 1 , 3 do JX = 1 , 3 STRESS ( JX , IX ) = 0.0_dp enddo enddo !     Work out processor grid dimensions ProcessorZ = Nodes / ProcessorY if ( ProcessorY * ProcessorZ . ne . Nodes ) & call die ( 'ERROR: ProcessorY must be a factor of the' // & ' number of processors!' ) Py = ( Node / ProcessorZ ) + 1 Pz = Node - ( Py - 1 ) * ProcessorZ + 1 !     Multiply by 8*PI/G2 to get the potential PI = 4.0_dp * atan ( 1.0_dp ) PI8 = PI * 8._dp U = 0.0_dp NG2 = Mesh ( 2 ) NG3 = Mesh ( 3 ) BlockSizeY = (( NG2 / NSM ) / ProcessorY ) * NSM BlockSizeZ = (( NG3 / NSM ) / ProcessorZ ) * NSM NRemY = ( NG2 - BlockSizeY * ProcessorY ) / NSM NRemZ = ( NG3 - BlockSizeZ * ProcessorZ ) / NSM J2min = ( Py - 1 ) * BlockSizeY + NSM * min ( Py - 1 , NRemY ) J2max = J2min + BlockSizeY - 1 if ( Py - 1. lt . NRemY ) J2max = J2max + NSM J2max = min ( J2max , NG2 - 1 ) J3min = ( Pz - 1 ) * BlockSizeZ + NSM * min ( Pz - 1 , NRemZ ) J3max = J3min + BlockSizeZ - 1 if ( Pz - 1. lt . NRemZ ) J3max = J3max + NSM J3max = min ( J3max , NG3 - 1 ) !$OMP parallel do default(shared), !$OMP&private(J3,J3L,I3,J2,J2L,I2,J1,I1,G,G2), !$OMP&private(J,VG,DU,C), reduction(+:U,STRESS) do J3 = J3min , J3max J3L = J3 - J3min if ( J3 . gt . NG3 / 2 ) then I3 = J3 - NG3 else I3 = J3 endif do J2 = J2min , J2max J2L = J2 - J2min if ( J2 . gt . NG2 / 2 ) then I2 = J2 - NG2 else I2 = J2 endif do J1 = 0 , N1 - 1 if ( J1 . gt . N1 / 2 ) then I1 = J1 - N1 else I1 = J1 endif G ( 1 ) = B ( 1 , 1 ) * I1 + B ( 1 , 2 ) * I2 + B ( 1 , 3 ) * I3 G ( 2 ) = B ( 2 , 1 ) * I1 + B ( 2 , 2 ) * I2 + B ( 2 , 3 ) * I3 G ( 3 ) = B ( 3 , 1 ) * I1 + B ( 3 , 2 ) * I2 + B ( 3 , 3 ) * I3 G2 = G ( 1 ) ** 2 + G ( 2 ) ** 2 + G ( 3 ) ** 2 J = 1 + J1 + N1 * J2L + N1 * N2 * J3L if ( G2 . LT . G2MAX . AND . G2 . GT . TINY ) then VG = PI8 / G2 DU = VG * ( CG ( 1 , J ) ** 2 + CG ( 2 , J ) ** 2 ) U = U + DU C = 2.0_dp * DU / G2 DO IX = 1 , 3 DO JX = 1 , 3 STRESS ( JX , IX ) = STRESS ( JX , IX ) + C * G ( IX ) * G ( JX ) ENDDO ENDDO CG ( 1 , J ) = VG * CG ( 1 , J ) CG ( 2 , J ) = VG * CG ( 2 , J ) else CG ( 1 , J ) = 0.0_dp CG ( 2 , J ) = 0.0_dp endif enddo enddo enddo !$OMP end parallel do NG = Mesh ( 1 ) * Mesh ( 2 ) * Mesh ( 3 ) U = 0.5_dp * U * VOLUME / DBLE ( NG ) ** 2 C = 0.5_dp / DBLE ( NG ) ** 2 do IX = 1 , 3 do JX = 1 , 3 STRESS ( JX , IX ) = C * STRESS ( JX , IX ) enddo STRESS ( IX , IX ) = STRESS ( IX , IX ) + U / VOLUME enddo !     Go back to real space call fft ( CG , Mesh , + 1 ) !     Copy potential to array V !$OMP parallel do default(shared), private(I) do I = 1 , NP V ( I ) = CG ( 1 , I ) enddo !$OMP end parallel do !     Free local memory call de_alloc ( CG , 'CG' , 'poison' ) #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif !     Stop time counter call timer ( 'POISON' , 2 ) #ifdef DEBUG call write_debug ( '    POS POISON' ) #endif end subroutine poison","tags":"","loc":"sourcefile/poison.f.html","title":"poison.F – SIESTA"},{"text":"This file depends on sourcefile~~atmfuncs.f~~EfferentGraph sourcefile~atmfuncs.f atmfuncs.f sourcefile~sys.f sys.F sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~precision.f precision.F sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atm_types.f atm_types.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~parallel.f parallel.F sourcefile~sys.f->sourcefile~parallel.f sourcefile~radial.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~atm_types.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~atmfuncs.f~~AfferentGraph sourcefile~atmfuncs.f atmfuncs.f sourcefile~delk.f90 delk.F90 sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~atmfuncs.f sourcefile~dhscf.f dhscf.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~vmat.f90 vmat.F90 sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~atmfuncs.f sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90->sourcefile~dhscf.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules atmfuncs Source Code atmfuncs.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module atmfuncs !! This file contains a set of routines which provide all the information !! about the basis set, pseudopotential, atomic mass, etc... of all the !! chemical species present in the calculation. !! !! The routines contained in this file can only be called after they !! are initialized by calling the subroutine 'atom' for all the !! different chemical species in the calculation: use precision use sys , only : die use atm_types use radial , only : rad_get , rad_func use spher_harm , only : rlylm implicit none type ( species_info ), pointer :: spp type ( rad_func ), pointer :: op type ( rad_func ), pointer :: pp type ( rad_func ), pointer :: func character ( len = 79 ) message integer , parameter :: max_l = 5 integer , parameter :: max_ilm = ( max_l + 1 ) * ( max_l + 1 ) real ( dp ), parameter :: tiny20 = 1.e-20_dp real ( dp ), parameter :: tiny12 = 1.e-12_dp private :: chk , max_l , max_ilm , message public :: nofis , nkbfis , izofis , massfis public :: rcore , rchlocal , rcut , chcore_sub , epskb , uion public :: atmpopfio , psch , zvalfis , floating , psover public :: lofio , symfio , cnfigfio , zetafio , mofio public :: labelfis , lomaxfis , nztfl , rphiatm , lmxkbfis public :: phiatm , all_phi public :: pol ! Added JMS Dec.2009 public :: orb_gindex , kbproj_gindex , vna_gindex , ldau_gindex private contains subroutine chk ( name , is ) character ( len =* ), intent ( in ) :: name integer , intent ( in ) :: is if (( is . lt . 1 ). or .( is . gt . nspecies )) then write ( message , '(2a,i3,a,i3)' ) $ name , \": Wrong species\" , is , \". Have\" , nspecies call die ( message ) endif end subroutine chk function floating ( is ) !! Returns `.true.` if the species is really a \"fake\" one, !! intended to provide some floating orbitals. logical floating integer , intent ( in ) :: is floating = izofis ( is ) . lt . 0 end function floating FUNCTION IZOFIS ( IS ) integer :: izofis !! Atomic number integer , intent ( in ) :: is !! Species index call chk ( 'izofis' , is ) izofis = species ( is )% z end function izofis FUNCTION ZVALFIS ( IS ) real ( dp ) :: zvalfis !! Valence charge integer , intent ( in ) :: is !! Species index call chk ( 'zvalfis' , is ) zvalfis = species ( is )% zval end function zvalfis FUNCTION LABELFIS ( IS ) character ( len = 20 ) :: labelfis !! Atomic label integer , intent ( in ) :: is !! Species index call chk ( 'labelfis' , is ) labelfis = species ( is )% label end function labelfis FUNCTION LMXKBFIS ( IS ) integer :: lmxkbfis !! Maximum ang mom of the KB projectors integer , intent ( in ) :: is !! Species index call chk ( 'lmxkbfis' , is ) lmxkbfis = species ( is )% lmax_projs end function lmxkbfis ! FUNCTION LOMAXFIS ( IS ) integer :: lomaxfis ! Maximum ang mom of the Basis Functions integer , intent ( in ) :: is ! Species index call chk ( 'lomaxfis' , is ) lomaxfis = species ( is )% lmax_basis end function lomaxfis ! FUNCTION MASSFIS ( IS ) real ( dp ) :: massfis ! Mass integer , intent ( in ) :: is ! Species index call chk ( 'massfis' , is ) massfis = species ( is )% mass end function massfis ! FUNCTION NKBFIS ( IS ) integer :: nkbfis ! Total number of KB projectors integer , intent ( in ) :: is ! Species index call chk ( 'nkbfis' , is ) nkbfis = species ( is )% nprojs end function nkbfis ! FUNCTION NOFIS ( IS ) integer :: nofis ! Total number of Basis functions integer , intent ( in ) :: is ! Species index call chk ( 'nofis' , is ) nofis = species ( is )% norbs end function nofis FUNCTION UION ( IS ) real ( dp ) uion integer , intent ( in ) :: is ! Species index call chk ( 'uion' , is ) uion = species ( is )% self_energy end function uion FUNCTION RCORE ( is ) real ( dp ) rcore integer , intent ( in ) :: is ! Species index C  Returns cutoff radius of the pseudo-core charge density for the non-linear C   core corrections for xc potential. C  Distances in Bohr call chk ( 'rcore' , is ) rcore = species ( is )% core % cutoff end function rcore FUNCTION RCHLOCAL ( is ) real ( dp ) rchlocal integer , intent ( in ) :: is ! Species index C  Returns cutoff radius of the Vlocal charge density C  Distances in Bohr call chk ( 'rchlocal' , is ) rchlocal = species ( is )% Chlocal % cutoff end function rchlocal !         AMENOFIS ! !---- Global index helpers------------------------------ FUNCTION orb_gindex ( IS , IO ) integer orb_gindex integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the global index of a basis orbital call chk ( 'orb_gindex' , is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . lt . 1 )) call die ( \"orb_gindex: Wrong io\" ) orb_gindex = species ( is )% orb_gindex ( io ) end function orb_gindex FUNCTION kbproj_gindex ( IS , IO ) integer kbproj_gindex integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! KBproj index ! (within atom, <0, for compatibility) C Returns the global index of a KB projector integer :: ko call chk ( 'kbproj_gindex' , is ) ko = - io if ( ( ko . gt . species ( is )% nprojs ) . or . $ ( ko . lt . 1 )) then call die ( \"kbproj_gindex: Wrong io\" ) endif kbproj_gindex = species ( is )% pj_gindex ( ko ) end function kbproj_gindex FUNCTION ldau_gindex ( IS , IO ) integer ldau_gindex integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the global index of a LDA+U projector call chk ( 'ldau_gindex' , is ) if ( ( io . gt . species ( is )% nprojsldau ) . or . $ ( io . lt . 1 )) call die ( \"ldau_gindex: Wrong io\" ) ldau_gindex = species ( is )% pjldau_gindex ( io ) end function ldau_gindex FUNCTION vna_gindex ( IS ) integer vna_gindex integer , intent ( in ) :: is ! Species index C Returns the global index for a Vna function call chk ( 'vna_gindex' , is ) vna_gindex = species ( is )% vna_gindex end function vna_gindex !------------------------------------------------ FUNCTION ATMPOPFIO ( IS , IO ) real ( dp ) atmpopfio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the population of the atomic basis orbitals in the atomic C ground state configuration. call chk ( 'atmpopfio' , is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . lt . 1 )) call die ( \"atmpopfio: Wrong io\" ) atmpopfio = species ( is )% orb_pop ( io ) end function atmpopfio FUNCTION CNFIGFIO ( IS , IO ) integer cnfigfio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the valence-shell configuration in the atomic ground state C (i.e. the principal quatum number for orbitals of angular momentum l) C   INTEGER CNFIGFIO: Principal quantum number of the shell to what C                     the orbital belongs ( for polarization orbitals C                     the quantum number corresponds to the shell which C                     is polarized by the orbital io) call chk ( 'cnfigfio' , is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . lt . 1 )) call die ( \"cnfigfio: Wrong io\" ) cnfigfio = species ( is )% orb_n ( io ) end function cnfigfio FUNCTION LOFIO ( IS , IO ) integer lofio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns total angular momentum quantum number of a given atomic basis C   basis orbital or Kleynman-Bylander projector. C    INTEGER  IO   : Orbital index (within atom) C                    IO > 0 => Basis orbitals C                    IO < 0 => Kleynman-Bylander projectors C                    IO = 0 => Local pseudopotential C************************OUTPUT***************************************** C   INTEGER LOFIO  : Quantum number L of orbital or KB projector call chk ( 'lofio' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"lofio: No such orbital\" ) lofio = spp % orb_l ( io ) else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"lofio: No such projector\" ) lofio = spp % pj_l ( - io ) else lofio = 0 endif end function lofio FUNCTION MOFIO ( IS , IO ) integer mofio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C   Returns m quantum number of a given atomic basis C   basis orbital or Kleynman-Bylander projector. C    INTEGER  IO   : Orbital index (within atom) C                    IO > 0 => Basis orbitals C                    IO < 0 => Kleynman-Bylander projectors C                    IO = 0 => Local pseudopotential C************************OUTPUT***************************************** C   INTEGER MOFIO  : Quantum number m of orbital or KB projector call chk ( 'mofio' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"mofio: No such orbital\" ) mofio = spp % orb_m ( io ) else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"mofio: No such projector\" ) mofio = spp % pj_m ( - io ) else mofio = 0 endif end function mofio FUNCTION ZETAFIO ( IS , IO ) integer zetafio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C   Returns zeta number of a C   basis orbital C    INTEGER  IO   : Orbital index (within atom) C                    IO > 0 => Basis orbitals C************************OUTPUT***************************************** C   INTEGER ZETAFIO  : Zeta number of orbital call chk ( 'mofio' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"zetafio: No such orbital\" ) zetafio = spp % orbnl_z ( spp % orb_index ( io )) else call die ( 'zetafio only deals with orbitals' ) endif end function zetafio function rcut ( is , io ) real ( dp ) rcut integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) ! io> => basis orbitals ! io<0  => KB projectors ! io=0 : Local screened pseudopotential C  Returns cutoff radius of Kleynman-Bylander projectors and C  atomic basis orbitals. C  Distances in Bohr call chk ( 'rcut' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"rcut: No such orbital\" ) op => spp % orbnl ( spp % orb_index ( io )) rcut = op % cutoff else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"rcut: No such projector\" ) pp => spp % pjnl ( spp % pj_index ( - io )) rcut = pp % cutoff else rcut = spp % vna % cutoff endif end function rcut ! ! FUNCTION SYMFIO ( IS , IO ) character ( len = 20 ) symfio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns a label describing the symmetry of the C   basis orbital or Kleynman-Bylander projector. C    INTEGER  IO   : Orbital index (within atom) C                    IO > 0 => Basis orbitals C                    IO < 0 => Kleynman-Bylander projectors C   INTEGER SYMFIO  : Symmetry of the orbital or KB projector C  2) Returns 's' for IO = 0 integer ilm , i , lorb , morb integer , parameter :: lmax_sym = 4 character ( len = 11 ) sym_label (( lmax_sym + 1 ) * ( lmax_sym + 1 )) data sym_label ( 1 ) . / 's' / data ( sym_label ( i ), i = 2 , 4 ) . / 'py' , 'pz' , 'px' / data ( sym_label ( i ), i = 5 , 9 ) . / 'dxy' , 'dyz' , 'dz2' , 'dxz' , 'dx2-y2' / data ( sym_label ( i ), i = 10 , 16 ) . / 'fy(3x2-y2)' , 'fxyz' , 'fz2y' , 'fz3' , . 'fz2x' , 'fz(x2-y2)' , 'fx(x2-3y2)' / data ( sym_label ( i ), i = 17 , 25 ) . / 'gxy(x2-y2)' , 'gzy(3x2-y2)' , 'gz2xy' , 'gz3y' , 'gz4' , . 'gz3x' , 'gz2(x2-y2)' , 'gzx(x2-3y2)' , 'gx4+y4' / call chk ( 'rcut' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"symfio: No such orbital\" ) else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"symfio: No such projector\" ) else symfio = 's' endif lorb = lofio ( is , io ) morb = mofio ( is , io ) if ( lorb . gt . lmax_sym ) then symfio = ' ' else ilm = lorb * lorb + lorb + morb + 1 if ( pol ( is , io )) then symfio = 'P' // sym_label ( ilm ) else symfio = sym_label ( ilm ) endif endif end function symfio ! !  End of FIOs ---------------------------------------------------- ! FUNCTION POL ( IS , IO ) logical pol integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) ! io>0 => basis orbitals C If true, the orbital IO is a perturbative polarization orbital spp => species ( is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . le . 0 )) call die ( \"pol: Wrong io\" ) spp => species ( is ) pol = spp % orbnl_ispol ( spp % orb_index ( io )) end function pol FUNCTION EPSKB ( IS , IO ) real ( dp ) epskb integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! KB proyector index (within atom) ! May be positive or negative ! (only ABS(IO) is used). C  Returns the energies epsKB_l of the Kleynman-Bylander projectors: C       <Psi|V_KB|Psi'> = <Psi|V_local|Psi'> + C                 Sum_lm( epsKB_l * <Psi|Phi_lm> * <Phi_lm|Psi'> ) C  where Phi_lm is returned by subroutine PHIATM. C  Energy in Rydbergs. integer ik spp => species ( is ) ik = abs ( io ) if (( ik . gt . spp % nprojs ) . or . $ ( ik . lt . 1 ) ) call die ( \"epskb: No such projector\" ) epskb = spp % pjnl_ekb ( spp % pj_index ( ik )) end function epskb !-------------------------------------------------------------------- subroutine vna_sub ( is , r , v , grv ) integer , intent ( in ) :: is ! Species index real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: v ! Value of local pseudopotential real ( dp ), intent ( out ) :: grv ( 3 ) ! Gradient of local pseudopotential C Returns local part of neutral-atom Kleynman-Bylander pseudopotential. C Distances in Bohr,  Energies in Rydbergs C  2) Returns exactly zero when |R| > RCUT(IS,0) real ( dp ) rmod , dvdr call chk ( 'vna_sub' , is ) v = 0.0_dp grv ( 1 : 3 ) = 0.0_dp if ( floating ( is )) return func => species ( is )% vna rmod = sqrt ( sum ( r * r )) if ( rmod . gt . func % cutoff ) return call rad_get ( func , rmod , v , dvdr ) rmod = rmod + tiny20 grv ( 1 : 3 ) = dvdr * r ( 1 : 3 ) / rmod end subroutine vna_sub subroutine psch ( is , r , ch , grch ) integer , intent ( in ) :: is ! Species index real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: ch ! Local pseudopot. charge dens. real ( dp ), intent ( out ) :: grch ( 3 ) ! Gradient of local ps. ch. dens. C Returns 'local-pseudotential charge density'. C Distances in Bohr, Energies in Rydbergs C Density in electrons/Bohr**3 C  2) Returns exactly zero when |R| > Rchloc real ( dp ) :: rmod , dchdr call chk ( 'psch' , is ) ch = 0.0_dp grch ( 1 : 3 ) = 0.0_dp if ( floating ( is )) return func => species ( is )% chlocal rmod = sqrt ( sum ( r * r )) if ( rmod . gt . func % cutoff ) return call rad_get ( func , rmod , ch , dchdr ) rmod = rmod + tiny20 grch ( 1 : 3 ) = dchdr * r ( 1 : 3 ) / rmod end subroutine psch subroutine chcore_sub ( is , r , ch , grch ) integer , intent ( in ) :: is ! Species index real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: ch ! Value of pseudo-core charge dens. real ( dp ), intent ( out ) :: grch ( 3 ) ! Gradient of pseudo-core ch. dens. C Returns returns pseudo-core charge density for non-linear core correction C in the xc potential. C Distances in Bohr, Energies in Rydbergs, Density in electrons/Bohr**3 C  2) Returns exactly zero when |R| > Rcore real ( dp ) rmod , dchdr call chk ( 'chcore_sub' , is ) ch = 0.0_dp grch ( 1 : 3 ) = 0.0_dp if ( floating ( is )) return func => species ( is )% core rmod = sqrt ( sum ( r * r )) rmod = rmod + tiny20 ! Moved here. JMS, Dec.2012 if ( rmod . gt . func % cutoff ) return call rad_get ( func , rmod , ch , dchdr ) !      rmod=rmod+tiny20                   ! Removed. JMS, Dec.2012 grch ( 1 : 3 ) = dchdr * r ( 1 : 3 ) / rmod end subroutine chcore_sub subroutine phiatm ( is , io , r , phi , grphi ) integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) !              IO > 0 =>  Basis orbitals !              IO = 0 =>  Local screened pseudopotential !              IO < 0 =>  Kleynman-Bylander projectors real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: phi ! Basis orbital, KB projector, or !  local pseudopotential real ( dp ), intent ( out ) :: grphi ( 3 ) ! Gradient of BO, KB proj, or Loc ps C  Returns Kleynman-Bylander local pseudopotential, nonlocal projectors, C  and atomic basis orbitals (and their gradients). C Distances in Bohr C 1) Each projector and basis function has a well defined total C    angular momentum (quantum number l). C 2) Basis functions are normalized and mutually orthogonal C 3) Projection functions are normalized and mutually orthogonal C 4) Normalization of KB projectors |Phi_lm> is such that C     <Psi|V_KB|Psi'> = <Psi|V_local|Psi'> + C                   Sum_lm( epsKB_l * <Psi|Phi_lm> * <Phi_lm|Psi'> ) C    where epsKB_l is returned by function EPSKB C 5) Prints a message and stops when no data exits for IS and/or IO C 6) Returns exactly zero when |R| > RCUT(IS,IO) C 7) PHIATM with IO = 0 is strictly equivalent to VNA_SUB real ( dp ) rmod , phir , dphidr real ( dp ) rly ( max_ilm ), grly ( 3 , max_ilm ) integer i , l , m , ik , ilm phi = 0.0_dp grphi ( 1 : 3 ) = 0.0_dp spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"phiatm: No such orbital\" ) func => spp % orbnl ( spp % orb_index ( io )) l = spp % orb_l ( io ) m = spp % orb_m ( io ) else if ( io . lt . 0 ) then if ( floating ( is )) return ik = - io if ( ik . gt . spp % nprojs ) call die ( \"phiatm: No such projector\" ) func => spp % pjnl ( spp % pj_index ( ik )) l = spp % pj_l ( ik ) m = spp % pj_m ( ik ) else ! io=0 if ( floating ( is )) return func => spp % vna l = 0 m = 0 endif rmod = sqrt ( sum ( r * r )) + tiny20 if ( rmod . gt . func % cutoff - tiny12 ) return call rad_get ( func , rmod , phir , dphidr ) if ( io . eq . 0 ) then phi = phir grphi ( 1 : 3 ) = dphidr * r ( 1 : 3 ) / rmod else ilm = l * l + l + m + 1 call rlylm ( l , r , rly , grly ) phi = phir * rly ( ilm ) do i = 1 , 3 grphi ( i ) = dphidr * rly ( ilm ) * r ( i ) / rmod + phir * grly ( i , ilm ) enddo endif end subroutine phiatm subroutine rphiatm ( is , io , r , phi , dphidr ) integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) !              IO > 0 =>  Basis orbitals !              IO = 0 =>  Local screened pseudopotential !              IO < 0 =>  Kleynman-Bylander projectors real ( dp ), intent ( in ) :: r ! Radial distance, relative to atom real ( dp ), intent ( out ) :: phi ! Basis orbital, KB projector, or !  local pseudopotential real ( dp ), intent ( out ) :: dphidr ! Radial derivative of BO, !  KB proj, or Loc pseudopot. C  Returns the radial component of C  Kleynman-Bylander local pseudopotential, nonlocal projectors, C  and atomic basis orbitals (and their radial drivatives) C Distances in Bohr C 1) Each projector and basis function has a well defined total C    angular momentum (quantum number l). C 2) Basis functions are normalized and mutually orthogonal C 3) Projection functions are normalized and mutually orthogonal C 4) Normalization of KB projectors |Phi_lm> is such that C     <Psi|V_KB|Psi'> = <Psi|V_local|Psi'> + C                   Sum_lm( epsKB_l * <Psi|Phi_lm> * <Phi_lm|Psi'> ) C    where epsKB_l is returned by function EPSKB C 6) Returns exactly zero when |R| > RCUT(IS,IO) C 7) RPHIATM with ITYPE = 0 is strictly equivalent to VNA_SUB real ( dp ) rmod , phir integer l , m , ik phi = 0.0_dp dphidr = 0._dp spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"rphiatm: No such orbital\" ) func => spp % orbnl ( spp % orb_index ( io )) l = spp % orb_l ( io ) m = spp % orb_m ( io ) else if ( io . lt . 0 ) then if ( floating ( is )) return ik = - io if ( ik . gt . spp % nprojs ) call die ( \"rphiatm: No such projector\" ) func => spp % pjnl ( spp % pj_index ( ik )) l = spp % pj_l ( ik ) m = spp % pj_m ( ik ) else if ( floating ( is )) return func => spp % vna l = 0 m = 0 endif rmod = r + tiny20 if ( rmod . gt . func % cutoff - tiny12 ) return call rad_get ( func , rmod , phir , dphidr ) if ( l . eq . 0 ) then phi = phir elseif ( l . eq . 1 ) then phi = phir * r dphidr = dphidr * r dphidr = dphidr + phir else phi = phir * r ** l dphidr = dphidr * r ** l dphidr = dphidr + l * phir * r ** ( l - 1 ) endif end subroutine rphiatm subroutine all_phi ( is , it , r , nphi , phi , grphi ) integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: it ! Orbital-type switch: ! IT > 0 => Basis orbitals ! IT < 0 => KB projectors real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom integer , intent ( out ) :: nphi ! Number of phi's real ( dp ), intent ( out ) :: phi (:) ! Basis orbital, KB projector, or !  local pseudopotential real ( dp ), optional , intent ( out ) :: grphi (:,:) ! Gradient of phi C  Returns Kleynman-Bylander local pseudopotential, nonlocal projectors, C  and atomic basis orbitals (and their gradients). C  Same as phiatm but returns all orbitals or KB projectors of the atom C  Written by D.Sanchez-Portal and J.M.Soler. Jan. 2000 C Distances in Bohr C 1) Each projector and basis function has a well defined total C    angular momentum (quantum number l). C 2) Basis functions are normalized and mutually orthogonal C 3) Projection functions are normalized and mutually orthogonal C 4) Normalization of KB projectors |Phi_lm> is such that C     <Psi|V_KB|Psi'> = <Psi|V_local|Psi'> + C                   Sum_lm( epsKB_l * <Psi|Phi_lm> * <Phi_lm|Psi'> ) C    where epsKB_l is returned by function EPSKB C 5) Prints a message and stops when no data exits for IS C 6) Returns exactly zero when |R| > RCUT(IS,IO) C 8) If arrays phi or grphi are too small, returns with the required C    value of nphi integer i , jlm , l , lmax , m , maxlm , n double precision rmod , phir , dphidr real ( dp ) rly ( max_ilm ), grly ( 3 , max_ilm ) integer , parameter :: maxphi = 100 integer :: ilm ( maxphi ) double precision :: rmax ( maxphi ) logical :: within ( maxphi ) call chk ( 'all_phi' , is ) spp => species ( is ) !     Find number of orbitals if ( it . gt . 0 ) then nphi = spp % norbs elseif ( it . lt . 0 ) then nphi = spp % nprojs else call die ( \"all_phi: Please use phiatm to get Vna...\" ) endif if ( nphi . gt . maxphi ) call die ( 'all_phi: maxphi too small' ) if ( it . gt . 0 ) then do i = 1 , nphi l = spp % orb_l ( i ) m = spp % orb_m ( i ) ilm ( i ) = l * ( l + 1 ) + m + 1 op => spp % orbnl ( spp % orb_index ( i )) rmax ( i ) = op % cutoff enddo else do i = 1 , nphi pp => spp % pjnl ( spp % pj_index ( i )) rmax ( i ) = pp % cutoff l = spp % pj_l ( i ) m = spp % pj_m ( i ) ilm ( i ) = l * ( l + 1 ) + m + 1 enddo endif !     Check size of output arrays if ( present ( grphi )) then if ( size ( grphi , 1 ). ne . 3 ) . call die ( 'all_phi: incorrect first dimension of grphi' ) n = min ( size ( phi ), size ( grphi , 2 ) ) else n = size ( phi ) endif !     Return if the caller did not provide arrays large enough... if ( n . lt . nphi ) return !     Initialize orbital values phi ( 1 : nphi ) = 0._dp if ( present ( grphi )) grphi (:, 1 : nphi ) = 0._dp if (( it . lt . 0 ) . and . floating ( is )) return !     Find for which orbitals rmod < rmax and test for quick return rmod = sqrt ( sum ( r * r )) + tiny20 within ( 1 : nphi ) = ( rmax ( 1 : nphi ) > rmod ) if (. not . any ( within ( 1 : nphi ))) return !     Find spherical harmonics maxlm = maxval ( ilm ( 1 : nphi ), mask = within ( 1 : nphi ) ) lmax = nint ( sqrt ( real ( maxlm , dp ))) - 1 call rlylm ( lmax , r , rly , grly ) !     Find values i_loop : do i = 1 , nphi !       Check if rmod > rmax if (. not . within ( i )) cycle i_loop !       Find radial part if ( it . gt . 0 ) then func => spp % orbnl ( spp % orb_index ( i )) else func => spp % pjnl ( spp % pj_index ( i )) endif call rad_get ( func , rmod , phir , dphidr ) !       Multiply radial and angular parts jlm = ilm ( i ) phi ( i ) = phir * rly ( jlm ) if ( present ( grphi )) . grphi (:, i ) = dphidr * rly ( jlm ) * r (:) / rmod + . phir * grly (:, jlm ) enddo i_loop end subroutine all_phi ! ! !     This routine takes two species arguments ! subroutine psover ( is1 , is2 , r , energ , dedr ) integer , intent ( in ) :: is1 , is2 ! Species indexes real ( dp ), intent ( in ) :: r ! Distance between atoms real ( dp ), intent ( out ) :: energ ! Value of the correction !  interaction energy real ( dp ), intent ( out ) :: dedr ! Radial derivative of the correction C Returns electrostatic correction to the ions interaction energy C due to the overlap of the two 'local pseudopotential charge densities' C Distances in Bohr, Energies in Rydbergs C  2) Returns exactly zero when |R| > Rchloc integer ismx , ismn , indx real ( dp ) r_local call chk ( 'psover' , is1 ) call chk ( 'psover' , is2 ) energ = 0.0_dp dedr = 0.0_dp if ( floating ( is1 ) . or . floating ( is2 )) return ismx = max ( is1 , is2 ) ismn = min ( is1 , is2 ) indx = (( ismx - 1 ) * ismx ) / 2 + ismn func => elec_corr ( indx ) if ( r . gt . func % cutoff - tiny12 ) return call rad_get ( func , r , energ , dedr ) r_local = r + tiny20 energ = 2.0_dp * energ / r_local dedr = ( - energ + 2.0_dp * dedr ) / r_local end subroutine psover ! !     Deprecated ! FUNCTION NZTFL ( IS , L ) integer nztfl integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: l ! Angular momentum of the basis funcs C Returns the number of different basis functions C with the same angular momentum and for a given species integer i call chk ( 'nztfl' , is ) spp => species ( is ) nztfl = 0 do i = 1 , spp % norbs if ( spp % orb_l ( i ). eq . l ) nztfl = nztfl + 1 enddo end function nztfl FUNCTION NKBL_FUNC ( IS , L ) integer nkbl_func integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: l ! Angular momentum of the basis funcs C Returns the number of different KB projectors C with the same angular momentum and for a given species integer i call chk ( 'nkbl_func' , is ) spp => species ( is ) nkbl_func = 0 do i = 1 , spp % nprojs if ( spp % pj_l ( i ). eq . l ) nkbl_func = nkbl_func + 1 enddo end function nkbl_func end module atmfuncs","tags":"","loc":"sourcefile/atmfuncs.f.html","title":"atmfuncs.f – SIESTA"},{"text":"This file depends on sourcefile~~mesh.f~~EfferentGraph sourcefile~mesh.f mesh.F sourcefile~precision.f precision.F sourcefile~mesh.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~mesh.f~~AfferentGraph sourcefile~mesh.f mesh.F sourcefile~delk.f90 delk.F90 sourcefile~delk.f90->sourcefile~mesh.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~forhar.f->sourcefile~mesh.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~vmat.f90 vmat.F90 sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~rhooda.f rhooda.F sourcefile~rhooda.f->sourcefile~mesh.f sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 var pansourcefilemeshfAfferentGraph = svgPanZoom('#sourcefilemeshfAfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules mesh Source Code mesh.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- module mesh !! Stores quantities that are connected with the mesh use precision , only : dp implicit none integer , pointer , save :: idop (:) !! `idop(mop)`: Extended-mesh-index displacement of points !! within a sphere of radius `rmax`. integer , pointer , save :: ipa (:) => null () !! `ipa(na)`: Mesh cell in which atom is real ( dp ), pointer , save :: dxa (:,:) => null () !! `dxa(3,na)`: Atom position within mesh-cell real ( dp ), pointer , save :: xdop (:,:) !! `xdop(3,mop)`: Vector to mesh points within rmax real ( dp ), pointer , save :: xdsp (:,:) !! `xdsp(3,nsp)`: Vector to mesh sub-points. !! Allocated in `[[dhscf_init(proc)]]`. integer , save :: mop !! Maximum number of non-zero orbital points integer , save :: ne ( 3 ) !! Number of mesh-Extension intervals in each direction integer , save :: nem ( 3 ) !! Extended-mesh divisions in each direction integer , save :: nmsc ( 3 ) !! Mesh divisions of each supercell vector integer , save :: nmuc ( 3 ) !! Mesh points in each unit cell vector integer , save :: nusc ( 3 ) !! Number of unit cells in each supercell dir integer , save :: meshLim ( 2 , 3 ) !! Upper an lower limits of the mesh in every process !! !! My processor's box of mesh points: !! `myBox(1,:)` : lower bounds !! `myBox(2,:)` : upper bounds integer , save :: nmeshg ( 3 ) !! Total number of mesh points in each direction integer , save :: nsm !! Number of mesh sub-divisions in each direction integer , save :: nsp !! Number of sub-points of each mesh point real ( dp ), save :: cmesh ( 3 , 3 ) !! Mesh-cell vectors real ( dp ), save :: rcmesh ( 3 , 3 ) !! Reciprocal mesh-cell vectors (WITHOUT  2*\\pi  factor) integer , pointer , save :: indexp (:) => null () !! `indexp(nep)`: Translation from extended to normal mesh index integer , pointer , save :: iatfold (:,:) => null () !! `iatfold(3,na)`: Supercell vector that keeps track of the !! of the folding of the atomic coordinates in the mesh end module mesh","tags":"","loc":"sourcefile/mesh.f.html","title":"mesh.F – SIESTA"},{"text":"This file depends on sourcefile~~state_init.f~~EfferentGraph sourcefile~state_init.f state_init.F sourcefile~m_mixing.f90 m_mixing.F90 sourcefile~state_init.f->sourcefile~m_mixing.f90 sourcefile~sys.f sys.F sourcefile~state_init.f->sourcefile~sys.f sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~state_init.f->sourcefile~siesta_options.f90 sourcefile~m_mixing_scf.f90 m_mixing_scf.F90 sourcefile~state_init.f->sourcefile~m_mixing_scf.f90 sourcefile~units.f90 units.f90 sourcefile~state_init.f->sourcefile~units.f90 sourcefile~parallel.f parallel.F sourcefile~state_init.f->sourcefile~parallel.f sourcefile~m_mixing.f90->sourcefile~parallel.f sourcefile~precision.f precision.F sourcefile~m_mixing.f90->sourcefile~precision.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~m_mixing_scf.f90->sourcefile~m_mixing.f90 sourcefile~m_mixing_scf.f90->sourcefile~parallel.f sourcefile~m_mixing_scf.f90->sourcefile~precision.f sourcefile~units.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~state_init.f~~AfferentGraph sourcefile~state_init.f state_init.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~state_init.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_state_init Source Code state_init.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_state_init private public :: state_init CONTAINS subroutine state_init ( istep ) use kpoint_scf_m , only : setup_kpoint_scf , kpoints_scf use kpoint_t_m , only : kpoint_delete , kpoint_nullify use m_os , only : file_exist use m_new_dm , only : new_dm use m_proximity_check , only : proximity_check use siesta_options use units , only : Ang use sparse_matrices , only : maxnh , numh , listh , listhptr use sparse_matrices , only : Dold , Dscf , DM_2D use sparse_matrices , only : Eold , Escf , EDM_2D use sparse_matrices , only : Hold , H , H_2D use sparse_matrices , only : xijo , xij_2D use sparse_matrices , only : S , S_1D use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : sparse_pattern use sparse_matrices , only : block_dist , single_dist use sparse_matrices , only : DM_history use create_Sparsity_SC , only : crtSparsity_SC use m_sparsity_handling , only : SpOrb_to_SpAtom use m_sparsity_handling , only : Sp_to_Spglobal use m_pivot_methods , only : sp2graphviz use siesta_geom use atomlist , only : iphorb , iphkb , indxua , & rmaxo , rmaxkb , rmaxv , rmaxldau , & lastkb , lasto , superc , indxuo , & no_u , no_s , no_l , iza , qtots use alloc , only : re_alloc , de_alloc , alloc_report use m_hsparse , only : hsparse use m_overlap , only : overlap use m_supercell , only : exact_sc_ag use siesta_cml , only : cml_p , cmlStartStep , mainXML use siesta_cml , only : cmlStartPropertyList use siesta_cml , only : cmlEndPropertyList use siesta_cml , only : cmlAddProperty use zmatrix , only : lUseZmatrix , write_zmatrix use m_energies , only : Emad use write_subs use m_ioxv , only : ioxv use m_iotdxv , only : iotdxv use m_steps use parallel , only : IOnode , node , nodes , BlockSize use m_spin , only : spin use m_rmaxh use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_normalize_dm , only : normalize_dm use m_eo use m_gamma use files , only : slabel use m_mpi_utils , only : globalize_or use m_mpi_utils , only : globalize_sum use domain_decom , only : domainDecom , use_dd , use_dd_perm use ldau_specs , only : switch_ldau , ldau_init use fdf , only : fdf_get use sys , only : message , die use m_sparse , only : xij_offset use ts_kpoint_scf_m , only : setup_ts_kpoint_scf , ts_kpoints_scf use m_ts_charge , only : TS_RHOCORR_METHOD , TS_RHOCORR_FERMI use m_ts_options , only : BTD_method use m_ts_options , only : TS_Analyze use m_ts_options , only : N_Elec , Elecs , IsVolt use m_ts_electype use m_ts_global_vars , only : TSrun , TSmode , onlyS use sys , only : bye use m_ts_io , only : fname_TSHS , ts_write_tshs use m_ts_sparse , only : ts_sparse_init use m_ts_tri_init , only : ts_tri_init , ts_tri_analyze use files , only : slabel , label_length #ifdef SIESTA__CHESS use m_chess , only : CheSS_init , get_CheSS_parameter #endif #ifdef CDF use iodm_netcdf , only : setup_dm_netcdf_file use iodmhs_netcdf , only : setup_dmhs_netcdf_file #endif use class_Sparsity use class_dSpData1D use class_dSpData2D use class_zSpData2D use class_dData2D #ifdef TEST_IO use m_test_io #endif #ifdef SIESTA__FLOOK use siesta_dicts , only : dict_repopulate_MD #endif implicit none integer :: istep , nnz real ( dp ) :: veclen ! Length of a unit-cell vector real ( dp ) :: rmax logical :: cell_can_change integer :: i , ix , iadispl , ixdispl logical :: auxchanged ! Auxiliary supercell changed? logical :: folding , folding1 logical :: diag_folding , diag_folding1 logical :: foundxv ! dummy for call to ioxv external :: madelung , timer real ( dp ), external :: volcel integer :: ts_kscell_file ( 3 , 3 ) = 0 real ( dp ) :: ts_kdispl_file ( 3 ) = 0.0 logical :: ts_Gamma_file = . true . character ( len = label_length + 6 ) :: fname real ( dp ) :: dummyef = 0.0 , dummyqtot = 0.0 #ifdef SIESTA__CHESS integer :: maxnh_kernel , maxnh_mult , no_l_kernel , no_l_mult integer , dimension (:), allocatable :: listh_kernel , listh_mult integer , dimension (:), allocatable :: numh_kernel , numh_mult real ( dp ) :: chess_value #endif type ( Sparsity ) :: g_Sp character ( len = 256 ) :: oname type ( dData2D ) :: tmp_2D real ( dp ) :: dummy_qspin ( 8 ) !------------------------------------------------------------------- BEGIN call timer ( 'IterGeom' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_init' ) #endif call timer ( 'state_init' , 1 ) istp = istp + 1 if ( IOnode ) then write ( 6 , '(/,t22,a)' ) repeat ( '=' , 36 ) select case ( idyn ) case ( 0 ) if ( nmove == 0 ) then write ( 6 , '(t25,a)' ) 'Single-point calculation' if ( cml_p ) call cmlStartStep ( mainXML , type = 'Single-Point' , $ index = istp ) else if ( broyden_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin Broyden opt. move = ' , $ istep else if ( fire_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin FIRE opt. move = ' , $ istep else write ( 6 , '(t25,a,i6)' ) 'Begin CG opt. move = ' , $ istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'Geom. Optim' , $ index = istp ) endif !        Print Z-matrix coordinates if ( lUseZmatrix ) then call write_Zmatrix () endif case ( 1 , 3 ) if ( iquench > 0 ) then write ( 6 , '(t25,a,i6)' ) 'Begin MD quenched step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD-quenched' , $ index = istep ) else write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , $ index = istep ) endif case ( 2 , 4 , 5 ) write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , index = istep ) case ( 6 ) write ( 6 , '(t25,a,i6)' ) 'Begin FC step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FC' , index = istep ) if ( istep . eq . 0 ) then write ( 6 , '(t25,a)' ) 'Undisplaced coordinates' else iadispl = ( istep - mod ( istep - 1 , 6 )) / 6 + ia1 ix = mod ( istep - 1 , 6 ) + 1 ixdispl = ( ix - mod ( ix - 1 , 2 ) + 1 ) / 2 write ( 6 , '(t26,a,i0,/,t26,a,i1,a,f10.6,a)' ) 'displace atom ' , & iadispl , 'in direction ' , ixdispl , ' by' , dx / Ang , ' Ang' endif case ( 8 ) write ( 6 , '(t25,a,i6)' ) 'Begin Server step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FS' , index = istep ) case ( 9 ) if ( istep == 0 ) then write ( 6 , '(t25,a,i7)' ) 'Explicit coord. initialization' else write ( 6 , '(t25,a,i7)' ) 'Explicit coord. step =' , istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'ECS' , index = istep ) case ( 10 ) write ( 6 , '(t25,a,i7)' ) 'LUA coord. step =' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'LUA' , index = istep ) end select write ( 6 , '(t22,a)' ) repeat ( '=' , 36 ) !     Print atomic coordinates call outcoor ( ucell , xa , na_u , ' ' , writec ) !     Save structural information in crystallographic format !     (in file SystemLabel.STRUCT_OUT), !     canonical Zmatrix (if applicable), and CML record call siesta_write_positions ( moved = . false .) endif ! IONode ! Write the XV file for single-point calculations, so that ! it is there at the end for those users who rely on it call ioxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , & foundxv ) ! Write TDXV file for TDDFT restart. if ( writetdwf . or . td_elec_dyn ) then call iotdxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , foundxv ) end if !     Actualize things if variable cell auxchanged = . false . cell_can_change = ( varcel . or . & ( idyn . eq . 8 ) ! Force/stress evaluation & ) if ( change_kgrid_in_md ) then cell_can_change = cell_can_change . or . & ( idyn . eq . 3 ) . or . ! Parrinello-Rahman & ( idyn . eq . 4 ) . or . ! Nose-Parrinello-Rahman & ( idyn . eq . 5 ) ! Anneal endif if ( cell_can_change . and . & ( istep . ne . inicoor ) . and . (. not . gamma ) ) then !       Will print k-points also call kpoint_delete ( kpoints_scf ) call setup_kpoint_scf ( ucell ) if ( TSmode ) then call kpoint_delete ( ts_kpoints_scf ) else call kpoint_nullify ( ts_kpoints_scf ) end if call setup_ts_kpoint_scf ( ucell , kpoints_scf ) call re_alloc ( eo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'eo' , 'state_init' ) call re_alloc ( qo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'qo' , 'state_init' ) !       Find required supercell if ( gamma ) then nsc ( 1 : 3 ) = 1 else do i = 1 , 3 veclen = sqrt ( ucell ( 1 , i ) ** 2 + ucell ( 2 , i ) ** 2 + ucell ( 3 , i ) ** 2 ) nsc ( i ) = 1 + 2 * ceiling ( rmaxh / veclen ) end do ! The above is kept for historical reasons, ! but a tight supercell can be found from the atom-graph info: call exact_sc_ag ( negl , ucell , na_u , isa , xa , nsc ) endif mscell = 0.0_dp do i = 1 , 3 mscell ( i , i ) = nsc ( i ) if ( nsc ( i ) /= nscold ( i )) auxchanged = . true . nscold ( i ) = nsc ( i ) enddo !       Madelung correction for charged systems if ( charnet . ne . 0.0_dp ) then call madelung ( ucell , shape , charnet , Emad ) endif endif !     End variable cell actualization !     Auxiliary supercell !     Do not move from here, as the coordinates might have changed !     even if not the unit cell call superc ( ucell , scell , nsc ) #ifdef SIESTA__FLOOK call dict_repopulate_MD () #endif !     Print unit cell and compute cell volume !     Possible BUG: !     Note that this volume is later used in write_subs and the md output !     routines, even if the cell later changes. if ( IOnode ) call outcell ( ucell ) volume_of_some_cell = volcel ( ucell ) !     Use largest possible range in program, except hsparse... !     2 * rmaxv: Vna overlap !     rmaxo + rmaxkb: Non-local KB action !     2 * (rmaxo + rmaxldau): Interaction through LDAU projector !     2.0_dp * (rmaxo+rmaxkb) : Orbital interaction through KB projectors rmax = max ( 2._dp * rmaxv , 2._dp * ( rmaxo + rmaxldau ), rmaxo + rmaxkb ) if ( . not . negl ) then rmax = max ( rmax , 2.0_dp * ( rmaxo + rmaxkb ) ) endif !     Check if any two atoms are unreasonably close call proximity_check ( rmax ) ! Clear history of mixing parameters call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) ! Ensure sparsity pattern is empty call delete ( sparse_pattern ) ! sadly deleting the sparse pattern does not necessarily ! mean that the arrays are de-associated. ! Remember that the reference counter could (in MD) ! be higher than 1, hence we need to create \"fake\" ! containers and let the new<class> delete the old ! sparsity pattern nullify ( numh , listhptr , listh ) allocate ( numh ( no_l ), listhptr ( no_l )) ! We do not need to allocate listh ! that will be allocated in hsparse #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then !         Calculate a sparsity pattern with some buffers... Only required !         for CheSS chess_value = get_chess_parameter ( 'chess_buffer_kernel' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_kernel = maxnh no_l_kernel = no_l allocate ( listh_kernel ( maxnh_kernel )) allocate ( numh_kernel ( no_l_kernel )) listh_kernel = listh numh_kernel = numh chess_value = get_chess_parameter ( 'chess_buffer_mult' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_mult = maxnh no_l_mult = no_l allocate ( listh_mult ( maxnh_mult )) allocate ( numh_mult ( no_l_mult )) listh_mult = listh numh_mult = numh end if #endif /* CHESS */ !     List of nonzero Hamiltonian matrix elements !     and, if applicable,  vectors between orbital centers !     Listh and xijo are allocated inside hsparse !     Note: We always generate xijo now, for COOP and other !           analyses. call delete ( xij_2D ) ! as xijo will be reallocated nullify ( xijo ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , $ set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ debug_folding = fdf_get ( 'debug-folding' ,. false .)) ! call globalize_or ( diag_folding1 , diag_folding ) call globalize_or ( folding1 , folding ) if ( diag_folding . and . gamma ) then call message ( \"WARNING\" , \"Gamma-point calculation \" // $ \"with interaction between periodic images\" ) call message ( \"WARNING\" , $ \"Some features might not work optimally:\" ) call message ( \"WARNING\" , $ \"e.g. DM initialization from atomic data\" ) if ( harrisfun ) call die ( \"Harris functional run needs \" // $ \"'force-aux-cell T'\" ) else if ( folding ) then if ( gamma ) then call message ( \"INFO\" , \"Gamma-point calculation \" // $ \"with multiply-connected orbital pairs\" ) call message ( \"INFO\" , $ \"Folding of H and S implicitly performed\" ) call check_cohp () else write ( 6 , \"(a,/,a)\" ) \"Non Gamma-point calculation \" // $ \"with multiply-connected orbital pairs \" // $ \"in auxiliary supercell.\" , $ \"Possible internal error. \" // $ \"Use 'debug-folding T' to debug.\" call die ( \"Inadequate auxiliary supercell\" ) endif endif ! call globalize_sum ( maxnh , nnz ) if ( cml_p ) then call cmlStartPropertyList ( mainXML , title = 'Orbital info' ) call cmlAddProperty ( xf = mainXML , value = no_u , $ title = 'Number of orbitals in unit cell' , $ dictref = 'siesta:no_u' , units = \"cmlUnits:countable\" ) call cmlAddProperty ( xf = mainXML , value = nnz , $ title = 'Number of non-zeros' , $ dictref = 'siesta:nnz' , units = \"cmlUnits:countable\" ) call cmlEndPropertyList ( mainXML ) endif ! #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then call CheSS_init ( node , nodes , maxnh , maxnh_kernel , maxnh_mult , & no_u , no_l , no_l_kernel , no_l_mult , BlockSize , & spin % spinor , qtots , listh , listh_kernel , listh_mult , & numh , numh_kernel , numh_mult ) deallocate ( listh_kernel ) deallocate ( numh_kernel ) deallocate ( listh_mult ) deallocate ( numh_mult ) end if #endif /* CHESS */ ! ! If using domain decomposition, redistribute orbitals ! for this geometry, based on the hsparse info. ! The first time round, the initial distribution is a ! simple block one (given by preSetOrbitLimits). ! ! Any DM, etc, read from file will be redistributed according ! to the new pattern. ! Inherited DMs from a previous geometry cannot be used if the ! orbital distribution changes. For now, we avoid changing the ! distribution (the variable use_dd_perm is .true. if domain ! decomposition is in effect). Names should be changed... if ( use_dd . and . (. not . use_dd_perm )) then call domainDecom ( no_u , no_l , maxnh ) ! maxnh intent(in) here maxnh = sum ( numh ( 1 : no_l )) ! We still need to re-create Julian Gale's ! indexing for O(N) in parallel. print \"(a5,i3,a20,3i8)\" , $ \"Node: \" , Node , \"no_u, no_l, maxnh: \" , no_u , no_l , maxnh call setup_ordern_indexes ( no_l , no_u , Nodes ) endif ! I would like to skip this alloc/move/dealloc/attach ! by allowing sparsity to have pointer targets. ! However, this poses a problem with intel compilers, ! as it apparently errors out when de-allocating a target pointer write ( oname , \"(a,i0)\" ) \"sparsity for geom step \" , istep call newSparsity ( sparse_pattern , no_l , no_u , maxnh , & numh , listhptr , listh , name = oname ) deallocate ( numh , listhptr , listh ) call attach ( sparse_pattern , & n_col = numh , list_ptr = listhptr , list_col = listh ) ! In case the user requests to create the connectivity graph if ( write_GRAPHVIZ > 0 ) then ! first create the unit-cell sparsity pattern call crtSparsity_SC ( sparse_pattern , g_Sp , UC = . true .) ! next move to global sparsity pattern call Sp_to_Spglobal ( block_dist , g_Sp , g_Sp ) if ( IONode ) then if ( write_GRAPHVIZ /= 2 ) & call sp2graphviz ( trim ( slabel ) // '.ORB.gv' , g_Sp ) ! Convert to atomic if ( write_GRAPHVIZ /= 1 ) then call SpOrb_to_SpAtom ( single_dist , g_Sp , na_u , lasto , g_Sp ) call sp2graphviz ( trim ( slabel ) // '.ATOM.gv' , g_Sp ) end if end if call delete ( g_Sp ) end if ! Copy over xijo array (we can first do it here... :( ) call newdData2D ( tmp_2D , xijo , 'xijo' ) deallocate ( xijo ) write ( oname , \"(a,i0)\" ) \"xijo at geom step \" , istep call newdSpData2D ( sparse_pattern , tmp_2D , block_dist , xij_2D , & name = oname ) call delete ( tmp_2D ) ! decrement container... xijo => val ( xij_2D ) ! Calculate the super-cell offsets... if ( Gamma ) then ! Here we create the super-cell offsets call re_alloc ( isc_off , 1 , 3 , 1 , 1 ) isc_off (:,:) = 0 else call xij_offset ( ucell , nsc , na_u , xa , lasto , & xij_2D , isc_off , & Bcast = . true .) end if ! When the user requests to only do an analyzation, we can call ! appropriate routines and quit if ( TS_Analyze ) then ! Force the creation of the full sparsity pattern call ts_sparse_init ( slabel , IsVolt , N_Elec , Elecs , & ucell , nsc , na_u , xa , lasto , block_dist , sparse_pattern , & Gamma , isc_off ) ! create the tri-diagonal matrix call ts_tri_analyze ( block_dist , sparse_pattern , N_Elec , & Elecs , ucell , na_u , lasto , nsc , isc_off , & BTD_method ) ! Print-out timers call timer ( 'TS-rgn2tri' , 3 ) ! Bye also waits for all processors call bye ( 'transiesta analyzation performed' ) end if write ( oname , \"(a,i0)\" ) \"EDM at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % EDM , block_dist , EDM_2D , & name = oname ) !if (ionode) call print_type(EDM_2D) Escf => val ( EDM_2D ) call re_alloc ( Dold , 1 , maxnh , 1 , spin % DM , name = 'Dold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) call re_alloc ( Hold , 1 , maxnh , 1 , spin % H , name = 'Hold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) if ( converge_EDM ) then call re_alloc ( Eold , 1 , maxnh , 1 , spin % EDM , name = 'Eold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) end if !     Allocate/reallocate storage associated with Hamiltonian/Overlap matrix write ( oname , \"(a,i0)\" ) \"H at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H , block_dist , H_2D , & name = oname ) !if (ionode) call print_type(H_2D) H => val ( H_2D ) write ( oname , \"(a,i0)\" ) \"H_vkb at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_vkb_1D , name = oname ) !if (ionode) call print_type(H_vkb_1D) write ( oname , \"(a,i0)\" ) \"H_kin at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_kin_1D , name = oname ) !if (ionode) call print_type(H_kin_1D) if ( switch_ldau ) then write ( oname , \"(a,i0)\" ) \"H_ldau at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % spinor , & block_dist , H_ldau_2D , name = oname ) ! Initialize to 0, LDA+U may re-calculate !   this matrix sporadically doing the SCF. ! Hence initialization MUST be performed upon ! re-allocation. call init_val ( H_ldau_2D ) if ( inicoor /= istep ) then ! Force initialization of the LDA+U ! when changing geometry ! For the first geometry this is controlled ! by the user via an fdf-key ldau_init = . true . end if end if if ( spin % SO_onsite ) then write ( oname , \"(a,i0)\" ) \"H_so (onsite) at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H - 2 , & block_dist , H_so_on_2D , name = oname ) else if ( spin % SO_offsite ) then write ( oname , \"(a,i0)\" ) \"H_so (offsite) at geom step \" , istep call newzSpData2D ( sparse_pattern , 4 , & block_dist , H_so_off_2D , name = oname ) endif write ( oname , \"(a,i0)\" ) \"S at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , S_1D , name = oname ) if ( ionode ) call print_type ( S_1D ) S => val ( S_1D ) !     Find overlap matrix call overlap ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , maxnh , & lasto , iphorb , isa , numh , listhptr , listh , S ) ! !     Here we could also read a Hamiltonian, either to proceed to !     the analysis section (with nscf=0) or to start a mix-H scf cycle. ! !     Initialize density matrix ! The resizing of Dscf is done inside new_dm call new_DM ( auxchanged , DM_history , DM_2D , EDM_2D ) Dscf => val ( DM_2D ) Escf => val ( EDM_2D ) if ( spin % H > 1 ) call print_spin ( dummy_qspin ) ! Initialize energy-density matrix to zero for first call to overfsm ! Only part of Escf is updated in TS, so if it is put as zero here ! a continuation run gives bad forces. if ( . not . TSrun ) then call normalize_DM ( first = . true . ) !$OMP parallel workshare default(shared) Escf (:,:) = 0.0_dp !$OMP end parallel workshare end if #ifdef TEST_IO ! We test the io-performance here call time_io ( spin % H , H_2D ) #endif !     If onlyS, Save overlap matrix and exit if ( onlyS ) then fname = fname_TSHS ( slabel , onlyS = . true . ) ! We include H as S, well-knowing that we only write one of ! them, there is no need to allocate space for no reason! call ts_write_tshs ( fname , & . true ., Gamma , ts_Gamma_file , & ucell , nsc , isc_off , na_u , no_s , spin % H , & ts_kscell_file , ts_kdispl_file , & xa , lasto , & H_2D , S_1D , indxuo , & dummyEf , dummyQtot , Temp , 0 , 0 ) call bye ( 'Save overlap matrix and exit' ) ! Exit siesta endif ! In case the user is requesting a Fermi-correction ! we need to delete the TS_FERMI file after each iteration if ( TSmode . and . TS_RHOCORR_METHOD == TS_RHOCORR_FERMI & . and . IONode ) then ! Delete the TS_FERMI file (enables ! reading it in and improve on the convergence) if ( file_exist ( 'TS_FERMI' ) ) then i = 23455 ! this should just not be used any were... ! Delete the file... open ( unit = i , file = 'TS_FERMI' ) close ( i , status = 'delete' ) end if end if #ifdef CDF if ( writedm_cdf ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh ) endif if ( writedm_cdf_history ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & istep ) endif if ( writedmhs_cdf ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s ) endif if ( writedmhs_cdf_history ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s , & istep ) endif #endif call timer ( 'state_init' , 2 ) END subroutine state_init subroutine check_cohp () use siesta_options , only : write_coop use sys , only : message if ( write_coop ) then call message ( \"WARNING\" , \"There are multiply-connected \" // $ \"orbitals.\" ) call message ( \"WARNING\" , \"Your COOP/COHP analysis might \" // $ \"be affected by folding.\" ) call message ( \"WARNING\" , 'Use \"force-aux-cell T \"' // $ 'or k-point sampling' ) endif end subroutine check_cohp END module m_state_init","tags":"","loc":"sourcefile/state_init.f.html","title":"state_init.F – SIESTA"},{"text":"This file depends on sourcefile~~state_analysis.f~~EfferentGraph sourcefile~state_analysis.f state_analysis.F sourcefile~parallel.f parallel.F sourcefile~state_analysis.f->sourcefile~parallel.f sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~state_analysis.f->sourcefile~siesta_options.f90 sourcefile~units.f90 units.f90 sourcefile~state_analysis.f->sourcefile~units.f90 sourcefile~precision.f precision.F sourcefile~units.f90->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~state_analysis.f~~AfferentGraph sourcefile~state_analysis.f state_analysis.F sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~state_analysis.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_state_analysis Source Code state_analysis.F Source Code ! --- ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt . ! See Docs/Contributors.txt for a list of contributors. ! --- MODULE m_state_analysis use write_subs private public :: state_analysis CONTAINS subroutine state_analysis ( istep ) use siesta_cml use m_born_charge , only : born_charge use parallel , only : IOnode use m_wallclock , only : wallclock use zmatrix , only : lUseZmatrix , iofaZmat , & CartesianForce_to_ZmatForce use atomlist , only : iaorb , iphorb , amass , no_u , lasto use atomlist , only : indxuo use m_spin , only : spin use m_fixed , only : fixed use sparse_matrices use siesta_geom USE siesta_options use units , only : amu , eV use m_stress use m_energies , only : Etot , FreeE , Eharrs , FreeEHarris , Entropy use m_energies , only : Ebs , Ef use m_ntm use m_forces use m_energies , only : update_FreeE , update_FreeEHarris use m_intramol_pressure , only : remove_intramol_pressure #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_FORCES #endif implicit none integer :: istep integer :: ia , jx , ix real ( dp ) :: volume logical :: eggbox_block = . true . ! Read eggbox info from data file? real ( dp ) :: qspin external :: eggbox , mulliken , moments real ( dp ), external :: volcel !------------------------------------------------------------------------- BEGIN call timer ( 'state_analysis' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_analysis' ) #endif if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'SCF Finalization' ) endif !     Write final Kohn-Sham and Free Energy FreeE = Etot - Temp * Entropy FreeEHarris = Eharrs - Temp * Entropy if ( cml_p ) call cmlStartPropertyList ( mainXML , & title = 'Energies and spin' ) if ( IOnode ) then if ( . not . harrisfun ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS(eV) =        ' , Etot / eV if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = FreeE / eV , & dictref = 'siesta:FreeE' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ebs / eV , & dictref = 'siesta:Ebs' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ef / eV , & dictref = 'siesta:E_Fermi' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif endif !     Substract egg box effect from energy if ( eggbox_block ) then call eggbox ( 'energy' , ucell , na_u , isa , ntm , xa , fa , Etot , & eggbox_block ) FreeE = Etot - Temp * Entropy if ( IOnode ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS - E_eggbox = ' , Etot / eV if ( cml_p ) call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS_egg' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif call update_FreeE ( Temp ) call update_FreeEHarris ( Temp ) call print_spin ( qspin ) if ( cml_p ) call cmlEndPropertyList ( mainXML ) !     Substract egg box effect from the forces if ( eggbox_block ) then call eggbox ( 'forces' , ucell , na_u , isa , ntm , xa , fa , Etot , eggbox_block ) endif if ( IOnode ) call write_raw_efs ( stress , na_u , fa , FreeE ) !     Compute stress without internal molecular pressure call remove_intramol_pressure ( ucell , stress , na_u , xa , fa , mstress ) !     Impose constraints to atomic movements by changing forces ........... if ( RemoveIntraMolecularPressure ) then !        Consider intramolecular pressure-removal as another !        kind of constraint call fixed ( ucell , mstress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) else call fixed ( ucell , stress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) endif #ifdef SIESTA__FLOOK ! We call it right after using the ! geometry constraints. ! In that way we can use both methods on top ! of each other! ! The easy, already implemented methods in fixed, ! and custom ones in Lua :) call slua_call ( LUA , LUA_FORCES ) #endif !     Calculate and output Zmatrix forces if ( lUseZmatrix . and . ( idyn . eq . 0 )) then call CartesianForce_to_ZmatForce ( na_u , xa , fa ) if ( IOnode ) call iofaZmat () endif !     Compute kinetic contribution to stress kin_stress ( 1 : 3 , 1 : 3 ) = 0.0_dp volume = volcel ( ucell ) do ia = 1 , na_u do jx = 1 , 3 do ix = 1 , 3 kin_stress ( ix , jx ) = kin_stress ( ix , jx ) - & amu * amass ( ia ) * va ( ix , ia ) * va ( jx , ia ) / volume enddo enddo enddo !     Add kinetic term to stress tensor tstress = stress + kin_stress !     Force output if ( IOnode ) then call siesta_write_forces ( istep ) call siesta_write_stress_pressure () call wallclock ( '--- end of geometry step' ) endif !     Population and moment analysis if ( spin % SO . and . orbmoms ) then call moments ( 1 , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) endif ! Call this unconditionally call mulliken ( mullipop , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) ! !     Call the born effective charge routine only in those steps (even) !     in which the dx  is positive. if ( bornz . and . ( mod ( istep , 2 ) . eq . 0 )) then call born_charge () endif !     End the xml module corresponding to the analysis if ( cml_p ) then call cmlEndModule ( mainXML ) endif call timer ( 'state_analysis' , 2 ) !--------------------------------------------------------------------------- END END subroutine state_analysis END MODULE m_state_analysis","tags":"","loc":"sourcefile/state_analysis.f.html","title":"state_analysis.F – SIESTA"},{"text":"The old dhscf has been split in two parts: an initialization routine dhscf_init , which is called after\n every geometry change but before\n the main scf loop, and a dhscf proper,\n which is called at every step\n of the scf loop Note The mesh initialization part is now done unconditionally in dhscf_init , i.e, after every geometry change, even if the\n change does not involve a cell change. The reason is to avoid\n complexity, since now the mesh parallel distributions will depend on\n the detailed atomic positions even if the cell does not change. Besides, the relative cost of a \"mesh only\" initialization is negligible.\n The only real observable effect would be a printout of \"initmesh\" data\n at every geometry iteration. This file depends on sourcefile~~dhscf.f~~EfferentGraph sourcefile~dhscf.f dhscf.F sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~precision.f precision.F sourcefile~dhscf.f->sourcefile~precision.f sourcefile~moremeshsubs.f moremeshsubs.F sourcefile~dhscf.f->sourcefile~moremeshsubs.f sourcefile~sys.f sys.F sourcefile~dhscf.f->sourcefile~sys.f sourcefile~siesta_options.f90 siesta_options.F90 sourcefile~dhscf.f->sourcefile~siesta_options.f90 sourcefile~mesh.f mesh.F sourcefile~dhscf.f->sourcefile~mesh.f sourcefile~forhar.f forhar.F sourcefile~dhscf.f->sourcefile~forhar.f sourcefile~rhoofd.f90 rhoofd.F90 sourcefile~dhscf.f->sourcefile~rhoofd.f90 sourcefile~m_iorho.f m_iorho.F sourcefile~dhscf.f->sourcefile~m_iorho.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~dhscf.f->sourcefile~atmfuncs.f sourcefile~vmat.f90 vmat.F90 sourcefile~dhscf.f->sourcefile~vmat.f90 sourcefile~units.f90 units.f90 sourcefile~dhscf.f->sourcefile~units.f90 sourcefile~parallel.f parallel.F sourcefile~dhscf.f->sourcefile~parallel.f sourcefile~m_rhog.f90 m_rhog.F90 sourcefile~dhscf.f->sourcefile~m_rhog.f90 sourcefile~dfscf.f dfscf.f sourcefile~dhscf.f->sourcefile~dfscf.f sourcefile~delk.f90->sourcefile~precision.f sourcefile~delk.f90->sourcefile~mesh.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~delk.f90->sourcefile~parallel.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~atm_types.f atm_types.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~moremeshsubs.f->sourcefile~precision.f sourcefile~moremeshsubs.f->sourcefile~sys.f sourcefile~moremeshsubs.f->sourcefile~mesh.f sourcefile~moremeshsubs.f->sourcefile~parallel.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~precision.f sourcefile~forhar.f->sourcefile~moremeshsubs.f sourcefile~forhar.f->sourcefile~mesh.f sourcefile~forhar.f->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~precision.f sourcefile~rhoofd.f90->sourcefile~sys.f sourcefile~rhoofd.f90->sourcefile~mesh.f sourcefile~rhoofd.f90->sourcefile~atmfuncs.f sourcefile~rhoofd.f90->sourcefile~parallel.f sourcefile~rhoofd.f90->sourcefile~atm_types.f sourcefile~m_iorho.f->sourcefile~precision.f sourcefile~m_iorho.f->sourcefile~sys.f sourcefile~m_iorho.f->sourcefile~parallel.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~radial.f radial.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~vmat.f90->sourcefile~precision.f sourcefile~vmat.f90->sourcefile~mesh.f sourcefile~vmat.f90->sourcefile~atmfuncs.f sourcefile~vmat.f90->sourcefile~parallel.f sourcefile~vmat.f90->sourcefile~atm_types.f sourcefile~units.f90->sourcefile~precision.f sourcefile~m_rhog.f90->sourcefile~precision.f sourcefile~m_rhog.f90->sourcefile~siesta_options.f90 sourcefile~m_rhog.f90->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~precision.f sourcefile~dfscf.f->sourcefile~sys.f sourcefile~dfscf.f->sourcefile~mesh.f sourcefile~dfscf.f->sourcefile~atmfuncs.f sourcefile~dfscf.f->sourcefile~parallel.f sourcefile~dfscf.f->sourcefile~atm_types.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~radial.f->sourcefile~precision.f var pansourcefiledhscffEfferentGraph = svgPanZoom('#sourcefiledhscffEfferentGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~dhscf.f~~AfferentGraph sourcefile~dhscf.f dhscf.F sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_dhscf Source Code dhscf.F Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! !! The old dhscf has been split in two parts: an initialization routine !! `[[dhscf_init(proc)]]`, which is called after !! every geometry change but before !! the main scf loop, and a `[[dhscf(proc)]]` proper, !! which is called at every step !! of the scf loop !!@note !! The mesh initialization part is now done *unconditionally* !! in `dhscf_init`, i.e, after *every* geometry change, even if the !! change does not involve a cell change. The reason is to avoid !! complexity, since now the mesh parallel distributions will depend on !! the detailed atomic positions even if the cell does not change. !!@endnote !! Besides, the relative cost of a \"mesh only\" initialization is negligible. !! The only real observable effect would be a printout of \"initmesh\" data !! at every geometry iteration. module m_dhscf !! To facilitate the communication among !! `[[dhscf_init(proc)]]` and `[[dhscf(proc)]]`, !! some arrays that hold data which do not change during the SCF loop !! have been made into module variables !! !! Some others are scratch, such as `nmpl`, `ntpl`, etc use precision , only : dp , grid_p use m_dfscf , only : dfscf implicit none real ( grid_p ), pointer :: rhopcc (:), rhoatm (:), Vna (:) real ( dp ) :: Uharrs !! Harris energy logical :: IsDiag , spiral character ( len = 10 ) :: shape integer :: nml ( 3 ), ntml ( 3 ), npcc , & nmpl , ntpl real ( dp ) :: bcell ( 3 , 3 ), cell ( 3 , 3 ), & dvol , field ( 3 ), rmax , scell ( 3 , 3 ) real ( dp ) :: G2mesh = 0.0_dp logical :: debug_dhscf = . false . character ( len =* ), parameter :: debug_fmt = & '('' -- dhscf : Node '',i3,tr1,a,4(tr1,e20.7))' public :: dhscf_init , dhscf CONTAINS subroutine dhscf_init ( nspin , norb , iaorb , iphorb , & nuo , nuotot , nua , na , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnd , numd , listdptr , listd , datm , & Fal , stressl ) use precision , only : dp , grid_p use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use fdf use sys , only : die use mesh , only : xdsp , nsm , nsp , meshLim use parsing #ifndef BSC_CELLXC use siestaXC , only : getXC ! Returns the XC functional used #else /* BSC_CELLXC */ use bsc_xcmod , only : nXCfunc , XCauth #endif /* BSC_CELLXC */ use alloc , only : re_alloc , de_alloc use siesta_options , only : harrisfun use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use meshsubs , only : PhiOnMesh use meshsubs , only : InitMesh use meshsubs , only : InitAtomMesh use meshsubs , only : setupExtMesh use meshsubs , only : distriPhiOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use meshdscf , only : createLocalDscfPointers use iogrid_netcdf , only : set_box_limits #ifdef NCDF_4 use m_ncdf_io , only : cdf_init_mesh #endif #ifdef BSC_CELLXC use cellxc_mod , only : setGGA #endif /* BSC_CELLXC */ use m_efield , only : initialize_efield , acting_efield use m_efield , only : get_field_from_dipole use m_efield , only : dipole_correction use m_efield , only : user_specified_field use m_doping_uniform , only : initialize_doping_uniform use m_doping_uniform , only : compute_doping_structs_uniform , $ doping_active use m_rhog , only : rhog , rhog_in use m_rhog , only : order_rhog use siesta_options , only : mix_charge #ifdef MPI use mpi_siesta #endif use m_mesh_node , only : init_mesh_node use m_charge_add , only : init_charge_add use m_hartree_add , only : init_hartree_add use m_ts_global_vars , only : TSmode use m_ts_options , only : IsVolt , N_Elec , Elecs use m_ts_voltage , only : ts_init_voltage use m_ts_hartree , only : ts_init_hartree_fix implicit none integer , intent ( in ) :: nspin , norb , iaorb ( norb ), iphorb ( norb ), & nuo , nuotot , nua , na , isa ( na ), & indxua ( na ), mscell ( 3 , 3 ), maxnd , & numd ( nuo ), listdptr ( nuo ), listd ( maxnd ) real ( dp ), intent ( in ) :: xa ( 3 , na ), ucell ( 3 , 3 ), datm ( norb ) real ( dp ), intent ( inout ) :: g2max integer , intent ( inout ) :: ntm ( 3 ) real ( dp ), intent ( inout ) :: Fal ( 3 , nua ), stressl ( 3 , 3 ) real ( dp ), parameter :: tiny = 1.e-12_dp integer :: io , ia , iphi , is , n , i , j integer :: nsc ( 3 ), nbcell , nsd real ( dp ) :: DStres ( 3 , 3 ), volume real ( dp ), external :: volcel , ddot real ( grid_p ) :: dummy_Drho ( 1 , 1 ), dummy_Vaux ( 1 ), & dummy_Vscf ( 1 ) logical , save :: frstme = . true . ! Keeps state real ( grid_p ), pointer :: Vscf (:,:), rhoatm_par (:) integer , pointer :: numphi (:), numphi_par (:) integer :: nm ( 3 ) ! For call to initMesh #ifndef BSC_CELLXC integer :: nXCfunc character ( len = 20 ) :: XCauth ( 10 ), XCfunc ( 10 ) #endif /* ! BSC_CELLXC */ ! Transport direction (unit-cell aligned) integer :: iE real ( dp ) :: ortho , field ( 3 ), field2 ( 3 ) !--------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE dhscf_init' ) #endif ! ---------------------------------------------------------------------- !     General initialisation ! ---------------------------------------------------------------------- !     Start time counter call timer ( 'DHSCF_Init' , 1 ) nsd = min ( nspin , 2 ) nullify ( Vscf , rhoatm_par ) if ( frstme ) then debug_dhscf = fdf_get ( 'Debug.DHSCF' , . false .) nullify ( xdsp , rhopcc , Vna , rhoatm ) !       nsm lives in module m_dhscf now    !! AG** nsm = fdf_integer ( 'MeshSubDivisions' , 2 ) nsm = max ( nsm , 1 ) !       Set mesh sub-division variables & perform one off allocation nsp = nsm * nsm * nsm call re_alloc ( xdsp , 1 , 3 , 1 , nsp , 'xdsp' , 'dhscf_init' ) !       Check spin-spiral wavevector (if defined) if ( spiral . and . nspin . lt . 4 ) & call die ( 'dhscf: ERROR: spiral defined but nspin < 4' ) endif ! First time #ifndef BSC_CELLXC ! Get functional(s) being used call getXC ( nXCfunc , XCfunc , XCauth ) #endif /* ! BSC_CELLXC */ if ( harrisfun ) then do n = 1 , nXCfunc if (. not .( leqi ( XCauth ( n ), 'PZ' ). or . leqi ( XCauth ( n ), 'CA' ))) then call die ( \"** Harris forces not implemented for non-LDA XC\" ) endif enddo endif ! ---------------------------------------------------------------------- !     Orbital initialisation : part 1 ! ---------------------------------------------------------------------- !     Find the maximum orbital radius rmax = 0.0_dp do io = 1 , norb ia = iaorb ( io ) ! Atomic index of each orbital iphi = iphorb ( io ) ! Orbital index of each  orbital in its atom is = isa ( ia ) ! Species index of each atom rmax = max ( rmax , rcut ( is , iphi ) ) enddo !     Start time counter for mesh initialization call timer ( 'DHSCF1' , 1 ) ! ---------------------------------------------------------------------- !     Unit cell handling ! ---------------------------------------------------------------------- !     Find diagonal unit cell and supercell call digcel ( ucell , mscell , cell , scell , nsc , IsDiag ) if (. not . IsDiag ) then if ( Node . eq . 0 ) then write ( 6 , '(/,a,3(/,a,3f12.6,a,i6))' ) & 'DHSCF: WARNING: New shape of unit cell and supercell:' , & ( 'DHSCF:' ,( cell ( i , j ), i = 1 , 3 ), '   x' , nsc ( j ), j = 1 , 3 ) endif endif !     Find the system shape call shaper ( cell , nua , isa , xa , shape , nbcell , bcell ) !     Find system volume volume = volcel ( cell ) ! ---------------------------------------------------------------------- !     Mesh initialization ! ---------------------------------------------------------------------- call InitMesh ( na , cell , norb , iaorb , iphorb , isa , rmax , & G2max , G2mesh , nsc , nmpl , nm , & nml , ntm , ntml , ntpl , dvol ) !     Setup box descriptors for each processor, !     held in module iogrid_netcdf call set_box_limits ( ntm , nsm ) ! Initialize information on local mesh for each node call init_mesh_node ( cell , ntm , meshLim , nsm ) ! Setup charge additions in the mesh call init_charge_add ( cell , ntm ) ! Setup Hartree additions in the mesh call init_hartree_add ( cell , ntm ) #ifdef NCDF_4 ! Initialize the box for each node... call cdf_init_mesh ( ntm , nsm ) #endif !     Stop time counter for mesh initialization call timer ( 'DHSCF1' , 2 ) ! ---------------------------------------------------------------------- !     End of mesh initialization ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     Initialize atomic orbitals, density and potential ! ---------------------------------------------------------------------- !     Start time counter for atomic initializations call timer ( 'DHSCF2' , 1 ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Initialise quantities relating to the atom-mesh positioning call InitAtomMesh ( UNIFORM , na , xa ) #ifdef BSC_CELLXC !     Check if we need extencils in cellxc call setGGA ( ) #endif /* BSC_CELLXC */ !     Compute the number of orbitals on the mesh and recompute the !     partions for every processor in order to have a similar load !     in each of them. nullify ( numphi ) call re_alloc ( numphi , 1 , nmpl , 'numphi' , 'dhscf_init' ) !$OMP parallel do default(shared), private(i) do i = 1 , nmpl numphi ( i ) = 0 enddo !$OMP end parallel do call distriPhiOnMesh ( nm , nmpl , norb , iaorb , iphorb , & isa , numphi ) !     Find if there are partial-core-corrections for any atom npcc = 0 do ia = 1 , na if ( rcore ( isa ( ia )) . gt . tiny ) npcc = 1 enddo !     Find partial-core-correction energy density !     Vscf and Vaux are not used here call re_alloc ( rhopcc , 1 , ntpl * npcc + 1 , 'rhopcc' , 'dhscf_init' ) if ( npcc . eq . 1 ) then call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , & nsd , dvol , volume , dummy_Vscf , dummy_Vaux , Fal , stressl , & . false ., . false . ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhopcc' , sqrt ( sum ( rhopcc ** 2 )) end if endif !     Find neutral-atom potential !     Drho is not used here call re_alloc ( Vna , 1 , ntpl , 'Vna' , 'dhscf_init' ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , dummy_DRho , Fal , stressl , & . false ., . false . ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Vna' , sqrt ( sum ( Vna ** 2 )) end if if ( nodes . gt . 1 ) then if ( node . eq . 0 ) then write ( 6 , \"(a)\" ) \"Setting up quadratic distribution...\" endif call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) !       Create extended mesh arrays for the second data distribution call setupExtMesh ( QUADRATIC , rmax ) !       Compute atom positions for the second data distribution call InitAtomMesh ( QUADRATIC , na , xa ) endif !     Calculate orbital values on mesh !     numphi has already been computed in distriPhiOnMesh !     in the UNIFORM distribution if ( nodes . eq . 1 ) then numphi_par => numphi else nullify ( numphi_par ) call re_alloc ( numphi_par , 1 , nmpl , 'numphi_par' , & 'dhscf_init' ) call distMeshData ( UNIFORM , numphi , QUADRATIC , & numphi_par , KEEP ) endif call PhiOnMesh ( nmpl , norb , iaorb , iphorb , isa , numphi_par ) if ( nodes . gt . 1 ) then call de_alloc ( numphi_par , 'numphi_par' , 'dhscf_init' ) endif call de_alloc ( numphi , 'numphi' , 'dhscf_init' ) ! ---------------------------------------------------------------------- !       Create sparse indexing for Dscf as needed for local mesh !       Note that this is done in the QUADRATIC distribution !       since 'endpht' (computed finally in PhiOnMesh and stored in !       meshphi module) is in that distribution. ! ---------------------------------------------------------------------- if ( Nodes . gt . 1 ) then call CreateLocalDscfPointers ( nmpl , nuotot , numd , listdptr , & listd ) endif ! ---------------------------------------------------------------------- !     Calculate terms relating to the neutral atoms on the mesh ! ---------------------------------------------------------------------- !     Find Harris (sum of atomic) electron density call re_alloc ( rhoatm_par , 1 , ntpl , 'rhoatm_par' , 'dhscf_init' ) call rhooda ( norb , nmpl , datm , rhoatm_par , iaorb , iphorb , isa ) !     rhoatm_par comes out of here in clustered form in QUADRATIC dist !     Routine Poison should use the uniform data distribution if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Create Rhoatm using UNIFORM distr, in sequential form call re_alloc ( rhoatm , 1 , ntpl , 'rhoatm' , 'dhscf_init' ) call distMeshData ( QUADRATIC , rhoatm_par , & UNIFORM , rhoatm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhoatm' , sqrt ( sum ( rhoatm ** 2 )) end if ! !  AG: The initialization of doping structs could be done here now, !      in the uniform distribution, and with a simple loop over !      rhoatm. if ( frstme ) call initialize_doping_uniform () if ( doping_active ) then call compute_doping_structs_uniform ( ntpl , rhoatm , nsd ) ! Will get the global number of hit points ! Then, the doping density to be added can be simply computed endif !     Allocate Temporal array call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf_init' ) !     Vscf is filled here but not used later !     Uharrs is computed (and saved) !     DStres is computed but not used later call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , rhoatm , & Uharrs , Vscf , DStres , nsm ) call de_alloc ( Vscf , 'Vscf' , 'dhscf_init' ) !     Always deallocate rhoatm_par, as it was used even if nodes=1 call de_alloc ( rhoatm_par , 'rhoatm_par' , 'dhscf_init' ) if ( mix_charge ) then call re_alloc ( rhog , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog' , 'dhscf_init' ) call re_alloc ( rhog_in , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog_in' , 'dhscf_init' ) call order_rhog ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nsm ) endif !     Stop time counter for atomic initializations call timer ( 'DHSCF2' , 2 ) ! ---------------------------------------------------------------------- !     At the end of initializations: !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution ! ---------------------------------------------------------------------- if ( frstme ) then call initialize_efield () end if ! Check if we need to add the potential ! corresponding to the voltage-drop. if ( TSmode ) then ! These routines are important if there are cell-changes call ts_init_hartree_fix ( cell , nua , xa , ntm , ntml ) if ( IsVolt ) then call ts_init_voltage ( cell , nua , xa , ntm ) end if if ( acting_efield ) then ! We do not allow the electric field for ! transiesta runs with V = 0, either. ! It does not make sense, only for fields perpendicular ! to the applied bias. ! We need to check that the e-field is perpendicular ! to the transport direction, and that the system is ! either a chain, or a slab. ! However, due to the allowance of a dipole correction ! along the transport direction for buffer calculations ! we have to allow all shapes. (atom is not transiesta ! compatible anyway) ! check that we do not violate the periodicity if ( Node . eq . 0 ) then write ( * , '(/,2(2a,/))' ) 'ts-WARNING: ' , & 'E-field/dipole-correction! ' , & 'ts-WARNING: ' , & 'I hope you know what you are doing!' end if ! This is either dipole or user, or both field (:) = user_specified_field (:) do iE = 1 , N_Elec field2 = Elecs ( iE )% cell (:, Elecs ( iE )% t_dir ) ortho = ddot ( 3 , field2 , 1 , field , 1 ) if ( abs ( ortho ) > 1.e-9_dp ) then call die ( ' User defined E - field must be & perpendicular to semi - infinite directions ' ) end if end do end if ! acting_efield ! We know that we currently allow people to do more than ! they probably should be allowed. However, there are many ! corner cases that may require dipole corrections, or ! electric fields to \"correct\" an intrinsic dipole. ! For instance, what should we do with a dipole in a transiesta ! calculation? ! Should we apply a field to counter act it in a device ! calculation? end if frstme = . false . call timer ( 'DHSCF_Init' , 2 ) #ifdef DEBUG call write_debug ( '    POS dhscf_init' ) #endif !------------------------------------------------------------------------- END end subroutine dhscf_init subroutine dhscf ( nspin , norb , iaorb , iphorb , nuo , & nuotot , nua , na , isa , xa , indxua , & ntm , ifa , istr , iHmat , & filesOut , maxnd , numd , & listdptr , listd , Dscf , datm , maxnh , Hmat , & Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , stress , Fal , stressl , & use_rhog_in , charge_density_only ) !! author: J.M. Soler !! date: August 1996 !! !! Calculates the self-consistent field contributions to Hamiltonian !! matrix elements, total energy and atomic forces. !! !! Coded by J.M. Soler, August 1996. July 1997. !! Modified by J.D. Gale, February 2000. !! !! !!### Units !! Energies in Rydbergs. !! Distances in Bohr. !! !!### Routines called internally !! * [[cellxc(proc)]]    : Finds total exch-corr energy and potential !! * [[cross(proc)]]   : Finds the cross product of two vectors !! * [[dfscf(proc)]]     : Finds SCF contribution to atomic forces !! * [[dipole(proc)]]    : Finds electric dipole moment !! * [[doping(proc)]]    : Adds a background charge for doped systems !! * [[write_rho(proc)]]     : Saves electron density on a file !! * [[poison(proc)]]    : Solves Poisson equation !! * [[reord(proc)]]     : Reorders electron density and potential arrays !! * [[rhooda(proc)]]    : Finds Harris electron density in the mesh !! * [[rhoofd(proc)]]    : Finds SCF electron density in the mesh !! * [[rhoofdsp(proc)]]  : Finds SCF electron density in the mesh for !!                    spiral arrangement of spins !! * [[timer(proc)]]     : Finds CPU times !! * [[vmat(proc)]]      : Finds matrix elements of SCF potential !! * [[vmatsp(proc)]]    : Finds matrix elements of SCF potential for !!                         spiral arrangement of spins !! * [[delk(proc)]] : Finds matrix elements of  exp(i \\vec{k} \\cdot \\vec{r})  !! * real*8 volcel( cell ) : Returns volume of unit cell !! !!### Internal variables and arrays !! * `real*8  bcell(3,3)`    : Bulk lattice vectors !! * `real*8  cell(3,3)`     : Auxiliary lattice vectors (same as ucell) !! * `real*8  const`         : Auxiliary variable (constant within a loop) !! * `real*8  DEc`           : Auxiliary variable to call cellxc !! * `real*8  DEx`           : Auxiliary variable to call cellxc !! * `real*8  dvol`          : Mesh-cell volume !! * `real*8  Ec`            : Correlation energy !! * `real*8  Ex`            : Exchange energy !! * `real*8  field(3)`      : External electric field !! * `integer i`             : General-purpose index !! * `integer ia`            : Atom index !! * `integer io`            : Orbital index !! * `integer ip`            : Point index !! * `integer is`            : Species index !! * `logical IsDiag`        : Is supercell diagonal? !! * `integer ispin`         : Spin index !! * `integer j`             : General-purpose index #ifndef BSC_CELLXC !! * `integer JDGdistr`      : J.D.Gale's parallel distribution of mesh points !! * `integer myBox(2,3)`    : My processor's mesh box #endif /* ! BSC_CELLXC */ !! * `integer nbcell`        : Number of independent bulk lattice vectors !! * `integer npcc`          : Partial core corrections? (0=no, 1=yes) !! * `integer nsd`           : Number of diagonal spin values (1 or 2) !! * `integer ntpl`          : Number of mesh Total Points in unit cell !!                           (including subpoints) locally !! * `real*4  rhoatm(ntpl)`  : Harris electron density !! * `real*4  rhopcc(ntpl)`  : Partial-core-correction density for xc !! * `real*4  DRho(ntpl)`    : Selfconsistent electron density difference !! * `real*8  rhotot`        : Total density at one point !! * `real*8  rmax`          : Maximum orbital radius !! * `real*8  scell(3,3)`    : Supercell vectors !! * `character shape*10`    : Name of system shape !! * `real*4  Vaux(ntpl)`    : Auxiliary potential array !! * `real*4  Vna(ntpl)`     : Sum of neutral-atom potentials !! * `real*8  volume`        : Unit cell volume !! * `real*4  Vscf(ntpl)`    : Hartree potential of selfconsistent density !! * `real*8  x0(3)`         : Center of molecule !! * `logical harrisfun`     : Harris functional or Kohn-Sham? use precision , only : dp , grid_p #ifndef BSC_CELLXC use parallel , only : ProcessorY #endif /* ! BSC_CELLXC */ !     Number of Mesh divisions of each cell vector (global) !     The status of this variable is confusing use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use units , only : Debye , eV , Ang use fdf use sys , only : die , bye use mesh , only : nsm , nsp use parsing use m_iorho , only : write_rho use m_forhar , only : forhar use alloc , only : re_alloc , de_alloc use files , only : slabel use files , only : filesOut_t ! derived type for output file names use siesta_options , only : harrisfun , save_initial_charge_density use siesta_options , only : analyze_charge_density_only use meshsubs , only : LocalChargeOnMesh use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use m_partial_charges , only : compute_partial_charges use m_partial_charges , only : want_partial_charges #ifndef BSC_CELLXC use siestaXC , only : cellXC ! Finds xc energy and potential use siestaXC , only : myMeshBox ! Returns my processor mesh box use siestaXC , only : jms_setMeshDistr => setMeshDistr ! Sets a distribution of mesh ! points over parallel processors #endif /* BSC_CELLXC */ use m_vmat , only : vmat use m_rhoofd , only : rhoofd #ifdef MPI use mpi_siesta #endif use iogrid_netcdf , only : write_grid_netcdf use iogrid_netcdf , only : read_grid_netcdf use siesta_options , only : read_charge_cdf use siesta_options , only : savebader use siesta_options , only : read_deformation_charge_cdf use siesta_options , only : mix_charge use m_efield , only : get_field_from_dipole , dipole_correction use m_efield , only : add_potential_from_field use m_efield , only : user_specified_field , acting_efield use m_doping_uniform , only : doping_active , doping_uniform use m_charge_add , only : charge_add use m_hartree_add , only : hartree_add #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif use m_rhofft , only : rhofft , FORWARD , BACKWARD use m_rhog , only : rhog_in , rhog use m_spin , only : spin use m_spin , only : Spiral , qSpiral use m_iotddft , only : write_tdrho use m_ts_global_vars , only : TSmode , TSrun use m_ts_options , only : IsVolt , Elecs , N_elec use m_ts_voltage , only : ts_voltage use m_ts_hartree , only : ts_hartree_fix implicit none integer , intent ( in ) :: nspin !! Number of different spin polarisations: !! nspin=1 => Unpolarized, nspin=2 => polarized !! nspin=4 => Noncollinear spin or spin-orbit. integer , intent ( in ) :: norb !! Total number of basis orbitals in supercell integer , intent ( in ) :: iaorb ( norb ) !! Atom to which each orbital belongs integer , intent ( in ) :: iphorb ( norb ) !! Orbital index (within atom) of each orbital integer , intent ( in ) :: nuo !! Number of orbitals in a unit cell in this node integer , intent ( in ) :: nuotot !! Number of orbitals in a unit cell integer , intent ( in ) :: nua !! Number of atoms in unit cell integer , intent ( in ) :: na !! Number of atoms in supercell integer , intent ( in ) :: isa ( na ) !! Species index of all atoms in supercell integer , intent ( in ) :: indxua ( na ) !! Index of equivalent atom in unit cell integer , intent ( in ) :: ifa !! Switch which fixes whether the SCF contrib: !! to atomic forces is calculated and added to fa. integer , intent ( in ) :: istr !! Switch which fixes whether the SCF contrib: !! to stress is calculated and added to stress. integer , intent ( in ) :: iHmat !! Switch which fixes whether the Hmat matrix !! elements are calculated or not. integer , intent ( in ) :: maxnd !! First dimension of listd and Dscf integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero density-matrix !! elements for each matrix row integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of rows of density-matrix integer , intent ( in ) :: listd ( * ) !! `listd(maxnd)`: Nonzero-density-matrix-element column !! indexes for each matrix row integer , intent ( in ) :: maxnh !! First dimension of listh and Hmat real ( dp ), intent ( in ) :: xa ( 3 , na ) !! Atomic positions of all atoms in supercell real ( dp ), intent ( in ) :: Dscf (:,:) !! `Dscf(maxnd,h_spin_dim)`: !! SCF density-matrix elements real ( dp ), intent ( in ) :: datm ( norb ) !! Harris density-matrix diagonal elements !! (atomic occupation charges of orbitals) real ( dp ), intent ( in ) :: Hmat (:,:) !! `Hmat(maxnh,h_spin_dim)`: !! Hamiltonian matrix in sparse form, !! to which are added the matrix elements !! `<ORB_I | DeltaV | ORB_J>`, where !! `DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris)` type ( filesOut_t ), intent ( inout ) :: filesOut !! Output file names (If blank => not saved) integer , intent ( inout ) :: ntm ( 3 ) !! Number of mesh divisions of each cell !! vector, including subgrid. real ( dp ), intent ( inout ) :: Fal ( 3 , nua ) !! Atomic forces, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative !! of `(Enascf - Enaatm + DUscf + Exc)` with !! respect to atomic positions, in Ry/Bohr. !! Contributions local to this node. real ( dp ), intent ( inout ) :: stressl ( 3 , 3 ) !! Stress tensor, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative of !! `(Enascf - Enaatm + DUscf + Exc) / volume` !! with respect to the strain tensor, in Ry. !! Contributions local to this node. real ( dp ) :: stress ( 3 , 3 ) real ( dp ), intent ( out ) :: Enaatm !! Integral of `Vna * rhoatm` real ( dp ), intent ( out ) :: Enascf !! Integral of `Vna * rhoscf` real ( dp ), intent ( out ) :: Uatm !! Harris hartree electron-interaction energy real ( dp ), intent ( out ) :: Uscf !! SCF hartree electron-interaction energy real ( dp ), intent ( out ) :: DUscf !! Electrostatic (Hartree) energy of !! `(rhoscf - rhoatm)` density real ( dp ), intent ( out ) :: DUext !! Interaction energy with external electric field real ( dp ), intent ( out ) :: Exc !! SCF exchange-correlation energy real ( dp ), intent ( out ) :: Dxc !! SCF double-counting correction to Exc !! `Dxc = integral of ( (epsxc - Vxc) * Rho )` !! All energies in Rydbergs real ( dp ), intent ( out ) :: dipol ( 3 ) !! Electric dipole (in a.u.) !! only when the system is a molecule logical , intent ( in ), optional :: use_rhog_in logical , intent ( in ), optional :: charge_density_only !     Local variables integer :: i , ia , ip , ispin , nsd , np_vac #ifndef BSC_CELLXC !     Interface to JMS's SiestaXC integer :: myBox ( 2 , 3 ) integer , save :: JDGdistr =- 1 real ( dp ) :: stressXC ( 3 , 3 ) #endif /* ! BSC_CELLXC */ real ( dp ) :: b1Xb2 ( 3 ), const , DEc , DEx , DStres ( 3 , 3 ), & Ec , Ex , rhotot , x0 ( 3 ), volume , Vmax_vac , Vmean_vac #ifdef BSC_CELLXC !     Dummy arrays for cellxc call real ( grid_p ) :: aux3 ( 3 , 1 ) real ( grid_p ) :: dummy_DVxcdn ( 1 , 1 , 1 ) #endif /* BSC_CELLXC */ logical :: use_rhog real ( dp ), external :: volcel , ddot external & cross , & dipole , & poison , & reord , rhooda , rhoofdsp , & timer , vmatsp , & readsp #ifdef BSC_CELLXC external bsc_cellxc #endif /* BSC_CELLXC */ !     Work arrays real ( grid_p ), pointer :: Vscf (:,:), Vscf_par (:,:), & DRho (:,:), DRho_par (:,:), & Vaux (:), Vaux_par (:), Chlocal (:), & Totchar (:), fsrc (:), fdst (:), & rhoatm_quad (:) => null (), & DRho_quad (:,:) => null () ! Temporary reciprocal spin quantity real ( grid_p ) :: rnsd #ifdef BSC_CELLXC real ( grid_p ), pointer :: Vscf_gga (:,:), DRho_gga (:,:) #endif /* BSC_CELLXC */ #ifdef MPI integer :: MPIerror real ( dp ) :: sbuffer ( 7 ), rbuffer ( 7 ) #endif #ifdef DEBUG call write_debug ( '    PRE DHSCF' ) #endif if ( spin % H /= size ( Dscf , dim = 2 ) ) then call die ( 'Spin components is not equal to options.' ) end if if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DM' , & ( sqrt ( sum ( Dscf (:, ispin ) ** 2 )), ispin = 1 , spin % H ) write ( * , debug_fmt ) Node , 'H' , & ( sqrt ( sum ( Hmat (:, ispin ) ** 2 )), ispin = 1 , spin % H ) end if !-------------------------------------------------------------------- BEGIN ! ---------------------------------------------------------------------- ! Start of SCF iteration part ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     At the end of DHSCF_INIT, and also at the end of any previous !     call to dhscf, we were in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna were in UNIFORM dist, sequential form !     The index array endpht was in the QUADRATIC distribution ! ---------------------------------------------------------------------- #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_restart ( ) #endif call timer ( 'DHSCF' , 1 ) call timer ( 'DHSCF3' , 1 ) nullify ( Vscf , Vscf_par , DRho , DRho_par , & Vaux , Vaux_par , Chlocal , Totchar ) #ifdef BSC_CELLXC nullify ( Vscf_gga , DRho_gga ) #endif /* BSC_CELLXC */ volume = volcel ( cell ) !------------------------------------------------------------------------- if ( analyze_charge_density_only ) then !! Use the functionality in the first block !! of the routine to get charge files and partial charges call setup_analysis_options () endif if ( filesOut % vna . ne . ' ' ) then ! Uniform dist, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vna' , 1 , ntml , Vna ) else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) end if #else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) #endif endif !     Allocate memory for DRho using the UNIFORM data distribution call re_alloc ( DRho , 1 , ntpl , 1 , nspin , 'DRho' , 'dhscf' ) ! Find number of diagonal spin values nsd = min ( nspin , 2 ) if ( nsd == 1 ) then rnsd = 1._grid_p else rnsd = 1._grid_p / nsd end if ! ---------------------------------------------------------------------- ! Find SCF electron density at mesh points. Store it in array DRho ! ---------------------------------------------------------------------- ! !     The reading routine works in the uniform distribution, in !     sequential form ! if ( present ( use_rhog_in )) then use_rhog = use_rhog_in else use_rhog = . false . endif if ( use_rhog ) then ! fourier transform back into drho call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog_in , BACKWARD ) else if ( read_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"Rho\" ) read_charge_cdf = . false . else if ( read_deformation_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"DeltaRho\" ) ! Add to diagonal components only do ispin = 1 , nsd do ip = 1 , ntpl !             rhoatm and Drho are in sequential mode DRho ( ip , ispin ) = DRho ( ip , ispin ) + rhoatm ( ip ) * rnsd enddo enddo read_deformation_charge_cdf = . false . else ! Set the QUADRATIC distribution and allocate memory for DRho_par ! since the construction of the density from the DM and orbital ! data needs that distribution if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_par , 1 , ntpl , 1 , nspin , & 'DRho_par' , 'dhscf' ) if ( Spiral ) then call rhoofdsp ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , nuo , nuotot , iaorb , & iphorb , isa , qspiral ) else call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , & nuo , nuotot , iaorb , iphorb , isa ) endif ! DRHO_par is here in QUADRATIC, clustered form !       Set the UNIFORM distribution again and copy DRho to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => DRho_par (:, ispin ) fdst => DRho (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( DRho_par , 'DRho_par' , 'dhscf' ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DRho' , & ( sqrt ( sum ( DRho (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if if ( save_initial_charge_density ) then ! This section is to be deprecated in favor ! of \"analyze_charge_density_only\" ! (except for the special name for the .nc file) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoInit' , nspin , & ntml , DRho ) else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) end if #else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) #endif call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after producing RHO_INIT from input DM\" ) endif endif if ( mix_charge ) then ! Save fourier transform of charge density call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog , FORWARD ) endif ! !     Proper place to integrate Hirshfeld and Voronoi code, !     since we have just computed rhoatm and Rho. if ( want_partial_charges ) then ! The endpht array is in the quadratic distribution, so ! we need to use it for this... if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_quad , 1 , ntpl , 1 , nspin , & 'DRho_quad' , 'dhscf' ) call re_alloc ( rhoatm_quad , 1 , ntpl , & 'rhoatm_quad' , 'dhscf' ) ! Redistribute grid-density do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_quad (:, ispin ) ! if nodes==1, this call will just reorder call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo call distMeshData ( UNIFORM , rhoatm , & QUADRATIC , rhoatm_quad , TO_CLUSTER ) call compute_partial_charges ( DRho_quad , rhoatm_quad , . nspin , iaorb , iphorb , . isa , nmpl , dvol ) call de_alloc ( rhoatm_quad , 'rhoatm_quad' , 'dhscf' ) call de_alloc ( Drho_quad , 'DRho_quad' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif ! ---------------------------------------------------------------------- ! Save electron density ! ---------------------------------------------------------------------- if ( filesOut % rho . ne . ' ' ) then !  DRho is already using a uniform, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Rho' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) end if #else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) #endif endif !----------------------------------------------------------------------- ! Save TD-electron density after every given number of steps- Rafi, Jan 2016 !----------------------------------------------------------------------- call write_tdrho ( filesOut ) if ( filesOut % tdrho . ne . ' ' ) then !  DRho is already using a uniform, sequential form call write_rho ( filesOut % tdrho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"TDRho\" ) endif ! ---------------------------------------------------------------------- ! Save the diffuse ionic charge and/or the total (ionic+electronic) charge ! ---------------------------------------------------------------------- if ( filesOut % psch . ne . ' ' . or . filesOut % toch . ne . ' ' ) then !       Find diffuse ionic charge on mesh ! Note that the *OnMesh routines, except PhiOnMesh, ! work with any distribution, thanks to the fact that ! the ipa, idop, and indexp arrays are distro-specific call re_alloc ( Chlocal , 1 , ntpl , 'Chlocal' , 'dhscf' ) call LocalChargeOnMesh ( na , isa , ntpl , Chlocal , indxua ) ! Chlocal comes out in clustered form, so we convert it call reord ( Chlocal , Chlocal , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Chlocal' , sqrt ( sum ( Chlocal ** 2 )) end if !       Save diffuse ionic charge if ( filesOut % psch . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Chlocal' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & 'Chlocal' ) end if #else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , 'Chlocal' ) #endif endif !       Save total (ionic+electronic) charge if ( filesOut % toch . ne . ' ' ) then ! ***************** ! **  IMPORTANT  ** ! The Chlocal array is re-used to minimize memory ! usage. In the this small snippet the Chlocal ! array will contain the total charge, and ! if the logic should change, (i.e. should Chlocal ! be retained) is the Totchar needed to be re-instantiated. ! ***************** !$OMP parallel default(shared), private(ispin,ip) do ispin = 1 , nsd !$OMP do do ip = 1 , ntpl Chlocal ( ip ) = Chlocal ( ip ) + DRho ( ip , ispin ) end do !$OMP end do end do !$OMP end parallel ! See note above #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoTot' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & \"TotalCharge\" ) end if #else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , \"TotalCharge\" ) #endif end if call de_alloc ( Chlocal , 'Chlocal' , 'dhscf' ) endif ! ---------------------------------------------------------------------- ! Save the total charge (model core + valence) for Bader analysis ! ---------------------------------------------------------------------- ! The test for toch guarantees that we are in \"analysis mode\" if ( filesOut % toch . ne . ' ' . and . savebader ) then call save_bader_charge () endif ! Find difference between selfconsistent and atomic densities !Both DRho and rhoatm are using a UNIFORM, sequential form !$OMP parallel do default(shared), private(ispin,ip), !$OMP&collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do ! ---------------------------------------------------------------------- ! Save electron density difference ! ---------------------------------------------------------------------- if ( filesOut % drho . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoDelta' , nspin , ntml , & DRho ) else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"DeltaRho\" ) end if #else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & DRho , \"DeltaRho\" ) #endif endif if ( present ( charge_density_only )) then if ( charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) RETURN endif endif ! End of analysis section ! Can exit now, if requested if ( analyze_charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after analyzing charge from input DM\" ) endif !------------------------------------------------------------- !     Transform spin density into sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif ! Add a background charge to neutralize the net charge, to ! model doped systems. It only adds the charge at points ! where there are atoms (i.e., not in vacuum). ! First, call with 'task=0' to add background charge if ( doping_active ) call doping_uniform ( cell , ntpl , 0 , $ DRho (:, 1 ), rhoatm ) ! Add doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '+' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Calculate the dipole moment ! ---------------------------------------------------------------------- dipol ( 1 : 3 ) = 0.0_dp if ( shape . ne . 'bulk' ) then ! Find center of system x0 ( 1 : 3 ) = 0.0_dp do ia = 1 , nua x0 ( 1 : 3 ) = x0 ( 1 : 3 ) + xa ( 1 : 3 , ia ) / nua enddo ! Find dipole ! This routine is distribution-blind ! and will reduce over all processors. call dipole ( cell , ntm , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), nsm , & DRho , x0 , dipol ) ! Orthogonalize dipole to bulk directions if ( shape . eq . 'chain' ) then const = ddot ( 3 , dipol , 1 , bcell , 1 ) / ddot ( 3 , bcell , 1 , bcell , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * bcell ( 1 : 3 , 1 ) else if ( shape . eq . 'slab' ) then call cross ( bcell ( 1 , 1 ), bcell ( 1 , 2 ), b1Xb2 ) const = ddot ( 3 , dipol , 1 , b1Xb2 , 1 ) / ddot ( 3 , b1Xb2 , 1 , b1Xb2 , 1 ) dipol ( 1 : 3 ) = const * b1Xb2 ( 1 : 3 ) end if if ( TSmode ) then if ( N_elec > 1 ) then ! Orthogonalize dipole to electrode transport directions do ia = 1 , N_Elec x0 = Elecs ( ia )% cell (:, Elecs ( ia )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * x0 end do else if ( ( shape == 'molecule' ) . or . ( shape == 'chain' ) ) then ! Only allow dipole correction for chains and molecules ! along the semi-infinite direciton. ! Note this is *only* for 1-electrode setups ! Note that since the above removes the periodic directions ! this should not do anything for 'chain' with the same semi-infinite ! direction x0 = Elecs ( 1 )% cell (:, Elecs ( 1 )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = const * x0 end if end if endif ! ---------------------------------------------------------------------- !     Find Hartree potential of DRho = rhoscf-rhoatm. Store it in Vaux ! ---------------------------------------------------------------------- !     Solve Poisson's equation call re_alloc ( Vaux , 1 , ntpl , 'Vaux' , 'dhscf' ) call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , DRho , & DUscf , Vaux , DStres , nsm ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Poisson' , sqrt ( sum ( Vaux (:) ** 2 )) end if ! Vscf is in the UNIFORM, sequential form, and only using ! the first spin index ! We require that even the SIESTA potential is \"fixed\" ! NOTE, this will only do something if !   TS.Hartree.Fix is set call ts_hartree_fix ( ntm , ntml , Vaux ) ! Add contribution to stress from electrostatic energy of rhoscf-rhoatm if ( istr . eq . 1 ) then stressl ( 1 : 3 , 1 : 3 ) = stressl ( 1 : 3 , 1 : 3 ) + DStres ( 1 : 3 , 1 : 3 ) endif ! ---------------------------------------------------------------------- !     Find electrostatic (Hartree) energy of full SCF electron density !     using the original data distribution ! ---------------------------------------------------------------------- Uatm = Uharrs Uscf = 0._dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Uscf) do ip = 1 , ntpl Uscf = Uscf + Vaux ( ip ) * rhoatm ( ip ) enddo !$OMP end parallel do Uscf = Uscf * dVol + Uatm + DUscf ! Call doping with 'task=1' to remove background charge added previously ! The extra charge thus only affects the Hartree energy and potential, ! but not the contribution to Enascf ( = \\Int_{Vna*\\rho}) if ( doping_active ) call doping_uniform ( cell , ntpl , 1 , $ DRho (:, 1 ), rhoatm ) ! Remove doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '-' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Add neutral-atom potential to Vaux ! ---------------------------------------------------------------------- Enaatm = 0.0_dp Enascf = 0.0_dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Enaatm,Enascf) do ip = 1 , ntpl Enaatm = Enaatm + Vna ( ip ) * rhoatm ( ip ) Enascf = Enascf + Vna ( ip ) * DRho ( ip , 1 ) Vaux ( ip ) = Vaux ( ip ) + Vna ( ip ) enddo !$OMP end parallel do Enaatm = Enaatm * dVol Enascf = Enaatm + Enascf * dVol ! ---------------------------------------------------------------------- ! Add potential from external electric field (if present) ! ---------------------------------------------------------------------- if ( acting_efield ) then if ( dipole_correction ) then field = get_field_from_dipole ( dipol , cell ) if ( Node == 0 ) then write ( 6 , '(a,3f12.4,a)' ) $ 'Dipole moment in unit cell   =' , dipol / Debye , ' D' write ( 6 , '(a,3f12.6,a)' ) $ 'Electric field for dipole correction =' , $ field / eV * Ang , ' eV/Ang/e' end if ! The dipole correction energy has an extra factor ! of one half because the field involved is internal. ! See the paper by Bengtsson DOI:10.1103/PhysRevB.59.12301 ! Hence we compute this part separately DUext = - 0.5_dp * ddot ( 3 , field , 1 , dipol , 1 ) else field = 0._dp DUext = 0._dp end if ! Add the external electric field field = field + user_specified_field ! This routine expects a sequential array, ! but it is distribution-blind call add_potential_from_field ( field , cell , nua , isa , xa , & ntm , nsm , Vaux ) ! Add energy of external electric field DUext = DUext - ddot ( 3 , user_specified_field , 1 , dipol , 1 ) endif ! --------------------------------------------------------------------- !     Transiesta: !     add the potential corresponding to the (possible) voltage-drop. !     note that ts_voltage is not sharing the reord wih efield since !     we should not encounter both at the same time. ! --------------------------------------------------------------------- if ( TSmode . and . IsVolt . and . TSrun ) then ! This routine expects a sequential array, ! in whatever distribution #ifdef TRANSIESTA_VOLTAGE_DEBUG !$OMP parallel workshare default(shared) Vaux (:) = 0._dp !$OMP end parallel workshare #endif call ts_voltage ( cell , ntm , ntml , Vaux ) #ifdef TRANSIESTA_VOLTAGE_DEBUG call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"TransiestaHartreePotential\" ) call timer ( 'ts_volt' , 3 ) call bye ( 'transiesta debug for Hartree potential' ) #endif endif ! ---------------------------------------------------------------------- ! Add potential from user defined geometries (if present) ! ---------------------------------------------------------------------- call hartree_add ( cell , ntpl , Vaux ) ! ---------------------------------------------------------------------- !     Save electrostatic potential ! ---------------------------------------------------------------------- if ( filesOut % vh . ne . ' ' ) then ! Note that only the first spin component is used #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vh' , 1 , ntml , & Vaux ) else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"ElectrostaticPotential\" ) end if #else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Vaux , \"ElectrostaticPotential\" ) #endif endif !     Get back spin density from sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(ip,rhotot) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) DRho ( ip , 1 ) = 0.5_dp * ( rhotot - DRho ( ip , 2 )) DRho ( ip , 2 ) = 0.5_dp * ( rhotot + DRho ( ip , 2 )) enddo !$OMP end parallel do endif ! ---------------------------------------------------------------------- #ifndef BSC_CELLXC ! Set uniform distribution of mesh points and find my processor mesh box ! This is the interface to JM Soler's own cellxc routine, which sets ! up the right distribution internally. ! ---------------------------------------------------------------------- call jms_setMeshDistr ( distrID = JDGdistr , nMesh = ntm , . nNodesX = 1 , nNodesY = ProcessorY , nBlock = nsm ) call myMeshBox ( ntm , JDGdistr , myBox ) ! ---------------------------------------------------------------------- #endif /* ! BSC_CELLXC */ ! Exchange-correlation energy ! ---------------------------------------------------------------------- call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf' ) if ( npcc . eq . 1 ) then !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & ( rhopcc ( ip ) + rhoatm ( ip )) * rnsd enddo enddo !$OMP end parallel do else !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do end if ! Write the electron density used by cellxc if ( filesOut % rhoxc . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoXC' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) end if #else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) #endif endif !     Everything now is in UNIFORM, sequential form call timer ( \"CellXC\" , 1 ) #ifdef BSC_CELLXC if ( nodes . gt . 1 ) then call setMeshDistr ( LINEAR , nsm , nsp , nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_gga , 1 , ntpl , 1 , nspin , 'Vscf_gga' , 'dhscf' ) call re_alloc ( DRho_gga , 1 , ntpl , 1 , nspin , 'DRho_gga' , 'dhscf' ) ! Redistribute all spin densities do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_gga (:, ispin ) call distMeshData ( UNIFORM , fsrc , LINEAR , fdst , KEEP ) enddo call bsc_cellxc ( 0 , 0 , cell , ntml , ntml , ntpl , 0 , aux3 , nspin , & DRho_gga , Ex , Ec , DEx , DEc , Vscf_gga , & dummy_DVxcdn , stressl ) #endif /* BSC_CELLXC */ #ifndef BSC_CELLXC call cellXC ( 0 , cell , ntm , myBox ( 1 , 1 ), myBox ( 2 , 1 ), . myBox ( 1 , 2 ), myBox ( 2 , 2 ), . myBox ( 1 , 3 ), myBox ( 2 , 3 ), nspin , . DRho , Ex , Ec , DEx , DEc , stressXC , Vscf ) #else /* BSC_CELLXC */ if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif ! Redistribute to the Vxc array do ispin = 1 , nspin fsrc => Vscf_gga (:, ispin ) fdst => Vscf (:, ispin ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) enddo #endif /* BSC_CELLXC */ if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'XC' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if #ifndef BSC_CELLXC !     Vscf is still sequential after the call to JMS's cellxc #else /* BSC_CELLXC */ call de_alloc ( DRho_gga , 'DRho_gga' , 'dhscf' ) call de_alloc ( Vscf_gga , 'Vscf_gga' , 'dhscf' ) #endif /* BSC_CELLXC */ Exc = Ex + Ec Dxc = DEx + DEc call timer ( \"CellXC\" , 2 ) !     Vscf contains only Vxc, and is UNIFORM and sequential !     Now we add up the other contributions to it, at !     the same time that we get DRho back to true DeltaRho form !$OMP parallel default(shared), private(ip,ispin) ! Hartree potential only has diagonal components do ispin = 1 , nsd if ( npcc . eq . 1 ) then !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - & ( rhoatm ( ip ) + rhopcc ( ip )) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do else !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do endif enddo !$OMP end parallel #ifndef BSC_CELLXC stress = stress + stressXC #endif /* ! BSC_CELLXC */ ! ---------------------------------------------------------------------- !     Save total potential ! ---------------------------------------------------------------------- if ( filesOut % vt . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vt' , nspin , ntml , & Vscf ) else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , & Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Vscf , & \"TotalPotential\" ) end if #else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Vscf , \"TotalPotential\" ) #endif endif ! ---------------------------------------------------------------------- ! Print vacuum level ! ---------------------------------------------------------------------- if ( filesOut % vt /= ' ' . or . filesOut % vh /= ' ' ) then forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) + rhoatm (:) * rnsd call vacuum_level ( ntpl , nspin , DRho , Vscf , . np_vac , Vmax_vac , Vmean_vac ) forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) - rhoatm (:) * rnsd if ( np_vac > 0 . and . Node == 0 ) print '(/,a,2f12.6,a)' , . 'dhscf: Vacuum level (max, mean) =' , . Vmax_vac / eV , Vmean_vac / eV , ' eV' endif if ( filesOut % ebs_dens /= '' ) then call save_ebs_density () endif ! ---------------------------------------------------------------------- !     Find SCF contribution to hamiltonian matrix elements ! ---------------------------------------------------------------------- if ( iHmat . eq . 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !        This is a work array, to which we copy Vscf call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo if ( Spiral ) then call vmatsp ( norb , nmpl , dvol , nspin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa , qspiral ) else call vmat ( norb , nmpl , dvol , spin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa ) endif call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then !          Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif #ifdef MPI !     Global reduction of Uscf/DUscf/Uatm/Enaatm/Enascf #ifndef BSC_CELLXC !     Note that Exc and Dxc are already reduced in the new cellxc #endif /* ! BSC_CELLXC */ sbuffer ( 1 ) = Uscf sbuffer ( 2 ) = DUscf sbuffer ( 3 ) = Uatm sbuffer ( 4 ) = Enaatm sbuffer ( 5 ) = Enascf #ifdef BSC_CELLXC sbuffer ( 6 ) = Exc sbuffer ( 7 ) = Dxc #else sbuffer ( 6 : 7 ) = 0._dp #endif /* BSC_CELLXC */ call MPI_AllReduce ( sbuffer , rbuffer , 7 , MPI_double_precision , & MPI_Sum , MPI_Comm_World , MPIerror ) Uscf = rbuffer ( 1 ) DUscf = rbuffer ( 2 ) Uatm = rbuffer ( 3 ) Enaatm = rbuffer ( 4 ) Enascf = rbuffer ( 5 ) #ifdef BSC_CELLXC Exc = rbuffer ( 6 ) Dxc = rbuffer ( 7 ) #endif /* BSC_CELLXC */ #endif /* MPI */ !     Add contribution to stress from the derivative of the Jacobian of --- !     r->r' (strained r) in the integral of Vna*(rhoscf-rhoatm) if ( istr . eq . 1 ) then do i = 1 , 3 stress ( i , i ) = stress ( i , i ) + ( Enascf - Enaatm ) / volume enddo endif !     Stop time counter for SCF iteration part call timer ( 'DHSCF3' , 2 ) ! ---------------------------------------------------------------------- !     End of SCF iteration part ! ---------------------------------------------------------------------- if ( ifa . eq . 1 . or . istr . eq . 1 ) then ! ---------------------------------------------------------------------- ! Forces and stress : SCF contribution ! ---------------------------------------------------------------------- !       Start time counter for force calculation part call timer ( 'DHSCF4' , 1 ) !       Find contribution of partial-core-correction if ( npcc . eq . 1 ) then call reord ( rhopcc , rhopcc , nml , nsm , TO_CLUSTER ) call reord ( Vaux , Vaux , nml , nsm , TO_CLUSTER ) ! The partial core calculation only acts on ! the diagonal spin-components (no need to ! redistribute un-used elements) do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_CLUSTER ) enddo call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , nsd , & dvol , volume , Vscf , Vaux , Fal , & stressl , ifa . ne . 0 , istr . ne . 0 ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) call reord ( Vaux , Vaux , nml , nsm , TO_SEQUENTIAL ) ! ** see above do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_SEQUENTIAL ) enddo if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'PartialCore' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nsd ) end if endif if ( harrisfun ) then !         Forhar deals internally with its own needs !         for distribution changes #ifndef BSC_CELLXC call forhar ( ntpl , nspin , nml , ntml , ntm , npcc , cell , #else /* BSC_CELLXC */ call forhar ( ntpl , nspin , nml , ntml , npcc , cell , #endif /* BSC_CELLXC */ & rhoatm , rhopcc , Vna , DRho , Vscf , Vaux ) !         Upon return, everything is UNIFORM, sequential form endif !     Transform spin density into sum and difference ! TODO NC/SO ! Should we perform local diagonalization? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif !       Find contribution of neutral-atom potential call reord ( Vna , Vna , nml , nsm , TO_CLUSTER ) call reord ( DRho , DRho , nml , nsm , TO_CLUSTER ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , DRho , Fal , stressl , & ifa . ne . 0 , istr . ne . 0 ) call reord ( DRho , DRho , nml , nsm , TO_SEQUENTIAL ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo !       Remember that Vaux contains everything except Vxc call re_alloc ( Vaux_par , 1 , ntpl , 'Vaux_par' , 'dhscf' ) call distMeshData ( UNIFORM , Vaux , & QUADRATIC , Vaux_par , TO_CLUSTER ) call dfscf ( ifa , istr , na , norb , nuo , nuotot , nmpl , nspin , & indxua , isa , iaorb , iphorb , & maxnd , numd , listdptr , listd , Dscf , datm , & Vscf_par , Vaux_par , dvol , volume , Fal , stressl ) call de_alloc ( Vaux_par , 'Vaux_par' , 'dhscf' ) call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif !       Stop time counter for force calculation part call timer ( 'DHSCF4' , 2 ) ! ---------------------------------------------------------------------- !       End of force and stress calculation ! ---------------------------------------------------------------------- endif !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution !     Stop time counter call timer ( 'DHSCF' , 2 ) ! ---------------------------------------------------------------------- !     Free locally allocated memory ! ---------------------------------------------------------------------- call de_alloc ( Vaux , 'Vaux' , 'dhscf' ) call de_alloc ( Vscf , 'Vscf' , 'dhscf' ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) #ifdef DEBUG call write_debug ( '    POS DHSCF' ) #endif !------------------------------------------------------------------------ END CONTAINS subroutine save_bader_charge () use meshsubs , only : ModelCoreChargeOnMesh #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif ! Auxiliary routine to output the Bader Charge ! real ( grid_p ), pointer :: BaderCharge (:) => null () call re_alloc ( BaderCharge , 1 , ntpl , name = 'BaderCharge' , & routine = 'dhscf' ) ! Find a model core charge by re-scaling the local charge call ModelCoreChargeOnMesh ( na , isa , ntpl , BaderCharge , indxua ) ! It comes out in clustered form, so we convert it call reord ( BaderCharge , BaderCharge , nml , nsm , TO_SEQUENTIAL ) do ispin = 1 , nsd BaderCharge ( 1 : ntpl ) = BaderCharge ( 1 : ntpl ) + DRho ( 1 : ntpl , ispin ) enddo #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoBader' , 1 , ntml , & BaderCharge ) else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) end if #else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) #endif call de_alloc ( BaderCharge , name = 'BaderCharge' ) end subroutine save_bader_charge subroutine setup_analysis_options () !! For the analyze-charge-density-only case, !! avoiding any diagonalization use siesta_options , only : hirshpop , voropop use siesta_options , only : saverho , savedrho , saverhoxc use siesta_options , only : savevh , savevt , savevna use siesta_options , only : savepsch , savetoch want_partial_charges = ( hirshpop . or . voropop ) if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' end subroutine setup_analysis_options subroutine save_ebs_density () !! Optional output of the \"band-structure energy density\", which !! is just the charge density weighted by the eigenvalues, i.e., !! using EDM instead of DM in rhoofd use sparse_matrices , only : Escf real ( grid_p ), pointer :: Ebs_dens (:,:) => null (), & Ebs_dens_quad (:,:) => null () !     Allocate memory for Ebs_dens using the UNIFORM data distribution call re_alloc ( Ebs_dens , 1 , ntpl , 1 , nspin , 'Ebs_dens' , 'dhscf' ) !     Switch to quadratic distribution for call to rhoofd if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( ebs_dens_quad , 1 , ntpl , 1 , nspin , & 'Ebs_dens_quad' , 'dhscf' ) call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Escf , Ebs_dens_quad , & nuo , nuotot , iaorb , iphorb , isa ) !     Ebs_dens_par is here in QUADRATIC, clustered form !     Set the UNIFORM distribution again and copy Ebs_dens to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => Ebs_dens_quad (:, ispin ) fdst => Ebs_dens (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( Ebs_dens_quad , 'Ebs_dens_quad' , 'dhscf' ) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Ebs_density' , $ nspin , ntml , Ebs_dens ) else call write_rho ( filesOut % ebs_dens , $ cell , ntm , nsm , ntpl , nspin , Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Ebs_dens , & \"Ebs_density\" ) end if #else call write_rho ( filesOut % ebs_dens , cell , ntm , nsm , ntpl , nspin , $ Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Ebs_dens , \"Ebs_density\" ) #endif call de_alloc ( Ebs_dens , 'Ebs_dens' , 'dhscf' ) end subroutine save_ebs_density end subroutine dhscf subroutine delk_wrapper ( isigneikr , norb , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) !! This is a wrapper to call [[delk(proc)]], using some of the module !! variables of `m_dhscf`, but from outside `dhscf` itself. !! !! The dhscf module variables used are: !! !! * nmpl !! * dvol !! * nml !! * nmpl !! * ntml !! * ntpl !! !! Some of them might be put somewhere else (mesh?) to allow some !! of the kitchen-sink functionality of dhscf to be made more modular. !! For example, this wrapper might live independently if enough mesh !! information is made available to it. use m_delk , only : delk ! The real workhorse, similar to vmat use moreMeshSubs , only : setMeshDistr use moreMeshSubs , only : UNIFORM , QUADRATIC use parallel , only : Nodes use mesh , only : nsm , nsp integer :: isigneikr , & norb , nuo , nuotot , maxnd , & iaorb ( * ), iphorb ( * ), isa ( * ), & numd ( nuo ), & listdptr ( nuo ), listd ( maxnd ) ! ---------------------------------------------------------------------- ! Calculate matrix elements of exp(i \\vec{k} \\cdot \\vec{r}) ! ---------------------------------------------------------------------- if ( isigneikr . eq . 1 . or . isigneikr . eq . - 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call delk ( isigneikr , norb , nmpl , dvol , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) if ( nodes . gt . 1 ) then !           Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif end subroutine delk_wrapper end module m_dhscf","tags":"","loc":"sourcefile/dhscf.f.html","title":"dhscf.F – SIESTA"},{"text":"This file depends on sourcefile~~reord.f~~EfferentGraph sourcefile~reord.f reord.f sourcefile~precision.f precision.F sourcefile~reord.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Subroutines reord Source Code reord.f Source Code ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! subroutine reord ( fclust , fseq , nm , nsm , itr ) !! author: J.M.Soler !! date: May 1995 !! Re-orders a clustered data array into a sequential one and viceversa use precision , only : grid_p use alloc , only : re_alloc , de_alloc IMPLICIT NONE INTEGER , intent ( in ) :: NM ( 3 ) !! Number of Mesh divisions in each cell vector INTEGER , intent ( in ) :: NSM !! Number of Sub-divisions in each Mesh division INTEGER , intent ( in ) :: ITR !! TRanslation-direction switch !! ITR=+1 => From clustered to sequential !! ITR=-1 => From sequential to clustered REAL ( grid_p ), intent ( inout ) :: FCLUST ( * ) !! CLUSTered data: !!    REAL*4 FCLUST(NSM,NSM,NSM,NM1,NM2,NM3) REAL ( grid_p ), intent ( inout ) :: FSEQ ( * ) !! SEQuential data: !! !!    REAL*4 FSEQ(NSM*NM1,NSM*NM2,NSM*NM3) INTEGER . I , I0 , I1 , I2 , I3 , IS , IS1 , IS2 , IS3 , . J , J0 , NSM3 , NTM ( 3 ), NAUX real ( grid_p ), dimension (:), pointer :: AUX integer , dimension (:), pointer :: JS CALL TIMER ( 'REORD' , 1 ) NTM ( 1 ) = NM ( 1 ) * NSM NTM ( 2 ) = NM ( 2 ) * NSM NTM ( 3 ) = NM ( 3 ) * NSM NSM3 = NSM ** 3 NAUX = NM ( 1 ) * NM ( 2 ) * NSM3 ! !  Allocate local memory ! nullify ( AUX ) call re_alloc ( AUX , 1 , NAUX , 'AUX' , 'reord' ) nullify ( JS ) call re_alloc ( JS , 1 , NSM3 , 'JS' , 'reord' ) IS = 0 DO IS3 = 0 , NSM - 1 DO IS2 = 0 , NSM - 1 DO IS1 = 0 , NSM - 1 IS = IS + 1 JS ( IS ) = 1 + IS1 + NTM ( 1 ) * IS2 + NTM ( 1 ) * NTM ( 2 ) * IS3 ENDDO ENDDO ENDDO IF ( ITR . GT . 0 ) THEN DO I3 = 0 , NM ( 3 ) - 1 DO I2 = 0 , NM ( 2 ) - 1 I0 = NSM3 * ( NM ( 1 ) * I2 + NM ( 1 ) * NM ( 2 ) * I3 ) J0 = NTM ( 1 ) * NSM * I2 DO IS = 1 , NSM3 I = I0 + IS J = J0 + JS ( IS ) DO I1 = 1 , NM ( 1 ) AUX ( J ) = FCLUST ( I ) I = I + NSM3 J = J + NSM ENDDO ENDDO ENDDO I = NM ( 1 ) * NM ( 2 ) * NSM3 * I3 DO J = 1 , NM ( 1 ) * NM ( 2 ) * NSM3 FSEQ ( I + J ) = AUX ( J ) ENDDO ENDDO ELSE DO I3 = 0 , NM ( 3 ) - 1 I = NM ( 1 ) * NM ( 2 ) * NSM3 * I3 DO J = 1 , NM ( 1 ) * NM ( 2 ) * NSM3 AUX ( J ) = FSEQ ( I + J ) ENDDO DO I2 = 0 , NM ( 2 ) - 1 I0 = NSM3 * ( NM ( 1 ) * I2 + NM ( 1 ) * NM ( 2 ) * I3 ) J0 = NTM ( 1 ) * NSM * I2 DO IS = 1 , NSM3 I = I0 + IS J = J0 + JS ( IS ) DO I1 = 1 , NM ( 1 ) FCLUST ( I ) = AUX ( J ) I = I + NSM3 J = J + NSM ENDDO ENDDO ENDDO ENDDO ENDIF ! !  Free local memory ! call de_alloc ( JS , 'JS' , 'reord' ) call de_alloc ( AUX , 'AUX' , 'reord' ) CALL TIMER ( 'REORD' , 2 ) END","tags":"","loc":"sourcefile/reord.f.html","title":"reord.f – SIESTA"},{"text":"This file depends on sourcefile~~delk.f90~~EfferentGraph sourcefile~delk.f90 delk.F90 sourcefile~atm_types.f atm_types.f sourcefile~delk.f90->sourcefile~atm_types.f sourcefile~precision.f precision.F sourcefile~delk.f90->sourcefile~precision.f sourcefile~m_planewavematrixvar.f90 m_planewavematrixvar.F90 sourcefile~delk.f90->sourcefile~m_planewavematrixvar.f90 sourcefile~mesh.f mesh.F sourcefile~delk.f90->sourcefile~mesh.f sourcefile~atmfuncs.f atmfuncs.f sourcefile~delk.f90->sourcefile~atmfuncs.f sourcefile~parallel.f parallel.F sourcefile~delk.f90->sourcefile~parallel.f sourcefile~atm_types.f->sourcefile~precision.f sourcefile~radial.f radial.f sourcefile~atm_types.f->sourcefile~radial.f sourcefile~m_planewavematrixvar.f90->sourcefile~precision.f sourcefile~mesh.f->sourcefile~precision.f sourcefile~atmfuncs.f->sourcefile~atm_types.f sourcefile~atmfuncs.f->sourcefile~precision.f sourcefile~sys.f sys.F sourcefile~atmfuncs.f->sourcefile~sys.f sourcefile~atmfuncs.f->sourcefile~radial.f sourcefile~sys.f->sourcefile~parallel.f sourcefile~radial.f->sourcefile~precision.f Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Files dependent on this one sourcefile~~delk.f90~~AfferentGraph sourcefile~delk.f90 delk.F90 sourcefile~dhscf.f dhscf.F sourcefile~dhscf.f->sourcefile~delk.f90 sourcefile~setup_h0.f setup_H0.F sourcefile~setup_h0.f->sourcefile~dhscf.f sourcefile~setup_hamiltonian.f setup_hamiltonian.F sourcefile~setup_hamiltonian.f->sourcefile~dhscf.f sourcefile~siesta_analysis.f siesta_analysis.F sourcefile~siesta_analysis.f->sourcefile~dhscf.f sourcefile~compute_energies.f90 compute_energies.F90 sourcefile~compute_energies.f90->sourcefile~dhscf.f sourcefile~siesta_forces.f90 siesta_forces.F90 sourcefile~siesta_forces.f90->sourcefile~setup_h0.f sourcefile~siesta_forces.f90->sourcefile~setup_hamiltonian.f sourcefile~siesta_forces.f90->sourcefile~compute_energies.f90 Help × Graph Key Nodes of different colours represent the following: Graph Key Source File Source File This Page's Entity This Page's Entity Solid arrows point from a file to a file which it depends on. A file\n    is dependent upon another if the latter must be compiled before the former\n    can be. Contents Modules m_delk Source Code delk.F90 Source Code !TODO find out fom of the formula  <\\phi_{\\mu}| exp&#94;( iexpikr * i * \\vec{k} \\cdot \\vec{r} ) |\\phi_{\\nu}> ! ! Copyright (C) 1996-2016\tThe SIESTA group !  This file is distributed under the terms of the !  GNU General Public License: see COPYING in the top directory !  or http://www.gnu.org/copyleft/gpl.txt. ! See Docs/Contributors.txt for a list of contributors. ! module m_delk implicit none private public :: delk contains subroutine delk ( iexpikr , no , np , dvol , nvmax , & numVs , listVsptr , listVs , & nuo , nuotot , iaorb , iphorb , isa ) !! author: J. Junquera !! date: February 2008 !! license: GNU GPL !! !! Finds the matrix elements of a plane wave !! <\\phi_{\\mu}|e&#94;{(\\boldsymbol{iexpikr} \\: \\cdot \\: i \\vec{k} \\vec{r})}|\\phi_{\\nu}> !! !! First version written by J. Junquera in Feb. 2008 !! !! Adapted from an existing version of `vmat` after the parallelization !! designed by BSC in July 2011. !! !!###Output !! `complex(dp) [[m_planewavematrixvar(module):delkmat]](nvmax)`: !! value of nonzero elements in each row !! of the matrix elements of  exp(i*\\vec{k}\\vec{r})  !  Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo , indxua use siesta_geom , only : xa use listsc_module , only : listsc use mesh , only : dxa , nsp , xdop , xdsp , ne , nem use mesh , only : cmesh , ipa , idop , nmsc , iatfold use mesh , only : meshLim use meshdscf , only : matrixMtoOC use meshdscf , only : needdscfl , listdl , numdl , nrowsdscfl , listdlptr use meshphi , only : directphi , endpht , lstpht , listp2 , phi use parallel , only : Nodes , node use alloc , only : re_alloc , de_alloc use parallelsubs , only : GlobalToLocalOrb use m_planewavematrixvar , only : delkmat , wavevector #ifdef MPI use mpi_siesta #endif #ifdef _OPENMP use omp_lib #endif integer , intent ( in ) :: iexpikr !! Prefactor of the dot product between the !! the k-vector and the position-vector in exponent. !! It might be +1 or -1 integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of columns in `C` (local) integer , intent ( in ) :: nvmax !! First dimension of `listV` and `Vs`, and max !! number of nonzero elements in any row of `[[m_planewavematrixvar(module):delkmat]]` integer :: nuo , nuotot integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms integer , intent ( in ) :: numVs ( nuo ) !! Number of non-zero elements in a row of `[[m_planewavematrixvar(module):delkmat]]` integer , intent ( in ) :: listVsptr ( nuo ) !! Pointer to the start of rows in `listVs` integer , intent ( in ) :: listVs ( nvmax ) !! List of non-zero elements of `[[m_planewavematrixvar(module):delkmat]]` real ( dp ), intent ( in ) :: dvol !! Volume per mesh point ! Internal variables and arrays integer , parameter :: minloc = 1000 ! Min buffer size integer , parameter :: maxoa = 100 ! Max # of orb/atom integer :: i , ia , ic , ii , ijl , il , imp , ind , iop integer :: ip , iphi , io , is , isp , iu , iul integer :: ix , j , jc , jl , last , lasta , lastop integer :: maxloc , maxloc2 , nc , nlocal , nphiloc integer :: nvmaxl , triang , lenx , leny , lenz , lenxy logical :: ParallelLocal real ( dp ) :: Vij ( 2 ), r2sp , dxsp ( 3 ), VClocal ( 2 , nsp ) integer , pointer :: ilc (:), ilocal (:), iorb (:) real ( dp ), pointer :: DscfL (:,:), t_DscfL (:,:,:), Clocal (:,:) real ( dp ), pointer :: Vlocal (:,:), phia (:,:), r2cut (:) integer :: NTH , TID ! Variables to compute the matrix element of the plane wave ! (not included in the original vmat subroutine) integer :: irel , iua , irealim , inmp ( 3 ) real ( dp ) :: kxij , aux ( 2 ), dist ( 3 ), kpoint ( 3 ) real ( dp ) :: dxp ( 3 ), displaat ( 3 ) real ( dp ) :: dxpgrid ( 3 , nsp ) complex ( dp ), pointer :: delkmats (:), t_delkmats (:,:) #ifdef _TRACE_ integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE delk' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 4 ) #endif !   Start time counter call timer ( 'delk' , 1 ) !   Initialize the matrix elements of exp(i*\\vec{k} \\vec{r}) !$OMP parallel workshare default(shared) delkmat (:) = 0.0_dp !$OMP end parallel workshare kpoint (:) = dble ( iexpikr ) * wavevector (:) !  For debugging !      kpoint(:) = 0.0_dp !  End debugging !   Find atomic cutoff radii nullify ( r2cut ) call re_alloc ( r2cut , 1 , nsmax , 'r2cut' , 'delk' ) r2cut = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) end do !   Set algorithm logical ParallelLocal = ( Nodes > 1 ) lenx = meshLim ( 2 , 1 ) - meshLim ( 1 , 1 ) + 1 leny = meshLim ( 2 , 2 ) - meshLim ( 1 , 2 ) + 1 lenz = meshLim ( 2 , 3 ) - meshLim ( 1 , 3 ) + 1 lenxy = lenx * leny !   Find value of maxloc maxloc2 = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxloc = maxloc2 + minloc maxloc = min ( maxloc , no ) triang = ( maxloc + 1 ) * ( maxloc + 2 ) / 2 if ( ParallelLocal ) then if ( nrowsDscfL > 0 ) then nvmaxl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else nvmaxl = 1 end if end if !   Allocate local memory !$OMP parallel default(shared), & !$OMP&shared(NTH,t_DscfL,t_delkmats), & !$OMP&private(TID,last,delkmats,irealim), & !$OMP&private(ip,nc,nlocal,ic,imp,i,il,iu,iul,ii,ind,j,ijl,jl), & !$OMP&private(lasta,lastop,ia,is,iop,isp,ix,dxsp,r2sp,nphiloc,iphi,jc), & !$OMP&private(Vij,VClocal,DscfL,ilocal,ilc,iorb,Vlocal,Clocal,phia), & !$OMP&private(irel,inmp,dxp,dxpgrid,dist,kxij,iua,displaat,aux) !$OMP single #ifdef _OPENMP NTH = omp_get_num_threads ( ) #else NTH = 1 #endif !$OMP end single ! implicit barrier, IMPORTANT #ifdef _OPENMP TID = omp_get_thread_num ( ) + 1 #else TID = 1 #endif nullify ( Clocal , phia , ilocal , ilc , iorb , Vlocal ) !$OMP critical ! Perhaps the critical section is not needed, ! however it \"tells\" the OS to allocate per ! thread, possibly waiting for each thread to ! place the memory in the best position. allocate ( Clocal ( nsp , maxloc2 ) ) allocate ( ilocal ( no ) , ilc ( maxloc2 ) , iorb ( maxloc ) ) allocate ( Vlocal ( triang , 2 ) ) if ( DirectPhi ) allocate ( phia ( maxoa , nsp ) ) !$OMP end critical !$OMP single if ( ParallelLocal ) then nullify ( t_DscfL ) call re_alloc ( t_DscfL , 1 , nvmaxl , 1 , 2 , 1 , NTH , & 'DscfL' , 'delk' ) else if ( NTH > 1 ) then nullify ( t_delkmats ) call re_alloc ( t_delkmats , 1 , nvmax , 1 , NTH , & 'delkmats' , 'delk' ) end if end if !$OMP end single if ( ParallelLocal ) then DscfL => t_DscfL ( 1 : nvmaxl , 1 : 2 , TID ) DscfL ( 1 : nvmaxl , 1 : 2 ) = 0.0_dp else if ( NTH > 1 ) then delkmats => t_delkmats ( 1 : nvmax , TID ) else delkmats => delkmat end if end if !   Full initializations done only once ilocal ( 1 : no ) = 0 iorb ( 1 : maxloc ) = 0 Vlocal ( 1 : triang , 1 : 2 ) = 0.0_dp last = 0 !   Loop over grid points !$OMP do do ip = 1 , np !      Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !      Find new required size of Vlocal nlocal = last do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) . eq . 0 ) nlocal = nlocal + 1 end do !      If overflooded, add Vlocal to delkmat and reinitialize it if ( nlocal > maxloc . and . last > 0 ) then if ( ParallelLocal ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) ! The variables we want to compute in this subroutine are complex numbers ! Here, when irealim =1 we refer to the real part, and ! when irealim = 2 we refer to the imaginary part DscfL ( ind ,:) = DscfL ( ind ,:) + Vlocal ( ijl ,:) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listsc ( i , iu , listdl ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) DscfL ( ind ,:) = DscfL ( ind ,:) + aux (:) * dVol end do end if end do else do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) call GlobalToLocalOrb ( iu , Node , Nodes , iul ) if ( i == iu ) then do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = listVs ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( Vlocal ( ijl , 1 ), Vlocal ( ijl , 2 ), kind = dp ) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = listsc ( i , iu , listVs ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( aux ( 1 ), aux ( 2 ), kind = dp ) * dVol end do end if end do end if !         Reset local arrays do ii = 1 , last ilocal ( iorb ( ii )) = 0 end do iorb ( 1 : last ) = 0 ijl = ( last + 1 ) * ( last + 2 ) / 2 Vlocal ( 1 : ijl , 1 : 2 ) = 0.0_dp last = 0 end if !      Look for required orbitals not yet in Vlocal if ( nlocal > last ) then do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) then last = last + 1 ilocal ( i ) = last iorb ( last ) = i end if end do end if if ( DirectPhi ) then lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !            Localize the position of the mesh point irel = idop ( iop ) + ipa ( ia ) call ipack ( - 1 , 3 , nem , inmp , irel ) inmp (:) = inmp (:) + ( meshLim ( 1 ,:) - 1 ) inmp (:) = inmp (:) - 2 * ne (:) dxp (:) = cmesh (:, 1 ) * inmp ( 1 ) + & cmesh (:, 2 ) * inmp ( 2 ) + & cmesh (:, 3 ) * inmp ( 3 ) do isp = 1 , nsp dxpgrid (:, isp ) = dxp (:) + xdsp (:, isp ) end do !            Generate phi values if ( ia /= lasta . or . iop /= lastop ) then lasta = ia lastop = iop is = isa ( ia ) do isp = 1 , nsp dxsp (:) = xdsp (:, isp ) + xdop (:, iop ) - dxa (:, ia ) r2sp = sum ( dxsp ** 2 ) if ( r2sp < r2cut ( is ) ) then !$OMP critical call all_phi ( is , + 1 , dxsp , nphiloc , phia (:, isp ) ) !$OMP end critical else phia (:, isp ) = 0.0_dp end if end do end if iphi = iphorb ( i ) Clocal (:, ic ) = phia ( iphi ,:) !            Pre-multiply V and Clocal(,ic) Vij (:) = 0._dp do isp = 1 , nsp kxij = kpoint ( 1 ) * dxpgrid ( 1 , isp ) + & kpoint ( 2 ) * dxpgrid ( 2 , isp ) + & kpoint ( 3 ) * dxpgrid ( 3 , isp ) VClocal ( 1 , isp ) = dcos ( kxij ) * Clocal ( isp , ic ) VClocal ( 2 , isp ) = dsin ( kxij ) * Clocal ( isp , ic ) Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , ic ) end do !            ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) !            Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !               Loop over sub-points Vij (:) = 0.0_dp do isp = 1 , nsp Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) * 2._dp else Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) end if end do end do else !         Loop on first orbital of mesh point do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !            Localize the position of the mesh point irel = idop ( iop ) + ipa ( ia ) call ipack ( - 1 , 3 , nem , inmp , irel ) inmp (:) = inmp (:) + ( meshLim ( 1 ,:) - 1 ) inmp (:) = inmp (:) - 2 * ne (:) dxp (:) = cmesh (:, 1 ) * inmp ( 1 ) + & cmesh (:, 2 ) * inmp ( 2 ) + & cmesh (:, 3 ) * inmp ( 3 ) do isp = 1 , nsp dxpgrid (:, isp ) = dxp (:) + xdsp (:, isp ) end do !            Retrieve phi values Clocal (:, ic ) = phi (:, imp ) !            Pre-multiply V and Clocal(,ic) Vij (:) = 0._dp do isp = 1 , nsp kxij = kpoint ( 1 ) * dxpgrid ( 1 , isp ) + & kpoint ( 2 ) * dxpgrid ( 2 , isp ) + & kpoint ( 3 ) * dxpgrid ( 3 , isp ) VClocal ( 1 , isp ) = dcos ( kxij ) * Clocal ( isp , ic ) VClocal ( 2 , isp ) = dsin ( kxij ) * Clocal ( isp , ic ) Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , ic ) end do !            ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) !            Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !               Loop over sub-points Vij (:) = 0.0_dp do isp = 1 , nsp Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) * 2._dp else Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) end if end do end do end if end do !$OMP end do nowait !   Add final Vlocal to delkmat if ( ParallelLocal . and . last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) DscfL ( ind ,:) = DscfL ( ind ,:) + Vlocal ( ijl ,:) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listsc ( i , iu , listdl ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) DscfL ( ind ,:) = DscfL ( ind ,:) + aux (:) * dVol end do end if end do else if ( last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) if ( i == iu ) then do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = listVs ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( Vlocal ( ijl , 1 ), Vlocal ( ijl , 2 ), kind = dp ) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = listsc ( i , iu , listVs ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( aux ( 1 ), aux ( 2 ), kind = dp ) * dVol end do end if end do end if !$OMP barrier if ( ParallelLocal . and . NTH > 1 ) then !$OMP do collapse(2) do irealim = 1 , 2 do ind = 1 , nvmaxl do ii = 2 , NTH t_DscfL ( ind , irealim , 1 ) = t_DscfL ( ind , irealim , 1 ) + & t_DscfL ( ind , irealim , ii ) end do end do end do !$OMP end do else if ( NTH > 1 ) then !$OMP do do ind = 1 , nvmax do ii = 1 , NTH delkmat ( ind ) = delkmat ( ind ) + t_delkmats ( ind , ii ) end do end do !$OMP end do end if !   Free local memory deallocate ( Clocal , ilocal , ilc , iorb , Vlocal ) if ( DirectPhi ) deallocate ( phia ) !$OMP master if ( ParallelLocal ) then !      Redistribute Hamiltonian from mesh to orbital based distribution DscfL => t_DscfL ( 1 : nvmaxl , 1 : 2 , 1 ) call matrixMtoOC ( nvmaxl , nvmax , numVs , listVsptr , nuo , DscfL , delkmat ) call de_alloc ( t_DscfL , 'DscfL' , 'delk' ) else if ( NTH > 1 ) then call de_alloc ( t_delkmats , 'delkmats' , 'delk' ) end if !$OMP end master !$OMP end parallel call de_alloc ( r2cut , 'r2cut' , 'delk' ) #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'delk' , 2 ) #ifdef DEBUG call write_debug ( '    POS delk' ) #endif contains !   In any case will the compiler most likely inline this !   small routine. So it should not pose any problem. pure function idx_ijl ( i , j ) result ( ij ) integer , intent ( in ) :: i , j integer :: ij if ( i > j ) then ij = i * ( i + 1 ) / 2 + j + 1 else ij = j * ( j + 1 ) / 2 + i + 1 end if end function idx_ijl end subroutine delk end module m_delk","tags":"","loc":"sourcefile/delk.f90.html","title":"delk.F90 – SIESTA"},{"text":"type, private :: meshDisType Private type to hold mesh distribution data. Contents Variables nMesh box indexp idop xdop ipa Source Code meshDisType Components Type Visibility Attributes Name Initial integer, public :: nMesh (3) Number of mesh div. in each axis. integer, public, pointer :: box (:,:,:) Mesh box bounds of each node: box(1,iAxis,iNode)=lower bounds box(2,iAxis,iNode)=upper bounds integer, public, pointer :: indexp (:) integer, public, pointer :: idop (:) real(kind=dp), public, pointer :: xdop (:,:) integer, public, pointer :: ipa (:) Source Code TYPE meshDisType !! Private type to hold mesh distribution data. integer :: nMesh ( 3 ) !! Number of mesh div. in each axis. integer , pointer :: box (:,:,:) !! Mesh box bounds of each node: !! box(1,iAxis,iNode)=lower bounds !! box(2,iAxis,iNode)=upper bounds integer , pointer :: indexp (:) integer , pointer :: idop (:) real ( dp ), pointer :: xdop (:,:) integer , pointer :: ipa (:) END TYPE meshDisType","tags":"","loc":"type/meshdistype.html","title":"meshDisType – SIESTA "},{"text":"type, private :: meshCommType Private type to hold communications to move data from one\n distribution to another. Contents Variables ncom src dst Source Code meshCommType Components Type Visibility Attributes Name Initial integer, public :: ncom Number of needed communications integer, public, pointer :: src (:) Sources of communications integer, public, pointer :: dst (:) Destination of communications Source Code TYPE meshCommType !! Private type to hold communications to move data from one !! distribution to another. integer :: ncom !! Number of needed communications integer , pointer :: src (:) !! Sources of communications integer , pointer :: dst (:) !! Destination of communications END TYPE meshCommType","tags":"","loc":"type/meshcommtype.html","title":"meshCommType – SIESTA "},{"text":"type, public :: tMixer Inherits type~~tmixer~~InheritsGraph type~tmixer tMixer type~tmixer->type~tmixer next, next_conv Fstack_dData1D Fstack_dData1D type~tmixer->Fstack_dData1D stack Help × Graph Key Nodes of different colours represent the following: Graph Key Type Type This Page's Entity This Page's Entity Solid arrows point from a derived type to the parent type which it\n    extends. Dashed arrows point from a derived type to the other\n    types it contains as a components, with a label listing the name(s) of\n    said component(s). Contents Variables name stack m v cur_itt start_itt n_hist n_itt restart restart_save action next next_conv w rv iv Source Code tMixer Components Type Visibility Attributes Name Initial character(len=24), public :: name type(Fstack_dData1D), public, allocatable :: stack (:) integer, public :: m = MIX_PULAY integer, public :: v = 0 integer, public :: cur_itt = 0 integer, public :: start_itt = 0 integer, public :: n_hist = 2 integer, public :: n_itt = 0 integer, public :: restart = 0 integer, public :: restart_save = 0 integer, public :: action = ACTION_MIX type( tMixer ), public, pointer :: next => null() type( tMixer ), public, pointer :: next_conv => null() real(kind=dp), public :: w = 0._dp real(kind=dp), public, pointer :: rv (:) => null() integer, public, pointer :: iv (:) => null() Source Code type tMixer ! Name of mixer character ( len = 24 ) :: name ! The different saved variables per iteration ! and their respective stacks type ( Fstack_dData1D ), allocatable :: stack (:) ! The method of the mixer integer :: m = MIX_PULAY ! In case the mixing method has a variant ! this denote the variant ! This value is thus specific for each method integer :: v = 0 ! The currently reached iteration integer :: cur_itt = 0 , start_itt = 0 ! Different mixers may have different histories integer :: n_hist = 2 ! Number of iterations using this mixer ! There are a couple of signals here !  == 0 : !     only use this mixer until convergence !   > 0 : !     after having runned n_itt step to \"next\" integer :: n_itt = 0 ! When mod(cur_itt,restart_itt) == 0 the history will ! be _reset_ integer :: restart = 0 integer :: restart_save = 0 ! This is an action token specifying the current ! action integer :: action = ACTION_MIX ! The next mixing method following this method type ( tMixer ), pointer :: next => null () ! The next mixing method following this method ! Only used if mixing method achieved convergence ! using this method type ( tMixer ), pointer :: next_conv => null () ! ** Parameters specific for the method: ! The mixing parameter used for this mixer real ( dp ) :: w = 0._dp ! linear array of real variables used specifically ! for this mixing type real ( dp ), pointer :: rv (:) => null () integer , pointer :: iv (:) => null () #ifdef MPI ! In case we have MPI the mixing scheme ! can implement a reduction scheme. ! This can be MPI_Comm_Self to not employ any ! reductions integer :: Comm = MPI_Comm_Self #endif end type tMixer","tags":"","loc":"type/tmixer.html","title":"tMixer – SIESTA "},{"text":"type, public :: rad_func Inherited by type~~rad_func~~InheritedByGraph type~rad_func rad_func type~species_info species_info type~species_info->type~rad_func orbnl, pjnl, pjldau, vna, chlocal, reduced_vlocal, core Help × Graph Key Nodes of different colours represent the following: Graph Key Type Type This Page's Entity This Page's Entity Solid arrows point from a derived type to the parent type which it\n    extends. Dashed arrows point from a derived type to the other\n    types it contains as a components, with a label listing the name(s) of\n    said component(s). Contents Variables n cutoff delta f d2 Source Code rad_func Components Type Visibility Attributes Name Initial integer, public :: n real(kind=dp), public :: cutoff real(kind=dp), public :: delta real(kind=dp), public, pointer :: f (:) => null() Actual data real(kind=dp), public, pointer :: d2 (:) => null() 2nd derivative Source Code type :: rad_func integer n real ( dp ) cutoff real ( dp ) delta real ( dp ), pointer :: f (:) => null () !! Actual data real ( dp ), pointer :: d2 (:) => null () !! 2nd derivative end type rad_func","tags":"","loc":"type/rad_func.html","title":"rad_func – SIESTA "},{"text":"type, public :: species_info Species_info: Consolidate all the pieces of information in on      !e place Inherits type~~species_info~~InheritsGraph type~species_info species_info type~rad_func rad_func type~species_info->type~rad_func orbnl, pjnl, pjldau, vna, chlocal, reduced_vlocal, core Help × Graph Key Nodes of different colours represent the following: Graph Key Type Type This Page's Entity This Page's Entity Solid arrows point from a derived type to the parent type which it\n    extends. Dashed arrows point from a derived type to the other\n    types it contains as a components, with a label listing the name(s) of\n    said component(s). Contents Variables symbol label z mass zval self_energy n_orbnl lmax_basis orbnl_l orbnl_n orbnl_z orbnl_ispol orbnl_pop lj_projs n_pjnl lmax_projs pjnl_l pjnl_j pjnl_n pjnl_ekb norbs orb_index orb_n orb_l orb_m orb_gindex orb_pop nprojs pj_index pj_n pj_l pj_j pj_m pj_gindex n_pjldaunl lmax_ldau_projs pjldaunl_l pjldaunl_n pjldaunl_U pjldaunl_J nprojsldau pjldau_index pjldau_n pjldau_l pjldau_m pjldau_gindex orbnl pjnl pjldau vna vna_gindex chlocal reduced_vlocal there_is_core core read_from_file Source Code species_info Components Type Visibility Attributes Name Initial character(len=2), public :: symbol character(len=20), public :: label integer, public :: z Atomic number real(kind=dp), public :: mass real(kind=dp), public :: zval Valence charge real(kind=dp), public :: self_energy Electrostatic self-energy integer, public :: n_orbnl = 0 num of nl orbs integer, public :: lmax_basis = 0 basis l cutoff integer, public, dimension(maxn_orbnl) :: orbnl_l l of each nl orb integer, public, dimension(maxn_orbnl) :: orbnl_n n of each nl orb integer, public, dimension(maxn_orbnl) :: orbnl_z z of each nl orb logical, public, dimension(maxn_orbnl) :: orbnl_ispol is it a pol. orb? real(kind=dp), public, dimension(maxn_orbnl) :: orbnl_pop population of nl orb (total of 2l+1 components) logical, public :: lj_projs = .false. integer, public :: n_pjnl = 0 num of \"nl\" projs integer, public :: lmax_projs = 0 l cutoff for projs integer, public, dimension(maxn_pjnl) :: pjnl_l l of each nl proj real(kind=dp), public, dimension(maxn_pjnl) :: pjnl_j j of each nl proj integer, public, dimension(maxn_pjnl) :: pjnl_n n of each nl proj real(kind=dp), public, dimension(maxn_pjnl) :: pjnl_ekb energy of integer, public :: norbs = 0 integer, public, dimension(maxnorbs) :: orb_index integer, public, dimension(maxnorbs) :: orb_n integer, public, dimension(maxnorbs) :: orb_l integer, public, dimension(maxnorbs) :: orb_m integer, public, dimension(maxnorbs) :: orb_gindex real(kind=dp), public, dimension(maxnorbs) :: orb_pop population of nl orb integer, public :: nprojs = 0 integer, public, dimension(maxnprojs) :: pj_index integer, public, dimension(maxnprojs) :: pj_n integer, public, dimension(maxnprojs) :: pj_l real(kind=dp), public, dimension(maxnprojs) :: pj_j integer, public, dimension(maxnprojs) :: pj_m integer, public, dimension(maxnprojs) :: pj_gindex integer, public :: n_pjldaunl = 0 num of \"nl\" projs. Not counting the \"m copies\" integer, public :: lmax_ldau_projs = 0 l cutoff for LDA+U proj integer, public, dimension(maxn_pjnl) :: pjldaunl_l l of each nl proj integer, public, dimension(maxn_pjnl) :: pjldaunl_n n of each nl proj Here, n is not the principal\n quantum number, but a sequential\n index from 1 to the total\n number of projectors for that l. In the case of LDA+U projectors,\n It is always equal to 1. real(kind=dp), public, dimension(maxn_pjnl) :: pjldaunl_U U of each nl projector real(kind=dp), public, dimension(maxn_pjnl) :: pjldaunl_J J of each nl projector integer, public :: nprojsldau = 0 Total number of LDA+U proj. Counting the \"m copies\" (including the (2l + 1) factor). integer, public, dimension(maxnprojs) :: pjldau_index integer, public, dimension(maxnprojs) :: pjldau_n integer, public, dimension(maxnprojs) :: pjldau_l integer, public, dimension(maxnprojs) :: pjldau_m integer, public, dimension(maxnprojs) :: pjldau_gindex type( rad_func ), public, dimension(:), pointer :: orbnl type( rad_func ), public, dimension(:), pointer :: pjnl type( rad_func ), public, dimension(:), pointer :: pjldau type( rad_func ), public :: vna integer, public :: vna_gindex = 0 type( rad_func ), public :: chlocal type( rad_func ), public :: reduced_vlocal logical, public :: there_is_core type( rad_func ), public :: core logical, public :: read_from_file Source Code type , public :: species_info !! Species_info: Consolidate all the pieces of information in one place character ( len = 2 ) :: symbol character ( len = 20 ) :: label integer :: z !! Atomic number real ( dp ) :: mass real ( dp ) :: zval !! Valence charge real ( dp ) :: self_energy !! Electrostatic self-energy !        Orbitals !             We keep track of just one orbital for each !             \"nl\" family integer :: n_orbnl = 0 !! num of nl orbs integer :: lmax_basis = 0 !! basis l cutoff integer , dimension ( maxn_orbnl ) :: orbnl_l !! l of each nl orb integer , dimension ( maxn_orbnl ) :: orbnl_n !! n of each nl orb integer , dimension ( maxn_orbnl ) :: orbnl_z !! z of each nl orb logical , dimension ( maxn_orbnl ) :: orbnl_ispol !! is it a pol. orb? real ( dp ), dimension ( maxn_orbnl ) :: orbnl_pop !! population of nl orb (total of 2l+1 components) !        KB Projectors !             For each l, there can be several projectors. Formally, we !             can can use the \"nl\" terminology for them. n will run from !             1 to the total number of projectors at that l. logical :: lj_projs = . false . integer :: n_pjnl = 0 !! num of \"nl\" projs integer :: lmax_projs = 0 !! l cutoff for projs integer , dimension ( maxn_pjnl ) :: pjnl_l !! l of each nl proj real ( dp ), dimension ( maxn_pjnl ) :: pjnl_j !! j of each nl proj integer , dimension ( maxn_pjnl ) :: pjnl_n !! n of each nl proj real ( dp ), dimension ( maxn_pjnl ) :: pjnl_ekb !! energy of !        Aggregate numbers of orbitals and projectors (including 2l+1 !        copies for each \"nl\"), and index arrays keeping track of !        which \"nl\" family they belong to, and their n, l, and m (to avoid !        a further dereference) integer :: norbs = 0 integer , dimension ( maxnorbs ) :: orb_index integer , dimension ( maxnorbs ) :: orb_n integer , dimension ( maxnorbs ) :: orb_l integer , dimension ( maxnorbs ) :: orb_m integer , dimension ( maxnorbs ) :: orb_gindex real ( dp ), dimension ( maxnorbs ) :: orb_pop !! population of nl orb integer :: nprojs = 0 integer , dimension ( maxnprojs ) :: pj_index integer , dimension ( maxnprojs ) :: pj_n integer , dimension ( maxnprojs ) :: pj_l real ( dp ), dimension ( maxnprojs ) :: pj_j integer , dimension ( maxnprojs ) :: pj_m integer , dimension ( maxnprojs ) :: pj_gindex !        LDA+U Projectors !        Here we follow the scheme used for the KB projectors integer :: n_pjldaunl = 0 !! num of \"nl\" projs. Not counting the \"m copies\" integer :: lmax_ldau_projs = 0 !! l cutoff for LDA+U proj integer , dimension ( maxn_pjnl ) :: pjldaunl_l !! l of each nl proj integer , dimension ( maxn_pjnl ) :: pjldaunl_n !! n of each nl proj !! Here, n is not the principal !! quantum number, but a sequential !! index from 1 to the total !! number of projectors for that l. !! In the case of LDA+U projectors, !! It is always equal to 1. real ( dp ), dimension ( maxn_pjnl ) :: pjldaunl_U !! U of each nl projector real ( dp ), dimension ( maxn_pjnl ) :: pjldaunl_J !! J of each nl projector integer :: nprojsldau = 0 !! Total number of LDA+U proj. !! Counting the \"m copies\" (including the `(2l + 1)` factor). integer , dimension ( maxnprojs ) :: pjldau_index integer , dimension ( maxnprojs ) :: pjldau_n integer , dimension ( maxnprojs ) :: pjldau_l integer , dimension ( maxnprojs ) :: pjldau_m integer , dimension ( maxnprojs ) :: pjldau_gindex type ( rad_func ), dimension (:), pointer :: orbnl type ( rad_func ), dimension (:), pointer :: pjnl type ( rad_func ), dimension (:), pointer :: pjldau type ( rad_func ) :: vna integer :: vna_gindex = 0 type ( rad_func ) :: chlocal type ( rad_func ) :: reduced_vlocal logical :: there_is_core type ( rad_func ) :: core logical :: read_from_file end type species_info","tags":"","loc":"type/species_info.html","title":"species_info – SIESTA "},{"text":"subroutine CROSS(A, B, AXB) Finds the cross product AxB of vectors A and B Arguments Type Intent Optional Attributes Name double precision :: A (3) double precision :: B (3) double precision :: AXB (3) Contents None","tags":"","loc":"proc/cross.html","title":"CROSS – SIESTA"},{"text":"subroutine rhooda(no, np, Datm, rhoatm, iaorb, iphorb, isa) Uses precision atmfuncs atomlist mesh meshphi proc~~rhooda~~UsesGraph proc~rhooda rhooda module~precision precision proc~rhooda->module~precision meshphi meshphi proc~rhooda->meshphi module~atmfuncs atmfuncs proc~rhooda->module~atmfuncs module~mesh mesh proc~rhooda->module~mesh atomlist atomlist proc~rhooda->atomlist module~atmfuncs->module~precision module~atm_types atm_types module~atmfuncs->module~atm_types spher_harm spher_harm module~atmfuncs->spher_harm module~radial radial module~atmfuncs->module~radial module~sys sys module~atmfuncs->module~sys module~mesh->module~precision module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finds the Harris density at the mesh points from the atomic occupations. Inverted so that grid points are the outer loop, J.D. Gale, Jan'99 Arguments Type Intent Optional Attributes Name integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of mesh points real(kind=dp), intent(in) :: Datm (no) Occupations of basis orbitals in free atom real(kind=grid_p), intent(out) :: rhoatm (nsp,np) Harris (sum of atoms) density at mesh points integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms Calls proc~~rhooda~~CallsGraph proc~rhooda rhooda phi phi proc~rhooda->phi indxuo indxuo proc~rhooda->indxuo endpht endpht proc~rhooda->endpht listp2 listp2 proc~rhooda->listp2 lstpht lstpht proc~rhooda->lstpht proc~phiatm phiatm proc~rhooda->proc~phiatm write_debug write_debug proc~rhooda->write_debug proc~rcut rcut proc~rhooda->proc~rcut proc~rad_get rad_get proc~phiatm->proc~rad_get rlylm rlylm proc~phiatm->rlylm proc~floating floating proc~phiatm->proc~floating proc~chk chk proc~rcut->proc~chk proc~die die proc~chk->proc~die splint splint proc~rad_get->splint proc~izofis IZOFIS proc~floating->proc~izofis io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~izofis->proc~chk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rhooda~~CalledByGraph proc~rhooda rhooda proc~dhscf_init dhscf_init proc~dhscf_init->proc~rhooda proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rhooda Source Code subroutine rhooda ( no , np , Datm , rhoatm , iaorb , iphorb , isa ) !! author: P.Ordejon and J.M.Soler !! date: May 1995 !! !! Finds the Harris density at the mesh points from the atomic occupations. !! !! Inverted so that grid points are the outer loop, J.D. Gale, Jan'99 ! Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , phiatm use atomlist , only : indxuo use mesh , only : nsp , dxa , xdop , xdsp use meshphi implicit none integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of mesh points integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms real ( grid_p ), intent ( out ) :: rhoatm ( nsp , np ) !! Harris (sum of atoms) density at mesh points real ( dp ), intent ( in ) :: Datm ( no ) !! Occupations of basis orbitals in free atom real ( dp ) :: phip integer :: i , ip , isp , iu , kn , iop , is , iphi , ia , ix real ( dp ) :: Ci , gradCi ( 3 ), r2o , r2sp , dxsp ( 3 ) #ifdef DEBUG call write_debug ( '      Pre rhooda' ) #endif !  Loop on mesh points do ip = 1 , np !  Initialise rhoatm do isp = 1 , nsp rhoatm ( isp , ip ) = 0.0_grid_p enddo !  Loop on orbitals of mesh point do kn = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( kn ) iu = indxuo ( i ) if ( DirectPhi ) then !  Generate phi value and loop on subpoints iphi = iphorb ( i ) ia = iaorb ( i ) is = isa ( ia ) r2o = rcut ( is , iphi ) ** 2 iop = listp2 ( kn ) do isp = 1 , nsp do ix = 1 , 3 dxsp ( ix ) = xdop ( ix , iop ) + xdsp ( ix , isp ) - dxa ( ix , ia ) enddo r2sp = dxsp ( 1 ) ** 2 + dxsp ( 2 ) ** 2 + dxsp ( 3 ) ** 2 if ( r2sp . lt . r2o ) then call phiatm ( is , iphi , dxsp , phip , gradCi ) Ci = phip rhoatm ( isp , ip ) = rhoatm ( isp , ip ) + Datm ( iu ) * Ci * Ci endif enddo else !  Loop on sub-points do isp = 1 , nsp Ci = phi ( isp , kn ) rhoatm ( isp , ip ) = rhoatm ( isp , ip ) + Datm ( iu ) * Ci * Ci enddo endif enddo enddo #ifdef DEBUG call write_debug ( '      POS rhooda' ) #endif end subroutine rhooda","tags":"","loc":"proc/rhooda.html","title":"rhooda – SIESTA"},{"text":"subroutine poison(CELL, N1, N2, N3, Mesh, RHO, U, V, STRESS, NSM) Uses precision parallel sys alloc m_fft cellsubs cellsubs m_chkgmx mpi_siesta proc~~poison~~UsesGraph proc~poison poison alloc alloc proc~poison->alloc cellsubs cellsubs proc~poison->cellsubs m_chkgmx m_chkgmx proc~poison->m_chkgmx module~precision precision proc~poison->module~precision module~parallel parallel proc~poison->module~parallel mpi_siesta mpi_siesta proc~poison->mpi_siesta m_fft m_fft proc~poison->m_fft module~sys sys proc~poison->module~sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Solves Poisson's Equation. Energy and potential returned in Rydberg units. Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: CELL (3,3) Unit cell vectors integer, intent(in) :: N1 Number of mesh divisions in each cell vector integer, intent(in) :: N2 Number of mesh divisions in each cell vector integer, intent(in) :: N3 Number of mesh divisions in each cell vector integer, intent(in) :: Mesh (3) Number of global mesh divisions real(kind=grid_p), intent(in) :: RHO (N1*N2*N3) Densitiy at mesh points real(kind=dp) :: U Electrostatic energy (in Ry) real(kind=grid_p) :: V (N1*N2*N3) Electrostatic potential (in Ry) V and Rho may be the same physical array real(kind=dp) :: STRESS (3,3) Electrostatic-energy contribution to stress\n tensor (in Ry/Bohr**3 ) assuming constant density\n (not charge), i.e. r->r' => rho'(r') = rho(r) For plane-wave and grid (finite difference)\n basis sets, density rescaling gives an extra\n term (not included) equal to -2*U/cell\\_volume for the diagonal elements of stress. For other\n basis sets, the extra term is, in general: IntegralOf( V * d\\_rho/d\\_strain ) / cell\\_volume integer, intent(out) :: NSM Number of sub-mesh points per mesh point along each axis Calls proc~~poison~~CallsGraph proc~poison poison mpitrace_event mpitrace_event proc~poison->mpitrace_event volcel volcel proc~poison->volcel reclat reclat proc~poison->reclat re_alloc re_alloc proc~poison->re_alloc fft fft proc~poison->fft timer timer proc~poison->timer mpi_barrier mpi_barrier proc~poison->mpi_barrier de_alloc de_alloc proc~poison->de_alloc chkgmx chkgmx proc~poison->chkgmx write_debug write_debug proc~poison->write_debug Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code poison Source Code subroutine poison ( CELL , N1 , N2 , N3 , Mesh , RHO , U , V , STRESS , & NSM ) !! author: J.M.Soler !! date: June 1995 !! !! Solves Poisson's Equation. Energy and potential returned in Rydberg units. !     Modules use precision , only : dp , grid_p use parallel , only : Node , Nodes , ProcessorY use sys , only : die use alloc , only : re_alloc , de_alloc use m_fft , only : fft ! 3-D fast Fourier transform use cellsubs , only : reclat ! Finds reciprocal lattice vectors use cellsubs , only : volcel ! Finds unit cell volume use m_chkgmx , only : chkgmx ! Checks planewave cutoff of a mesh #ifdef MPI use mpi_siesta #endif implicit          none !     Input/output variables integer , intent ( in ) :: N1 , N2 , N3 !! Number of mesh divisions in each cell vector integer , intent ( in ) :: Mesh ( 3 ) !! Number of global mesh divisions real ( grid_p ), intent ( in ) :: RHO ( N1 * N2 * N3 ) !! Densitiy at mesh points real ( dp ), intent ( in ) :: CELL ( 3 , 3 ) !! Unit cell vectors integer , intent ( out ) :: NSM !! Number of sub-mesh points per mesh point along each axis real ( grid_p ) :: V ( N1 * N2 * N3 ) !! Electrostatic potential (in Ry) !! V and Rho may be the same physical array real ( dp ) :: U !! Electrostatic energy (in Ry) real ( dp ) :: STRESS ( 3 , 3 ) !! Electrostatic-energy contribution to stress !! tensor (in Ry/Bohr**3) assuming constant density !! (not charge), i.e. r->r' => rho'(r') = rho(r) !! For plane-wave and grid (finite difference) !! basis sets, density rescaling gives an extra !! term (not included) equal to -2*U/cell\\_volume !! for the diagonal elements of stress. For other !! basis sets, the extra term is, in general: !! IntegralOf( V * d\\_rho/d\\_strain ) / cell\\_volume !     Local variables integer :: I , I1 , I2 , I3 , IX , J , J1 , J2 , J3 , JX , & NP , NG , NG2 , NG3 , & ProcessorZ , Py , Pz , J2min , J2max , & J3min , J3max , J2L , J3L , NRemY , NRemZ , & BlockSizeY , BlockSizeZ real ( dp ) :: C , B ( 3 , 3 ), DU , G ( 3 ), G2 , G2MAX , & PI , VG , VOLUME , PI8 real ( grid_p ), pointer :: CG (:,:) real ( dp ), parameter :: K0 ( 3 ) = ( / 0.0 , 0.0 , 0.0 / ), TINY = 1.0e-15 #ifdef MPI integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE POISON' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 2 ) #endif !     Start time counter call timer ( 'POISON' , 1 ) !     Allocate local memory nullify ( CG ) call re_alloc ( CG , 1 , 2 , 1 , n1 * n2 * n3 , 'CG' , 'poison' ) !     Find unit cell volume VOLUME = VOLCEL ( CELL ) !     Find reciprocal lattice vectors call reclat ( CELL , B , 1 ) !     Find maximun planewave cutoff NP = N1 * N2 * N3 G2MAX = 1.0e30_dp call CHKGMX ( K0 , B , Mesh , G2MAX ) !     Copy density to complex array !$OMP parallel do default(shared), private(I) do I = 1 , NP CG ( 1 , I ) = RHO ( I ) CG ( 2 , I ) = 0.0_grid_p enddo !$OMP end parallel do !     Find fourier transform of density call fft ( CG , Mesh , - 1 ) !     Initialize stress contribution do IX = 1 , 3 do JX = 1 , 3 STRESS ( JX , IX ) = 0.0_dp enddo enddo !     Work out processor grid dimensions ProcessorZ = Nodes / ProcessorY if ( ProcessorY * ProcessorZ . ne . Nodes ) & call die ( 'ERROR: ProcessorY must be a factor of the' // & ' number of processors!' ) Py = ( Node / ProcessorZ ) + 1 Pz = Node - ( Py - 1 ) * ProcessorZ + 1 !     Multiply by 8*PI/G2 to get the potential PI = 4.0_dp * atan ( 1.0_dp ) PI8 = PI * 8._dp U = 0.0_dp NG2 = Mesh ( 2 ) NG3 = Mesh ( 3 ) BlockSizeY = (( NG2 / NSM ) / ProcessorY ) * NSM BlockSizeZ = (( NG3 / NSM ) / ProcessorZ ) * NSM NRemY = ( NG2 - BlockSizeY * ProcessorY ) / NSM NRemZ = ( NG3 - BlockSizeZ * ProcessorZ ) / NSM J2min = ( Py - 1 ) * BlockSizeY + NSM * min ( Py - 1 , NRemY ) J2max = J2min + BlockSizeY - 1 if ( Py - 1. lt . NRemY ) J2max = J2max + NSM J2max = min ( J2max , NG2 - 1 ) J3min = ( Pz - 1 ) * BlockSizeZ + NSM * min ( Pz - 1 , NRemZ ) J3max = J3min + BlockSizeZ - 1 if ( Pz - 1. lt . NRemZ ) J3max = J3max + NSM J3max = min ( J3max , NG3 - 1 ) !$OMP parallel do default(shared), !$OMP&private(J3,J3L,I3,J2,J2L,I2,J1,I1,G,G2), !$OMP&private(J,VG,DU,C), reduction(+:U,STRESS) do J3 = J3min , J3max J3L = J3 - J3min if ( J3 . gt . NG3 / 2 ) then I3 = J3 - NG3 else I3 = J3 endif do J2 = J2min , J2max J2L = J2 - J2min if ( J2 . gt . NG2 / 2 ) then I2 = J2 - NG2 else I2 = J2 endif do J1 = 0 , N1 - 1 if ( J1 . gt . N1 / 2 ) then I1 = J1 - N1 else I1 = J1 endif G ( 1 ) = B ( 1 , 1 ) * I1 + B ( 1 , 2 ) * I2 + B ( 1 , 3 ) * I3 G ( 2 ) = B ( 2 , 1 ) * I1 + B ( 2 , 2 ) * I2 + B ( 2 , 3 ) * I3 G ( 3 ) = B ( 3 , 1 ) * I1 + B ( 3 , 2 ) * I2 + B ( 3 , 3 ) * I3 G2 = G ( 1 ) ** 2 + G ( 2 ) ** 2 + G ( 3 ) ** 2 J = 1 + J1 + N1 * J2L + N1 * N2 * J3L if ( G2 . LT . G2MAX . AND . G2 . GT . TINY ) then VG = PI8 / G2 DU = VG * ( CG ( 1 , J ) ** 2 + CG ( 2 , J ) ** 2 ) U = U + DU C = 2.0_dp * DU / G2 DO IX = 1 , 3 DO JX = 1 , 3 STRESS ( JX , IX ) = STRESS ( JX , IX ) + C * G ( IX ) * G ( JX ) ENDDO ENDDO CG ( 1 , J ) = VG * CG ( 1 , J ) CG ( 2 , J ) = VG * CG ( 2 , J ) else CG ( 1 , J ) = 0.0_dp CG ( 2 , J ) = 0.0_dp endif enddo enddo enddo !$OMP end parallel do NG = Mesh ( 1 ) * Mesh ( 2 ) * Mesh ( 3 ) U = 0.5_dp * U * VOLUME / DBLE ( NG ) ** 2 C = 0.5_dp / DBLE ( NG ) ** 2 do IX = 1 , 3 do JX = 1 , 3 STRESS ( JX , IX ) = C * STRESS ( JX , IX ) enddo STRESS ( IX , IX ) = STRESS ( IX , IX ) + U / VOLUME enddo !     Go back to real space call fft ( CG , Mesh , + 1 ) !     Copy potential to array V !$OMP parallel do default(shared), private(I) do I = 1 , NP V ( I ) = CG ( 1 , I ) enddo !$OMP end parallel do !     Free local memory call de_alloc ( CG , 'CG' , 'poison' ) #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif !     Stop time counter call timer ( 'POISON' , 2 ) #ifdef DEBUG call write_debug ( '    POS POISON' ) #endif end subroutine poison","tags":"","loc":"proc/poison.html","title":"poison – SIESTA"},{"text":"subroutine reord(FCLUST, FSEQ, NM, NSM, ITR) Uses precision alloc proc~~reord~~UsesGraph proc~reord reord module~precision precision proc~reord->module~precision alloc alloc proc~reord->alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Re-orders a clustered data array into a sequential one and viceversa Arguments Type Intent Optional Attributes Name real(kind=grid_p), intent(inout) :: FCLUST (*) CLUSTered data: REAL*4 FCLUST(NSM,NSM,NSM,NM1,NM2,NM3) real(kind=grid_p), intent(inout) :: FSEQ (*) SEQuential data: REAL*4 FSEQ(NSM*NM1,NSM*NM2,NSM*NM3) integer, intent(in) :: NM (3) Number of Mesh divisions in each cell vector integer, intent(in) :: NSM Number of Sub-divisions in each Mesh division integer, intent(in) :: ITR TRanslation-direction switch ITR=+1 => From clustered to sequential ITR=-1 => From sequential to clustered Calls proc~~reord~~CallsGraph proc~reord reord re_alloc re_alloc proc~reord->re_alloc de_alloc de_alloc proc~reord->de_alloc timer timer proc~reord->timer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~reord~~CalledByGraph proc~reord reord proc~dhscf_init dhscf_init proc~dhscf_init->proc~reord interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->proc~reord proc~dhscf->interface~distmeshdata proc~forhar forhar proc~dhscf->proc~forhar proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->proc~reord proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf interface~distmeshdata->proc~distmeshdata_rea proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian proc~forhar->interface~distmeshdata var panprocreordCalledByGraph = svgPanZoom('#procreordCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents None","tags":"","loc":"proc/reord.html","title":"reord – SIESTA"},{"text":"public subroutine rhoofd(no, np, maxnd, numd, listdptr, listd, nspin, Dscf, rhoscf, nuo, nuotot, iaorb, iphorb, isa) Uses precision atmfuncs atm_types atomlist m_spin listsc_module mesh meshdscf meshdscf meshphi parallel sys alloc parallelsubs proc~~rhoofd~~UsesGraph proc~rhoofd rhoofd listsc_module listsc_module proc~rhoofd->listsc_module module~precision precision proc~rhoofd->module~precision module~atmfuncs atmfuncs proc~rhoofd->module~atmfuncs meshphi meshphi proc~rhoofd->meshphi parallelsubs parallelsubs proc~rhoofd->parallelsubs alloc alloc proc~rhoofd->alloc atomlist atomlist proc~rhoofd->atomlist module~parallel parallel proc~rhoofd->module~parallel module~mesh mesh proc~rhoofd->module~mesh meshdscf meshdscf proc~rhoofd->meshdscf m_spin m_spin proc~rhoofd->m_spin module~sys sys proc~rhoofd->module~sys module~atm_types atm_types proc~rhoofd->module~atm_types module~atmfuncs->module~precision module~atmfuncs->module~sys module~atmfuncs->module~atm_types spher_harm spher_harm module~atmfuncs->spher_harm module~radial radial module~atmfuncs->module~radial module~mesh->module~precision module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finds the SCF density at the mesh points from the density matrix. Re-ordered so that mesh is the outer loop and the orbitals are\n handled as lower-half triangular. J.D.Gale and J.M.Soler, Feb'99 Version of rhoofd that optionally uses a direct algorithm to save\n memory. Modified by J.D.Gale, November'99 Arguments Type Intent Optional Attributes Name integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of mesh points integer, intent(in) :: maxnd First dimension of listd and Dscf , and\n maximum number of nonzero elements in\n any row of Dscf integer, intent(in) :: numd (nuo) Number of nonzero elemts in each row of Dscf integer, intent(in) :: listdptr (nuo) Pointer to start of rows in listd integer, intent(in) :: listd (maxnd) List of nonzero elements in each row of Dscf integer, intent(in) :: nspin Number of spin components real(kind=dp), intent(in) :: Dscf (:,:) real*8  Dscf(maxnd) - Rows of Dscf that are non-zero real(kind=grid_p), intent(out) :: rhoscf (nsp,np,nspin) SCF density at mesh points integer, intent(in) :: nuo Number of orbitals in unit cell locally integer, intent(in) :: nuotot Number of orbitals in unit cell in total integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms Calls proc~~rhoofd~~CallsGraph proc~rhoofd rhoofd phi phi proc~rhoofd->phi indxuo indxuo proc~rhoofd->indxuo listsc listsc proc~rhoofd->listsc endpht endpht proc~rhoofd->endpht re_alloc re_alloc proc~rhoofd->re_alloc listp2 listp2 proc~rhoofd->listp2 needdscfl needdscfl proc~rhoofd->needdscfl timer timer proc~rhoofd->timer listdlptr listdlptr proc~rhoofd->listdlptr matrixotom matrixotom proc~rhoofd->matrixotom lstpht lstpht proc~rhoofd->lstpht globaltolocalorb globaltolocalorb proc~rhoofd->globaltolocalorb de_alloc de_alloc proc~rhoofd->de_alloc numdl numdl proc~rhoofd->numdl proc~rcut rcut proc~rhoofd->proc~rcut proc~die die proc~rhoofd->proc~die dscfl dscfl proc~rhoofd->dscfl listdl listdl proc~rhoofd->listdl proc~chk chk proc~rcut->proc~chk io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~chk->proc~die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rhoofd~~CalledByGraph proc~rhoofd rhoofd proc~dhscf dhscf proc~dhscf->proc~rhoofd proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rhoofd Source Code subroutine rhoofd ( no , np , maxnd , numd , listdptr , listd , nspin , & Dscf , rhoscf , nuo , nuotot , iaorb , iphorb , isa ) !! author: P.Ordejon and J.M.Soler !! date: May 1995 !! license: GNU GPL !! !! Finds the SCF density at the mesh points from the density matrix. !! !! Re-ordered so that mesh is the outer loop and the orbitals are !! handled as lower-half triangular. J.D.Gale and J.M.Soler, Feb'99 !! !! Version of rhoofd that optionally uses a direct algorithm to save !! memory. Modified by J.D.Gale, November'99 use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo use m_spin , only : SpOrb use listsc_module , only : LISTSC use mesh , only : nsp , dxa , xdop , xdsp , meshLim use meshdscf , only : matrixOtoM use meshdscf , only : nrowsDscfL , listdl , listdlptr , NeedDscfL , & numdl , DscfL use meshphi , only : DirectPhi , endpht , lstpht , listp2 , phi use parallel , only : Nodes , node use sys , only : die use alloc , only : re_alloc , de_alloc use parallelsubs , only : GlobalToLocalOrb #ifdef MPI use mpi_siesta #endif implicit none integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of mesh points integer , intent ( in ) :: maxnd !! First dimension of `listd` and `Dscf`, and !! maximum number of nonzero elements in !! any row of `Dscf` integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero elemts in each row of `Dscf` integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of rows in `listd` integer , intent ( in ) :: listd ( maxnd ) !! List of nonzero elements in each row of `Dscf` integer , intent ( in ) :: nspin !! Number of spin components integer , intent ( in ) :: nuo !! Number of orbitals in unit cell locally integer , intent ( in ) :: nuotot !! Number of orbitals in unit cell in total integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms real ( dp ), intent ( in ) :: Dscf (:,:) !! `real*8  Dscf(maxnd)` - Rows of `Dscf` that are non-zero real ( grid_p ), intent ( out ) :: rhoscf ( nsp , np , nspin ) !! SCF density at mesh points external :: memory , timer !     Internal variables and arrays integer , parameter :: minloc = 1000 ! Min buffer size for local copy of Dscf integer , parameter :: maxoa = 100 ! Max # of orbitals per atom logical :: ParallelLocal integer :: i , ia , ic , ii , ijl , il , imp , ind integer :: ispin , io , iop , ip , iphi , is integer :: isp , iu , iul , j , jc , last , lasta integer :: lastop , maxloc , maxloc2 , triang , nc integer :: maxndl , nphiloc , lenx , leny , lenxy , lenz ! Total hamiltonian size integer :: h_spin_dim real ( dp ) :: r2sp , dxsp ( 3 ) integer , pointer :: ilc (:), ilocal (:), iorb (:) real ( dp ), pointer :: r2cut (:), Clocal (:,:), Dlocal (:,:), phia (:,:) #ifdef _TRACE_ integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE rhoofd' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 1 ) #endif !     Start time counter call timer ( 'rhoofd' , 1 ) ! Get spin-size h_spin_dim = size ( Dscf , 2 ) !     Set algorithm logical ParallelLocal = ( Nodes > 1 ) if ( ParallelLocal ) then if ( nrowsDscfL > 0 ) then maxndl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else maxndl = 1 end if nullify ( DscfL ) call re_alloc ( DscfL , 1 , maxndl , 1 , h_spin_dim , 'DscfL' , 'rhoofd' ) !       Redistribute Dscf to DscfL form call matrixOtoM ( maxnd , numd , listdptr , maxndl , nuo , & h_spin_dim , Dscf , DscfL ) end if !     Find atomic cutoff radii nullify ( r2cut ) call re_alloc ( r2cut , 1 , nsmax , 'r2cut' , 'rhoofd' ) r2cut = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) end do ! Find size of buffers to store partial copies of Dscf and C maxloc2 = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxloc = maxloc2 + minloc maxloc = min ( maxloc , no ) triang = ( maxloc + 1 ) * ( maxloc + 2 ) / 2 lenx = meshLim ( 2 , 1 ) - meshLim ( 1 , 1 ) + 1 leny = meshLim ( 2 , 2 ) - meshLim ( 1 , 2 ) + 1 lenz = meshLim ( 2 , 3 ) - meshLim ( 1 , 3 ) + 1 lenxy = lenx * leny !$OMP parallel default(shared), & !$OMP&shared(rhoscf), & !$OMP&private(ilocal,ilc,iorb,Dlocal,Clocal,phia), & !$OMP&private(ip,nc,ic,imp,i,il,last,j,iu,iul,ii,ind,io), & !$OMP&private(ijl,ispin,lasta,lastop,ia,is,iop,isp,iphi), & !$OMP&private(jc,nphiloc,dxsp,r2sp) ! Allocate local memory nullify ( ilocal , ilc , iorb , Dlocal , Clocal , phia ) !$OMP critical allocate ( ilocal ( no ), ilc ( maxloc2 ), iorb ( maxloc ) ) allocate ( Dlocal ( triang , nspin ), Clocal ( nsp , maxloc2 ) ) if ( DirectPhi ) allocate ( phia ( maxoa , nsp ) ) !$OMP end critical ! Initializations Dlocal (:,:) = 0.0_dp ilocal (:) = 0 iorb (:) = 0 last = 0 !$OMP do do ip = 1 , np !    Initializations rhoscf (:, ip ,:) = 0.0_grid_p !    Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !       iorb(il)>0 means that row il of Dlocal must not be overwritten !       iorb(il)=0 means that row il of Dlocal is empty !       iorb(il)<0 means that row il of Dlocal contains a valid row of !             Dscf, but which is not required at this point do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) if ( il > 0 ) iorb ( il ) = i end do !    Look for required rows of Dscf not yet stored in Dlocal do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) then !          Look for an available row in Dlocal do il = 1 , maxloc !             last runs circularly over rows of Dlocal last = last + 1 if ( last > maxloc ) last = 1 if ( iorb ( last ) <= 0 ) goto 10 end do call die ( 'rhoofd: no slot available in Dlocal' ) 10 continue !          Copy row i of Dscf into row last of Dlocal j = abs ( iorb ( last )) if ( j /= 0 ) ilocal ( j ) = 0 ilocal ( i ) = last iorb ( last ) = i il = last iu = indxuo ( i ) if ( ParallelLocal ) then iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = DscfL ( ind , 1 ) Dlocal ( ijl , 2 ) = DscfL ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( DscfL ( ind , 3 ) + DscfL ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( DscfL ( ind , 4 ) + DscfL ( ind , 8 )) else Dlocal ( ijl ,:) = DscfL ( ind ,:) end if end do else do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = LISTSC ( i , iu , listdl ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = DscfL ( ind , 1 ) Dlocal ( ijl , 2 ) = DscfL ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( DscfL ( ind , 3 ) + DscfL ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( DscfL ( ind , 4 ) + DscfL ( ind , 8 )) else Dlocal ( ijl ,:) = DscfL ( ind ,:) end if end do end if else call GlobalToLocalOrb ( iu , Node , Nodes , iul ) if ( i == iu ) then do ii = 1 , numd ( iul ) ind = listdptr ( iul ) + ii j = listd ( ind ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = Dscf ( ind , 1 ) Dlocal ( ijl , 2 ) = Dscf ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( Dscf ( ind , 3 ) + Dscf ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( Dscf ( ind , 4 ) + Dscf ( ind , 8 )) else Dlocal ( ijl ,:) = Dscf ( ind ,:) end if end do else do ii = 1 , numd ( iul ) ind = listdptr ( iul ) + ii j = LISTSC ( i , iu , listd ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) if ( SpOrb ) then Dlocal ( ijl , 1 ) = Dscf ( ind , 1 ) Dlocal ( ijl , 2 ) = Dscf ( ind , 2 ) Dlocal ( ijl , 3 ) = 0.5 * ( Dscf ( ind , 3 ) + Dscf ( ind , 7 )) Dlocal ( ijl , 4 ) = 0.5 * ( Dscf ( ind , 4 ) + Dscf ( ind , 8 )) else Dlocal ( ijl ,:) = Dscf ( ind ,:) end if end do end if end if end if end do !    Check algorithm if ( DirectPhi ) then lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !          Generate or retrieve phi values if ( ia /= lasta . or . iop /= lastop ) then lasta = ia lastop = iop is = isa ( ia ) do isp = 1 , nsp dxsp (:) = xdsp (:, isp ) + xdop (:, iop ) - dxa (:, ia ) r2sp = sum ( dxsp ** 2 ) if ( r2sp < r2cut ( is ) ) then !$OMP critical call all_phi ( is , + 1 , dxsp , nphiloc , phia (:, isp ) ) !$OMP end critical else phia (:, isp ) = 0.0_dp end if end do end if iphi = iphorb ( i ) !          Retrieve phi values Clocal (:, ic ) = dsqrt ( 2._dp ) * phia ( iphi ,:) !          Loop on second orbital of mesh point do jc = 1 , ic - 1 ijl = idx_ijl ( il , ilc ( jc )) do ispin = 1 , nspin !                Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * Clocal ( isp , ic ) * Clocal ( isp , jc ) end do end do end do !          ilc(ic) == il ijl = idx_ijl ( il , ilc ( ic )) do ispin = 1 , nspin !             Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * 0.5_dp * Clocal ( isp , ic ) ** 2 end do end do end do else !       Store values do ic = 1 , nc imp = endpht ( ip - 1 ) + ic il = ilocal ( lstpht ( imp )) ilc ( ic ) = il !          Retrieve phi values Clocal (:, ic ) = dsqrt ( 2._dp ) * phi (:, imp ) !          Loop on second orbital of mesh point do jc = 1 , ic - 1 ijl = idx_ijl ( il , ilc ( jc )) do ispin = 1 , nspin !                Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * Clocal ( isp , ic ) * Clocal ( isp , jc ) end do end do end do !          ilc(ic) == il ijl = idx_ijl ( il , ilc ( ic )) do ispin = 1 , nspin !             Loop over sub-points do isp = 1 , nsp rhoscf ( isp , ip , ispin ) = rhoscf ( isp , ip , ispin ) + & Dlocal ( ijl , ispin ) * 0.5_dp * Clocal ( isp , ic ) ** 2 end do end do end do end if !    Restore iorb for next point do imp = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( imp ) il = ilocal ( i ) iorb ( il ) = - i end do end do !$OMP end do ! Free local memory !$OMP critical deallocate ( ilocal , ilc , iorb , Dlocal , Clocal ) if ( DirectPhi ) deallocate ( phia ) !$OMP end critical !$OMP end parallel call de_alloc ( r2cut , 'r2cut' , 'rhoofd' ) if ( ParallelLocal ) then call de_alloc ( DscfL , 'DscfL' , 'rhoofd' ) end if #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'rhoofd' , 2 ) #ifdef DEBUG call write_debug ( '    POS rhoofd' ) #endif contains ! In any case will the compiler most likely inline this ! small routine. So it should not pose any problem. pure function idx_ijl ( i , j ) result ( ij ) integer , intent ( in ) :: i , j integer :: ij if ( i > j ) then ij = i * ( i + 1 ) / 2 + j + 1 else ij = j * ( j + 1 ) / 2 + i + 1 end if end function idx_ijl end subroutine rhoofd","tags":"","loc":"proc/rhoofd.html","title":"rhoofd – SIESTA"},{"text":"public subroutine setup_H0(g2max) Uses siesta_options sparse_matrices sparse_matrices sparse_matrices m_nlefsm m_spin sparse_matrices siesta_geom atmfuncs atomlist metaforce molecularmechanics m_nlefsm m_kinefsm m_naefs m_dnaefs m_dhscf m_energies m_ntm m_spin spinorbit alloc class_dSpData1D class_dSpData2D class_zSpData2D m_mpi_utils proc~~setup_h0~~UsesGraph proc~setup_h0 setup_H0 class_dSpData1D class_dSpData1D proc~setup_h0->class_dSpData1D m_naefs m_naefs proc~setup_h0->m_naefs m_nlefsm m_nlefsm proc~setup_h0->m_nlefsm module~atmfuncs atmfuncs proc~setup_h0->module~atmfuncs module~siesta_options siesta_options proc~setup_h0->module~siesta_options siesta_geom siesta_geom proc~setup_h0->siesta_geom sparse_matrices sparse_matrices proc~setup_h0->sparse_matrices m_kinefsm m_kinefsm proc~setup_h0->m_kinefsm atomlist atomlist proc~setup_h0->atomlist module~m_dhscf m_dhscf proc~setup_h0->module~m_dhscf m_dnaefs m_dnaefs proc~setup_h0->m_dnaefs m_energies m_energies proc~setup_h0->m_energies molecularmechanics molecularmechanics proc~setup_h0->molecularmechanics m_spin m_spin proc~setup_h0->m_spin spinorbit spinorbit proc~setup_h0->spinorbit m_ntm m_ntm proc~setup_h0->m_ntm class_zSpData2D class_zSpData2D proc~setup_h0->class_zSpData2D alloc alloc proc~setup_h0->alloc m_mpi_utils m_mpi_utils proc~setup_h0->m_mpi_utils metaforce metaforce proc~setup_h0->metaforce class_dSpData2D class_dSpData2D proc~setup_h0->class_dSpData2D module~precision precision module~atmfuncs->module~precision spher_harm spher_harm module~atmfuncs->spher_harm module~sys sys module~atmfuncs->module~sys module~atm_types atm_types module~atmfuncs->module~atm_types module~radial radial module~atmfuncs->module~radial module~m_dhscf->module~precision module~m_dfscf m_dfscf module~m_dhscf->module~m_dfscf module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name real(kind=dp), intent(inout) :: g2max Calls proc~~setup_h0~~CallsGraph proc~setup_h0 setup_H0 proc~dhscf_init dhscf_init proc~setup_h0->proc~dhscf_init globalize_sum globalize_sum proc~setup_h0->globalize_sum val val proc~setup_h0->val proc~uion UION proc~setup_h0->proc~uion naefs naefs proc~setup_h0->naefs timer timer proc~setup_h0->timer nlefsm_so_off nlefsm_so_off proc~setup_h0->nlefsm_so_off spinorb spinorb proc~setup_h0->spinorb meta meta proc~setup_h0->meta kinefsm kinefsm proc~setup_h0->kinefsm write_debug write_debug proc~setup_h0->write_debug dnaefs dnaefs proc~setup_h0->dnaefs dscf dscf proc~setup_h0->dscf isa isa proc~setup_h0->isa twobody twobody proc~setup_h0->twobody nlefsm nlefsm proc~setup_h0->nlefsm proc~dhscf_init->timer proc~dhscf_init->write_debug volcel volcel proc~dhscf_init->volcel ts_init_voltage ts_init_voltage proc~dhscf_init->ts_init_voltage re_alloc re_alloc proc~dhscf_init->re_alloc de_alloc de_alloc proc~dhscf_init->de_alloc ddot ddot proc~dhscf_init->ddot setupextmesh setupextmesh proc~dhscf_init->setupextmesh distriphionmesh distriphionmesh proc~dhscf_init->distriphionmesh proc~rcore RCORE proc~dhscf_init->proc~rcore initmesh initmesh proc~dhscf_init->initmesh leqi leqi proc~dhscf_init->leqi proc~setmeshdistr setMeshDistr proc~dhscf_init->proc~setmeshdistr initatommesh initatommesh proc~dhscf_init->initatommesh fdf_integer fdf_integer proc~dhscf_init->fdf_integer init_hartree_add init_hartree_add proc~dhscf_init->init_hartree_add ts_init_hartree_fix ts_init_hartree_fix proc~dhscf_init->ts_init_hartree_fix partialcoreonmesh partialcoreonmesh proc~dhscf_init->partialcoreonmesh proc~die die proc~dhscf_init->proc~die proc~reord reord proc~dhscf_init->proc~reord init_mesh_node init_mesh_node proc~dhscf_init->init_mesh_node setgga setgga proc~dhscf_init->setgga phionmesh phionmesh proc~dhscf_init->phionmesh proc~rcut rcut proc~dhscf_init->proc~rcut initialize_efield initialize_efield proc~dhscf_init->initialize_efield createlocaldscfpointers createlocaldscfpointers proc~dhscf_init->createlocaldscfpointers init_charge_add init_charge_add proc~dhscf_init->init_charge_add neutralatomonmesh neutralatomonmesh proc~dhscf_init->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata elecs elecs proc~dhscf_init->elecs fdf_get fdf_get proc~dhscf_init->fdf_get shaper shaper proc~dhscf_init->shaper user_specified_field user_specified_field proc~dhscf_init->user_specified_field proc~rhooda rhooda proc~dhscf_init->proc~rhooda digcel digcel proc~dhscf_init->digcel cdf_init_mesh cdf_init_mesh proc~dhscf_init->cdf_init_mesh set_box_limits set_box_limits proc~dhscf_init->set_box_limits getxc getxc proc~dhscf_init->getxc compute_doping_structs_uniform compute_doping_structs_uniform proc~dhscf_init->compute_doping_structs_uniform proc~chk chk proc~uion->proc~chk proc~chk->proc~die proc~rcore->proc~chk io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~reord->timer proc~reord->re_alloc proc~reord->de_alloc proc~rcut->proc~chk proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~rhooda->write_debug proc~rhooda->proc~rcut phi phi proc~rhooda->phi indxuo indxuo proc~rhooda->indxuo endpht endpht proc~rhooda->endpht listp2 listp2 proc~rhooda->listp2 lstpht lstpht proc~rhooda->lstpht proc~phiatm phiatm proc~rhooda->proc~phiatm proc~floating floating proc~phiatm->proc~floating proc~rad_get rad_get proc~phiatm->proc~rad_get rlylm rlylm proc~phiatm->rlylm proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~die proc~boxintersection boxIntersection proc~distmeshdata_int->proc~boxintersection proc~distmeshdata_rea->timer proc~distmeshdata_rea->write_debug proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->proc~die proc~distmeshdata_rea->proc~reord mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~distmeshdata_rea->proc~boxintersection mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint proc~izofis->proc~chk var panprocsetup_h0CallsGraph = svgPanZoom('#procsetup_h0CallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setup_H0 Source Code subroutine setup_H0 ( G2max ) C Computes non - self - consistent part of the Hamiltonian C and initializes data structures on the grid . USE siesta_options , only : g2cut use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : Dscf use m_nlefsm , only : nlefsm_SO_off use m_spin , only : spin use sparse_matrices , only : listh , listhptr , numh , maxnh use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use m_nlefsm , only : nlefsm use m_kinefsm , only : kinefsm use m_naefs , only : naefs use m_dnaefs , only : dnaefs use m_dhscf , only : dhscf_init use m_energies , only : Eions , Ena , DEna , Emm , Emeta , Eso use m_ntm use m_spin , only : spin use spinorbit , only : spinorb use alloc , only : re_alloc , de_alloc use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none real ( dp ), intent ( inout ) :: g2max real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ), dummy_dm ( 1 , 1 ) real ( dp ) :: dummy_E integer :: ia , is real ( dp ) :: dummy_Eso integer :: ispin , i , j complex ( dp ) :: Dc #ifdef MPI real ( dp ) :: buffer1 #endif real ( dp ), pointer :: H_val (:), H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) #ifdef DEBUG call write_debug ( '    PRE setup_H0' ) #endif !----------------------------------------------------------------------BEGIN call timer ( 'Setup_H0' , 1 ) C Self - energy of isolated ions Eions = 0.0_dp do ia = 1 , na_u is = isa ( ia ) Eions = Eions + uion ( is ) enddo !     In these routines, add a flag to tell them NOT to compute !     forces and stresses in this first pass, only energies. !     Neutral-atom: energy call naefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , Ena , dummy_fa , dummy_stress , & forces_and_stress = . false .) call dnaefs ( na_u , na_s , scell , xa , indxua , rmaxv , & isa , DEna , dummy_fa , dummy_stress , & forces_and_stress = . false .) Ena = Ena + DEna C Metadynamics energy if ( lMetaForce ) then call meta ( xa , na_u , ucell , Emeta , dummy_fa , dummy_stress , $ . false .,. false .) endif C Add on force field contribution to energy call twobody ( na_u , xa , isa , ucell , Emm , & ifa = 0 , fa = dummy_fa , istr = 0 , stress = dummy_stress ) ! !     Now we compute matrix elements of the Kinetic and Non-local !     parts of H !     Kinetic: matrix elements only H_val => val ( H_kin_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare call kinefsm ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , & maxnh , maxnh , lasto , iphorb , isa , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) !     Non-local-pseudop:  matrix elements only H_val => val ( H_vkb_1D ) !$OMP parallel workshare default(shared) H_val (:) = 0.0_dp !$OMP end parallel workshare Eso = 0.0d0 if ( . not . spin % SO_offsite ) then call nlefsm ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & 1 , & dummy_dm , dummy_E , dummy_fa , dummy_stress , & H_val , & matrix_elements_only = . true .) else H_so_off => val ( H_so_off_2D ) H_so_off = dcmplx ( 0._dp , 0._dp ) call nlefsm_SO_off ( scell , na_u , na_s , isa , xa , indxua , & maxnh , maxnh , lasto , lastkb , iphorb , iphKB , & numh , listhptr , listh , numh , listhptr , listh , & spin % Grid , & dummy_E , dummy_Eso , dummy_fa , & dummy_stress , H_val , H_so_off , & matrix_elements_only = . true .) ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! do i = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( i , 1 ), Dscf ( i , 5 ), dp ) Eso = Eso + real ( H_so_off ( i , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( i , 2 ), Dscf ( i , 6 ), dp ) Eso = Eso + real ( H_so_off ( i , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( i , 3 ), Dscf ( i , 4 ), dp ) Eso = Eso + real ( H_so_off ( i , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( i , 7 ), - Dscf ( i , 8 ), dp ) Eso = Eso + real ( H_so_off ( i , 3 ) * Dc , dp ) enddo #ifdef MPI ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 #endif endif ! .................. ! If in the future the spin-orbit routine is able to compute ! forces and stresses, then \"last\" will be needed. If we are not ! computing forces and stresses, calling it in the first iteration ! should be enough ! if ( spin % SO_onsite ) then H_so_on => val ( H_so_on_2D ) !$OMP parallel workshare default(shared) H_so_on (:,:) = 0._dp !$OMP end parallel workshare call spinorb ( no_u , no_l , iaorb , iphorb , isa , indxuo , & maxnh , numh , listhptr , listh , Dscf , H_so_on , Eso ) end if C This will take care of possible changes to the mesh and atomic - related C mesh structures for geometry changes g2max = g2cut call dhscf_init ( spin % Grid , no_s , iaorb , iphorb , & no_l , no_u , na_u , na_s , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnh , numh , listhptr , listh , datm , & dummy_fa , dummy_stress ) call timer ( 'Setup_H0' , 2 ) #ifdef DEBUG call write_debug ( '    POS setup_H0' ) #endif !---------------------------------------------------------------------- END END subroutine setup_H0","tags":"","loc":"proc/setup_h0.html","title":"setup_H0 – SIESTA"},{"text":"private function scalar_product(a, b) result(sp) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: a (:) real(kind=dp), intent(in) :: b (:) Return Value real(kind=dp) Contents Source Code scalar_product Source Code function scalar_product ( a , b ) result ( sp ) real ( dp ), intent ( in ) :: a (:), b (:) real ( dp ) :: sp integer :: ip , ispin , j real ( dp ) :: weight sp = 0 ip = 1 ! Use standard definition of the scalar product as conjg(A)*B, ! but take the real part, as per DIIS equations ! If q1sq is not zero, g2_diis(ip) determines the weight, as in KF do ispin = 1 , nspin do j = 1 , ng_diis weight = ( g2_diis ( ip ) + q1sq ) / g2_diis ( ip ) sp = sp + ( a ( ip ) * b ( ip ) + a ( ip + 1 ) * b ( ip + 1 )) * weight ip = ip + 2 enddo enddo ! Note: This is a \"local scalar product\" ! For efficiency reasons, the All_reduce call is done ! on the whole matrix in m_diis end function scalar_product","tags":"","loc":"proc/scalar_product.html","title":"scalar_product – SIESTA"},{"text":"public subroutine mix_rhog(iscf) Uses siesta_options precision fdf m_diis proc~~mix_rhog~~UsesGraph proc~mix_rhog mix_rhog module~precision precision proc~mix_rhog->module~precision fdf fdf proc~mix_rhog->fdf module~siesta_options siesta_options proc~mix_rhog->module~siesta_options m_diis m_diis proc~mix_rhog->m_diis Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. ! maybe           if (g2(j) <= certain cutoff) then Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~mix_rhog~~CallsGraph proc~mix_rhog mix_rhog n_items n_items proc~mix_rhog->n_items diis diis proc~mix_rhog->diis fdf_get fdf_get proc~mix_rhog->fdf_get Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_rhog~~CalledByGraph proc~mix_rhog mix_rhog proc~siesta_forces siesta_forces proc~siesta_forces->proc~mix_rhog Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_rhog Source Code subroutine mix_rhog ( iscf ) use siesta_options , only : wmix use precision , only : dp use fdf , only : fdf_get use m_diis , only : diis integer , intent ( in ) :: iscf integer :: j , i real ( dp ) :: alpha real ( dp ), allocatable :: coeff (:) logical :: mix_first_iter if ( using_diis_for_rhog ) then ! Do not use the first diff in the cycle for DIIS if ( iscf > 1 ) call add_rhog_in_and_diff_to_stack () !call print_type(rhog_stack) if ( n_items ( rhog_stack ) > 1 ) then allocate ( coeff ( n_items ( rhog_stack ))) ! Get the DIIS coefficients call diis ( rhog_stack , scalar_product , coeff ) ! This will replace the small-G set coefficients ! by the DIIS-optimal ones call get_optimal_rhog_in () deallocate ( coeff ) endif endif ! Do Kerker mixing on the whole fourier series ! Check whether we want to mix in the first step mix_first_iter = fdf_get ( \"SCF.MixCharge.SCF1\" ,. false .) if (( iscf == 1 ) . and . (. not . mix_first_iter )) then ! Do not mix. Take the output density rhog_in (:,:,:) = rhog (:,:,:) else do j = 1 , size ( rhog_in , dim = 2 ) !!! maybe           if (g2(j) <= certain cutoff) then if (. true .) then alpha = wmix * g2 ( j ) / ( g2 ( j ) + q0sq ) if ( alpha == 0 ) alpha = wmix ! for G=0 rhog_in (:, j ,:) = alpha * rhog (:, j ,:) + & ( 1.0_dp - alpha ) * rhog_in (:, j ,:) else rhog_in (:, j ,:) = rhog (:, j ,:) endif enddo endif CONTAINS subroutine add_rhog_in_and_diff_to_stack () ! Store rho_in(G) and rho_diff(G) as single vectors ! in a circular stack of the appropriate size type ( dData1D ) :: vin , vdiff type ( Pair_dData1D ) :: pair integer :: ip , i , j , ispin character ( len = 20 ) :: msg ip = 0 do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then ! if this G is treated within DIIS do i = 1 , 2 ip = ip + 1 rg_in ( ip ) = rhog_in ( i , j , ispin ) rg_diff ( ip ) = rhog ( i , j , ispin ) - rg_in ( ip ) enddo endif enddo enddo write ( msg , \"(a,i3)\" ) \"scf step: \" , iscf call newdData1D ( vin , rg_in , name = \"(rhog_in -- \" // trim ( msg ) // \")\" ) call newdData1D ( vdiff , rg_diff , name = \"(rhog_diff -- \" // trim ( msg ) // \")\" ) call new ( pair , vin , vdiff , \"(pair in-diff -- \" // trim ( msg ) // \")\" ) call push ( rhog_stack , pair ) ! Store in stack call delete ( vin ) call delete ( vdiff ) call delete ( pair ) end subroutine add_rhog_in_and_diff_to_stack subroutine get_optimal_rhog_in () ! Synthesize the DIIS-optimal rho_in(G) and rho_out(G) ! from the DIIS coefficients. real ( dp ), dimension (:), pointer :: vin , vdiff type ( Pair_dData1D ), pointer :: pairp type ( dData1D ), pointer :: vp integer :: ip , i , j , ispin , k ! zero-out the components treated with DIIS do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then rhog_in (:, j , ispin ) = 0.0_dp rhog (:, j , ispin ) = 0.0_dp endif enddo enddo do k = 1 , n_items ( rhog_stack ) pairp => get_pointer ( rhog_stack , k ) call firstp ( pairp , vp ) vin => val ( vp ) call secondp ( pairp , vp ) vdiff => val ( vp ) ip = 0 do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then do i = 1 , 2 ip = ip + 1 rhog_in ( i , j , ispin ) = rhog_in ( i , j , ispin ) + & coeff ( k ) * vin ( ip ) rhog ( i , j , ispin ) = rhog ( i , j , ispin ) + & coeff ( k ) * ( vdiff ( ip ) + vin ( ip )) enddo endif enddo enddo enddo end subroutine get_optimal_rhog_in end subroutine mix_rhog","tags":"","loc":"proc/mix_rhog.html","title":"mix_rhog – SIESTA"},{"text":"public subroutine compute_charge_diff(drhog) Uses precision m_spin fdf parallel proc~~compute_charge_diff~~UsesGraph proc~compute_charge_diff compute_charge_diff module~parallel parallel proc~compute_charge_diff->module~parallel module~precision precision proc~compute_charge_diff->module~precision m_spin m_spin proc~compute_charge_diff->m_spin fdf fdf proc~compute_charge_diff->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name real(kind=dp), intent(out) :: drhog Calls proc~~compute_charge_diff~~CallsGraph proc~compute_charge_diff compute_charge_diff fdf_get fdf_get proc~compute_charge_diff->fdf_get Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_charge_diff~~CalledByGraph proc~compute_charge_diff compute_charge_diff proc~siesta_forces siesta_forces proc~siesta_forces->proc~compute_charge_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_charge_diff Source Code subroutine compute_charge_diff ( drhog ) use precision use m_spin , only : nspin use fdf , only : fdf_get use parallel , only : Node #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( out ) :: drhog #ifdef MPI real ( dp ) :: buffer1 #endif integer :: j , js , i , ispin logical :: debug_stars real ( dp ) :: ss ! Note that we now use the complex norm instead of the ! abs-norm... drhog = - huge ( 1.0_dp ) do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) ss = 0.0_dp do i = 1 , 2 ss = ss + ( rhog ( i , j , ispin ) - rhog_in ( i , j , ispin )) ** 2 !drhog = max(drhog,abs(rhog(i,j,ispin)-rhog_in(i,j,ispin))) enddo drhog = max ( drhog , ss ) enddo enddo drhog = sqrt ( drhog ) if ( Node == 0 ) print \"(a,f12.6)\" , \" Max |\\Delta rho(G)|: \" , drhog ! Print info about the first 10 stars debug_stars = fdf_get ( \"SCF.DebugRhogMixing\" ,. false .) if ( debug_stars . and . Node == 0 ) then print \"(a8,2x,a20,4x,a20,2x,a8)\" , \"G2\" , & \"rho_in(G) (R, C)\" , \"Diff_rho(G) (R, C)\" , \"damping\" do ispin = 1 , nspin do js = 1 , 10 j = gindex ( star_index ( js )) print \"(f8.4,2x,2f10.5,4x,2f10.5,2x,f8.4)\" , g2 ( j ), & rhog_in (:, j , ispin ) , ( rhog (:, j , ispin ) - rhog_in (:, j , ispin )), & g2 ( j ) / ( g2 ( j ) + q0sq ) enddo enddo endif #ifdef MPI !     Ensure that drhog is the same on all nodes call globalize_max ( drhog , buffer1 ) drhog = buffer1 #endif end subroutine compute_charge_diff","tags":"","loc":"proc/compute_charge_diff.html","title":"compute_charge_diff – SIESTA"},{"text":"public subroutine order_rhog(cell, n1, n2, n3, mesh, nsm) Uses parallel alloc fdf sorting m_mpi_utils m_mpi_utils m_mpi_utils atomlist proc~~order_rhog~~UsesGraph proc~order_rhog order_rhog alloc alloc proc~order_rhog->alloc sorting sorting proc~order_rhog->sorting fdf fdf proc~order_rhog->fdf atomlist atomlist proc~order_rhog->atomlist module~parallel parallel proc~order_rhog->module~parallel m_mpi_utils m_mpi_utils proc~order_rhog->m_mpi_utils Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: cell (3,3) integer, intent(in) :: n1 integer, intent(in) :: n2 integer, intent(in) :: n3 integer, intent(in) :: mesh (3) integer, intent(in) :: nsm Calls proc~~order_rhog~~CallsGraph proc~order_rhog order_rhog volcel volcel proc~order_rhog->volcel globalize_sum globalize_sum proc~order_rhog->globalize_sum globalize_max globalize_max proc~order_rhog->globalize_max re_alloc re_alloc proc~order_rhog->re_alloc reclat reclat proc~order_rhog->reclat fdf_get fdf_get proc~order_rhog->fdf_get fdf_defined fdf_defined proc~order_rhog->fdf_defined globalize_min globalize_min proc~order_rhog->globalize_min ordix ordix proc~order_rhog->ordix Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code order_rhog Source Code subroutine order_rhog ( cell , n1 , n2 , n3 , mesh , nsm ) ! Sets up indexes for the handling of rho(G) use parallel , only : Node , Nodes , ProcessorY use alloc , only : re_alloc use fdf , only : fdf_get , fdf_defined use sorting , only : ordix use m_mpi_utils , only : globalize_max use m_mpi_utils , only : globalize_min use m_mpi_utils , only : globalize_sum use atomlist , only : qtot implicit none real ( dp ), intent ( in ) :: cell ( 3 , 3 ) integer , intent ( in ) :: n1 , n2 , n3 integer , intent ( in ) :: mesh ( 3 ) integer , intent ( in ) :: nsm ! Local variables real ( dp ) :: B ( 3 , 3 ), g ( 3 ), celvol integer :: I , I1 , I2 , I3 , IX , J , J1 , J2 , J3 , JX , & NP , NG , NG2 , NG3 , & ProcessorZ , Py , Pz , J2min , J2max , & J3min , J3max , J2L , J3L , NRemY , NRemZ , & BlockSizeY , BlockSizeZ external :: reclat real ( dp ), external :: volcel real ( dp ), parameter :: tiny = 1.e-10_dp ! for regularization integer :: n_rhog_depth integer :: ng_diis_min , ng_diis_max , ng_diis_sum real ( dp ) :: pi , qtf2 , length , length_max , q0_size ! !     Find the genuine thomas fermi k0&#94;2. This does not !     seem to be too relevant for the preconditioning, as !     it tends to be too big. ! pi = 4 * atan ( 1.0_dp ) celvol = volcel ( cell ) qtf2 = 4 * ( 3 * qtot / ( pi * celvol )) ** ( 1.0_dp / 3.0_dp ) ! Find the maximum length of a lattice vector ! This could set a better scale for the Kerker preconditioning length_max = 0.0_dp do i = 1 , 3 length = sqrt ( dot_product ( cell (:, i ), cell (:, i ))) length_max = max ( length , length_max ) enddo q0_size = ( 2 * pi / length_max ) if ( Node == 0 ) then print \"(a,f12.6)\" , \"Thomas-Fermi K2 (Ry):\" , qtf2 print \"(a,f12.6)\" , \"L max (bohr):\" , length_max print \"(a,f12.6)\" , \"q0_size = 2pi/L (Bohr&#94;-1):\" , q0_size print \"(a,f12.6)\" , \"q0_size&#94;2 (Ry) :\" , q0_size ** 2 endif if ( fdf_defined ( \"Thomas.Fermi.K2\" )) then if ( Node == 0 ) then print \"(a,f12.6)\" , \"Please use 'SCF.Kerker.q0sq' \" // & \"instead of 'Thomas.Fermi.K2'\" endif endif q0sq = fdf_get ( \"SCF.Kerker.q0sq\" , 0.0_dp , \"Ry\" ) ! in Ry if ( Node == 0 ) then print \"(a,f12.6)\" , \"Kerker preconditioner q0&#94;2 (Ry):\" , q0sq endif q0sq = q0sq + tiny call re_alloc ( g2 , 1 , n1 * n2 * n3 , \"g2\" , \"order_rhog\" ) call re_alloc ( g2mask , 1 , n1 * n2 * n3 , \"g2mask\" , \"order_rhog\" ) call re_alloc ( gindex , 1 , n1 * n2 * n3 , \"gindex\" , \"order_rhog\" ) rhog_cutoff = fdf_get ( \"SCF.RhoG-DIIS-Cutoff\" , 9.0_dp , \"Ry\" ) ! in Ry ng_diis = 0 !     Find reciprocal lattice vectors call reclat ( CELL , B , 1 ) !     Work out processor grid dimensions jg0 = - 1 ProcessorZ = Nodes / ProcessorY Py = ( Node / ProcessorZ ) + 1 Pz = Node - ( Py - 1 ) * ProcessorZ + 1 NG2 = Mesh ( 2 ) NG3 = Mesh ( 3 ) BlockSizeY = (( NG2 / NSM ) / ProcessorY ) * NSM BlockSizeZ = (( NG3 / NSM ) / ProcessorZ ) * NSM NRemY = ( NG2 - BlockSizeY * ProcessorY ) / NSM NRemZ = ( NG3 - BlockSizeZ * ProcessorZ ) / NSM J2min = ( Py - 1 ) * BlockSizeY + NSM * min ( Py - 1 , NRemY ) J2max = J2min + BlockSizeY - 1 if ( Py - 1. lt . NRemY ) J2max = J2max + NSM J2max = min ( J2max , NG2 - 1 ) J3min = ( Pz - 1 ) * BlockSizeZ + NSM * min ( Pz - 1 , NRemZ ) J3max = J3min + BlockSizeZ - 1 if ( Pz - 1. lt . NRemZ ) J3max = J3max + NSM J3max = min ( J3max , NG3 - 1 ) do J3 = J3min , J3max if ( J3 . gt . NG3 / 2 ) then I3 = J3 - NG3 else I3 = J3 endif do J2 = J2min , J2max if ( J2 . gt . NG2 / 2 ) then I2 = J2 - NG2 else I2 = J2 endif do J1 = 0 , N1 - 1 if ( J1 . gt . N1 / 2 ) then I1 = J1 - N1 else I1 = J1 endif G ( 1 ) = B ( 1 , 1 ) * I1 + B ( 1 , 2 ) * I2 + B ( 1 , 3 ) * I3 G ( 2 ) = B ( 2 , 1 ) * I1 + B ( 2 , 2 ) * I2 + B ( 2 , 3 ) * I3 G ( 3 ) = B ( 3 , 1 ) * I1 + B ( 3 , 2 ) * I2 + B ( 3 , 3 ) * I3 J2L = J2 - J2min J3L = J3 - J3min J = 1 + J1 + N1 * J2L + N1 * N2 * J3L G2 ( J ) = G ( 1 ) ** 2 + G ( 2 ) ** 2 + G ( 3 ) ** 2 if ( max ( abs ( i1 ), abs ( i2 ), abs ( i3 )) == 0 ) then jg0 = j endif ! Do not include G=0 in the DIIS subset g2mask ( j ) = (( g2 ( j ) <= rhog_cutoff ) . and . ( g2 ( j ) > 0 )) !   if (g2mask(j)) then !      print \"(i5,f10.4,4x,3i6)\", j, g2(j), i1, i2, i3 !   endif enddo enddo enddo ! This will work only in serial form for now ! Sort by module of G call ordix ( g2 , 1 , n1 * n2 * n3 , gindex ) ! Get index of star representatives call get_star_reps ( g2 , gindex , star_index ) ng_diis = count ( g2mask ) call globalize_min ( ng_diis , ng_diis_min ) call globalize_max ( ng_diis , ng_diis_max ) call globalize_sum ( ng_diis , ng_diis_sum ) if ( Node == 0 ) then print \"(a,i10)\" , \"Number of G's in DIIS: \" , ng_diis_sum #ifdef MPI print \"(a,3i10)\" , \"Distrib of G's in DIIS (min, ave, max): \" , & ng_diis_min , nint ( dble ( ng_diis_sum ) / Nodes ), ng_diis_max #endif endif n_rhog_depth = fdf_get ( \"SCF.RhoG-DIIS-Depth\" , 0 ) ! Note that some nodes might not have any Gs in the DIIS procedure ! But we still go ahead using_diis_for_rhog = (( ng_diis_max > 1 ) . and . ( n_rhog_depth > 1 )) if ( using_diis_for_rhog ) call set_up_diis () CONTAINS subroutine get_star_reps ( a , aindex , star_index ) ! Determine star representatives by checking ! when the (sorted) modulus of G changes... real ( dp ), intent ( in ) :: a (:) integer , intent ( in ) :: aindex (:) integer , pointer :: star_index (:) integer :: j , ng , jj , jp1 , ns ng = size ( a ) ns = 0 do jj = 1 , ng - 1 j = aindex ( jj ) jp1 = aindex ( jj + 1 ) if ( abs ( a ( j ) - a ( jp1 )) > 1.e-7_dp ) ns = ns + 1 enddo ! call re_alloc ( star_index , 1 , ns , \"star_index\" , \"m_rhog\" ) ! ns = 0 do jj = 1 , ng - 1 j = aindex ( jj ) jp1 = aindex ( jj + 1 ) if ( abs ( a ( j ) - a ( jp1 )) > 1.e-7_dp ) then ns = ns + 1 star_index ( ns ) = jj endif enddo if ( Node == 0 ) print * , \"Number of stars: \" , ns end subroutine get_star_reps subroutine set_up_diis () #ifdef MPI use m_mpi_utils , only : globalize_max , globalize_min #endif integer :: ip , ispin , n real ( dp ) :: max_g2 , min_g2 , q1sq_def #ifdef MPI real ( dp ) :: global_max , global_min #endif if ( Node == 0 ) print \"(/,a)\" , \"Setting up DIIS for rho(G) -------\" if ( Node == 0 ) print \"(a,i6)\" , & \"Number of G's treated with DIIS in Node 0: \" , ng_diis call re_alloc ( g2_diis , 1 , 2 * ng_diis * nspin , \"g2_diis\" , \"order_rhog\" ) call re_alloc ( rg_in , 1 , 2 * ng_diis * nspin , \"rg_in\" , \"order_rhog\" ) call re_alloc ( rg_diff , 1 , 2 * ng_diis * nspin , \"rg_diff\" , \"order_rhog\" ) ! Create the g2_diis array holding |G|&#94;2 for each G in the DIIS set ip = 0 do ispin = 1 , nspin do j = 1 , size ( rhog_in , dim = 2 ) if ( g2mask ( j )) then do i = 1 , 2 ! A bit superfluous ip = ip + 1 g2_diis ( ip ) = g2 ( j ) enddo endif enddo enddo min_g2 = minval ( g2_diis ) max_g2 = maxval ( g2_diis ) #ifdef MPI call globalize_min ( min_g2 , global_min ) min_g2 = global_min call globalize_max ( max_g2 , global_max ) max_g2 = global_max #endif if ( Node == 0 ) then print \"(a,2f10.3)\" , & \"Minimum and maximum g2 in DIIS: \" , min_g2 , max_g2 endif ! KF parameter for scalar-product weight ! Weight smallest g 20 times more than largest g ! (when possible -- if not, set factor n<20) n = floor ( max_g2 / min_g2 ) - 1 n = min ( n , 20 ) q1sq_def = ( n - 1 ) * max_g2 / (( max_g2 / min_g2 ) - n ) if ( Node == 0 ) then print \"(a,f10.3,a,i3)\" , & \"Metric preconditioner cutoff default (Ry): \" , q1sq_def , & \" n:\" , n endif q1sq = fdf_get ( \"SCF.RhoG.Metric.Preconditioner.Cutoff\" , q1sq_def , \"Ry\" ) if ( Node == 0 ) then print \"(a,f10.3)\" , & \"Metric preconditioner cutoff (Ry): \" , q1sq print \"(a,2f10.4)\" , \"Max and min weights: \" , & ( min_g2 + q1sq ) / min_g2 , & ( max_g2 + q1sq ) / max_g2 endif call new ( rhog_stack , n_rhog_depth , \"(rhog DIIS stack)\" ) end subroutine set_up_diis end subroutine order_rhog","tags":"","loc":"proc/order_rhog.html","title":"order_rhog – SIESTA"},{"text":"public subroutine resetRhog(continuation) Uses alloc proc~~resetrhog~~UsesGraph proc~resetrhog resetRhog alloc alloc proc~resetrhog->alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name logical, intent(in), optional :: continuation If .true. we don't de-allocate anything, we only reset the history Calls proc~~resetrhog~~CallsGraph proc~resetrhog resetRhog reset reset proc~resetrhog->reset de_alloc de_alloc proc~resetrhog->de_alloc delete delete proc~resetrhog->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code resetRhog Source Code subroutine resetRhog ( continuation ) use alloc , only : de_alloc !< If .true. we don't de-allocate anything, we only reset the history logical , intent ( in ), optional :: continuation logical :: lcontinuation lcontinuation = . false . if ( present ( continuation ) ) lcontinuation = continuation if ( lcontinuation ) then call reset ( rhog_stack ) else call de_alloc ( rhog_in ) call de_alloc ( rhog ) call de_alloc ( g2 ) call de_alloc ( g2mask ) call de_alloc ( gindex ) call de_alloc ( star_index ) call de_alloc ( g2_diis ) call de_alloc ( rg_in ) call de_alloc ( rg_diff ) call delete ( rhog_stack ) end if end subroutine resetRhog","tags":"","loc":"proc/resetrhog.html","title":"resetRhog – SIESTA"},{"text":"public subroutine initMeshDistr(iDistr, oDistr, nm, wload) Computes a new data distribution and the communications needed to\n move data from/to the current distribution to the existing ones. The limits of the new distributions are stored in the current module\n in meshDistr : meshDistr(oDistr)\n meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) If this is the first distribution, we split the mesh uniformly among\n the several processes (we only split it in dimensions Y and Z). For the other data distributions we should split the vector wload.\n The subroutine splitwload will return the limits of the new data\n distribution. The subroutine compMeshComm will return the communications\n needed to move data from/to the current distribution to/from the\n previous ones. Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index of the input vector integer, intent(in) :: oDistr The new data distribution index integer, intent(in) :: nm (3) Number of Mesh divisions of each cell vector integer, intent(in), optional :: wload (*) Weights of every point of the mesh using the input distribut      !ion Calls proc~~initmeshdistr~~CallsGraph proc~initmeshdistr initMeshDistr re_alloc re_alloc proc~initmeshdistr->re_alloc proc~die die proc~initmeshdistr->proc~die timer timer proc~initmeshdistr->timer io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code initMeshDistr Source Code subroutine initMeshDistr ( iDistr , oDistr , nm , wload ) !! Computes a new data distribution and the communications needed to !! move data from/to the current distribution to the existing ones. !! !! The limits of the new distributions are stored in the current module !! in `[[moreMeshSubs(module):meshDistr(variable)]]`: !! !!     meshDistr(oDistr) !!     meshCommu(((oDistr-2)*(oDistr-1))/2+1:(oDistr-1)*oDistr/2) !! !! If this is the first distribution, we split the mesh uniformly among !! the several processes (we only split it in dimensions Y and Z). !! !! For the other data distributions we should split the vector wload. !! The subroutine splitwload will return the limits of the new data !! distribution. The subroutine compMeshComm will return the communications !! needed to move data from/to the current distribution to/from the !! previous ones. implicit none integer , optional , intent ( in ) :: iDistr !!  Distribution index of the input vector integer , intent ( in ) :: oDistr !!  The new data distribution index integer , intent ( in ) :: nm ( 3 ) !!  Number of Mesh divisions of each cell vector integer , optional , intent ( in ) :: wload ( * ) !!  Weights of every point of the mesh using the input distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'initMeshDistr ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: ii , jj , PY , PZ , PP , ProcessorZ , & blocY , blocZ , nremY , nremZ , & iniY , iniZ , dimY , dimZ , nsize type ( meshDisType ), pointer :: distr logical , save :: firstime = . true . integer , pointer :: box (:,:,:), mybox (:,:) call timer ( 'INITMESH' , 1 ) !     Check the number of mesh distribution if ( oDistr . gt . maxDistr ) & call die ( errMsg // 'oDistr.gt.maxDistr' ) !     Reset data if necessay if ( firstime ) then do ii = 1 , maxDistr nullify ( meshDistr ( ii )% box ) nullify ( meshDistr ( ii )% indexp ) nullify ( meshDistr ( ii )% idop ) nullify ( meshDistr ( ii )% xdop ) nullify ( meshDistr ( ii )% ipa ) enddo do ii = 1 , ( maxDistr * ( maxDistr - 1 )) / 2 nullify ( meshCommu ( ii )% src ) nullify ( meshCommu ( ii )% dst ) enddo do ii = 1 , maxDistr do jj = 1 , 3 nullify ( exteCommu ( ii , jj )% src ) nullify ( exteCommu ( ii , jj )% dst ) enddo enddo #ifdef ASYNCHRONOUS nullify ( tBuff1 ) nullify ( tBuff2 ) #endif firstime = . false . endif distr => meshDistr ( oDistr ) !     Allocate memory for the current distribution nullify ( distr % box ) call re_alloc ( distr % box , 1 , 2 , 1 , 3 , 1 , Nodes , & 'distr%box' , moduName ) !     The first distribution should be the uniform distribution if ( oDistr . eq . 1 ) then ProcessorZ = Nodes / ProcessorY blocY = ( nm ( 2 ) / ProcessorY ) blocZ = ( nm ( 3 ) / ProcessorZ ) nremY = nm ( 2 ) - blocY * ProcessorY nremZ = nm ( 3 ) - blocZ * ProcessorZ PP = 1 iniY = 1 do PY = 1 , ProcessorY dimY = blocY if ( PY . LE . nremY ) dimY = dimY + 1 iniZ = 1 do PZ = 1 , ProcessorZ dimZ = blocZ if ( PZ . LE . nremZ ) dimZ = dimZ + 1 distr % box ( 1 , 1 , PP ) = 1 distr % box ( 2 , 1 , PP ) = nm ( 1 ) distr % box ( 1 , 2 , PP ) = iniY distr % box ( 2 , 2 , PP ) = iniY + dimY - 1 distr % box ( 1 , 3 , PP ) = iniZ distr % box ( 2 , 3 , PP ) = iniZ + dimZ - 1 iniZ = iniZ + dimZ PP = PP + 1 enddo iniY = iniY + dimY enddo else !       In order to compute the other data distributions, we should split !       the vector \"wload\" among the several processes #ifdef MPI if (. NOT . present ( iDistr ) . OR . & . NOT . present ( wload ) ) then call die ( errMsg // 'Wrong parameters' ) endif call splitwload ( Nodes , node + 1 , nm , wload , & meshDistr ( iDistr ), meshDistr ( oDistr ) ) call reordMeshNumbering ( meshDistr ( 1 ), distr ) !       Precompute the communications needed to move data between the new data !       distribution and the previous ones. jj = (( oDistr - 2 ) * ( oDistr - 1 )) / 2 + 1 do ii = 1 , oDistr - 1 call compMeshComm ( meshDistr ( ii ), distr , meshCommu ( jj ) ) jj = jj + 1 enddo #endif endif if ( Node == 0 ) then write ( 6 , \"(a,i3)\" ) \"New grid distribution: \" , oDistr do PP = 1 , Nodes write ( 6 , \"(i12,3x,3(i5,a1,i5))\" ) $ PP , $ ( distr % box ( 1 , jj , PP ), \":\" , distr % box ( 2 , jj , PP ), jj = 1 , 3 ) enddo endif call timer ( 'INITMESH' , 2 ) end subroutine initMeshDistr","tags":"","loc":"proc/initmeshdistr.html","title":"initMeshDistr – SIESTA"},{"text":"public subroutine allocASynBuffer(ndistr) Uses mesh proc~~allocasynbuffer~~UsesGraph proc~allocasynbuffer allocASynBuffer module~mesh mesh proc~allocasynbuffer->module~mesh module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Allocate memory buffers for asynchronous communications.\n It does nothing for synchronous communications. The output values are stored in the current module: tBuff1 : Buffer for distribution 1 tBuff2 : Buffer for other distributions Arguments Type Intent Optional Attributes Name integer :: ndistr Total number of distributions Calls proc~~allocasynbuffer~~CallsGraph proc~allocasynbuffer allocASynBuffer re_alloc re_alloc proc~allocasynbuffer->re_alloc proc~boxintersection boxIntersection proc~allocasynbuffer->proc~boxintersection de_alloc de_alloc proc~allocasynbuffer->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocASynBuffer Source Code subroutine allocASynBuffer ( ndistr ) !! Allocate memory buffers for asynchronous communications. !! It does nothing for synchronous communications. !! The output values are stored in the current module: !! `[[moreMeshSubs(module):tBuff1(variable)]]` : Buffer for distribution 1 !! `[[moreMeshSubs(module):tBuff2(variable)]]` : Buffer for other distributions use mesh , only : nsm implicit none integer :: ndistr !! Total number of distributions integer :: ii , jj , imax1 , imax2 , lsize , nsp , Lbox ( 2 , 3 ) integer , pointer :: box1 (:,:), box2 (:,:), nsize (:) logical :: inters #ifdef ASYNCHRONOUS !     Allocate local memory nsp = nsm * nsm * nsm call re_alloc ( nsize , 1 , ndistr , 'nsize' , moduName ) !     Check the size of the local box for every data distribution do ii = 1 , ndistr box1 => meshDistr ( ii )% box (:,:, node + 1 ) nsize ( ii ) = ( box1 ( 2 , 1 ) - box1 ( 1 , 1 ) + 1 ) * & ( box1 ( 2 , 2 ) - box1 ( 1 , 2 ) + 1 ) * & ( box1 ( 2 , 3 ) - box1 ( 1 , 3 ) + 1 ) * nsp enddo !     Check the size of the intersections between the first data distributions !     and the others data distributions. !     Buffers don't need to store intersections imax1 = 0 imax2 = 0 box1 => meshDistr ( 1 )% box (:,:, node + 1 ) do ii = 2 , ndistr box2 => meshDistr ( ii )% box (:,:, node + 1 ) call boxIntersection ( box1 , box2 , Lbox , inters ) if ( inters ) then lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp else lsize = 0 endif imax1 = max ( imax1 , nsize ( 1 ) - lsize ) imax2 = max ( imax2 , nsize ( ii ) - lsize ) enddo !     Deallocate local memory call de_alloc ( nsize , 'nsize' , moduName ) !     Allocate memory for asynchronous communications call re_alloc ( tBuff1 , 1 , imax1 , 'tBuff1' , moduName ) call re_alloc ( tBuff2 , 1 , imax2 , 'tBuff2' , moduName ) #endif end subroutine allocASynBuffer","tags":"","loc":"proc/allocasynbuffer.html","title":"allocASynBuffer – SIESTA"},{"text":"public subroutine allocExtMeshDistr(iDistr, nep, mop) Uses mesh proc~~allocextmeshdistr~~UsesGraph proc~allocextmeshdistr allocExtMeshDistr module~mesh mesh proc~allocextmeshdistr->module~mesh module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nep integer, intent(in) :: mop Calls proc~~allocextmeshdistr~~CallsGraph proc~allocextmeshdistr allocExtMeshDistr re_alloc re_alloc proc~allocextmeshdistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocExtMeshDistr Source Code subroutine allocExtMeshDistr ( iDistr , nep , mop ) use mesh , only : indexp , idop , xdop implicit none !     Input variables integer , intent ( in ) :: iDistr , nep , mop !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % indexp , 1 , nep , 'distr%indexp' , moduName ) call re_alloc ( distr % idop , 1 , mop , 'distr%idop' , moduName ) call re_alloc ( distr % xdop , 1 , 3 , 1 , mop , 'distr%xdop' , moduName ) indexp => distr % indexp idop => distr % idop xdop => distr % xdop end subroutine allocExtMeshDistr","tags":"","loc":"proc/allocextmeshdistr.html","title":"allocExtMeshDistr – SIESTA"},{"text":"public subroutine allocIpaDistr(iDistr, na) Uses mesh proc~~allocipadistr~~UsesGraph proc~allocipadistr allocIpaDistr module~mesh mesh proc~allocipadistr->module~mesh module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: na Calls proc~~allocipadistr~~CallsGraph proc~allocipadistr allocIpaDistr re_alloc re_alloc proc~allocipadistr->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code allocIpaDistr Source Code subroutine allocIpaDistr ( iDistr , na ) use mesh , only : ipa implicit none !     Input variables integer , intent ( in ) :: iDistr , na !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) call re_alloc ( distr % ipa , 1 , na , 'distr%ipa' , moduName ) ipa => meshDistr ( iDistr )% ipa end subroutine allocIpaDistr","tags":"","loc":"proc/allocipadistr.html","title":"allocIpaDistr – SIESTA"},{"text":"public subroutine setMeshDistr(iDistr, nsm, nsp, nml, nmpl, ntml, ntpl) Uses mesh proc~~setmeshdistr~~UsesGraph proc~setmeshdistr setMeshDistr module~mesh mesh proc~setmeshdistr->module~mesh module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Fixes the new data limits and dimensions of the mesh to those of\n the data distribution iDistr . Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index of the input vector integer, intent(in) :: nsm Number of mesh sub-divisions in each direction integer, intent(in) :: nsp Number of sub-points of each mesh point integer, intent(out) :: nml (3) Local number of Mesh divisions in each cell vector integer, intent(out) :: nmpl Local number of Mesh divisions integer, intent(out) :: ntml (3) Local number of Mesh points in each cell vector integer, intent(out) :: ntpl Local number of Mesh points Called by proc~~setmeshdistr~~CalledByGraph proc~setmeshdistr setMeshDistr proc~dhscf_init dhscf_init proc~dhscf_init->proc~setmeshdistr proc~dhscf dhscf proc~dhscf->proc~setmeshdistr proc~forhar forhar proc~dhscf->proc~forhar proc~delk_wrapper delk_wrapper proc~delk_wrapper->proc~setmeshdistr proc~forhar->proc~setmeshdistr proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setMeshDistr Source Code subroutine setMeshDistr ( iDistr , nsm , nsp , nml , nmpl , ntml , ntpl ) !! Fixes the new data limits and dimensions of the mesh to those of !! the data distribution `iDistr`. use mesh , only : meshLim , indexp , ipa , idop , xdop implicit none integer , intent ( in ) :: iDistr !! Distribution index of the input vector integer , intent ( in ) :: nsm !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: nsp !! Number of sub-points of each mesh point integer , intent ( out ) :: nml ( 3 ) !! Local number of Mesh divisions in each cell vector integer , intent ( out ) :: nmpl !! Local number of Mesh divisions integer , intent ( out ) :: ntml ( 3 ) !! Local number of Mesh points in each cell vector integer , intent ( out ) :: ntpl !! Local number of Mesh points !     Local variables type ( meshDisType ), pointer :: distr distr => meshDistr ( iDistr ) meshLim = distr % box ( 1 : 2 , 1 : 3 , node + 1 ) nml ( 1 ) = ( MeshLim ( 2 , 1 ) - MeshLim ( 1 , 1 )) + 1 nml ( 2 ) = ( MeshLim ( 2 , 2 ) - MeshLim ( 1 , 2 )) + 1 nml ( 3 ) = ( MeshLim ( 2 , 3 ) - MeshLim ( 1 , 3 )) + 1 nmpl = nml ( 1 ) * nml ( 2 ) * nml ( 3 ) ntml = nml * nsm ntpl = nmpl * nsp indexp => distr % indexp idop => distr % idop xdop => distr % xdop ipa => distr % ipa !--------------------------------------------------------------------------- END end subroutine setMeshDistr","tags":"","loc":"proc/setmeshdistr.html","title":"setMeshDistr – SIESTA"},{"text":"public subroutine resetMeshDistr(iDistr) Reset the data of the distribution iDistr .\n Deallocate associated arrays of the current distribution. Modifies data of the current module. Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index to be reset Calls proc~~resetmeshdistr~~CallsGraph proc~resetmeshdistr resetMeshDistr de_alloc de_alloc proc~resetmeshdistr->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code resetMeshDistr Source Code subroutine resetMeshDistr ( iDistr ) !! Reset the data of the distribution `iDistr`. !! Deallocate associated arrays of the current distribution. !! Modifies data of the current module. implicit none integer , optional , intent ( in ) :: iDistr !! Distribution index to be reset integer :: idis , ini , fin , icom type ( meshDisType ), pointer :: distr type ( meshCommType ), pointer :: mcomm if ( present ( iDistr )) then ini = iDistr fin = iDistr else ini = 1 fin = maxDistr endif do idis = ini , fin distr => meshDistr ( idis ) distr % nMesh = 0 if ( associated ( distr % box )) then call de_alloc ( distr % box , 'distr%box' , 'moreMeshSubs' ) endif if ( associated ( distr % indexp )) then call de_alloc ( distr % indexp , 'distr%indexp' , & 'moreMeshSubs' ) endif if ( associated ( distr % idop )) then call de_alloc ( distr % idop , 'distr%idop' , & 'moreMeshSubs' ) endif if ( associated ( distr % xdop )) then call de_alloc ( distr % xdop , 'distr%xdop' , & 'moreMeshSubs' ) endif if ( associated ( distr % ipa )) then call de_alloc ( distr % ipa , 'distr%ipa' , & 'moreMeshSubs' ) endif do icom = 1 , 3 mcomm => exteCommu ( idis , icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo do icom = (( idis - 2 ) * ( idis - 1 )) / 2 + 1 , (( idis - 1 ) * idis ) / 2 mcomm => meshCommu ( icom ) if ( associated ( mcomm % src )) then call de_alloc ( mcomm % src , 'mcomm%src' , 'moreMeshSubs' ) endif if ( associated ( mcomm % dst )) then call de_alloc ( mcomm % dst , 'mcomm%dst' , 'moreMeshSubs' ) endif mcomm % ncom = 0 enddo enddo #ifdef ASYNCHRONOUS if ( associated ( tBuff1 )) then call de_alloc ( tBuff1 , 'tBuff1' , 'moreMeshSubs' ) endif if ( associated ( tBuff2 )) then call de_alloc ( tBuff2 , 'tBuff2' , 'moreMeshSubs' ) endif #endif end subroutine resetMeshDistr","tags":"","loc":"proc/resetmeshdistr.html","title":"resetMeshDistr – SIESTA"},{"text":"private subroutine distMeshData_rea(iDistr, fsrc, oDistr, fdst, itr) Uses mesh mpi_siesta proc~~distmeshdata_rea~2~~UsesGraph proc~distmeshdata_rea~2 distMeshData_rea module~mesh mesh proc~distmeshdata_rea~2->module~mesh mpi_siesta mpi_siesta proc~distmeshdata_rea~2->mpi_siesta module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_rea~2~~CallsGraph proc~distmeshdata_rea~2 distMeshData_rea mpitrace_event mpitrace_event proc~distmeshdata_rea~2->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea~2->proc~boxintersection proc~reord reord proc~distmeshdata_rea~2->proc~reord re_alloc re_alloc proc~distmeshdata_rea~2->re_alloc timer timer proc~distmeshdata_rea~2->timer mpi_barrier mpi_barrier proc~distmeshdata_rea~2->mpi_barrier de_alloc de_alloc proc~distmeshdata_rea~2->de_alloc write_debug write_debug proc~distmeshdata_rea~2->write_debug proc~die die proc~distmeshdata_rea~2->proc~die proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_rea Source Code subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea","tags":"","loc":"proc/distmeshdata_rea~2.html","title":"distMeshData_rea – SIESTA"},{"text":"private subroutine distMeshData_rea(iDistr, fsrc, oDistr, fdst, itr) Uses mesh mpi_siesta proc~~distmeshdata_rea~~UsesGraph proc~distmeshdata_rea distMeshData_rea module~mesh mesh proc~distmeshdata_rea->module~mesh mpi_siesta mpi_siesta proc~distmeshdata_rea->mpi_siesta module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_rea~~CallsGraph proc~distmeshdata_rea distMeshData_rea mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~reord reord proc~distmeshdata_rea->proc~reord re_alloc re_alloc proc~distmeshdata_rea->re_alloc timer timer proc~distmeshdata_rea->timer mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier de_alloc de_alloc proc~distmeshdata_rea->de_alloc write_debug write_debug proc~distmeshdata_rea->write_debug proc~die die proc~distmeshdata_rea->proc~die proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~distmeshdata_rea~~CalledByGraph proc~distmeshdata_rea distMeshData_rea interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_rea proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~forhar forhar proc~dhscf->proc~forhar proc~forhar->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian var panprocdistmeshdata_reaCalledByGraph = svgPanZoom('#procdistmeshdata_reaCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_rea Source Code subroutine distMeshData_rea ( iDistr , fsrc , oDistr , fdst , itr ) use mesh , only : nsm , nmeshg #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr real ( grid_p ), intent ( in ) :: fsrc ( * ) real ( grid_p ), intent ( out ) :: fdst ( * ) !     Local variables integer :: i , I1 , I2 , I3 , N1 , N2 , N3 , NN , ind , & J1 , J2 , J3 , K1 , K2 , K3 , KS , KR , & icom , ncom , nsp , me , nsize , lsize , & NSRC ( 3 ), NDST ( 3 ), Lbox ( 2 , 3 ), ierr , & nm ( 3 ), status ( MPI_Status_Size ), & Xsize , Ysize , Zsize logical :: inters integer , pointer :: request (:), src (:), dst (:), & Sbox (:,:), Dbox (:,:), JS (:) real ( grid_p ), pointer :: sBuff (:), rBuff (:) type ( meshDisType ), pointer :: idis , odis #ifdef DEBUG call write_debug ( '    PRE distMeshData' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 6 ) #endif call timer ( 'COMM_BSC' , 1 ) nm ( 1 : 3 ) = nmeshg ( 1 : 3 ) / nsm if ( nodes == 1 ) then if ( itr . gt . 0 ) then ! Note that in reord the first argument is always ! clustered call reord ( fsrc , fdst , nm , nsm , TO_SEQUENTIAL ) else if ( itr . lt . 0 ) then call reord ( fdst , fsrc , nm , nsm , TO_CLUSTER ) else ! Copy source to destination ! This will be executed only in serial mode, ! so we know that the size is the total number ! of (small) points, but maybe this information ! should be more explicit. nsize = product ( nmeshg ( 1 : 3 )) fdst ( 1 : nsize ) = fsrc ( 1 : nsize ) endif else ! nodes > 1 !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif nullify ( request ) call re_alloc ( request , 1 , ncom , 'request' , 'distmeshdata' ) idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) nsp = nsm * nsm * nsm me = node + 1 nullify ( JS ) call re_alloc ( JS , 1 , nsp , 'JS' , 'distmeshdata' ) if ( iDistr . eq . UNIFORM ) then sBuff => tBuff1 (:) rBuff => tBuff2 (:) else if ( oDistr . eq . UNIFORM ) then sBuff => tBuff2 (:) rBuff => tBuff1 (:) else !           Asynchronous buffers are sized to move data from/to !           UNIFORM distribution. Check subroutine allocASynBuffer !           to contemplate different cases call die ( 'Asynchronous temporal buffer error' ) endif endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = ( Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 ) * nsm NSRC ( 2 ) = ( Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 ) * nsm NSRC ( 3 ) = ( Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 ) * nsm Dbox => odis % box (:,:, ME ) NDST ( 1 ) = ( Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 ) * nsm NDST ( 2 ) = ( Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 ) * nsm NDST ( 3 ) = ( Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 ) * nsm if ( itr . eq . 1 ) then !         From clustered to sequential NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NDST ( 1 ) ENDDO I3 = I3 + NDST ( 1 ) * NDST ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = fsrc ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM K2 = K2 + NDST ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSP + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 ) J1 = J1 + 1 K1 = K1 + 1 ENDDO enddo J2 = J2 + NSRC ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 + JS ( NN )) = rBuff ( J1 ) J1 = J1 + 1 ENDDO K1 = K1 + NSM enddo K2 = K2 + NDST ( 1 ) * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . - 1 ) then !         From sequencial to clustered NN = 1 I3 = 0 DO N3 = 0 , NSM - 1 I2 = 0 DO N2 = 0 , NSM - 1 I1 = I2 + I3 DO N1 = 0 , NSM - 1 JS ( NN ) = I1 NN = NN + 1 I1 = I1 + 1 ENDDO I2 = I2 + NSRC ( 1 ) ENDDO I3 = I3 + NSRC ( 1 ) * NSRC ( 2 ) ENDDO KS = 1 KR = 1 do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM K2 = K2 + NDST ( 1 ) * NSM * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo else !               We should send data to process dst(icom)-1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP sBuff ( K1 ) = fsrc ( J1 + JS ( NN )) K1 = K1 + 1 ENDDO J1 = J1 + NSM enddo J2 = J2 + NSRC ( 1 ) * NSM enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) * NSM enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else C We should receive data from process src ( icom ) - 1 lsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * nsp #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = Lbox ( 1 , 3 ), Lbox ( 2 , 3 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM * NSM do I2 = Lbox ( 1 , 2 ), Lbox ( 2 , 2 ) K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSP + 1 + K2 + K3 do I1 = Lbox ( 1 , 1 ), Lbox ( 2 , 1 ) DO NN = 1 , NSP fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 ENDDO enddo K2 = K2 + NDST ( 1 ) * NSM * NSM enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) * NSM enddo endif enddo else if ( itr . eq . 0 ) then KS = 1 KR = 1 !         From sequencial to sequencial or from clustered to clustered do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 lsize = Xsize * Ysize * Zsize J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) * NSM K1 = KS do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) * NSM do I2 = 1 , Ysize J1 = ( Lbox ( 1 , 1 ) - Sbox ( 1 , 1 )) * NSM + 1 + J2 + J3 do I1 = 1 , Xsize sBuff ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call mpi_isend ( sBuff ( KS ), lsize , MPI_grid_real , & dst ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KS = K1 endif else !             We should receive data from process src(icom)-1 lsize = Xsize * Ysize * Zsize #ifdef MPI call mpi_irecv ( rBuff ( KR ), lsize , MPI_grid_real , & src ( icom ) - 1 , 0 , MPI_COMM_WORLD , & request ( icom ), ierr ) #endif KR = KR + lsize endif enddo J1 = 1 do icom = 1 , ncom !           Wait for received data and move it to the destination buffer if ( src ( icom ). ne . ME ) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * NSM Ysize = ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * NSM Zsize = ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) * NSM #ifdef MPI CALL MPI_WAIT ( request ( icom ), status , ierr ) #endif K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) * NSM do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) * NSM do I2 = 1 , Ysize K1 = ( Lbox ( 1 , 1 ) - Dbox ( 1 , 1 )) * NSM + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = rBuff ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif call de_alloc ( JS , 'JS' , 'distmeshdata' ) call de_alloc ( request , 'request' , 'distmeshdata' ) endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , ierr ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'COMM_BSC' , 2 ) #ifdef DEBUG call write_debug ( '    POS distMeshData' ) #endif end subroutine distMeshData_rea","tags":"","loc":"proc/distmeshdata_rea.html","title":"distMeshData_rea – SIESTA"},{"text":"private subroutine distMeshData_int(iDistr, fsrc, oDistr, fdst, itr) Uses mpi_siesta proc~~distmeshdata_int~~UsesGraph proc~distmeshdata_int distMeshData_int mpi_siesta mpi_siesta proc~distmeshdata_int->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr Calls proc~~distmeshdata_int~~CallsGraph proc~distmeshdata_int distMeshData_int re_alloc re_alloc proc~distmeshdata_int->re_alloc proc~boxintersection boxIntersection proc~distmeshdata_int->proc~boxintersection de_alloc de_alloc proc~distmeshdata_int->de_alloc proc~die die proc~distmeshdata_int->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~distmeshdata_int~~CalledByGraph proc~distmeshdata_int distMeshData_int interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_int proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~forhar forhar proc~dhscf->proc~forhar proc~forhar->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian var panprocdistmeshdata_intCalledByGraph = svgPanZoom('#procdistmeshdata_intCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distMeshData_int Source Code subroutine distMeshData_int ( iDistr , fsrc , oDistr , fdst , itr ) #ifdef MPI use mpi_siesta #endif implicit none !     Passed arguments integer , intent ( in ) :: iDistr , oDistr , itr integer , intent ( in ) :: fsrc ( * ) integer , intent ( out ) :: fdst ( * ) !     Local variables character ( len =* ), parameter :: myName = moduName // 'distMeshData ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: I1 , I2 , I3 , J1 , J2 , J3 , K1 , K2 , K3 , & ind , ncom , icom , NSRC ( 3 ), NDST ( 3 ), & ME , MaxSize , Xsize , Ysize , Zsize , & Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:), Sbox (:,:), & Dbox (:,:) type ( meshDisType ), pointer :: idis , odis logical :: inters integer , pointer :: TBUF (:) #ifdef MPI integer :: MPIerror , Status ( MPI_Status_Size ) #endif if ( nodes == 1 ) then call die ( \"Called _int version of distMeshData for n=1\" ) else !       The communications are stored in a triangular structure. if ( iDistr . gt . oDistr ) then ind = (( iDistr - 1 ) * ( iDistr - 2 )) / 2 + oDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% dst dst => meshCommu ( ind )% src else ind = (( oDistr - 1 ) * ( oDistr - 2 )) / 2 + iDistr ncom = meshCommu ( ind )% ncom src => meshCommu ( ind )% src dst => meshCommu ( ind )% dst endif idis => meshDistr ( iDistr ) odis => meshDistr ( oDistr ) ME = Node + 1 !       Compute the maximum size of the buffer needed to transfer data !       among the several processes maxSize = 0 do icom = 1 , ncom if ( src ( icom ). ne . dst ( icom )) then Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 MaxSize = max ( MaxSize , Xsize * Ysize * Zsize ) endif enddo if ( MaxSize . gt . 0 ) then nullify ( TBUF ) call re_alloc ( TBUF , 1 , MaxSize , 'TBUF' , 'moreMeshSubs' ) endif Sbox => idis % box (:,:, ME ) NSRC ( 1 ) = Sbox ( 2 , 1 ) - Sbox ( 1 , 1 ) + 1 NSRC ( 2 ) = Sbox ( 2 , 2 ) - Sbox ( 1 , 2 ) + 1 NSRC ( 3 ) = Sbox ( 2 , 3 ) - Sbox ( 1 , 3 ) + 1 Dbox => odis % box (:,:, ME ) NDST ( 1 ) = Dbox ( 2 , 1 ) - Dbox ( 1 , 1 ) + 1 NDST ( 2 ) = Dbox ( 2 , 2 ) - Dbox ( 1 , 2 ) + 1 NDST ( 3 ) = Dbox ( 2 , 3 ) - Dbox ( 1 , 3 ) + 1 if ( itr . eq . 0 ) then !         From sequencial to sequencial do icom = 1 , ncom Sbox => idis % box (:,:, src ( icom )) Dbox => odis % box (:,:, dst ( icom )) call boxIntersection ( Sbox , Dbox , Lbox , inters ) Xsize = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ysize = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Zsize = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( src ( icom ). eq . ME ) then if ( dst ( icom ). eq . ME ) then !               SRC and DST are the current process J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) K2 = K2 + NDST ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo else !               We should send data to process dst(icom)-1 J3 = ( Lbox ( 1 , 3 ) - Sbox ( 1 , 3 )) * NSRC ( 1 ) * NSRC ( 2 ) K1 = 1 do I3 = 1 , Zsize J2 = ( Lbox ( 1 , 2 ) - Sbox ( 1 , 2 )) * NSRC ( 1 ) do I2 = 1 , Ysize J1 = Lbox ( 1 , 1 ) - Sbox ( 1 , 1 ) + 1 + J2 + J3 do I1 = 1 , Xsize TBUF ( K1 ) = fsrc ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo J2 = J2 + NSRC ( 1 ) enddo J3 = J3 + NSRC ( 1 ) * NSRC ( 2 ) enddo #ifdef MPI call MPI_Send ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , dst ( icom ) - 1 , 1 , & MPI_Comm_world , MPIerror ) #endif endif else !             We should receive data from process src(icom)-1 #ifdef MPI call mpi_recv ( TBUF , Xsize * Ysize * Zsize , & MPI_Integer , src ( icom ) - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) #endif J1 = 1 K3 = ( Lbox ( 1 , 3 ) - Dbox ( 1 , 3 )) * NDST ( 1 ) * NDST ( 2 ) do I3 = 1 , Zsize K2 = ( Lbox ( 1 , 2 ) - Dbox ( 1 , 2 )) * NDST ( 1 ) do I2 = 1 , Ysize K1 = Lbox ( 1 , 1 ) - Dbox ( 1 , 1 ) + 1 + K2 + K3 do I1 = 1 , Xsize fdst ( K1 ) = TBUF ( J1 ) K1 = K1 + 1 J1 = J1 + 1 enddo K2 = K2 + NDST ( 1 ) enddo K3 = K3 + NDST ( 1 ) * NDST ( 2 ) enddo endif enddo else if ( Node . eq . 0 ) then write ( * , * ) 'ERROR: Wrong parameter for function distMeshData' endif call die () endif if ( MaxSize . gt . 0 ) then call de_alloc ( TBUF , 'TBUF' , 'moreMeshSubs' ) endif endif end subroutine distMeshData_int","tags":"","loc":"proc/distmeshdata_int.html","title":"distMeshData_int – SIESTA"},{"text":"private subroutine boxIntersection(ibox1, ibox2, obox, inters) Checks the three axis of the input boxes to see if there is\n intersection between the input boxes. If it exists, returns\n the resulting box. Arguments Type Intent Optional Attributes Name integer, intent(in) :: ibox1 (2,3) Input box integer, intent(in) :: ibox2 (2,3) Input box integer, intent(out) :: obox (2,3) Intersection between ibox1 and ibox2 logical, intent(out) :: inters TRUE , if there is an intersection. Otherwise FALSE . Called by proc~~boxintersection~~CalledByGraph proc~boxintersection boxIntersection proc~reordmeshnumbering reordMeshNumbering proc~reordmeshnumbering->proc~boxintersection proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->proc~boxintersection proc~distextmeshdata distExtMeshData proc~distextmeshdata->proc~boxintersection proc~compmeshcomm compMeshComm proc~compmeshcomm->proc~boxintersection proc~allocasynbuffer allocASynBuffer proc~allocasynbuffer->proc~boxintersection proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->proc~boxintersection proc~gathextmeshdata gathExtMeshData proc~gathextmeshdata->proc~boxintersection proc~splitwload splitwload proc~splitwload->proc~boxintersection proc~distmeshdata_int distMeshData_int proc~distmeshdata_int->proc~boxintersection proc~initmeshextencil initMeshExtencil proc~initmeshextencil->proc~boxintersection proc~reordmeshnumbering~2 reordMeshNumbering proc~reordmeshnumbering~2->proc~boxintersection interface~distmeshdata distMeshData interface~distmeshdata->proc~distmeshdata_rea interface~distmeshdata->proc~distmeshdata_int proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~forhar forhar proc~dhscf->proc~forhar proc~forhar->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian var panprocboxintersectionCalledByGraph = svgPanZoom('#procboxintersectionCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code boxIntersection Source Code subroutine boxIntersection ( ibox1 , ibox2 , obox , inters ) !! Checks the three axis of the input boxes to see if there is !! intersection between the input boxes. If it exists, returns !! the resulting box. implicit none !     Passed arguments integer , intent ( in ) :: ibox1 ( 2 , 3 ), ibox2 ( 2 , 3 ) !! Input box integer , intent ( out ) :: obox ( 2 , 3 ) !! Intersection between `ibox1` and `ibox2` logical , intent ( out ) :: inters !! `TRUE`, if there is an intersection. Otherwise `FALSE`. !     Local variables integer :: iaxis inters = . true . do iaxis = 1 , 3 obox ( 1 , iaxis ) = max ( ibox1 ( 1 , iaxis ), ibox2 ( 1 , iaxis )) obox ( 2 , iaxis ) = min ( ibox1 ( 2 , iaxis ), ibox2 ( 2 , iaxis )) if ( obox ( 2 , iaxis ). lt . obox ( 1 , iaxis )) inters = . false . enddo end subroutine boxIntersection","tags":"","loc":"proc/boxintersection.html","title":"boxIntersection – SIESTA"},{"text":"public subroutine initMeshExtencil(iDistr, nm) Uses scheComm proc~~initmeshextencil~~UsesGraph proc~initmeshextencil initMeshExtencil scheComm scheComm proc~initmeshextencil->scheComm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Compute the needed communications in order to send/receive the\n extencil (when the data is ordered in the distribution iDistr )\n The results are stored in the variable exteCommu (iDistr,1:3) of the current module. For every dimension of the problem, search all the neighbors that\n we have. Given the current data distribution we compute the limits\n of our extencil and we check its intersection with all the other\n processes. Once we know all our neighbors we call subroutine scheduleComm in order to minimize the number\n of communications steps. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: nm (3) Number of Mesh divisions in each cell vector Calls proc~~initmeshextencil~~CallsGraph proc~initmeshextencil initMeshExtencil re_alloc re_alloc proc~initmeshextencil->re_alloc proc~boxintersection boxIntersection proc~initmeshextencil->proc~boxintersection de_alloc de_alloc proc~initmeshextencil->de_alloc schedulecomm schedulecomm proc~initmeshextencil->schedulecomm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code initMeshExtencil Source Code subroutine initMeshExtencil ( iDistr , nm ) !! Compute the needed communications in order to send/receive the !! extencil (when the data is ordered in the distribution `iDistr`) !! The results are stored in the variable !! `[[moreMeshSubs(module):exteCommu(variable)]](iDistr,1:3)` !! of the current module. !! !! For every dimension of the problem, search all the neighbors that !! we have. Given the current data distribution we compute the limits !! of our extencil and we check its intersection with all the other !! processes. Once we know all our neighbors we call subroutine !! `[[scheduleComm(proc)]]` in order to minimize the number !! of communications steps. use scheComm implicit none !     Passed arguments integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: nm ( 3 ) !! Number of Mesh divisions in each cell vector !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), Ibox ( 2 , 3 ), & ii , iaxis , ncom , Gcom , Lcom , P1 , P2 integer , pointer :: src (:), dst (:), Dbox (:,:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm type ( COMM_T ) :: comm logical :: inters idis => meshDistr ( iDistr ) do iaxis = 1 , 3 !       One communication structure for every dimension mcomm => exteCommu ( iDistr , iaxis ) !       Count the number of communications needed to send/receive !       the extencil ncom = 0 do P1 = 1 , Nodes !         Create the extencil boxes for both sides of the current !         partition Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) ncom = ncom + 1 endif enddo enddo Gcom = ncom !       Create a list of communications needed to send/receive !       the extencil if ( Gcom . gt . 0 ) then nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) ncom = 0 do P1 = 1 , Nodes Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , P1 ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do P2 = P1 + 1 , Nodes Dbox => idis % box (:,:, P2 ) call boxIntersection ( Dbox , Ubox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 else call boxIntersection ( Dbox , Lbox , Ibox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif endif enddo enddo comm % np = Nodes !         reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !         Count the number of communications needed by the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !         Store the ordered list of communications needed by the current !         process to send/receive the extencil. if ( Lcom . gt . 0 ) then nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , & 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , & 'moreMeshSubs' ) ncom = 0 do P1 = 1 , comm % ncol ii = comm % ind ( P1 , Node + 1 ) if ( ii . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( ii ) mcomm % dst ( ncom ) = dst ( ii ) endif enddo mcomm % ncom = Lcom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) endif call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) endif enddo end subroutine initMeshExtencil","tags":"","loc":"proc/initmeshextencil.html","title":"initMeshExtencil – SIESTA"},{"text":"public subroutine distExtMeshData(iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, DENS, BDENS) Uses mpi_siesta proc~~distextmeshdata~~UsesGraph proc~distextmeshdata distExtMeshData mpi_siesta mpi_siesta proc~distextmeshdata->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Send/receive the extencil information from the DENS matrix to the\n temporal array BDENS . We have a different code for every axis. We should find if we\n intersects with a neightbour node throught the upper, the lower\n or both sides. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: DENS (maxp,NSPIN) Electron density matrix real(kind=gp), intent(out) :: BDENS (BS,2*NN,NSPIN) Auxiliary arrays to store the extencil from other partitions Calls proc~~distextmeshdata~~CallsGraph proc~distextmeshdata distExtMeshData re_alloc re_alloc proc~distextmeshdata->re_alloc proc~boxintersection boxIntersection proc~distextmeshdata->proc~boxintersection de_alloc de_alloc proc~distextmeshdata->de_alloc proc~die die proc~distextmeshdata->proc~die mpi_sendrecv mpi_sendrecv proc~distextmeshdata->mpi_sendrecv io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code distExtMeshData Source Code subroutine distExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , dens , BDENS ) !! Send/receive the extencil information from the `DENS` matrix to the !! temporal array `BDENS`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: DENS ( maxp , NSPIN ) !! Electron density matrix real ( gp ), intent ( out ) :: BDENS ( BS , 2 * NN , NSPIN ) !! Auxiliary arrays to store the extencil from other partitions !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM if (. not . associated ( mcomm % dst )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif if (. not . associated ( mcomm % src )) then write ( 6 , * ) 'ERROR: Trying to communicate extencil ' , & 'with an uninitialized mesh distribution' call die () endif nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = NN , 1 , - 1 tt = tt + 1 BDENS ( uu , ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY do ix = 1 , NN tt = tt + 1 BDENS ( uu , NN + ix , ispin ) = RBUF ( tt ) enddo uu = uu + 1 enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = dimB ( 2 ) - NN + 1 , dimB ( 2 ) uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = NN , 1 , - 1 uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = iniZ , endZ do iy = 1 , NN uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iy , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ) - NN + 1 , dimB ( 3 ) do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = dens ( uu , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = NN , 1 , - 1 do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 BDENS ( uu , NN + iz , ispin ) = RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine distExtMeshData","tags":"","loc":"proc/distextmeshdata.html","title":"distExtMeshData – SIESTA"},{"text":"public subroutine gathExtMeshData(iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, BVXC, VXC) Uses mpi_siesta proc~~gathextmeshdata~~UsesGraph proc~gathextmeshdata gathExtMeshData mpi_siesta mpi_siesta proc~gathextmeshdata->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Send/receive the extencil information from the BVXC temporal array\n to the array VXC . We have a different code for every axis. We should find if we\n intersects with a neightbour node throught the upper, the lower\n or both sides. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: BVXC (BS,2*NN,NSPIN) Auxiliar array that contains the extencil of the\n exch-corr potential real(kind=gp), intent(out) :: VXC (maxp,NSPIN) Exch-corr potential Calls proc~~gathextmeshdata~~CallsGraph proc~gathextmeshdata gathExtMeshData re_alloc re_alloc proc~gathextmeshdata->re_alloc proc~boxintersection boxIntersection proc~gathextmeshdata->proc~boxintersection de_alloc de_alloc proc~gathextmeshdata->de_alloc mpi_sendrecv mpi_sendrecv proc~gathextmeshdata->mpi_sendrecv Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code gathExtMeshData Source Code subroutine gathExtMeshData ( iDistr , iaxis , BS , NSM , NN , NSPIN , & maxp , NMeshG , BVXC , VXC ) !! Send/receive the extencil information from the `BVXC` temporal array !! to the array `VXC`. !! !! We have a different code for every axis. We should find if we !! intersects with a neightbour node throught the upper, the lower !! or both sides. use mpi_siesta implicit none integer , intent ( in ) :: iDistr !! Distribution index to be used integer , intent ( in ) :: iaxis !! Axe to be splitted integer , intent ( in ) :: BS !! Dimension of a plane in the current axe integer , intent ( in ) :: NSM !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: NN !! Size of the extencil integer , intent ( in ) :: NSPIN !! Number of pollarizations integer , intent ( in ) :: maxp !! Total number of points integer , intent ( in ) :: NMeshG ( 3 ) !! Number of Mesh points in each cell vector real ( gp ), intent ( in ) :: BVXC ( BS , 2 * NN , NSPIN ) !! Auxiliar array that contains the extencil of the !! exch-corr potential real ( gp ), intent ( out ) :: VXC ( maxp , NSPIN ) !! Exch-corr potential !     Local variables integer :: Ubox ( 2 , 3 ), Lbox ( 2 , 3 ), IUbox ( 2 , 3 ), & ILbox ( 2 , 3 ), nm ( 3 ), ispin , Cnode , & iniX , endX , iniY , endY , iniZ , endZ , & ix , iy , iz , tt , uu , dimB ( 3 ), ii , PP logical :: inter1 , inter2 integer , pointer :: Dbox (:,:) real ( gp ), pointer :: SBUF (:), RBUF (:) type ( meshDisType ), pointer :: idis type ( meshCommType ), pointer :: mcomm integer :: MPIerror , Status ( MPI_Status_Size ) idis => meshDistr ( iDistr ) mcomm => exteCommu ( iDistr , iaxis ) nm = NMeshG / NSM Cnode = Node + 1 dimB ( 1 ) = ( idis % box ( 2 , 1 , Cnode ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM dimB ( 2 ) = ( idis % box ( 2 , 2 , Cnode ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM dimB ( 3 ) = ( idis % box ( 2 , 3 , Cnode ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM nullify ( SBUF , RBUF ) call re_alloc ( SBUF , 1 , BS * NN * nspin , 'SBUF' , 'moreMeshSubs' ) call re_alloc ( RBUF , 1 , BS * NN * nspin , 'RBUF' , 'moreMeshSubs' ) Ubox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Ubox ( 1 , iaxis ) = Ubox ( 1 , iaxis ) - 1 if ( Ubox ( 1 , iaxis ). lt . 1 ) Ubox ( 1 , iaxis ) = nm ( iaxis ) Ubox ( 2 , iaxis ) = Ubox ( 1 , iaxis ) Lbox ( 1 : 2 , 1 : 3 ) = idis % box ( 1 : 2 , 1 : 3 , Cnode ) Lbox ( 2 , iaxis ) = Lbox ( 2 , iaxis ) + 1 if ( Lbox ( 2 , iaxis ). gt . nm ( iaxis )) Lbox ( 2 , iaxis ) = 1 Lbox ( 1 , iaxis ) = Lbox ( 2 , iaxis ) do ii = 1 , mcomm % ncom if ( Cnode . eq . mcomm % src ( ii )) then PP = mcomm % dst ( ii ) else PP = mcomm % src ( ii ) endif Dbox => idis % box (:,:, PP ) call boxIntersection ( Dbox , Ubox , IUbox , inter1 ) call boxIntersection ( Dbox , Lbox , ILbox , inter2 ) if ( inter1 ) then if ( iaxis . eq . 1 ) then iniX = 1 endX = NN iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( IUbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( IUbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( IUbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( IUbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( IUbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( IUbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter2 ) then tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif if ( inter2 ) then if ( iaxis . eq . 1 ) then iniX = dimB ( 1 ) - NN + 1 endX = dimB ( 1 ) iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do ix = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 2 ) + iniY do iy = iniY , endY tt = tt + 1 SBUF ( tt ) = BVXC ( uu , ix , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do ix = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do ix = dimB ( 1 ), dimB ( 1 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iniY - 1 ) * dimB ( 1 ) + ix do iy = iniY , endY tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + dimB ( 1 ) enddo enddo enddo enddo endif else if ( iaxis . eq . 2 ) then iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniZ = ( ILbox ( 1 , 3 ) - idis % box ( 1 , 3 , Cnode )) * NSM + 1 endZ = ( ILbox ( 2 , 3 ) - idis % box ( 1 , 3 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iy = NN + 1 , 2 * NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iy , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iy = 1 , NN do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iy = dimB ( 2 ), dimB ( 2 ) - NN + 1 , - 1 do iz = iniZ , endZ uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif else iniX = ( ILbox ( 1 , 1 ) - idis % box ( 1 , 1 , Cnode )) * NSM + 1 endX = ( ILbox ( 2 , 1 ) - idis % box ( 1 , 1 , Cnode ) + 1 ) * NSM iniY = ( ILbox ( 1 , 2 ) - idis % box ( 1 , 2 , Cnode )) * NSM + 1 endY = ( ILbox ( 2 , 2 ) - idis % box ( 1 , 2 , Cnode ) + 1 ) * NSM tt = 0 do ispin = 1 , nspin do iz = NN + 1 , 2 * NN do iy = iniY , endY uu = ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 SBUF ( tt ) = BVXC ( uu , iz , ispin ) uu = uu + 1 enddo enddo enddo enddo call MPI_SendRecv ( SBUF , tt , MPI_grid_real , PP - 1 , 0 , & RBUF , tt , MPI_grid_real , PP - 1 , 0 , & MPI_Comm_world , Status , MPIerror ) if ( inter1 ) then tt = 0 do ispin = 1 , nspin do iz = 1 , NN do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo else tt = 0 do ispin = 1 , nspin do iz = dimB ( 3 ), dimB ( 3 ) - NN + 1 , - 1 do iy = iniY , endY uu = ( iz - 1 ) * dimB ( 1 ) * dimB ( 2 ) + ( iy - 1 ) * dimB ( 1 ) + iniX do ix = iniX , endX tt = tt + 1 VXC ( uu , ispin ) = VXC ( uu , ispin ) + RBUF ( tt ) uu = uu + 1 enddo enddo enddo enddo endif endif endif enddo call de_alloc ( RBUF , 'RBUF' , 'moreMeshSubs' ) call de_alloc ( SBUF , 'SBUF' , 'moreMeshSubs' ) end subroutine gathExtMeshData","tags":"","loc":"proc/gathextmeshdata.html","title":"gathExtMeshData – SIESTA"},{"text":"private subroutine splitwload(Nodes, Node, nm, wload, iDistr, oDistr) Uses mpi_siesta proc~~splitwload~~UsesGraph proc~splitwload splitwload mpi_siesta mpi_siesta proc~splitwload->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Compute the limits of a new distribution, trying to split the load\n of the array wload . We use the nested disection algorithm in\n order to split the mesh in the 3 dimensions. We use the nested disection algorithm to split the load associated\n to the vector wload among all the processes. The problem is that\n every process have a different part of wload. Every time that we want\n to split a piece of the mesh, we should find which processors have that\n information. wload is a 3D array. In every iteration of the algorithm we should\n decide the direction of the cut. Then we should made a reduction of\n this 3-D array to a 1-D array (according to the selected direction). Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes Total number of nodes integer, intent(in) :: Node Current process ID (from 1 to Node) integer, intent(in) :: nm (3) Number of mesh sub-divisions in each direction integer, intent(in) :: wload (*) Weights of every point of the mesh. type( meshDisType ), intent(in) :: iDistr Input distribution type( meshDisType ), intent(out) :: oDistr Output distribution Calls proc~~splitwload~~CallsGraph proc~splitwload splitwload proc~boxintersection boxIntersection proc~splitwload->proc~boxintersection mpi_bcast mpi_bcast proc~splitwload->mpi_bcast re_alloc re_alloc proc~splitwload->re_alloc mpi_recv mpi_recv proc~splitwload->mpi_recv timer timer proc~splitwload->timer de_alloc de_alloc proc~splitwload->de_alloc proc~reduce3dto1d reduce3Dto1D proc~splitwload->proc~reduce3dto1d mpi_send mpi_send proc~splitwload->mpi_send Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code splitwload Source Code subroutine splitwload ( Nodes , Node , nm , wload , iDistr , oDistr ) !! Compute the limits of a new distribution, trying to split the load !! of the array `wload`. We use the nested disection algorithm in !! order to split the mesh in the 3 dimensions. !! !! We use the nested disection algorithm to split the load associated !! to the vector `wload` among all the processes. The problem is that !! every process have a different part of wload. Every time that we want !! to split a piece of the mesh, we should find which processors have that !! information. !! !! `wload` is a 3D array. In every iteration of the algorithm we should !! decide the direction of the cut. Then we should made a reduction of !! this 3-D array to a 1-D array (according to the selected direction). use mpi_siesta implicit none integer , intent ( in ) :: Nodes !! Total number of nodes integer , intent ( in ) :: Node !! Current process ID (from 1 to Node) integer , intent ( in ) :: nm ( 3 ) !! Number of mesh sub-divisions in each direction integer , intent ( in ) :: wload ( * ) !! Weights of every point of the mesh. type ( meshDisType ), intent ( in ) :: iDistr !! Input distribution type ( meshDisType ), intent ( out ) :: oDistr !! Output distribution !     Local variables character ( len =* ), parameter :: myName = moduName // 'splitwload ' character ( len =* ), parameter :: errMsg = myName // 'ERROR: ' integer :: PP , Lbox ( 2 , 3 ), Ldim , & QQ , P1 , P2 , POS , ini integer ( i8b ), pointer :: lwload (:), gwload (:), recvB (:) logical :: found , inters integer :: mGdim , mLdim , nAxis , nms ( 3 ) integer ( i8b ) :: h1 , h2 integer , pointer :: PROCS (:) integer :: MPIerror , Status ( MPI_Status_Size ) call timer ( 'SPLOAD' , 1 ) !     At the begining of the algorithm all the mesh is assigned to the !     first node:  oDistr%box(*,*,1) = nm oDistr % box ( 1 , 1 , 1 ) = 1 oDistr % box ( 2 , 1 , 1 ) = nm ( 1 ) oDistr % box ( 1 , 2 , 1 ) = 1 oDistr % box ( 2 , 2 , 1 ) = nm ( 2 ) oDistr % box ( 1 , 3 , 1 ) = 1 oDistr % box ( 2 , 3 , 1 ) = nm ( 3 ) oDistr % box ( 1 : 2 , 1 : 3 , 2 : Nodes ) = 0 nms = nm !     Array PROCS will contain the number of processes that are associated to !     every box. At the begining all the mesh is assigned to process 1, then !     PROCS(1)=Nodes, while the rest are equal to zero nullify ( PROCS , lwload , gwload , recvB ) call re_alloc ( PROCS , 1 , Nodes , 'PROCS' , 'moreMeshSubs' ) PROCS ( 1 ) = Nodes PROCS ( 2 : Nodes ) = 0 found = . true . do while ( found ) !       Choose the direction to cut the mesh nAxis = 3 if ( nms ( 2 ). gt . nms ( nAxis )) nAxis = 2 if ( nms ( 1 ). gt . nms ( nAxis )) nAxis = 1 nms ( nAxis ) = ( nms ( nAxis ) + 1 ) / 2 !       Check if we still have to keep cutting the mesh found = . false . do PP = Nodes , 1 , - 1 if ( PROCS ( PP ). GT . 1 ) then !           There are more than one processes associated to the mesh !           of process PP. We are going to split the mesh in two parts !           of p1 and p2 processors. p1 = PROCS ( PP ) / 2 p2 = PROCS ( PP ) - p1 found = . true . !           Check if the current partition has intersection with the piece of !           mesh that we want to cut. call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, Node ), & Lbox , inters ) if ( Node . eq . PP ) then mGdim = oDistr % box ( 2 , nAxis , PP ) - oDistr % box ( 1 , nAxis , PP ) + 1 call re_alloc ( gwload , 1 , mGdim , 'gwload' , 'moreMeshSubs' ) call re_alloc ( recvB , 1 , mGdim , 'recvB' , 'moreMeshSubs' ) endif if ( inters ) then !             If there is an intersection I should reduce the intersected part !             from a 3-D array to a 1-D array. mLdim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 call re_alloc ( lwload , 1 , mLdim , 'lwload' , & 'moreMeshSubs' ) call reduce3Dto1D ( nAxis , iDistr % box (:,:, Node ), Lbox , & wload , lwload ) endif if ( Node . eq . PP ) then !             If, I'm the process PP I should receive the information from other !             processes gwload = 0 do QQ = 1 , Nodes call boxIntersection ( oDistr % box (:,:, PP ), & iDistr % box (:,:, QQ ), & Lbox , inters ) if ( inters ) then Ldim = Lbox ( 2 , nAxis ) - Lbox ( 1 , nAxis ) + 1 ini = Lbox ( 1 , nAxis ) - oDistr % box ( 1 , nAxis , PP ) if ( PP . eq . QQ ) then gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + lwload ( 1 : Ldim ) else call mpi_recv ( recvB , Ldim , MPI_INTEGER8 , QQ - 1 , 1 , & MPI_Comm_world , Status , MPIerror ) gwload ( ini + 1 : ini + Ldim ) = & gwload ( ini + 1 : ini + Ldim ) + recvB ( 1 : Ldim ) endif endif enddo call de_alloc ( recvB , 'recvB' , 'moreMeshSubs' ) !             Process PP computes where to cut the mesh call vecBisec ( mGdim , gwload ( 1 : mGdim ), & PROCS ( PP ), POS , h1 , h2 ) call de_alloc ( gwload , 'gwload' , 'moreMeshSubs' ) else if ( inters ) then !             If, I'm not the process PP I should send the information to !             the process PP call MPI_Send ( lwload , mLdim , & MPI_INTEGER8 , PP - 1 , 1 , MPI_Comm_World , & MPIerror ) endif if ( associated ( lwload )) & call de_alloc ( lwload , 'lwload' , 'moreMeshSubs' ) !           Process PP send the position of the cut to the rest of processes call MPI_Bcast ( pos , 1 , MPI_integer , PP - 1 , & MPI_Comm_World , MPIerror ) !           We have splitted the piece of mesh associated to process PP !           in two parts. One would be stored in position PP and the other !           would be stored in position PP+P1 QQ = PP + P1 oDistr % box ( 1 : 2 , 1 : 3 , QQ ) = oDistr % box ( 1 : 2 , 1 : 3 , PP ) pos = oDistr % box ( 1 , naxis , QQ ) + pos oDistr % box ( 1 , naxis , QQ ) = pos oDistr % box ( 2 , naxis , PP ) = pos - 1 !           We should actualize the numbers of processes associated to PP and QQ PROCS ( PP ) = P1 PROCS ( QQ ) = P2 endif enddo enddo call de_alloc ( PROCS , 'PROCS' , 'moreMeshSubs' ) call timer ( 'SPLOAD' , 2 ) end subroutine splitwload","tags":"","loc":"proc/splitwload.html","title":"splitwload – SIESTA"},{"text":"private subroutine reduce3Dto1D(iaxis, Ibox, Lbox, wload, lwload) Given a 3-D array, wload , we will make a reduction of its values\n to one of its dimensions ( iaxis ). Ibox gives the limits of the\n input array wload and Lbox gives the limits of the part that we\n want to reduce. First we compute the 3 dimensions of the input array and the\n intersection. We accumulate the values of the intersection into a\n 1-D array. IF (iaxis=1) lwload(II) = SUM(wload(II,*,*))\n IF (iaxis=2) lwload(II) = SUM(wload(*,II,*))\n IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iaxis Axe to be reduced integer, intent(in) :: Ibox (2,3) Limits of the input array integer, intent(in) :: Lbox (2,3) Limits of the intersection that we want to reduce integer, intent(in) :: wload (*) 3-D array that we want to reduce to one of\n its dimensions integer(kind=i8b), intent(out) :: lwload (*) 1-D array. Reduction of the intersected part\n of wload Called by proc~~reduce3dto1d~~CalledByGraph proc~reduce3dto1d reduce3Dto1D proc~splitwload splitwload proc~splitwload->proc~reduce3dto1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reduce3Dto1D Source Code subroutine reduce3Dto1D ( iaxis , Ibox , Lbox , wload , lwload ) !! Given a 3-D array, `wload`, we will make a reduction of its values !! to one of its dimensions (`iaxis`). `Ibox` gives the limits of the !! input array `wload` and `Lbox` gives the limits of the part that we !! want to reduce. !! !! First we compute the 3 dimensions of the input array and the !! intersection. We accumulate the values of the intersection into a !! 1-D array. !! !!     IF (iaxis=1) lwload(II) = SUM(wload(II,*,*)) !!     IF (iaxis=2) lwload(II) = SUM(wload(*,II,*)) !!     IF (iaxis=3) lwload(II) = SUM(wload(*,*,II)) implicit none integer , intent ( in ) :: iaxis !! Axe to be reduced integer , intent ( in ) :: Ibox ( 2 , 3 ) !! Limits of the input array integer , intent ( in ) :: Lbox ( 2 , 3 ) !! Limits of the intersection that we want to reduce integer , intent ( in ) :: wload ( * ) !! 3-D array that we want to reduce to one of !! its dimensions integer ( i8b ), intent ( out ) :: lwload ( * ) !! 1-D array. Reduction of the intersected part !! of wload !     Local variables integer :: Idim ( 3 ), Ldim ( 3 ), ind , ind1 , ind2 , ind3 , & I1 , I2 , I3 !     Dimensions of the input array Idim ( 1 ) = Ibox ( 2 , 1 ) - Ibox ( 1 , 1 ) + 1 Idim ( 2 ) = Ibox ( 2 , 2 ) - Ibox ( 1 , 2 ) + 1 Idim ( 3 ) = Ibox ( 2 , 3 ) - Ibox ( 1 , 3 ) + 1 !     Dimensions of the intersection. Ldim ( 1 ) = Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 Ldim ( 2 ) = Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 Ldim ( 3 ) = Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 if ( iaxis . eq . 1 ) then !       Reduction into the X-axis lwload ( 1 : Ldim ( 1 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I1 ) = lwload ( I1 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else if ( iaxis . eq . 2 ) then !       Reduction into the Y-axis lwload ( 1 : Ldim ( 2 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I2 ) = lwload ( I2 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo else !       Reduction into the Z-axis lwload ( 1 : Ldim ( 3 )) = 0 ind3 = ( Lbox ( 1 , 3 ) - Ibox ( 1 , 3 )) * Idim ( 1 ) * Idim ( 2 ) ind1 = Lbox ( 1 , 1 ) - Ibox ( 1 , 1 ) do I3 = 1 , Ldim ( 3 ) ind2 = ( Lbox ( 1 , 2 ) - Ibox ( 1 , 2 )) * Idim ( 1 ) do I2 = 1 , Ldim ( 2 ) ind = ind3 + ind2 + ind1 + 1 do I1 = 1 , Ldim ( 1 ) lwload ( I3 ) = lwload ( I3 ) + wload ( ind ) ind = ind + 1 enddo ind2 = ind2 + Idim ( 1 ) enddo ind3 = ind3 + Idim ( 1 ) * Idim ( 2 ) enddo endif end subroutine reduce3Dto1D","tags":"","loc":"proc/reduce3dto1d.html","title":"reduce3Dto1D – SIESTA"},{"text":"private subroutine vecBisec(nval, values, nparts, pos, h1, h2) Bisection of the load associated to an array. We want to split array values in nparts , but in this call to vecBisec we are going to make only one cut. First, we split nparts in two parts: p1=nparts/2 and p2=nparts-p1 . Then we compute the total\n load of the array values ( total ) and the desired load for the\n first part: halfG = (total*p1)/nparts Finally, we try to find the position inside values where we are\n nearer of the the desired solution. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nval Dimension of the input array integer(kind=i8b), intent(in) :: values (nval) Input array integer, intent(in) :: nparts Numbers of partitions that we want to make from\n the input array (in this call we only make one cut) integer, intent(out) :: pos Position of the cut integer(kind=i8b), intent(out) :: h1 Load of the first part integer(kind=i8b), intent(out) :: h2 Load of the second part Contents Source Code vecBisec Source Code subroutine vecBisec ( nval , values , nparts , pos , h1 , h2 ) !! Bisection of the load associated to an array. !! !! We want to split array `values` in `nparts`, but in this call to !! `vecBisec` we are going to make only one cut. First, we split `nparts` !! in two parts: `p1=nparts/2` and `p2=nparts-p1`. Then we compute the total !! load of the array `values` (`total`) and the desired load for the !! first part: !! !!     halfG = (total*p1)/nparts !! !! Finally, we try to find the position inside `values` where we are !! nearer of the the desired solution. implicit none integer , intent ( in ) :: nval !! Dimension of the input array integer ( i8b ), intent ( in ) :: values ( nval ) !! Input array integer , intent ( in ) :: nparts !! Numbers of partitions that we want to make from !! the input array (in this call we only make one cut) integer , intent ( out ) :: pos !! Position of the cut integer ( i8b ), intent ( out ) :: h1 !! Load of the first part integer ( i8b ), intent ( out ) :: h2 !! Load of the second part !     Local variables integer :: p1 , p2 , ii integer ( i8b ) :: total , halfG , halfL if ( nparts . gt . 1 ) then !       Split the number of parts in 2 p1 = nparts / 2 p2 = nparts - p1 !       Compute the total load of the array total = 0 do ii = 1 , nval total = total + values ( ii ) enddo !       Desired load of the first part halfG = ( total * p1 ) / nparts halfL = 0 pos = 0 !       Loop until we reach the solution do while ( halfL . lt . halfG ) pos = pos + 1 if ( pos . eq . nval + 1 ) STOP 'ERROR in vecBisec' halfL = halfL + values ( pos ) enddo !       Check if the previous position is better than the !       current position if (( halfL - values ( pos ) * p2 / nparts ). gt . halfG ) then halfL = halfL - values ( pos ) pos = pos - 1 endif h1 = halfL h2 = total - halfL endif end subroutine vecBisec","tags":"","loc":"proc/vecbisec.html","title":"vecBisec – SIESTA"},{"text":"private subroutine reordMeshNumbering(distr1, distr2) Warning This version of subroutine is called if\n the directive REORD1 is defined. Given a new distribution, distr2 , reasign each box to the proper\n process. We use the following criteria: Minimize the number of communications. Data don't need to\nbe communicated if it belongs to the same process in\ndifferent data distributions The output values are stored in the current module: meshDistr (distr2)%box(:,:,:) Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution Calls proc~~reordmeshnumbering~~CallsGraph proc~reordmeshnumbering reordMeshNumbering re_alloc re_alloc proc~reordmeshnumbering->re_alloc proc~boxintersection boxIntersection proc~reordmeshnumbering->proc~boxintersection de_alloc de_alloc proc~reordmeshnumbering->de_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reordMeshNumbering Source Code subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is defined. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > Minimize the number of communications. Data don't need to !! > be communicated if it belongs to the same process in !! > different data distributions !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters !     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) !     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo !     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 !       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo !       Save the permutation for this node (a negative number means that !       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI !       Change the permutation of those nodes who pretend to use the !       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering","tags":"","loc":"proc/reordmeshnumbering.html","title":"reordMeshNumbering – SIESTA"},{"text":"private subroutine reordMeshNumbering(distr1, distr2) Uses fdf proc~~reordmeshnumbering~2~~UsesGraph proc~reordmeshnumbering~2 reordMeshNumbering fdf fdf proc~reordmeshnumbering~2->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Warning This version of subroutine is called if\n the directive REORD1 is NOT defined. Here an integer parameter PROCS_PER_NODE is also\n used as an input. Value of PROCS_PER_NODE is either\n read from .fdf-file or set to 4 as default. Given a new distribution, distr2 , reasign each box to the proper\n process. We use the following criteria: Minimize the number of communications. Data don't need to\n  be communicated if it belongs to the same process in\n  different data distributions Distribute memory needs among different NODES (group of processes\n  that shares the same memory) The output values are stored in the current module: meshDistr (distr2)%box(:,:,:) Behavior Compute the size of all the boxes of the second distribution Reorder the list of boxes according to its size Create a list of buckets Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution Calls proc~~reordmeshnumbering~2~~CallsGraph proc~reordmeshnumbering~2 reordMeshNumbering myqsort myqsort proc~reordmeshnumbering~2->myqsort re_alloc re_alloc proc~reordmeshnumbering~2->re_alloc proc~boxintersection boxIntersection proc~reordmeshnumbering~2->proc~boxintersection de_alloc de_alloc proc~reordmeshnumbering~2->de_alloc fdf_get fdf_get proc~reordmeshnumbering~2->fdf_get Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code reordMeshNumbering Source Code subroutine reordMeshNumbering ( distr1 , distr2 ) !!@warning !! This version of subroutine is called if !! the directive `REORD1` is defined. !!@endwarning !! !! Given a new distribution, `distr2`, reasign each box to the proper !! process. We use the following criteria: !! > Minimize the number of communications. Data don't need to !! > be communicated if it belongs to the same process in !! > different data distributions !! !! The output values are stored in the current module: !!        `[[moreMeshSubs(module):meshDistr(variable)]](distr2)%box(:,:,:)` implicit none type ( meshDisType ), intent ( in ) :: distr1 !! First distribution type ( meshDisType ), intent ( out ) :: distr2 !! Second distribution !     Local variables integer :: P1 , P2 , P3 , Lbox ( 2 , 3 ), II , I1 , & PermI integer , pointer :: Isiz (:,:), perm (:), weig (:), & invp (:), box (:,:,:) => null () logical :: inters !     Allocate local arrays nullify ( Isiz , perm , invp , weig ) call re_alloc ( Isiz , 1 , Nodes , 1 , Nodes , 'Isiz' , 'moreMeshSubs' ) call re_alloc ( perm , 1 , Nodes , 'perm' , 'moreMeshSubs' ) call re_alloc ( invp , 1 , Nodes , 'invp' , 'moreMeshSubs' ) call re_alloc ( weig , 1 , Nodes , 'weig' , 'moreMeshSubs' ) !     Check the intersections sizes between the two distributions Isiz ( 1 : nodes , 1 : nodes ) = 0 do P1 = 1 , Nodes II =- 1 do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then Isiz ( P2 , P1 ) = ( Lbox ( 2 , 1 ) - Lbox ( 1 , 1 ) + 1 ) * & ( Lbox ( 2 , 2 ) - Lbox ( 1 , 2 ) + 1 ) * & ( Lbox ( 2 , 3 ) - Lbox ( 1 , 3 ) + 1 ) if ( Isiz ( P2 , P1 ). gt . II ) then II = Isiz ( P2 , P1 ) I1 = P2 endif endif enddo weig ( P1 ) = II perm ( P1 ) = I1 enddo !     Choose a proper permutation for every row invp ( 1 : Nodes ) = 0 do P1 = 1 , Nodes II = - 1 !       Choose the node with higher weight among those not permuted before do P2 = 1 , Nodes if ( perm ( P2 ). gt . 0 ) then if ( weig ( P2 ). gt . II ) then II = weig ( P2 ) I1 = P2 endif endif enddo !       Save the permutation for this node (a negative number means that !       the node has been permuted. PermI = perm ( I1 ) invp ( PermI ) = I1 perm ( I1 ) = - PermI !       Change the permutation of those nodes who pretend to use the !       permutation permI do P2 = 1 , Nodes if ( perm ( P2 ). eq . PermI ) then II = - 1 do P3 = 1 , Nodes if ( invp ( P3 ). eq . 0 . and . Isiz ( P3 , P2 ). gt . II ) then II = Isiz ( P3 , P2 ) I1 = P3 endif enddo weig ( P2 ) = II perm ( P2 ) = I1 endif enddo enddo call re_alloc ( box , 1 , 2 , 1 , 3 , 1 , Nodes , 'box' , 'moreMeshSubs' ) box ( 1 : 2 , 1 : 3 , 1 : Nodes ) = distr2 % box ( 1 : 2 , 1 : 3 , 1 : Nodes ) do P1 = 1 , Nodes II = - perm ( P1 ) distr2 % box ( 1 : 2 , 1 : 3 , P1 ) = box ( 1 : 2 , 1 : 3 , II ) enddo call de_alloc ( box , 'box' , 'moreMeshSubs' ) call de_alloc ( weig , 'weig' , 'moreMeshSubs' ) call de_alloc ( invp , 'invp' , 'moreMeshSubs' ) call de_alloc ( perm , 'perm' , 'moreMeshSubs' ) call de_alloc ( Isiz , 'Isiz' , 'moreMeshSubs' ) end subroutine reordMeshNumbering","tags":"","loc":"proc/reordmeshnumbering~2.html","title":"reordMeshNumbering – SIESTA"},{"text":"private subroutine compMeshComm(distr1, distr2, mcomm) Uses scheComm proc~~compmeshcomm~~UsesGraph proc~compmeshcomm compMeshComm scheComm scheComm proc~compmeshcomm->scheComm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Find the communications needed to transform one array that uses\n distribution distr1 to distribution distr2 Count the number of intersections between the source distribution\n and the destiny distribution. Every intersection represents a\n communication. Then we call scheduleComm to optimize the order of these\n communications. Finally, we save the communications that belongs to\n the current process in the variable mcomm Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 Source distribution type( meshDisType ), intent(in) :: distr2 Destination distribution type( meshCommType ), intent(out) :: mcomm Communications needed Calls proc~~compmeshcomm~~CallsGraph proc~compmeshcomm compMeshComm re_alloc re_alloc proc~compmeshcomm->re_alloc proc~boxintersection boxIntersection proc~compmeshcomm->proc~boxintersection de_alloc de_alloc proc~compmeshcomm->de_alloc schedulecomm schedulecomm proc~compmeshcomm->schedulecomm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compMeshComm Source Code subroutine compMeshComm ( distr1 , distr2 , mcomm ) !! Find the communications needed to transform one array that uses !! distribution `distr1` to distribution `distr2` !! !! Count the number of intersections between the source distribution !! and the destiny distribution. Every intersection represents a !! communication. Then we call [[scheduleComm(proc)]] !! to optimize the order of these !! communications. Finally, we save the communications that belongs to !! the current process in the variable `mcomm` use scheComm implicit none type ( meshDisType ), intent ( in ) :: distr1 !! Source distribution type ( meshDisType ), intent ( in ) :: distr2 !! Destination distribution type ( meshCommType ), intent ( out ) :: mcomm !! Communications needed !     Local variables integer :: P1 , P2 , ncom , Gcom , Lcom , & Lind , Lbox ( 2 , 3 ) integer , pointer :: src (:), dst (:) logical :: inters type ( COMM_T ) :: comm !     count the number of intersections between Source distribution and !     destiny distribution. Every intersection represents a communication. ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) ncom = ncom + 1 enddo enddo Gcom = ncom !     Allocate local arrays nullify ( src , dst ) call re_alloc ( src , 1 , Gcom , 'src' , 'moreMeshSubs' ) call re_alloc ( dst , 1 , Gcom , 'dst' , 'moreMeshSubs' ) !     Make a list of communications ncom = 0 do P1 = 1 , Nodes do P2 = 1 , Nodes call boxIntersection ( distr1 % box (:,:, P1 ), & distr2 % box (:,:, P2 ), Lbox , inters ) if ( inters ) then ncom = ncom + 1 src ( ncom ) = P1 dst ( ncom ) = P2 endif enddo enddo comm % np = Nodes !     reschedule the communications in order to minimize the time call scheduleComm ( Gcom , src , dst , comm ) !     Count the number of communications of the current process ncom = 0 do P1 = 1 , comm % ncol if ( comm % ind ( P1 , Node + 1 ). ne . 0 ) ncom = ncom + 1 enddo Lcom = ncom !     Allocate memory to store data of the communications of the !     current process. nullify ( mcomm % src , mcomm % dst ) call re_alloc ( mcomm % src , 1 , Lcom , 'mcomm%src' , 'moreMeshSubs' ) call re_alloc ( mcomm % dst , 1 , Lcom , 'mcomm%dst' , 'moreMeshSubs' ) !     Save the list of communications for the current process ncom = 0 do P1 = 1 , comm % ncol Lind = comm % ind ( P1 , Node + 1 ) if ( Lind . ne . 0 ) then ncom = ncom + 1 mcomm % src ( ncom ) = src ( Lind ) mcomm % dst ( ncom ) = dst ( Lind ) endif enddo mcomm % ncom = ncom call de_alloc ( comm % ind , 'comm%ind' , 'scheComm' ) call de_alloc ( dst , 'dst' , 'moreMeshSubs' ) call de_alloc ( src , 'src' , 'moreMeshSubs' ) end subroutine compMeshComm","tags":"","loc":"proc/compmeshcomm.html","title":"compMeshComm – SIESTA"},{"text":"public interface distMeshData Move data from vector fsrc , that uses distribution iDistr , to vector fdst , that uses distribution oDistr . It also re-orders a clustered\n data array into a sequential one and viceversa.\n If this is a sequencial execution, it only reorders the data. Note There are two subroutines: one to deal with real data and\n the other with integers. Both are called using the same interface. Note AG : Note that the integer version does NOT have the exact functionality\n of the real version. In particular, the integer version has no provision\n for a \"serial fallback\", and so this case has been trapped. Check the communications that this process should do to move data\n from iDistr to oDistr . We have 3 kind of communications (send, receive\n and keep on the same node). We have 3 kind of reorderings (clustered to\n sequential, sequential to clustered and keep the same ordering). For the sequencial code we call subroutine reord INPUT iDistr : Distribution index of the input vector. fsrc : Input vector. oDistr : Distribution index of the output vector. itr : TRanslation-direction switch: itr =+1 => From clustered to sequential itr =-1 => From sequential to clustered itr =0  => Keep the status OUTPUT fdst : Output vector. Calls interface~~distmeshdata~~CallsGraph interface~distmeshdata distMeshData proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~reord reord proc~distmeshdata_rea->proc~reord re_alloc re_alloc proc~distmeshdata_rea->re_alloc timer timer proc~distmeshdata_rea->timer mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier de_alloc de_alloc proc~distmeshdata_rea->de_alloc write_debug write_debug proc~distmeshdata_rea->write_debug proc~die die proc~distmeshdata_rea->proc~die proc~distmeshdata_int->proc~boxintersection proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~die proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by interface~~distmeshdata~~CalledByGraph interface~distmeshdata distMeshData proc~dhscf_init dhscf_init proc~dhscf_init->interface~distmeshdata proc~dhscf dhscf proc~dhscf->interface~distmeshdata proc~forhar forhar proc~dhscf->proc~forhar proc~forhar->interface~distmeshdata proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures distMeshData_rea distMeshData_int Module Procedures private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr","tags":"","loc":"interface/distmeshdata.html","title":"distMeshData – SIESTA"},{"text":"public subroutine forhar(NTPL, NSPIN, NML, NTML, NPCC, CELL, RHOATM, RHOPCC, VNA, DRHOOUT, VHARRIS1, VHARRIS2) Build the potentials needed for computing Harris forces: Todo Formulas in description (V_{NA} + V_{Hartree}(DeltaRho_{in}) - DV_{xc}(Rho_{in})/Dn * (Rho_{out}-Rho_{in})) (V_{NA} + V_{Hartree}(DeltaRho_in) + V_{xc}(Rho_{in}) In the first SCF step, V_{Hartree}(DeltaRho_{in}) is zero, because\n in that case, Rho_{SCF}(r) = Rho_{atm}(r) and therefore, DeltaRho(r) = 0 This calculation will be skipped. If Harris + Spin polarized in the first SCF step, then Vharris2 will\n multiply to D Rho(Harris)/D R inside dfscf, and the change of\n the harris density respect the displacement\n on one atom will not depend on spin. We add in Vharris2 the\n contributions of both spins. Arguments Type Intent Optional Attributes Name integer :: NTPL Number of Mesh Total Points in unit cell (including subpoints      !) locally. integer, intent(in) :: NSPIN Spin polarizations integer :: NML (3) integer :: NTML (3) integer, intent(in) :: NPCC Partial core corrections? ( 0 =no, 1 =yes) real(kind=dp), intent(in) :: CELL (3,3) Cell vectors real(kind=grid_p), intent(in) :: RHOATM (NTPL) Harris density at mesh points real(kind=grid_p), intent(in) :: RHOPCC (NTPL) Partial-core-correction density for xc real(kind=grid_p), intent(in) :: VNA (NTPL) Sum of neutral atoms potentials real(kind=grid_p), intent(inout) :: DRHOOUT (NTPL,NSPIN) Charge density at the mesh points in current step.\n The charge density that enters in forhar is Drho_{out} - Rho_{atm} . real(kind=grid_p), intent(inout), TARGET :: VHARRIS1 (NTPL,NSPIN) V_{na} + V_{Hartree}(DeltaRho_{in}) + V_{xc}(Rho_{in}) real(kind=grid_p), intent(inout) :: VHARRIS2 (NTPL) V_{na} + V_{Hartree}(Rho_{in}) +\n DV_{xc}(Rho_{in})/DRho_{in} * (Rho_{out}-Rho_{in}) If Harris forces are computed in the first SCF step,\n it does not depend on spin. Calls proc~~forhar~~CallsGraph proc~forhar forhar mymeshbox mymeshbox proc~forhar->mymeshbox proc~setmeshdistr setMeshDistr proc~forhar->proc~setmeshdistr re_alloc re_alloc proc~forhar->re_alloc interface~distmeshdata distMeshData proc~forhar->interface~distmeshdata de_alloc de_alloc proc~forhar->de_alloc jms_setmeshdistr jms_setmeshdistr proc~forhar->jms_setmeshdistr bsc_cellxc bsc_cellxc proc~forhar->bsc_cellxc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~reord reord proc~distmeshdata_rea->proc~reord timer timer proc~distmeshdata_rea->timer mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier write_debug write_debug proc~distmeshdata_rea->write_debug proc~die die proc~distmeshdata_rea->proc~die proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~boxintersection proc~distmeshdata_int->proc~die proc~reord->re_alloc proc~reord->de_alloc proc~reord->timer io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~forhar~~CalledByGraph proc~forhar forhar proc~dhscf dhscf proc~dhscf->proc~forhar proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code forhar Source Code subroutine forhar ( NTPL , NSPIN , NML , NTML , NTM , NPCC , $ CELL , RHOATM , #else /* BSC_CELLXC */ subroutine forhar ( NTPL , NSPIN , NML , NTML , NPCC , CELL , RHOATM , #endif /* BSC_CELLXC */ & RHOPCC , VNA , DRHOOUT , VHARRIS1 , VHARRIS2 ) !! author: J.Junquera !! date: 09/00 !! !! Build the potentials needed for computing Harris forces: !! !!@todo !! Formulas in description !!@endtodo !! !! !! (V_{NA} + V_{Hartree}(DeltaRho_{in}) - DV_{xc}(Rho_{in})/Dn * (Rho_{out}-Rho_{in})) !! !! !! !! (V_{NA} + V_{Hartree}(DeltaRho_in) + V_{xc}(Rho_{in}) !! !! !! In the first SCF step,  V_{Hartree}(DeltaRho_{in})  is zero, because !! in that case,  Rho_{SCF}(r) = Rho_{atm}(r)  and therefore,  DeltaRho(r) = 0  !! This calculation will be skipped. !! !! If Harris + Spin polarized in the first SCF step, then Vharris2 will !! multiply to  D Rho(Harris)/D R  inside dfscf, and the change of !! the harris density respect the displacement !! on one atom will not depend on spin. We add in Vharris2 the !! contributions of both spins. INTEGER :: NTPL !! Number of Mesh Total Points in unit cell (including subpoints) locally. INTEGER :: NML ( 3 ), NTML ( 3 ) #ifndef BSC_CELLXC INTEGER , intent ( IN ) :: NTM ( 3 ) !! Number of mesh divisions of each cell !! vector, including subgrid INTEGER , INTENT ( IN ) :: NSPIN !! Spin polarizations INTEGER , INTENT ( IN ) :: NPCC !! Partial core corrections? (`0`=no,`1`=yes) #else /* BSC_CELLXC */ INTEGER , INTENT ( IN ) :: NSPIN , NPCC #endif /* BSC_CELLXC */ REAL ( dp ), INTENT ( IN ) :: CELL ( 3 , 3 ) !! Cell vectors REAL ( grid_p ), INTENT ( IN ) :: VNA ( NTPL ) !! Sum of neutral atoms potentials REAL ( grid_p ), INTENT ( IN ) :: RHOATM ( NTPL ) !! Harris density at mesh points REAL ( grid_p ), INTENT ( IN ) :: RHOPCC ( NTPL ) !! Partial-core-correction density for xc REAL ( grid_p ), INTENT ( INOUT ) :: DRHOOUT ( NTPL , NSPIN ) !! Charge density at the mesh points in current step. !! The charge density that enters in forhar is  Drho_{out} - Rho_{atm} . REAL ( grid_p ), TARGET , INTENT ( INOUT ) :: VHARRIS1 ( NTPL , NSPIN ) !!  V_{na} + V_{Hartree}(DeltaRho_{in}) + V_{xc}(Rho_{in})  real ( grid_p ), intent ( INOUT ) :: VHARRIS2 ( NTPL ) !!  !! V_{na} + V_{Hartree}(Rho_{in}) + !! DV_{xc}(Rho_{in})/DRho_{in} * (Rho_{out}-Rho_{in}) !!  !! !! If Harris forces are computed in the first SCF step, !! it does not depend on spin. #ifndef BSC_CELLXC EXTERNAL REORD #else /* BSC_CELLXC */ EXTERNAL bsc_cellxc #endif /* BSC_CELLXC */ ! AG: Note:  REAL*4 variables are really REAL(kind=grid_p) ! C ***** INTERNAL VARIABLES ********************************************* C REAL * 4 DVXDN ( NTPL , NSPIN , NSPIN ): Derivative of exchange - correlation C potential respect the charge density C ********************************************************************** C ---------------------------------------------------------------------- C Internal variables and arrays C ---------------------------------------------------------------------- #ifndef BSC_CELLXC INTEGER IP , ISPIN , ISPIN2 , myBox ( 2 , 3 ) REAL ( dp ) EX , EC , DEX , DEC , STRESS ( 3 , 3 ) INTEGER , SAVE :: JDGdistr =- 1 real ( grid_p ), pointer :: drhoin (:,:), & dvxcdn (:,:,:) #else /* BSC_CELLXC */ INTEGER :: IP , ISPIN , ISPIN2 , NMPL REAL ( dp ) :: EX , EC , DEX , DEC , STRESSL ( 3 , 3 ) real ( grid_p ) :: aux3 ( 3 , 1 ) !! dummy arrays for cellxc real ( grid_p ), pointer :: drhoin (:,:), drhoin_par (:,:), & dvxcdn (:,:,:), dvxcdn_par (:,:,:), & vharris1_par (:,:), fsrc (:), fdst (:) INTEGER :: ntpl_3 #endif /* BSC_CELLXC */ nullify ( drhoin , dvxcdn ) call re_alloc ( drhoin , 1 , ntpl , 1 , nspin , 'drhoin' , 'forhar' ) call re_alloc ( dvxcdn , 1 , ntpl , 1 , nspin , 1 , nspin , & 'dvxcdn' , 'forhar' ) C ---------------------------------------------------------------------- C Initialize some variables C ---------------------------------------------------------------------- VHARRIS1 (:,:) = 0.0_grid_p VHARRIS2 (:) = 0.0_grid_p DRHOIN (:,:) = 0.0_grid_p DVXCDN (:,:,:) = 0.0_grid_p #ifndef BSC_CELLXC C ---------------------------------------------------------------------- C Set uniform distribution of mesh points and find my processor mesh box C ---------------------------------------------------------------------- call jms_setMeshDistr ( distrID = JDGdistr , nMesh = NTM , . nNodesX = 1 , nNodesY = ProcessorY , nBlock = NSM ) call myMeshBox ( NTM , JDGdistr , myBox ) #else /* BSC_CELLXC */ STRESSL (:,:) = 0.0_dp #endif /* BSC_CELLXC */ C ---------------------------------------------------------------------- C Compute exchange - correlation energy and potential and C their derivatives respect the input charge , that is , Harris charge C or the sum of atomic charges . C ---------------------------------------------------------------------- DO ISPIN = 1 , NSPIN DRHOIN ( 1 : NTPL , ISPIN ) = RHOATM ( 1 : NTPL ) / NSPIN IF ( NPCC . EQ . 1 ) . DRHOIN ( 1 : NTPL , ISPIN ) = DRHOIN ( 1 : NTPL , ISPIN ) + . RHOPCC ( 1 : NTPL ) / NSPIN ENDDO #ifdef BSC_CELLXC #ifdef MPI ! The input distribution is UNIFORM, but we need to work ! with the LINEAR one call setMeshDistr ( LINEAR , nsm , nsp , nml , nmpl , ntml , ntpl ) #endif ntpl_3 = ntpl nullify ( drhoin_par , vharris1_par , dvxcdn_par ) call re_alloc ( drhoin_par , 1 , ntpl_3 , 1 , nspin , & 'drhoin_par' , 'forhar' ) call re_alloc ( vharris1_par , 1 , ntpl_3 , 1 , nspin , & 'vharris1_par' , 'forhar' ) call re_alloc ( dvxcdn_par , 1 , ntpl_3 , 1 , nspin , 1 , nspin , & 'dvxcdn_par' , 'forhar' ) #endif /* BSC_CELLXC */ DO ISPIN = 1 , NSPIN #ifndef BSC_CELLXC CALL REORD ( DRHOIN ( 1 , ISPIN ), DRHOIN ( 1 , ISPIN ), NML , NSM , + 1 ) #else /* BSC_CELLXC */ fsrc => drhoin (:, ispin ) fdst => drhoin_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , LINEAR , fdst , KEEP ) #endif /* BSC_CELLXC */ ENDDO #ifndef BSC_CELLXC CALL CELLXC ( 0 , CELL , NTM , myBox ( 1 , 1 ), myBox ( 2 , 1 ), . myBox ( 1 , 2 ), myBox ( 2 , 2 ), . myBox ( 1 , 3 ), myBox ( 2 , 3 ), NSPIN , DRHOIN , . EX , EC , DEX , DEC , STRESS , VHARRIS1 , DVXCDN ) #else /* BSC_CELLXC */ CALL bsc_cellxc ( 0 , 1 , CELL , NTML , NTML , NTPL , 0 , AUX3 , NSPIN , & DRHOIN_PAR , EX , EC , DEX , DEC , VHARRIS1_PAR , & DVXCDN_PAR , STRESSL ) #endif /* BSC_CELLXC */ DO ISPIN = 1 , NSPIN #ifndef BSC_CELLXC CALL REORD ( DRHOIN ( 1 , ISPIN ), DRHOIN ( 1 , ISPIN ), NML , NSM , - 1 ) CALL REORD ( VHARRIS1 ( 1 , ISPIN ), VHARRIS1 ( 1 , ISPIN ), NML , NSM , - 1 ) #else /* BSC_CELLXC */ fsrc => VHARRIS1_PAR (:, ISPIN ) fdst => VHARRIS1 (:, ispin ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) #endif /* BSC_CELLXC */ DO ISPIN2 = 1 , NSPIN #ifndef BSC_CELLXC CALL REORD ( DVXCDN ( 1 , ISPIN , ISPIN2 ), DVXCDN ( 1 , ISPIN , ISPIN2 ), . NML , NSM , - 1 ) #else /* BSC_CELLXC */ fsrc => DVXCDN_PAR (:, ISPIN , ISPIN2 ) fdst => DVXCDN (:, ISPIN , ISPIN2 ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) #endif /* BSC_CELLXC */ ENDDO ENDDO #ifdef BSC_CELLXC call de_alloc ( dvxcdn_par , 'dvxcdn_par' , 'forhar' ) call de_alloc ( vharris1_par , 'vharris1_par' , 'forhar' ) call de_alloc ( drhoin_par , 'drhoin_par' , 'forhar' ) #ifdef MPI call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) #endif #endif /* BSC_CELLXC */ DO ISPIN = 1 , NSPIN IF ( NPCC . EQ . 1 ) & DRHOIN ( 1 : NTPL , ISPIN ) = DRHOIN ( 1 : NTPL , ISPIN ) - & RHOPCC ( 1 : NTPL ) / NSPIN DO IP = 1 , NTPL VHARRIS1 ( IP , ISPIN ) = VHARRIS1 ( IP , ISPIN ) + VNA ( IP ) ENDDO ENDDO C ---------------------------------------------------------------------- C Compute the product DV_xc ( Rho_in ) / DRho_in * ( Rho_out - Rho_in ). C Since the charge that enters into forhar is DRHOOUT = Rho_out - Rhoatm C no extra transformation on the charge density is needed . C ---------------------------------------------------------------------- DO ISPIN = 1 , NSPIN DO ISPIN2 = 1 , NSPIN DO IP = 1 , NTPL VHARRIS2 ( IP ) = VHARRIS2 ( IP ) + & DVXCDN ( IP , ISPIN2 , ISPIN ) * DRHOOUT ( IP , ISPIN2 ) ENDDO ENDDO ENDDO C ---------------------------------------------------------------------- C Since V_Hartree ( DeltaRho_in ) = 0.0 , we only add to vharris2 the neutral C atom potential C ---------------------------------------------------------------------- DO IP = 1 , NTPL VHARRIS2 ( IP ) = VNA ( IP ) - VHARRIS2 ( IP ) ENDDO call de_alloc ( dvxcdn , 'dvxcdn' , 'forhar' ) call de_alloc ( drhoin , 'drhoin' , 'forhar' ) end subroutine forhar","tags":"","loc":"proc/forhar.html","title":"forhar – SIESTA"},{"text":"private elemental function parcount(Nodes, N) Convert a (positive) counter into a counter divisable by\n the number of Nodes. It works by this: PN = Nodes .PARCOUNT. N We make it elemental for obvious reasons Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes integer, intent(in) :: N Return Value integer Called by proc~~parcount~~CalledByGraph proc~parcount parcount interface~operator(.parcount.) operator(.PARCOUNT.) interface~operator(.parcount.)->proc~parcount Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code parcount Source Code elemental function parcount ( Nodes , N ) !! Convert a (positive) counter into a counter divisable by !! the number of Nodes. !! !! It works by this: !! `PN = Nodes .PARCOUNT. N` !! !! We make it elemental for obvious reasons integer , intent ( in ) :: Nodes , N integer :: parcount if ( mod ( N , Nodes ) == 0 ) then parcount = N else parcount = N + Nodes - mod ( N , Nodes ) end if end function parcount","tags":"","loc":"proc/parcount.html","title":"parcount – SIESTA"},{"text":"public subroutine parallel_init() Uses mpi_siesta proc~~parallel_init~~UsesGraph proc~parallel_init parallel_init mpi_siesta mpi_siesta proc~parallel_init->mpi_siesta Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Initializes Node, Nodes, and IOnode Arguments None Calls proc~~parallel_init~~CallsGraph proc~parallel_init parallel_init mpi_comm_size mpi_comm_size proc~parallel_init->mpi_comm_size mpi_comm_rank mpi_comm_rank proc~parallel_init->mpi_comm_rank Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code parallel_init Source Code subroutine parallel_init () !! Initializes Node, Nodes, and IOnode #ifdef MPI use mpi_siesta , only : MPI_Comm_World logical , save :: initialized = . false . integer :: MPIerror if (. not . initialized ) then call MPI_Comm_Rank ( MPI_Comm_World , Node , MPIerror ) call MPI_Comm_Size ( MPI_Comm_World , Nodes , MPIerror ) IOnode = ( Node == 0 ) initialized = . true . end if #endif end subroutine parallel_init","tags":"","loc":"proc/parallel_init.html","title":"parallel_init – SIESTA"},{"text":"public interface operator(.PARCOUNT.) Calls interface~~operator(.parcount.)~~CallsGraph interface~operator(.parcount.) operator(.PARCOUNT.) proc~parcount parcount interface~operator(.parcount.)->proc~parcount Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures parcount Module Procedures private elemental function parcount (Nodes, N) Convert a (positive) counter into a counter divisable by\n the number of Nodes. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes integer, intent(in) :: N Return Value integer","tags":"","loc":"interface/operator(.parcount.).html","title":"operator(.PARCOUNT.) – SIESTA"},{"text":"public subroutine siesta_forces(istep) Uses units precision sys files siesta_cml m_state_init m_setup_hamiltonian m_setup_H0 m_compute_dm m_compute_max_diff m_scfconvergence_test m_post_scf_work m_mixer m_mixing_scf m_mixing_scf m_mixing_scf m_rhog siesta_options parallel m_state_analysis m_steps m_spin sparse_matrices sparse_matrices m_convergence m_convergence siesta_geom m_energies m_forces m_stress siesta_master siesta_master m_save_density_matrix m_iodm_old atomlist m_dm_charge m_pexsi_solver write_subs write_subs m_compute_energies m_mpi_utils fdf m_check_walltime m_energies m_ts_options m_ts_method m_ts_global_vars siesta_geom sparse_matrices sparse_matrices m_ts_charge m_ts_charge m_ts_charge m_ts_charge m_transiesta kpoint_scf_m m_energies m_initwf proc~~siesta_forces~~UsesGraph proc~siesta_forces siesta_forces m_check_walltime m_check_walltime proc~siesta_forces->m_check_walltime m_spin m_spin proc~siesta_forces->m_spin write_subs write_subs proc~siesta_forces->write_subs m_ts_options m_ts_options proc~siesta_forces->m_ts_options siesta_cml siesta_cml proc~siesta_forces->siesta_cml module~parallel parallel proc~siesta_forces->module~parallel m_energies m_energies proc~siesta_forces->m_energies m_dm_charge m_dm_charge proc~siesta_forces->m_dm_charge module~m_setup_h0 m_setup_H0 proc~siesta_forces->module~m_setup_h0 module~sys sys proc~siesta_forces->module~sys kpoint_scf_m kpoint_scf_m proc~siesta_forces->kpoint_scf_m module~m_state_analysis m_state_analysis proc~siesta_forces->module~m_state_analysis m_steps m_steps proc~siesta_forces->m_steps m_mixer m_mixer proc~siesta_forces->m_mixer module~m_compute_max_diff m_compute_max_diff proc~siesta_forces->module~m_compute_max_diff siesta_geom siesta_geom proc~siesta_forces->siesta_geom module~m_state_init m_state_init proc~siesta_forces->module~m_state_init m_ts_method m_ts_method proc~siesta_forces->m_ts_method m_ts_charge m_ts_charge proc~siesta_forces->m_ts_charge m_save_density_matrix m_save_density_matrix proc~siesta_forces->m_save_density_matrix m_stress m_stress proc~siesta_forces->m_stress module~m_compute_energies m_compute_energies proc~siesta_forces->module~m_compute_energies m_mpi_utils m_mpi_utils proc~siesta_forces->m_mpi_utils files files proc~siesta_forces->files m_pexsi_solver m_pexsi_solver proc~siesta_forces->m_pexsi_solver module~units units proc~siesta_forces->module~units module~siesta_options siesta_options proc~siesta_forces->module~siesta_options m_convergence m_convergence proc~siesta_forces->m_convergence m_ts_global_vars m_ts_global_vars proc~siesta_forces->m_ts_global_vars m_post_scf_work m_post_scf_work proc~siesta_forces->m_post_scf_work m_forces m_forces proc~siesta_forces->m_forces m_scfconvergence_test m_scfconvergence_test proc~siesta_forces->m_scfconvergence_test m_transiesta m_transiesta proc~siesta_forces->m_transiesta module~m_setup_hamiltonian m_setup_hamiltonian proc~siesta_forces->module~m_setup_hamiltonian siesta_master siesta_master proc~siesta_forces->siesta_master m_iodm_old m_iodm_old proc~siesta_forces->m_iodm_old module~precision precision proc~siesta_forces->module~precision module~m_compute_dm m_compute_dm proc~siesta_forces->module~m_compute_dm fdf fdf proc~siesta_forces->fdf m_initwf m_initwf proc~siesta_forces->m_initwf sparse_matrices sparse_matrices proc~siesta_forces->sparse_matrices atomlist atomlist proc~siesta_forces->atomlist module~m_rhog m_rhog proc~siesta_forces->module~m_rhog module~m_mixing_scf m_mixing_scf proc~siesta_forces->module~m_mixing_scf module~m_state_analysis->write_subs module~m_compute_max_diff->module~precision module~units->module~precision module~m_rhog->m_spin module~m_rhog->module~precision class_Fstack_Pair_dData1D class_Fstack_Pair_dData1D module~m_rhog->class_Fstack_Pair_dData1D class_dData1D class_dData1D module~m_rhog->class_dData1D class_Pair_dData1D class_Pair_dData1D module~m_rhog->class_Pair_dData1D module~m_mixing m_mixing module~m_mixing_scf->module~m_mixing class_Fstack_dData1D class_Fstack_dData1D module~m_mixing_scf->class_Fstack_dData1D module~m_mixing->module~precision module~m_mixing->class_Fstack_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. This subroutine represents central SIESTA operation logic. Todo It might be better to split the two,\n  putting the grid initialization into state_init (link!) and moving the\n  calculation of H_0 to the body of the loop, done if first_scf=.true. This would suit analysis runs in which nscf = 0 The dHmax variable only has meaning for Hamiltonian\n  mixing, or when requiring the Hamiltonian to be converged. SCF loop The current structure of the loop tries to reproduce the\n  historical Siesta usage. It should be made more clear. Two changes: The number of scf iterations performed is exactly\n     equal to the number specified (i.e., the \"forces\"\n     phase is not counted as a final scf step) At the change to a TranSiesta GF run the variable \"first_scf\"\n     is implicitly reset to \"true\". Start of SCF cycle Conditions of exit: At the top, to catch a non-positive nscf and # of iterations At the bottom, based on convergence Note implications for TranSiesta when mixing H .\n  Now H will be recomputed instead of simply being\n  inherited, however, this is required as if\n  we have bias calculations as the electric\n  field across the junction needs to be present. end of SCF cycle Arguments Type Intent Optional Attributes Name integer, intent(inout) :: istep Calls proc~~siesta_forces~~CallsGraph proc~siesta_forces siesta_forces proc~state_init state_init proc~siesta_forces->proc~state_init scfconvergence_test scfconvergence_test proc~siesta_forces->scfconvergence_test set_tolerance set_tolerance proc~siesta_forces->set_tolerance fdf_get fdf_get proc~siesta_forces->fdf_get message message proc~siesta_forces->message proc~mixers_scf_history_init mixers_scf_history_init proc~siesta_forces->proc~mixers_scf_history_init interface~compute_max_diff compute_max_diff proc~siesta_forces->interface~compute_max_diff write_spmatrix write_spmatrix proc~siesta_forces->write_spmatrix proc~bye bye proc~siesta_forces->proc~bye save_density_matrix save_density_matrix proc~siesta_forces->save_density_matrix proc~state_analysis state_analysis proc~siesta_forces->proc~state_analysis proc~compute_charge_diff compute_charge_diff proc~siesta_forces->proc~compute_charge_diff proc~mixing_scf_converged mixing_scf_converged proc~siesta_forces->proc~mixing_scf_converged dm_charge dm_charge proc~siesta_forces->dm_charge proc~setup_hamiltonian setup_hamiltonian proc~siesta_forces->proc~setup_hamiltonian post_scf_work post_scf_work proc~siesta_forces->post_scf_work check_walltime check_walltime proc~siesta_forces->check_walltime barrier barrier proc~siesta_forces->barrier proc~compute_dm compute_dm proc~siesta_forces->proc~compute_dm proc~compute_energies compute_energies proc~siesta_forces->proc~compute_energies initwf initwf proc~siesta_forces->initwf reset reset proc~siesta_forces->reset mixer mixer proc~siesta_forces->mixer proc~mix_rhog mix_rhog proc~siesta_forces->proc~mix_rhog transiesta transiesta proc~siesta_forces->transiesta die die proc~siesta_forces->die timer timer proc~siesta_forces->timer proc~state_init->fdf_get proc~state_init->proc~bye proc~state_init->timer volcel volcel proc~state_init->volcel mscell mscell proc~state_init->mscell re_alloc re_alloc proc~state_init->re_alloc newddata2d newddata2d proc~state_init->newddata2d superc superc proc~state_init->superc ioxv ioxv proc~state_init->ioxv setup_ts_kpoint_scf setup_ts_kpoint_scf proc~state_init->setup_ts_kpoint_scf setup_ordern_indexes setup_ordern_indexes proc~state_init->setup_ordern_indexes setup_dmhs_netcdf_file setup_dmhs_netcdf_file proc~state_init->setup_dmhs_netcdf_file new_dm new_dm proc~state_init->new_dm exact_sc_ag exact_sc_ag proc~state_init->exact_sc_ag cmladdproperty cmladdproperty proc~state_init->cmladdproperty sporb_to_spatom sporb_to_spatom proc~state_init->sporb_to_spatom newzspdata2d newzspdata2d proc~state_init->newzspdata2d siesta_write_positions siesta_write_positions proc~state_init->siesta_write_positions write_debug write_debug proc~state_init->write_debug domaindecom domaindecom proc~state_init->domaindecom globalize_or globalize_or proc~state_init->globalize_or crtsparsity_sc crtsparsity_sc proc~state_init->crtsparsity_sc proximity_check proximity_check proc~state_init->proximity_check overlap overlap proc~state_init->overlap cmlendpropertylist cmlendpropertylist proc~state_init->cmlendpropertylist newdspdata1d newdspdata1d proc~state_init->newdspdata1d newsparsity newsparsity proc~state_init->newsparsity kpoint_nullify kpoint_nullify proc~state_init->kpoint_nullify hsparse hsparse proc~state_init->hsparse outcoor outcoor proc~state_init->outcoor cmlstartpropertylist cmlstartpropertylist proc~state_init->cmlstartpropertylist isc_off isc_off proc~state_init->isc_off setup_dm_netcdf_file setup_dm_netcdf_file proc~state_init->setup_dm_netcdf_file nsc nsc proc~state_init->nsc proc~mixers_history_init mixers_history_init proc~state_init->proc~mixers_history_init chess_init chess_init proc~state_init->chess_init iotdxv iotdxv proc~state_init->iotdxv file_exist file_exist proc~state_init->file_exist val val proc~state_init->val attach attach proc~state_init->attach sp_to_spglobal sp_to_spglobal proc~state_init->sp_to_spglobal init_val init_val proc~state_init->init_val dict_repopulate_md dict_repopulate_md proc~state_init->dict_repopulate_md kpoint_delete kpoint_delete proc~state_init->kpoint_delete get_chess_parameter get_chess_parameter proc~state_init->get_chess_parameter ts_tri_analyze ts_tri_analyze proc~state_init->ts_tri_analyze ucell ucell proc~state_init->ucell nscold nscold proc~state_init->nscold delete delete proc~state_init->delete escf escf proc~state_init->escf time_io time_io proc~state_init->time_io proc~check_cohp check_cohp proc~state_init->proc~check_cohp listhptr listhptr proc~state_init->listhptr xij_offset xij_offset proc~state_init->xij_offset setup_kpoint_scf setup_kpoint_scf proc~state_init->setup_kpoint_scf ts_sparse_init ts_sparse_init proc~state_init->ts_sparse_init normalize_dm normalize_dm proc~state_init->normalize_dm proc~die die proc~state_init->proc~die globalize_sum globalize_sum proc~state_init->globalize_sum newdspdata2d newdspdata2d proc~state_init->newdspdata2d proc~message message proc~state_init->proc~message madelung madelung proc~state_init->madelung numh numh proc~state_init->numh write_zmatrix write_zmatrix proc~state_init->write_zmatrix ts_write_tshs ts_write_tshs proc~state_init->ts_write_tshs fname_tshs fname_tshs proc~state_init->fname_tshs proc~mixers_scf_history_init->proc~mixers_history_init proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff->proc~compute_max_diff_2d proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff->proc~compute_max_diff_1d mpi_finalize mpi_finalize proc~bye->mpi_finalize pxfflush pxfflush proc~bye->pxfflush cmlfinishfile cmlfinishfile proc~bye->cmlfinishfile proc~state_analysis->timer proc~state_analysis->volcel moments moments proc~state_analysis->moments cartesianforce_to_zmatforce cartesianforce_to_zmatforce proc~state_analysis->cartesianforce_to_zmatforce print_spin print_spin proc~state_analysis->print_spin siesta_write_stress_pressure siesta_write_stress_pressure proc~state_analysis->siesta_write_stress_pressure slua_call slua_call proc~state_analysis->slua_call update_freeeharris update_freeeharris proc~state_analysis->update_freeeharris update_freee update_freee proc~state_analysis->update_freee born_charge born_charge proc~state_analysis->born_charge proc~state_analysis->cmladdproperty mulliken mulliken proc~state_analysis->mulliken eggbox eggbox proc~state_analysis->eggbox cmlendmodule cmlendmodule proc~state_analysis->cmlendmodule proc~state_analysis->write_debug fixed fixed proc~state_analysis->fixed kin_stress kin_stress proc~state_analysis->kin_stress cmlstartmodule cmlstartmodule proc~state_analysis->cmlstartmodule amass amass proc~state_analysis->amass va va proc~state_analysis->va siesta_write_forces siesta_write_forces proc~state_analysis->siesta_write_forces wallclock wallclock proc~state_analysis->wallclock remove_intramol_pressure remove_intramol_pressure proc~state_analysis->remove_intramol_pressure proc~compute_charge_diff->fdf_get proc~mixing_scf_converged->reset proc~setup_hamiltonian->proc~bye proc~setup_hamiltonian->timer proc~setup_hamiltonian->re_alloc h h proc~setup_hamiltonian->h proc~dhscf dhscf proc~setup_hamiltonian->proc~dhscf write_hsx write_hsx proc~setup_hamiltonian->write_hsx hubbard_term hubbard_term proc~setup_hamiltonian->hubbard_term proc~setup_hamiltonian->val hold hold proc~setup_hamiltonian->hold de_alloc de_alloc proc~setup_hamiltonian->de_alloc dimag dimag proc~setup_hamiltonian->dimag update_e0 update_e0 proc~setup_hamiltonian->update_e0 proc~setup_hamiltonian->proc~die proc~setup_hamiltonian->globalize_sum dscf dscf proc~setup_hamiltonian->dscf proc~compute_dm->proc~bye proc~compute_dm->transiesta proc~compute_dm->timer zminim zminim proc~compute_dm->zminim write_orb_indx write_orb_indx proc~compute_dm->write_orb_indx eold eold proc~compute_dm->eold write_dmh_netcdf write_dmh_netcdf proc~compute_dm->write_dmh_netcdf dminim dminim proc~compute_dm->dminim ordern ordern proc~compute_dm->ordern diagon diagon proc~compute_dm->diagon proc~compute_dm->val dold dold proc~compute_dm->dold pexsi_solver pexsi_solver proc~compute_dm->pexsi_solver proc~compute_dm->escf chess_wrapper chess_wrapper proc~compute_dm->chess_wrapper proc~compute_dm->normalize_dm compute_ebs_shift compute_ebs_shift proc~compute_dm->compute_ebs_shift mpi_bcast mpi_bcast proc~compute_dm->mpi_bcast proc~compute_dm->dscf write_hs_formatted write_hs_formatted proc~compute_dm->write_hs_formatted proc~compute_energies->fdf_get proc~compute_energies->update_freee update_dena update_dena proc~compute_energies->update_dena update_etot update_etot proc~compute_energies->update_etot proc~mix_rhog->fdf_get n_items n_items proc~mix_rhog->n_items diis diis proc~mix_rhog->diis proc~compute_max_diff_2d->die proc~dhscf->proc~bye proc~dhscf->timer proc~dhscf->volcel proc~dhscf->re_alloc proc~dhscf->write_debug proc~dhscf->de_alloc proc~dhscf->proc~die elecs elecs proc~dhscf->elecs proc~forhar forhar proc~dhscf->proc~forhar ts_voltage ts_voltage proc~dhscf->ts_voltage mpi_barrier mpi_barrier proc~dhscf->mpi_barrier ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofdsp rhoofdsp proc~dhscf->rhoofdsp get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field hartree_add hartree_add proc~dhscf->hartree_add proc~dfscf dfscf proc~dhscf->proc~dfscf proc~reord reord proc~dhscf->proc~reord compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata proc~vmat vmat proc~dhscf->proc~vmat mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce vacuum_level vacuum_level proc~dhscf->vacuum_level localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh proc~rhoofd rhoofd proc~dhscf->proc~rhoofd proc~write_rho write_rho proc~dhscf->proc~write_rho proc~mixers_history_init->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new proc~compute_max_diff_1d->die proc~check_cohp->proc~message proc~die->pxfflush proc~die->cmlfinishfile mpi_abort mpi_abort proc~die->mpi_abort io_close io_close proc~die->io_close io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort proc~message->pxfflush proc~message->io_close proc~message->io_assign proc~forhar->re_alloc proc~forhar->de_alloc proc~forhar->jms_setmeshdistr proc~forhar->bsc_cellxc proc~forhar->proc~setmeshdistr proc~forhar->mymeshbox proc~forhar->interface~distmeshdata proc~dfscf->timer proc~dfscf->re_alloc proc~dfscf->de_alloc proc~dfscf->proc~die indxuo indxuo proc~dfscf->indxuo listsc listsc proc~dfscf->listsc endpht endpht proc~dfscf->endpht listp2 listp2 proc~dfscf->listp2 needdscfl needdscfl proc~dfscf->needdscfl listdlptr listdlptr proc~dfscf->listdlptr matrixotom matrixotom proc~dfscf->matrixotom lstpht lstpht proc~dfscf->lstpht globaltolocalorb globaltolocalorb proc~dfscf->globaltolocalorb numdl numdl proc~dfscf->numdl proc~rcut rcut proc~dfscf->proc~rcut alloc_default alloc_default proc~dfscf->alloc_default dscfl dscfl proc~dfscf->dscfl listdl listdl proc~dfscf->listdl proc~reord->timer proc~reord->re_alloc proc~reord->de_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~vmat->timer proc~vmat->re_alloc proc~vmat->de_alloc matrixmtoo matrixmtoo proc~vmat->matrixmtoo phi phi proc~vmat->phi proc~vmat->indxuo proc~vmat->listsc proc~vmat->endpht proc~vmat->listp2 proc~vmat->needdscfl proc~vmat->listdlptr proc~vmat->lstpht proc~vmat->globaltolocalorb proc~vmat->numdl proc~vmat->proc~rcut proc~vmat->listdl proc~rhoofd->timer proc~rhoofd->re_alloc proc~rhoofd->de_alloc proc~rhoofd->proc~die proc~rhoofd->phi proc~rhoofd->indxuo proc~rhoofd->listsc proc~rhoofd->endpht proc~rhoofd->listp2 proc~rhoofd->needdscfl proc~rhoofd->listdlptr proc~rhoofd->matrixotom proc~rhoofd->lstpht proc~rhoofd->globaltolocalorb proc~rhoofd->numdl proc~rhoofd->proc~rcut proc~rhoofd->dscfl proc~rhoofd->listdl proc~write_rho->write_debug proc~write_rho->de_alloc proc~write_rho->mpi_barrier proc~write_rho->io_close proc~write_rho->io_assign mpi_wait mpi_wait proc~write_rho->mpi_wait mpi_irecv mpi_irecv proc~write_rho->mpi_irecv proc~distmeshdata_rea->timer proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->write_debug proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->proc~die proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->proc~reord mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~die proc~distmeshdata_int->proc~boxintersection proc~chk chk proc~rcut->proc~chk proc~chk->proc~die var panprocsiesta_forcesCallsGraph = svgPanZoom('#procsiesta_forcesCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code siesta_forces Source Code subroutine siesta_forces ( istep ) !! This subroutine represents central SIESTA operation logic. #ifdef MPI use mpi_siesta #endif use units , only : eV , Ang use precision , only : dp use sys , only : bye use files , only : slabel use siesta_cml #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call use flook_siesta , only : LUA_INIT_MD , LUA_SCF_LOOP use siesta_dicts , only : dict_variable_add use m_ts_options , only : ts_scf_mixs use variable , only : cunpack #ifndef NCDF_4 use dictionary , only : assign #endif use m_mixing , only : mixers_history_init #endif use m_state_init use m_setup_hamiltonian use m_setup_H0 use m_compute_dm use m_compute_max_diff use m_scfconvergence_test use m_post_scf_work use m_mixer , only : mixer use m_mixing_scf , only : mixing_scf_converged use m_mixing_scf , only : mixers_scf_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_rhog , only : mix_rhog , compute_charge_diff use siesta_options use parallel , only : IOnode , SIESTA_worker use m_state_analysis use m_steps use m_spin , only : spin use sparse_matrices , only : DM_2D , S_1D use sparse_matrices , only : H , Hold , Dold , Dscf , Eold , Escf , maxnh use m_convergence , only : converger_t use m_convergence , only : reset , set_tolerance use siesta_geom , only : na_u ! Number of atoms in unit cell use m_energies , only : Etot ! Total energy use m_forces , only : fa , cfa ! Forces and constrained forces use m_stress , only : cstress ! Constrained stress tensor use siesta_master , only : forcesToMaster ! Send forces to master prog use siesta_master , only : siesta_server ! Is siesta a server? use m_save_density_matrix , only : save_density_matrix use m_iodm_old , only : write_spmatrix use atomlist , only : no_u , lasto , Qtot use m_dm_charge , only : dm_charge use m_pexsi_solver , only : prevDmax use write_subs , only : siesta_write_forces use write_subs , only : siesta_write_stress_pressure #ifdef NCDF_4 use dictionary use m_ncdf_siesta , only : cdf_init_file , cdf_save_settings use m_ncdf_siesta , only : cdf_save_state , cdf_save_basis #endif use m_compute_energies , only : compute_energies use m_mpi_utils , only : broadcast , barrier use fdf #ifdef SIESTA__PEXSI use m_pexsi , only : pexsi_finalize_scfloop #endif use m_check_walltime use m_energies , only : DE_NEGF use m_ts_options , only : N_Elec use m_ts_method use m_ts_global_vars , only : TSmode , TSinit , TSrun use siesta_geom , only : nsc , xa , ucell , isc_off use sparse_matrices , only : sparse_pattern , block_dist use sparse_matrices , only : S use m_ts_charge , only : ts_get_charges use m_ts_charge , only : TS_RHOCORR_METHOD use m_ts_charge , only : TS_RHOCORR_FERMI use m_ts_charge , only : TS_RHOCORR_FERMI_TOLERANCE use m_transiesta , only : transiesta use kpoint_scf_m , only : gamma_scf use m_energies , only : Ef use m_initwf , only : initwf integer , intent ( inout ) :: istep integer :: iscf logical :: first_scf , SCFconverged real ( dp ) :: dDmax ! Max. change in DM elements real ( dp ) :: dHmax ! Max. change in H elements real ( dp ) :: dEmax ! Max. change in EDM elements real ( dp ) :: drhog ! Max. change in rho(G) (experimental) real ( dp ), target :: G2max ! actually used meshcutoff type ( converger_t ) :: conv_harris , conv_freeE ! For initwf integer :: istpp #ifdef SIESTA__FLOOK ! len=24 from m_mixing.F90 character ( len = 1 ), target :: next_mixer ( 24 ) character ( len = 24 ) :: nnext_mixer integer :: imix #endif logical :: time_is_up character ( len = 40 ) :: tmp_str real ( dp ) :: Qcur #ifdef NCDF_4 type ( dict ) :: d_sav #endif #ifdef MPI integer :: MPIerror #endif external :: die , message #ifdef DEBUG call write_debug ( '    PRE siesta_forces' ) #endif #ifdef SIESTA__PEXSI ! Broadcast relevant things for program logic ! These were set in read_options, called only by \"SIESTA_workers\". call broadcast ( nscf , comm = true_MPI_Comm_World ) #endif !  Initialization tasks for a given geometry: if ( SIESTA_worker ) then call state_init ( istep ) end if #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_init\" ) #endif if ( fdf_get ( \"Sonly\" ,. false .) ) then if ( SIESTA_worker ) then call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) end if call bye ( \"S only\" ) end if Qcur = Qtot #ifdef SIESTA__FLOOK ! Add the iscf constant to the list of variables ! that are available only in this part of the routine. call dict_variable_add ( 'SCF.iteration' , iscf ) call dict_variable_add ( 'SCF.converged' , SCFconverged ) call dict_variable_add ( 'SCF.charge' , Qcur ) call dict_variable_add ( 'SCF.dD' , dDmax ) call dict_variable_add ( 'SCF.dH' , dHmax ) call dict_variable_add ( 'SCF.dE' , dEmax ) call dict_variable_add ( 'SCF.drhoG' , drhog ) ! We have to set the meshcutoff here ! because the asked and required ones are not ! necessarily the same call dict_variable_add ( 'Mesh.Cutoff.Minimum' , G2cut ) call dict_variable_add ( 'Mesh.Cutoff.Used' , G2max ) if ( mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , wmix ) else call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) ! Just to populate the table in the dictionary call dict_variable_add ( 'SCF.Mixer.Switch' , next_mixer ) end if ! Initialize to no switch next_mixer = ' ' #endif !  This call computes the **non-scf** part of  H  and initializes the !  real-space grid structures: if ( SIESTA_worker ) call setup_H0 ( G2max ) !!@todo !* It might be better to split the two, !  putting the grid initialization into **state_init (link!)** and moving the !  calculation of  H_0  to the body of the loop, done `if first_scf=.true.` !  This would suit _analysis_ runs in which **nscf = 0** !!@endtodo #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after setup_H0\" ) #endif #ifdef SIESTA__FLOOK ! Communicate with lua, just before entering the SCF loop ! This is mainly to be able to communicate ! mesh-related quantities (g2max) call slua_call ( LUA , LUA_INIT_MD ) #endif #ifdef NCDF_4 ! Initialize the NC file if ( write_cdf ) then ! Initialize the file... call cdf_init_file ( trim ( slabel ) // '.nc' , is_MD = . false .) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif ! Save the settings call cdf_save_settings ( trim ( slabel ) // '.nc' ) #ifdef MPI call MPI_Barrier ( MPI_Comm_World , MPIerror ) #endif d_sav = ( 'sp' . kv . 1 ) // ( 'S' . kv . 1 ) d_sav = d_sav // ( 'nsc' . kv . 1 ) // ( 'xij' . kv . 1 ) d_sav = d_sav // ( 'xa' . kv . 1 ) // ( 'cell' . kv . 1 ) d_sav = d_sav // ( 'isc_off' . kv . 1 ) call cdf_save_state ( trim ( slabel ) // '.nc' , d_sav ) call delete ( d_sav ) ! Save the basis set call cdf_save_basis ( trim ( slabel ) // '.nc' ) end if #endif !* The dHmax variable only has meaning for Hamiltonian !  mixing, or when requiring the Hamiltonian to be converged. dDmax = - 1._dp dHmax = - 1._dp dEmax = - 1._dp drhog = - 1._dp ! Setup convergence criteria: if ( SIESTA_worker ) then if ( converge_Eharr ) then call reset ( conv_harris ) call set_tolerance ( conv_harris , tolerance_Eharr ) end if if ( converge_FreeE ) then call reset ( conv_FreeE ) call set_tolerance ( conv_FreeE , tolerance_FreeE ) end if end if !!# SCF loop !* The current structure of the loop tries to reproduce the !  historical Siesta usage. It should be made more clear. !* Two changes: ! !  1. The number of scf iterations performed is exactly !     equal to the number specified (i.e., the \"forces\" !     phase is not counted as a final scf step) !  2. At the change to a TranSiesta GF run the variable \"first_scf\" !     is implicitly reset to \"true\". ! !!## Start of SCF cycle ! !* Conditions of exit: ! !  * At the top, to catch a non-positive nscf and # of iterations !  * At the bottom, based on convergence ! iscf = 0 do while ( iscf < nscf ) iscf = iscf + 1 !* Note implications for TranSiesta when mixing H. !  Now H will be recomputed instead of simply being !  inherited, however, this is required as if !  we have bias calculations as the electric !  field across the junction needs to be present. first_scf = ( iscf == 1 ) if ( SIESTA_worker ) then ! Check whether we are short of time to continue call check_walltime ( time_is_up ) if ( time_is_up ) then ! Save DM/H if we were not saving it... !   Do any other bookeeping not done by \"die\" call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge' // & ' before wall time exhaustion' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call barrier () ! A non-root node might get first to the 'die' call call die ( \"OUT_OF_TIME: Time is up.\" ) end if call timer ( 'IterSCF' , 1 ) if ( cml_p ) & call cmlStartStep ( xf = mainXML , type = 'SCF' , index = iscf ) if ( mixH ) then if ( first_scf ) then if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then call get_H_from_file () else call setup_hamiltonian ( iscf ) end if end if call compute_DM ( iscf ) ! Maybe set Dold to zero if reading charge or H... call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) else call setup_hamiltonian ( iscf ) call compute_max_diff ( Hold , H , dHmax ) call compute_DM ( iscf ) call compute_max_diff ( Dold , Dscf , dDmax ) if ( converge_EDM ) & call compute_max_diff ( Eold , Escf , dEmax ) end if ! This iteration has completed calculating the new DM call compute_energies ( iscf ) if ( mix_charge ) then call compute_charge_diff ( drhog ) end if ! Note: For DM and H convergence checks. At this point: ! If mixing the DM: !        Dscf=DM_out, Dold=DM_in(mixed), H=H_in, Hold=H_in(prev step) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H_in - H_in(prev step)) ! If mixing the Hamiltonian: !        Dscf=DM_out, Dold=DM_in, H=H_(DM_out), Hold=H_in(mixed) !        dDmax=maxdiff(DM_out,DM_in) !        dHmax=maxdiff(H(DM_out),H_in) call scfconvergence_test ( first_scf , iscf , & dDmax , dHmax , dEmax , & conv_harris , conv_freeE , & SCFconverged ) ! ** Check this heuristic if ( mixH ) then prevDmax = dHmax else prevDmax = dDmax end if ! Calculate current charge based on the density matrix call dm_charge ( spin , DM_2D , S_1D , Qcur ) ! Check whether we should step to the next mixer call mixing_scf_converged ( SCFconverged ) if ( SCFconverged . and . iscf < min_nscf ) then SCFconverged = . false . if ( IONode ) then write ( * , \"(a,i0)\" ) & \"SCF cycle continued for minimum number of iterations: \" , & min_nscf end if end if ! In case the user has requested a Fermi-level correction ! Then we start by correcting the fermi-level if ( TSrun . and . SCFconverged . and . & TS_RHOCORR_METHOD == TS_RHOCORR_FERMI ) then if ( abs ( Qcur - Qtot ) > TS_RHOCORR_FERMI_TOLERANCE ) then ! Call transiesta with fermi-correct call transiesta ( iscf , spin % H , & block_dist , sparse_pattern , Gamma_Scf , ucell , nsc , & isc_off , no_u , na_u , lasto , xa , maxnh , H , S , & Dscf , Escf , Ef , Qtot , . true ., DE_NEGF ) ! We will not have not converged as we have just ! changed the Fermi-level SCFconverged = . false . end if end if if ( monitor_forces_in_scf ) call compute_forces () ! Mix_after_convergence preserves the old behavior of ! the program. if ( (. not . SCFconverged ) . or . mix_after_convergence ) then ! Mix for next step if ( mix_charge ) then call mix_rhog ( iscf ) else call mixer ( iscf ) end if ! Save for possible restarts if ( mixH ) then call write_spmatrix ( H , file = \"H_MIXED\" , when = writeH ) call save_density_matrix ( file = \"DM_OUT\" , when = writeDM ) else call save_density_matrix ( file = \"DM_MIXED\" , when = writeDM ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = writeH ) end if end if call timer ( 'IterSCF' , 2 ) call print_timings ( first_scf , istep == inicoor ) if ( cml_p ) call cmlEndStep ( mainXML ) #ifdef SIESTA__FLOOK ! Communicate with lua call slua_call ( LUA , LUA_SCF_LOOP ) ! Retrieve an easy character string nnext_mixer = cunpack ( next_mixer ) if ( len_trim ( nnext_mixer ) > 0 . and . . not . mix_charge ) then if ( TSrun ) then do imix = 1 , size ( ts_scf_mixs ) if ( ts_scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( ts_scf_mixs ) scf_mix => ts_scf_mixs ( imix ) exit end if end do else do imix = 1 , size ( scf_mixs ) if ( scf_mixs ( imix )% name == nnext_mixer ) then call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( imix ) exit end if end do end if ! Check that we indeed have changed the mixer if ( IONode . and . scf_mix % name /= nnext_mixer ) then write ( * , '(2a)' ) 'siesta-lua: WARNING: trying to change ' , & 'to a non-existing mixer! Not changing anything!' else if ( IONode ) then write ( * , '(2a)' ) 'siesta-lua: Switching mixer method to: ' , & trim ( nnext_mixer ) end if ! Reset for next loop next_mixer = ' ' ! Update the references call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif ! ... except that we might continue for TranSiesta if ( SCFconverged ) then call transiesta_switch () ! might reset SCFconverged and iscf end if else ! non-siesta worker call compute_DM ( iscf ) end if #ifdef SIESTA__PEXSI call broadcast ( iscf , comm = true_MPI_Comm_World ) call broadcast ( SCFconverged , comm = true_MPI_Comm_World ) #endif !  Exit if converged: if ( SCFconverged ) exit end do !! **end of SCF cycle** #ifdef SIESTA__PEXSI if ( isolve == SOLVE_PEXSI ) then call pexsi_finalize_scfloop () end if #endif if ( . not . SIESTA_worker ) return call end_of_cycle_save_operations () if ( . not . SCFconverged ) then if ( SCFMustConverge ) then call message ( 'FATAL' , 'SCF_NOT_CONV: SCF did not converge' // & ' in maximum number of steps (required).' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call barrier () call die ( 'ABNORMAL_TERMINATION' ) else if ( . not . harrisfun ) then call message ( 'WARNING' , & 'SCF_NOT_CONV: SCF did not converge  in maximum number of steps.' ) write ( tmp_str , \"(2(i5,tr1),f12.6)\" ) istep , iscf , prevDmax call message ( ' (info)' , \"Geom step, scf iteration, dmax:\" // trim ( tmp_str )) end if end if ! To write the initial wavefunctions to be used in a ! consequent TDDFT run. if ( writetdwf ) then istpp = 0 call initwf ( istpp , totime ) end if if ( TSmode . and . TSinit . and .(. not . SCFConverged ) ) then ! Signal that the DM hasn't converged, so we cannot ! continue to the transiesta routines call die ( 'ABNORMAL_TERMINATION' ) end if ! Clean-up here to limit memory usage call mixers_scf_history_init ( ) ! End of standard SCF loop. ! Do one more pass to compute forces and stresses ! Note that this call will no longer overwrite H while computing the ! final energies, forces and stresses... if ( fdf_get ( \"compute-forces\" ,. true .) ) then call post_scf_work ( istep , iscf , SCFconverged ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after post_scf_work\" ) #endif end if ! ... so H at this point is the latest generator of the DM, except ! if mixing H beyond self-consistency or terminating the scf loop ! without convergence while mixing H call state_analysis ( istep ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after state_analysis\" ) #endif ! If siesta is running as a subroutine, send forces to master program if ( siesta_server ) & call forcesToMaster ( na_u , Etot , cfa , cstress ) #ifdef DEBUG call write_debug ( '    POS siesta_forces' ) #endif contains ! Read the Hamiltonian from a file subroutine get_H_from_file () use sparse_matrices , only : maxnh , numh , listh , listhptr use atomlist , only : no_l use m_spin , only : spin use m_iodm_old , only : read_spmatrix logical :: found call read_spmatrix ( maxnh , no_l , spin % H , numh , & listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) end subroutine get_H_from_file ! Computes forces and stresses with the current DM_out subroutine compute_forces () use siesta_options , only : recompute_H_after_scf use m_final_H_f_stress , only : final_H_f_stress use write_subs real ( dp ), allocatable :: fa_old (:,:), Hsave (:,:) allocate ( fa_old ( size ( fa , dim = 1 ), size ( fa , dim = 2 ))) fa_old (:,:) = fa (:,:) if ( recompute_H_after_scf ) then allocate ( Hsave ( size ( H , dim = 1 ), size ( H , dim = 2 ))) Hsave (:,:) = H (:,:) end if call final_H_f_stress ( istep , iscf , . false . ) if ( recompute_H_after_scf ) then H (:,:) = Hsave (:,:) deallocate ( Hsave ) end if if ( ionode ) then print * , \"Max diff in force (eV/Ang): \" , & maxval ( abs ( fa - fa_old )) * Ang / eV call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () endif deallocate ( fa_old ) end subroutine compute_forces ! Print out timings of the first SCF loop only subroutine print_timings ( first_scf , first_md ) use timer_options , only : use_tree_timer use m_ts_global_vars , only : TSrun logical , intent ( in ) :: first_scf , first_md character ( len = 20 ) :: routine ! If this is not the first iteration, ! we immediately return. if ( . not . first_scf ) return if ( . not . first_md ) return routine = 'IterSCF' if ( TSrun ) then ! with Green function generation ! The tree-timer requires direct ! children of the routine to be ! queried. ! This is not obeyed in the TS case... :( if ( . not . use_tree_timer ) then routine = 'TS' end if endif call timer ( routine , 3 ) end subroutine print_timings ! Depending on various conditions, save the DMin ! or the DMout, and possibly keep a copy of H ! NOTE: Only if the scf cycle converged before exit it ! is guaranteed that the DM is \"pure out\" and that ! we can recover the right H if mixing H. ! subroutine end_of_cycle_save_operations () logical :: DM_write , H_write ! Depending on the option we should overwrite the ! Hamiltonian if ( mixH . and . . not . mix_after_convergence ) then ! Make sure that we keep the H actually used ! to generate the last DM, if needed. H = Hold end if DM_write = write_DM_at_end_of_cycle . and . & . not . writeDM H_write = write_H_at_end_of_cycle . and . & . not . writeH if ( mix_after_convergence ) then ! If we have been saving them, there is no point in doing ! it one more time if ( mixH ) then call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_MIXED\" , when = H_write ) else call save_density_matrix ( file = \"DM_MIXED\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if else call save_density_matrix ( file = \"DM_OUT\" , when = DM_write ) call write_spmatrix ( H , file = \"H_DMGEN\" , when = H_write ) end if end subroutine end_of_cycle_save_operations subroutine transiesta_switch () use precision , only : dp use parallel , only : IONode use class_dSpData2D use class_Fstack_dData1D use densematrix , only : resetDenseMatrix use siesta_options , only : fire_mix , broyden_maxit use siesta_options , only : dDtol , dHtol use sparse_matrices , only : DM_2D , EDM_2D use atomlist , only : lasto use siesta_geom , only : nsc , isc_off , na_u , xa , ucell use m_energies , only : Ef use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mix , scf_mixs use m_rhog , only : resetRhoG use m_ts_global_vars , only : TSinit , TSrun use m_ts_global_vars , only : ts_print_transiesta use m_ts_method use m_ts_options , only : N_Elec , Elecs use m_ts_options , only : DM_bulk use m_ts_options , only : val_swap use m_ts_options , only : ts_Dtol , ts_Htol use m_ts_options , only : ts_hist_keep use m_ts_options , only : ts_siesta_stop use m_ts_options , only : ts_scf_mixs use m_ts_electype integer :: iEl , na_a integer , allocatable :: allowed_a (:) real ( dp ), pointer :: DM (:,:), EDM (:,:) ! We are done with the initial diagon run ! Now we start the TRANSIESTA (Green functions) run if ( . not . TSmode ) return if ( . not . TSinit ) return ! whether we are in siesta initialization step TSinit = . false . ! whether transiesta is running TSrun = . true . ! If transiesta should stop immediately if ( ts_siesta_stop ) then if ( IONode ) then write ( * , '(a)' ) 'ts: Stopping transiesta (user option)!' end if return end if ! Reduce memory requirements call resetDenseMatrix () ! Signal to continue... ! These two variables are from the top-level ! routine (siesta_forces) SCFconverged = . false . iscf = 0 ! DANGER (when/if going back to the DIAGON run, we should ! re-instantiate the original mixing value) call val_swap ( dDtol , ts_Dtol ) call val_swap ( dHtol , ts_Htol ) ! Clean up mixing history if ( mix_charge ) then call resetRhoG (. true .) else if ( associated ( ts_scf_mixs , target = scf_mixs ) ) then do iel = 1 , size ( scf_mix % stack ) call reset ( scf_mix % stack ( iel ), - ts_hist_keep ) ! Reset iteration count as certain ! mixing schemes require this for consistency scf_mix % cur_itt = n_items ( scf_mix % stack ( iel )) end do else call mixers_history_init ( scf_mixs ) end if end if ! Transfer scf_mixing to the transiesta mixing routine scf_mix => ts_scf_mixs ( 1 ) #ifdef SIESTA__FLOOK if ( . not . mix_charge ) then call dict_variable_add ( 'SCF.Mixer.Weight' , scf_mix % w ) call dict_variable_add ( 'SCF.Mixer.Restart' , scf_mix % restart ) call dict_variable_add ( 'SCF.Mixer.Iterations' , scf_mix % n_itt ) end if #endif call ts_print_transiesta () ! In case of transiesta and DM_bulk. ! In case we ask for initialization of the DM in bulk ! we read in the DM files from the electrodes and ! initialize the bulk to those values if ( DM_bulk > 0 ) then if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Initializing bulk DM in electrodes.' end if na_a = 0 do iEl = 1 , na_u if ( . not . a_isDev ( iEl ) ) na_a = na_a + 1 end do allocate ( allowed_a ( na_a )) na_a = 0 do iEl = 1 , na_u ! We allow the buffer atoms as well (this will even out the ! potential around the back of the electrode) if ( . not . a_isDev ( iEl ) ) then na_a = na_a + 1 allowed_a ( na_a ) = iEl end if end do do iEl = 1 , N_Elec if ( IONode ) then write ( * , '(/,2a)' ) 'transiesta: ' , & 'Reading in electrode TSDE for ' // & trim ( Elecs ( iEl )% Name ) end if ! Copy over the DM in the lead ! Notice that the EDM matrix that is copied over ! will be equivalent at Ef == 0 call copy_DM ( Elecs ( iEl ), na_u , xa , lasto , nsc , isc_off , & ucell , DM_2D , EDM_2D , na_a , allowed_a ) end do ! Clean-up deallocate ( allowed_a ) if ( IONode ) then write ( * , * ) ! new-line end if ! The electrode EDM is aligned at Ef == 0 ! We need to align the energy matrix DM => val ( DM_2D ) EDM => val ( EDM_2D ) iEl = size ( DM ) call daxpy ( iEl , Ef , DM ( 1 , 1 ), 1 , EDM ( 1 , 1 ), 1 ) end if end subroutine transiesta_switch end subroutine siesta_forces","tags":"","loc":"proc/siesta_forces.html","title":"siesta_forces – SIESTA"},{"text":"public function mix_method(str) result(m) Uses fdf proc~~mix_method~~UsesGraph proc~mix_method mix_method fdf fdf proc~mix_method->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Return the integer specification of the mixing type @param[in] str the character representation of the mixing type\n @return the integer corresponding to the mixing type Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: str Return Value integer Calls proc~~mix_method~~CallsGraph proc~mix_method mix_method die die proc~mix_method->die leqi leqi proc~mix_method->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_method~~CalledByGraph proc~mix_method mix_method proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mix_method Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_method Source Code function mix_method ( str ) result ( m ) use fdf , only : leqi character ( len =* ), intent ( in ) :: str integer :: m if ( leqi ( str , 'linear' ) ) then m = MIX_LINEAR else if ( leqi ( str , 'pulay' ) . or . & leqi ( str , 'diis' ) . or . & leqi ( str , 'anderson' ) ) then m = MIX_PULAY else if ( leqi ( str , 'broyden' ) ) then m = MIX_BROYDEN else if ( leqi ( str , 'fire' ) ) then m = MIX_FIRE call die ( 'mixing: FIRE currently not supported.' ) else call die ( 'mixing: Unknown mixing variant.' ) end if end function mix_method","tags":"","loc":"proc/mix_method.html","title":"mix_method – SIESTA"},{"text":"public function mix_method_variant(m, str) result(v) Uses fdf proc~~mix_method_variant~~UsesGraph proc~mix_method_variant mix_method_variant fdf fdf proc~mix_method_variant->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Return the variant of the mixing method @param[in] m the integer type of the mixing method\n @param[in] str the character specification of the mixing method variant\n @return the variant of the mixing method Arguments Type Intent Optional Attributes Name integer, intent(in) :: m character(len=*), intent(in) :: str Return Value integer Calls proc~~mix_method_variant~~CallsGraph proc~mix_method_variant mix_method_variant leqi leqi proc~mix_method_variant->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mix_method_variant~~CalledByGraph proc~mix_method_variant mix_method_variant proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mix_method_variant Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mix_method_variant Source Code function mix_method_variant ( m , str ) result ( v ) use fdf , only : leqi integer , intent ( in ) :: m character ( len =* ), intent ( in ) :: str integer :: v v = 0 select case ( m ) case ( MIX_LINEAR ) ! no variants case ( MIX_PULAY ) v = 0 ! We do not implement tho non-stable version ! There is no need to have an inferior Pulay mixer... if ( leqi ( str , 'original' ) . or . & leqi ( str , 'kresse' ) . or . leqi ( str , 'stable' ) ) then ! stable version, will nearly always succeed on inversion v = 0 else if ( leqi ( str , 'original+svd' ) . or . & leqi ( str , 'kresse+svd' ) . or . leqi ( str , 'stable+svd' ) ) then ! stable version, will nearly always succeed on inversion v = 2 else if ( leqi ( str , 'gr' ) . or . & leqi ( str , 'guarenteed-reduction' ) . or . & leqi ( str , 'bowler-gillan' ) ) then ! Guarenteed reduction version v = 1 else if ( leqi ( str , 'gr+svd' ) . or . & leqi ( str , 'guarenteed-reduction+svd' ) . or . & leqi ( str , 'bowler-gillan+svd' ) ) then ! Guarenteed reduction version v = 3 end if case ( MIX_BROYDEN ) ! Currently only one variant v = 0 case ( MIX_FIRE ) ! no variants end select end function mix_method_variant","tags":"","loc":"proc/mix_method_variant.html","title":"mix_method_variant – SIESTA"},{"text":"private function mixing_ncoeff(mix) result(n) Function to retrieve the number of coefficients\n calculated in this iteration.\n This is so external routines can query the size\n of the arrays used. @param[in] mix the used mixer Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer Calls proc~~mixing_ncoeff~~CallsGraph proc~mixing_ncoeff mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_ncoeff~~CalledByGraph proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_ncoeff proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_coeff->proc~mixing_ncoeff interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_ncoeff Source Code function mixing_ncoeff ( mix ) result ( n ) type ( tMixer ), intent ( in ) :: mix integer :: n n = 0 select case ( mix % m ) case ( MIX_PULAY ) n = n_items ( mix % stack ( 2 )) case ( MIX_BROYDEN ) n = n_items ( mix % stack ( 2 )) end select end function mixing_ncoeff","tags":"","loc":"proc/mixing_ncoeff.html","title":"mixing_ncoeff – SIESTA"},{"text":"private function getstackval(mix, sidx, hidx) result(d1) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix integer, intent(in) :: sidx integer, intent(in), optional :: hidx Return Value real(kind=dp),\n  pointer,(:) Calls proc~~getstackval~~CallsGraph proc~getstackval getstackval get_pointer get_pointer proc~getstackval->get_pointer n_items n_items proc~getstackval->n_items val val proc~getstackval->val Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code getstackval Source Code function getstackval ( mix , sidx , hidx ) result ( d1 ) type ( tMixer ), intent ( in ) :: mix integer , intent ( in ) :: sidx integer , intent ( in ), optional :: hidx real ( dp ), pointer :: d1 (:) type ( dData1D ), pointer :: dD1 if ( present ( hidx ) ) then dD1 => get_pointer ( mix % stack ( sidx ), hidx ) else dD1 => get_pointer ( mix % stack ( sidx ), & n_items ( mix % stack ( sidx ))) end if d1 => val ( dD1 ) end function getstackval","tags":"","loc":"proc/getstackval.html","title":"getstackval – SIESTA"},{"text":"private function is_next(mix, method, next) result(bool) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in), target :: mix integer, intent(in) :: method type( tMixer ), optional pointer :: next Return Value logical Contents Source Code is_next Source Code function is_next ( mix , method , next ) result ( bool ) type ( tMixer ), intent ( in ), target :: mix integer , intent ( in ) :: method type ( tMixer ), pointer , optional :: next logical :: bool type ( tMixer ), pointer :: m bool = . false . m => mix % next do while ( associated ( m ) ) if ( m % m == MIX_LINEAR ) then m => m % next else if ( m % m == method ) then bool = . true . exit else ! Quit if it does not do anything exit end if ! this will prevent cyclic combinations if ( associated ( m , mix ) ) exit end do if ( present ( next ) ) then next => m end if end function is_next","tags":"","loc":"proc/is_next.html","title":"is_next – SIESTA"},{"text":"private function current_itt(mix) result(itt) Get current iteration count This is abstracted because the initial iteration\n and the current iteration may be uniquely defined. Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer Called by proc~~current_itt~~CalledByGraph proc~current_itt current_itt proc~mixing_init mixing_init proc~mixing_init->proc~current_itt proc~mixers_history_init mixers_history_init proc~mixers_history_init->proc~current_itt proc~mixing_finalize mixing_finalize proc~mixing_finalize->proc~current_itt proc~mixers_init mixers_init proc~mixers_init->proc~mixers_history_init proc~state_init state_init proc~state_init->proc~mixers_history_init proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init proc~mixing_1d->proc~mixing_finalize proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_scf_init->proc~mixers_init proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->proc~mixers_history_init interface~mixing mixing interface~mixing->proc~mixing_1d proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code current_itt Source Code function current_itt ( mix ) result ( itt ) type ( tMixer ), intent ( in ) :: mix integer :: itt itt = mix % cur_itt - mix % start_itt end function current_itt","tags":"","loc":"proc/current_itt.html","title":"current_itt – SIESTA"},{"text":"private function stack_check(stack, n) result(check) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: stack integer, intent(in) :: n Return Value logical Calls proc~~stack_check~~CallsGraph proc~stack_check stack_check get_pointer get_pointer proc~stack_check->get_pointer n_items n_items proc~stack_check->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~stack_check~~CalledByGraph proc~stack_check stack_check proc~push_stack_data push_stack_data proc~push_stack_data->proc~stack_check proc~push_f push_F proc~push_f->proc~stack_check proc~update_f update_F proc~update_f->proc~stack_check proc~update_f->proc~push_f Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code stack_check Source Code function stack_check ( stack , n ) result ( check ) type ( Fstack_dData1D ), intent ( inout ) :: stack integer , intent ( in ) :: n logical :: check ! Local arrays type ( dData1D ), pointer :: dD1 if ( n_items ( stack ) == 0 ) then check = . true . else ! Check that the stack stored arrays are ! of same size... dD1 => get_pointer ( stack , 1 ) check = n == size ( dD1 ) end if end function stack_check","tags":"","loc":"proc/stack_check.html","title":"stack_check – SIESTA"},{"text":"private function norm(n, x1, x2) Calculate the norm of two arrays Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: x1 (n) real(kind=dp), intent(in) :: x2 (n) Return Value real(kind=dp) Called by proc~~norm~~CalledByGraph proc~norm norm proc~mixing_init mixing_init proc~mixing_init->proc~norm proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code norm Source Code function norm ( n , x1 , x2 ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: x1 ( n ), x2 ( n ) real ( dp ) :: norm ! Currently we use an external routine integer :: i ! Calculate dot product norm = 0._dp !$OMP parallel do default(shared), private(i) & !$OMP& reduction(+:norm) do i = 1 , n norm = norm + x1 ( i ) * x2 ( i ) end do !$OMP end parallel do end function norm","tags":"","loc":"proc/norm.html","title":"norm – SIESTA"},{"text":"public subroutine mixers_init(prefix, mixers, Comm) Uses parallel fdf proc~~mixers_init~~UsesGraph proc~mixers_init mixers_init module~parallel parallel proc~mixers_init->module~parallel fdf fdf proc~mixers_init->fdf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Initialize a set of mixers by reading in fdf information.\n @param[in] prefix the fdf-label prefixes\n @param[pointer] mixers the mixers that are to be initialized\n @param[in] Comm @opt optional MPI-communicator Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), pointer :: mixers (:) integer, intent(in), optional :: Comm Calls proc~~mixers_init~~CallsGraph proc~mixers_init mixers_init fdf_block fdf_block proc~mixers_init->fdf_block fdf_bnnames fdf_bnnames proc~mixers_init->fdf_bnnames die die proc~mixers_init->die proc~mixers_history_init mixers_history_init proc~mixers_init->proc~mixers_history_init fdf_bline fdf_bline proc~mixers_init->fdf_bline proc~mixers_reset mixers_reset proc~mixers_init->proc~mixers_reset fdf_bnames fdf_bnames proc~mixers_init->fdf_bnames fdf_get fdf_get proc~mixers_init->fdf_get fdf_brewind fdf_brewind proc~mixers_init->fdf_brewind new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt delete delete proc~mixers_history_init->delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_init~~CalledByGraph proc~mixers_init mixers_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_init Source Code subroutine mixers_init ( prefix , mixers , Comm ) use parallel , only : IONode , Node use fdf ! FDF-prefix for searching keywords character ( len =* ), intent ( in ) :: prefix ! The array of mixers (has to be nullified upon entry) type ( tMixer ), pointer :: mixers (:) integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf type ( parsed_line ), pointer :: pline ! number of history steps saved integer :: n_hist , n_restart , n_save real ( dp ) :: w integer :: nm , im , im2 character ( len = 10 ) :: lp character ( len = 70 ) :: method , variant ! Default mixing options... if ( fdf_get ( 'Mixer.Debug' ,. false .) ) then debug_mix = IONode debug_msg = 'mix:' end if if ( fdf_get ( 'Mixer.Debug.MPI' ,. false .) ) then debug_mix = . true . write ( debug_msg , '(a,i0,a)' ) 'mix (' , Node , '):' end if lp = trim ( prefix ) // '.Mixer' ! ensure nullification call mixers_reset ( mixers ) ! Return immediately if the user hasn't defined ! an fdf-block for the mixing options... if ( . not . fdf_block ( trim ( lp ) // 's' , bfdf ) ) return ! update mixing weight and kick mixing weight w = fdf_get ( trim ( lp ) // '.Weight' , 0.1_dp ) ! Get history length n_hist = fdf_get ( trim ( lp ) // '.History' , 6 ) ! Restart after this number of iterations n_restart = fdf_get ( trim ( lp ) // '.Restart' , 0 ) n_save = fdf_get ( trim ( lp ) // '.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Read in the options regarding the mixing options nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 end do if ( nm == 0 ) then call die ( 'mixing: No mixing schemes selected. & &Please at least add one mixer.' ) end if ! Allocate all denoted mixers... allocate ( mixers ( nm )) mixers (:)% w = w mixers (:)% n_hist = n_hist mixers (:)% restart = n_restart mixers (:)% restart_save = n_save ! Rewind to grab names. call fdf_brewind ( bfdf ) nm = 0 do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle nm = nm + 1 mixers ( nm )% name = fdf_bnames ( pline , 1 ) end do ! Now read all mixers for this segment and their options do im = 1 , nm call read_block ( mixers ( im ) ) end do ! Create history stack and associate correct ! stack pointers call mixers_history_init ( mixers ) #ifdef MPI if ( present ( Comm ) ) then mixers (:)% Comm = Comm else mixers (:)% Comm = MPI_Comm_World end if #endif contains subroutine read_block ( m ) type ( tMixer ), intent ( inout ), target :: m character ( len = 64 ) :: opt ! create block string opt = trim ( lp ) // '.' // trim ( m % name ) if ( . not . fdf_block ( opt , bfdf ) ) then call die ( 'Block: ' // trim ( opt ) // ' does not exist!' ) end if ! Default to the pulay method... ! This enables NOT writing this in the block method = 'pulay' variant = ' ' ! read method do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'method' ) ) then method = fdf_bnames ( pline , 2 ) else if ( leqi ( opt , 'variant' ) ) then variant = fdf_bnames ( pline , 2 ) end if end do ! Retrieve the method and the variant m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! Define separate defaults which are ! not part of the default input options select case ( m % m ) case ( MIX_LINEAR ) m % n_hist = 0 end select call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'iterations' ) & . or . leqi ( opt , 'itt' ) ) then m % n_itt = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'history' ) ) then m % n_hist = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'weight' ) . or . leqi ( opt , 'w' ) ) then m % w = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'restart' ) ) then m % restart = fdf_bintegers ( pline , 1 ) else if ( leqi ( opt , 'restart.save' ) ) then m % restart_save = fdf_bintegers ( pline , 1 ) m % restart_save = max ( 0 , m % restart_save ) end if end do ! Initialize the mixer by setting the correct ! standard options and allocate space in the mixers... call mixer_init ( m ) ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) if ( leqi ( opt , 'next' ) ) then nullify ( m % next ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next ) ) then call die ( 'mixing: Could not find next mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next , target = m ) ) then call die ( 'mixing: Next *must* not be it-self. & &Please change accordingly.' ) end if else if ( leqi ( opt , 'next.conv' ) ) then nullify ( m % next_conv ) opt = fdf_bnames ( pline , 2 ) do im2 = 1 , nm if ( leqi ( opt , mixers ( im2 )% name ) ) then m % next_conv => mixers ( im2 ) exit end if end do if ( . not . associated ( m % next_conv ) ) then call die ( 'mixing: Could not find next convergence mixer. & &Ensure all mixers exist and their names.' ) end if if ( associated ( m % next_conv , target = m ) ) then call die ( 'mixing: next.conv *must* not be it-self. & &Please change accordingly.' ) end if end if end do ! Ensure that if a next have not been specified ! it will continue indefinitely. if ( . not . associated ( m % next ) ) then m % n_itt = 0 end if ! Read the options for this mixer call fdf_brewind ( bfdf ) ! read options do while ( fdf_bline ( bfdf , pline ) ) ! skip lines without associated content if ( fdf_bnnames ( pline ) == 0 ) cycle opt = fdf_bnames ( pline , 1 ) ! Do options so that a pulay option may refer to ! the actual names of the constants if ( m % m == MIX_PULAY ) then ! The linear mixing weight if ( leqi ( opt , 'weight.linear' ) & . or . leqi ( opt , 'w.linear' ) ) then m % rv ( 1 ) = fdf_breals ( pline , 1 ) else if ( leqi ( opt , 'svd.cond' ) ) then ! This is only applicable to the Pulay ! mixing scheme... m % rv ( I_SVD_COND ) = fdf_bvalues ( pline , 1 ) end if end if ! Generic options for all advanced methods... if ( leqi ( opt , 'next.p' ) ) then ! Only allow stepping to the next when ! having a next associated if ( associated ( m % next ) ) then m % rv ( I_P_NEXT ) = fdf_bvalues ( pline , 1 ) end if else if ( leqi ( opt , 'restart.p' ) ) then m % rv ( I_P_RESTART ) = fdf_bvalues ( pline , 1 ) end if end do end subroutine read_block end subroutine mixers_init","tags":"","loc":"proc/mixers_init.html","title":"mixers_init – SIESTA"},{"text":"public subroutine mixer_init(mix) Initialize a single mixer depending on the preset\n options. Useful for external correct setup. @param[inout] mix mixer to be initialized Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix Calls proc~~mixer_init~~CallsGraph proc~mixer_init mixer_init die die proc~mixer_init->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixer_init Source Code subroutine mixer_init ( mix ) type ( tMixer ), intent ( inout ) :: mix integer :: n ! Correct amount of history in the mixing. if ( 0 < mix % restart . and . & mix % restart < mix % n_hist ) then ! This is if we restart this scheme, ! then it does not make sense to have a history ! greater than the restart count mix % n_hist = mix % restart end if if ( 0 < mix % n_itt . and . & mix % n_itt < mix % n_hist ) then ! If this only runs for n_itt itterations, ! it makes no sense to have a history greater ! than this. mix % n_hist = mix % n_itt end if select case ( mix % m ) case ( MIX_LINEAR ) allocate ( mix % rv ( I_SVD_COND : 0 )) ! Kill any history settings that do not apply to the ! linear mixer. mix % restart = 0 mix % restart_save = 0 case ( MIX_PULAY ) allocate ( mix % rv ( I_SVD_COND : 1 )) mix % rv ( 1 ) = mix % w ! We allocate the double residual (n_hist-1) mix % n_hist = max ( 2 , mix % n_hist ) if ( mix % v == 1 . or . mix % v == 3 ) then ! The GR method requires an even number ! of restart steps ! And then we ensure the history to be aligned ! with a restart (restart has precedence) mix % restart = mix % restart + mod ( mix % restart , 2 ) end if case ( MIX_BROYDEN ) ! allocate temporary array mix % n_hist = max ( 2 , mix % n_hist ) n = 1 + mix % n_hist allocate ( mix % rv ( I_SVD_COND : n )) mix % rv ( 1 : n ) = mix % w end select if ( mix % restart < 0 ) then call die ( 'mixing: restart count must be positive' ) end if mix % restart_save = min ( mix % n_hist - 1 , mix % restart_save ) mix % restart_save = max ( 0 , mix % restart_save ) ! This is the restart parameter ! I.e. if |f_k / f - 1| < rp ! only works for positive rp mix % rv ( I_PREVIOUS_RES ) = huge ( 1._dp ) mix % rv ( I_P_RESTART ) = - 1._dp mix % rv ( I_P_NEXT ) = - 1._dp mix % rv ( I_SVD_COND ) = 1.e-8_dp end subroutine mixer_init","tags":"","loc":"proc/mixer_init.html","title":"mixer_init – SIESTA"},{"text":"public subroutine mixers_history_init(mixers) Initialize all history for the mixers Routine for clearing all history and setting up the\n arrays so that they may be used subsequently. @param[inout] mixers the mixers to be initialized Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout), target :: mixers (:) Calls proc~~mixers_history_init~~CallsGraph proc~mixers_history_init mixers_history_init new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt delete delete proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_history_init~~CalledByGraph proc~mixers_history_init mixers_history_init proc~state_init state_init proc~state_init->proc~mixers_history_init proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_init mixers_init proc~mixers_scf_init->proc~mixers_init proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->proc~mixers_history_init proc~mixers_init->proc~mixers_history_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_history_init Source Code subroutine mixers_history_init ( mixers ) type ( tMixer ), intent ( inout ), target :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns logical :: is_GR do im = 1 , size ( mixers ) m => mixers ( im ) if ( debug_mix . and . current_itt ( m ) >= 1 ) then write ( * , '(a,a)' ) trim ( debug_msg ), & ' resetting history of all mixers' exit end if end do ! Clean up all arrays and reference counted ! objects do im = 1 , size ( mixers ) m => mixers ( im ) ! reset history track m % start_itt = 0 m % cur_itt = 0 ! do not try and de-allocate something not ! allocated if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do ! clean-up deallocate ( m % stack ) end if ! Re-populate select case ( m % m ) case ( MIX_LINEAR ) ! do nothing case ( MIX_PULAY ) is_GR = ( m % v == 1 ) . or . ( m % v == 3 ) if ( . not . is_GR ) then allocate ( m % stack ( 3 )) else allocate ( m % stack ( 2 )) end if ! These arrays contains these informations !   s1 = m%stack(1) !   s2 = m%stack(2) !   s3 = m%stack(3) ! Here <> is input function, x[in], and ! <>' is the corresponding output, x[out]. ! First iteration: !   s1 = { 1' - 1 } !   s3 = { 1' } ! Second iteration !   s2 = { 2' - 2 - (1' - 1) } !   s1 = { 2 - 1 , 2' - 2 } !   s3 = { 2' } ! Third iteration !   s2 = { 2' - 2 - (1' - 1) , 3' - 3 - (2' - 2) } !   s1 = { 2 - 1 , 3 - 2, 3' - 3 } !   s3 = { 3' } ! and so on ! allocate x[i+1] - x[i] call new ( m % stack ( 1 ), m % n_hist ) ! allocate F[i+1] - F[i] call new ( m % stack ( 2 ), m % n_hist - 1 ) if ( . not . is_GR ) then call new ( m % stack ( 3 ), 1 ) end if case ( MIX_BROYDEN ) ! Same as original Pulay allocate ( m % stack ( 3 )) call new ( m % stack ( 1 ), m % n_hist ) call new ( m % stack ( 2 ), m % n_hist - 1 ) call new ( m % stack ( 3 ), 1 ) end select end do end subroutine mixers_history_init","tags":"","loc":"proc/mixers_history_init.html","title":"mixers_history_init – SIESTA"},{"text":"public subroutine mixers_reset(mixers) Reset the mixers, i.e. clean everything Also deallocates (and nullifies) the input array! @param[inout] mixers array of mixers to be cleaned Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mixers (:) Calls proc~~mixers_reset~~CallsGraph proc~mixers_reset mixers_reset delete delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_reset~~CalledByGraph proc~mixers_reset mixers_reset proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->proc~mixers_reset proc~mixers_init mixers_init proc~mixers_init->proc~mixers_reset proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->proc~mixers_reset proc~mixers_scf_init->proc~mixers_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_reset Source Code subroutine mixers_reset ( mixers ) type ( tMixer ), pointer :: mixers (:) type ( tMixer ), pointer :: m integer :: im , is , ns if ( . not . associated ( mixers ) ) return do im = 1 , size ( mixers ) m => mixers ( im ) if ( allocated ( m % stack ) ) then ns = size ( m % stack ) do is = 1 , ns call delete ( m % stack ( is )) end do deallocate ( m % stack ) end if if ( associated ( m % rv ) ) then deallocate ( m % rv ) nullify ( m % rv ) end if if ( associated ( m % iv ) ) then deallocate ( m % iv ) nullify ( m % iv ) end if end do deallocate ( mixers ) nullify ( mixers ) end subroutine mixers_reset","tags":"","loc":"proc/mixers_reset.html","title":"mixers_reset – SIESTA"},{"text":"public subroutine mixers_print(prefix, mixers) Uses parallel proc~~mixers_print~~UsesGraph proc~mixers_print mixers_print module~parallel parallel proc~mixers_print->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Print (to std-out) information regarding the mixers @param[in] prefix the prefix (fdf) for the mixers\n @param[in] mixers array of mixers allocated Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) Calls proc~~mixers_print~~CallsGraph proc~mixers_print mixers_print die die proc~mixers_print->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_print~~CalledByGraph proc~mixers_print mixers_print proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->proc~mixers_print Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_print Source Code subroutine mixers_print ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m character ( len = 50 ) :: fmt logical :: bool integer :: i if ( . not . IONode ) return fmt = 'mix.' // trim ( prefix ) // ':' if ( debug_mix ) then write ( * , '(2a,t50,''= '',l)' ) trim ( fmt ), & ' Debug messages' , debug_mix end if ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Linear mixing' , trim ( m % name ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w if ( m % n_hist > 0 . and . (& associated ( m % next ) & . or . associated ( m % next_conv )) ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Carried history steps' , m % n_hist end if case ( MIX_PULAY ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Pulay mixing' , trim ( m % name ) select case ( m % v ) case ( 0 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable' case ( 1 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR' case ( 2 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'stable-SVD' case ( 3 ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Variant' , 'GR-SVD' end select write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Linear mixing weight' , m % rv ( 1 ) write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Mixing weight' , m % w write ( * , '(2a,t50,''= '',e10.4)' ) trim ( fmt ), & '    SVD condition' , m % rv ( I_SVD_COND ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_BROYDEN ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Broyden mixing' , trim ( m % name ) !write(*,'(2a,t50,''= '',a)') trim(fmt), & !     '    Variant','original' write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    History steps' , m % n_hist write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Jacobian weight' , m % w write ( * , '(2a,t50,''= '',f12.6)' ) trim ( fmt ), & '    Weight prime' , m % rv ( 1 ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Step mixer parameter' , m % rv ( I_P_NEXT ) end if bool = . false . if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(2a,t50,''= '',f6.4)' ) trim ( fmt ), & '    Restart parameter' , m % rv ( I_P_RESTART ) bool = . true . end if if ( m % restart > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart steps' , m % restart bool = . true . end if if ( bool ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Restart save steps' , m % restart_save end if case ( MIX_FIRE ) write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & ' Fire mixing' , trim ( m % name ) end select if ( m % n_itt > 0 ) then write ( * , '(2a,t50,''= '',i0)' ) trim ( fmt ), & '    Number of mixing iterations' , m % n_itt if ( associated ( m % next ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer' , trim ( m % next % name ) else call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if end if if ( associated ( m % next_conv ) ) then write ( * , '(2a,t50,''= '',a)' ) trim ( fmt ), & '    Following mixer upon convergence' , trim ( m % next_conv % name ) end if end do end subroutine mixers_print","tags":"","loc":"proc/mixers_print.html","title":"mixers_print – SIESTA"},{"text":"public subroutine mixers_print_block(prefix, mixers) Uses parallel proc~~mixers_print_block~~UsesGraph proc~mixers_print_block mixers_print_block module~parallel parallel proc~mixers_print_block->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Print (to std-out) the fdf-blocks that recreate the mixer settings @param[in] prefix the fdf-prefix for reading the blocks\n @param[in] mixers array of mixers that should be printed\n    their fdf-blocks Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) Calls proc~~mixers_print_block~~CallsGraph proc~mixers_print_block mixers_print_block die die proc~mixers_print_block->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_print_block~~CalledByGraph proc~mixers_print_block mixers_print_block proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->proc~mixers_print_block Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_print_block Source Code subroutine mixers_print_block ( prefix , mixers ) use parallel , only : IONode character ( len =* ), intent ( in ) :: prefix type ( tMixer ), intent ( in ), target :: mixers (:) type ( tMixer ), pointer :: m logical :: bool integer :: i if ( . not . IONode ) return ! Write block of input write ( * , '(/3a)' ) '%block ' , trim ( prefix ), '.Mixers' do i = 1 , size ( mixers ) m => mixers ( i ) write ( * , '(t3,a)' ) trim ( m % name ) end do write ( * , '(3a)' ) '%endblock ' , trim ( prefix ), '.Mixers' ! Print out options for all mixers do i = 1 , size ( mixers ) m => mixers ( i ) ! Write out this block write ( * , '(/4a)' ) '%block ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) write ( * , '(t3,a)' ) '# Mixing method' ! Write out method select case ( m % m ) case ( MIX_LINEAR ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'linear' case ( MIX_PULAY ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'pulay' select case ( m % v ) case ( 0 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable' case ( 1 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR' case ( 2 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'stable+SVD' case ( 3 ) write ( * , '(t2,2(tr1,a))' ) 'variant' , 'GR+SVD' end select case ( MIX_BROYDEN ) write ( * , '(t2,2(tr1,a))' ) 'method' , 'broyden' ! currently no variants exists end select ! remark write ( * , '(/,t3,a)' ) '# Mixing options' ! Weight ! For Broyden this is the inverse Jacobian write ( * , '(t3,a,f6.4)' ) 'weight ' , m % w select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) write ( * , '(t3,a,f6.4)' ) 'weight.linear ' , m % rv ( 1 ) end select if ( m % n_hist > 0 ) then write ( * , '(t3,a,i0)' ) 'history ' , m % n_hist end if bool = . false . if ( m % restart > 0 ) then write ( * , '(t3,a,i0)' ) 'restart ' , m % restart bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_RESTART ) > 0._dp ) then write ( * , '(t3,a,e10.5)' ) 'restart.p ' , m % rv ( I_P_RESTART ) bool = . true . end if end select if ( bool ) then write ( * , '(t3,a,i0)' ) 'restart.save ' , m % restart_save end if ! remark bool = . false . if ( m % n_itt > 0 ) then write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,i0)' ) 'iterations ' , m % n_itt bool = . true . end if select case ( m % m ) case ( MIX_PULAY , MIX_BROYDEN ) if ( m % rv ( I_P_NEXT ) > 0._dp ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t3,a,f6.4)' ) 'next.p ' , m % rv ( I_P_NEXT ) bool = . true . end if end select if ( bool . and . associated ( m % next ) ) then write ( * , '(t2,2(tr1,a))' ) 'next' , trim ( m % next % name ) else if ( bool ) then call die ( 'Something went wrong, if the mixer does not go & &indefinitely it should have a following method.' ) end if if ( associated ( m % next_conv ) ) then if ( . not . bool ) & write ( * , '(/,t3,a)' ) '# Continuation options' write ( * , '(t2,2(tr1,a))' ) 'next.conv' , trim ( m % next_conv % name ) end if write ( * , '(4a)' ) '%endblock ' , trim ( prefix ), '.Mixer.' , trim ( m % name ) end do write ( * , * ) ! new-line end subroutine mixers_print_block","tags":"","loc":"proc/mixers_print_block.html","title":"mixers_print_block – SIESTA"},{"text":"private subroutine mixing_init(mix, n, xin, F) Initialize the mixing algorithm @param[pointer] mix the mixing method\n @param[in] n size of the arrays to be used in the algorithm\n @param[in] xin array of the input variables\n @param[in] xout array of the output variables Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) Calls proc~~mixing_init~~CallsGraph proc~mixing_init mixing_init proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_init~~CalledByGraph proc~mixing_init mixing_init proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_init interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_init Source Code subroutine mixing_init ( mix , n , xin , F ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n ! In/out of the function real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), pointer :: res (:), rres (:) integer :: i , ns real ( dp ) :: dnorm , dtmp logical :: p_next , p_restart ! Initialize action for mixer mix % action = ACTION_MIX ! Step iterator (so first mixing has cur_itt == 1) mix % cur_itt = mix % cur_itt + 1 ! If we are going to skip to next, we signal it ! before entering if ( mix % n_itt > 0 . and . & mix % n_itt <= current_itt ( mix ) ) then mix % action = IOR ( mix % action , ACTION_NEXT ) end if ! Check whether the residual norm is below a certain ! criteria p_next = mix % rv ( I_P_NEXT ) > 0._dp p_restart = mix % rv ( I_P_RESTART ) > 0._dp ! Check whether a parameter next/restart is required if ( p_restart . or . p_next ) then ! Calculate norm: ||f_k|| dnorm = norm ( n , F , F ) #ifdef MPI dtmp = dnorm call MPI_AllReduce ( dtmp , dnorm , 1 , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) #endif ! Calculate the relative difference dtmp = abs ( dnorm / mix % rv ( I_PREVIOUS_RES ) - 1._dp ) ! We first check for next, that has precedence if ( p_next ) then if ( dtmp < mix % rv ( I_P_NEXT ) ) then ! Signal stepping mixer mix % action = IOR ( mix % action , ACTION_NEXT ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < np  :  ' , & dtmp , ' < ' , mix % rv ( I_P_NEXT ) end if if ( p_restart ) then if ( dtmp < mix % rv ( I_P_RESTART ) ) then ! Signal restart mix % action = IOR ( mix % action , ACTION_RESTART ) end if if ( debug_mix . and . current_itt ( mix ) > 1 ) & write ( * , '(a,2(a,e8.3))' ) trim ( debug_msg ), & ' | ||f_k|| - ||f_k-1|| |/||f_k-1|| < rp  :  ' , & dtmp , ' < ' , mix % rv ( I_P_RESTART ) end if ! Store the new residual norm mix % rv ( I_PREVIOUS_RES ) = dnorm end if ! Push information to the stack select case ( mix % m ) case ( MIX_LINEAR ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' linear' call init_linear () case ( MIX_PULAY ) if ( debug_mix ) then select case ( mix % v ) case ( 0 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay' case ( 1 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay, GR' case ( 2 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD' case ( 3 ) write ( * , '(2a)' ) trim ( debug_msg ), ' Pulay-SVD, GR' end select end if call init_pulay () case ( MIX_BROYDEN ) if ( debug_mix ) & write ( * , '(2a)' ) trim ( debug_msg ), ' Broyden' call init_broyden () end select contains subroutine init_linear () ! information for this depends on the ! following method call fake_history_from_linear ( mix % next ) call fake_history_from_linear ( mix % next_conv ) end subroutine init_linear subroutine init_pulay () logical :: GR_linear select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do end if case ( 1 , 3 ) ! Whether this is the linear cycle... GR_linear = mod ( current_itt ( mix ), 2 ) == 1 ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F , mix % rv ( 1 )) ns = n_items ( mix % stack ( 1 )) if ( GR_linear . and . current_itt ( mix ) > 1 . and . & ns > 1 ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = rres ( i ) + res ( i ) end do !$OMP end parallel do else if ( ns > 1 . and . . not . GR_linear ) then ! now we can calculate RRes[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) end if end select end subroutine init_pulay subroutine init_broyden () ! Add the residual to the stack call push_F ( mix % stack ( 1 ), n , F ) ns = n_items ( mix % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( mix % stack ( 2 ), mix % stack ( 1 )) ! Update the residual to reflect the input residual res => getstackval ( mix , 1 , ns - 1 ) rres => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - rres ( i ) + xin ( i ) end do !$OMP end parallel do else ! Store F[x_in] (used to create the input residual) call push_stack_data ( mix % stack ( 3 ), n ) res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if end subroutine init_broyden subroutine fake_history_from_linear ( next ) type ( tMixer ), pointer :: next real ( dp ), pointer :: t1 (:), t2 (:) integer :: ns , nh , i , nhl if ( . not . associated ( next ) ) return ! Reduce to # history of linear nhl = mix % n_hist ! if the number of fake-history steps saved is ! zero we immediately return. ! Only if mix%n_hist > 0 will the below ! occur. if ( nhl == 0 ) return ! Check for the type of following method select case ( next % m ) case ( MIX_PULAY ) ! Here it depends on the variant select case ( next % v ) case ( 0 , 2 ) ! stable pulay mixing ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns > 1 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select case ( MIX_BROYDEN ) ! Add the residual to the stack call push_F ( next % stack ( 1 ), n , F ) ns = n_items ( next % stack ( 1 )) ! Add the residuals of the residuals if applicable if ( ns >= 2 ) then ! Create F[i+1] - F[i] call push_diff ( next % stack ( 2 ), next % stack ( 1 )) ! Update the residual to reflect the input residual t1 => getstackval ( next , 1 , ns - 1 ) t2 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = t1 ( i ) - t2 ( i ) + xin ( i ) t2 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do else call push_stack_data ( next % stack ( 3 ), n ) t1 => getstackval ( next , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n t1 ( i ) = xin ( i ) + F ( i ) end do !$OMP end parallel do end if ! Clean up the data... if ( ns >= nhl ) then call reset ( next % stack ( 1 ), 1 ) call reset ( next % stack ( 2 ), 1 ) ns = ns - 1 end if nh = max_size ( next % stack ( 1 )) if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' next%n_hist = ' , ns , ' / ' , nh end select end subroutine fake_history_from_linear end subroutine mixing_init","tags":"","loc":"proc/mixing_init.html","title":"mixing_init – SIESTA"},{"text":"private subroutine mixing_coeff(mix, n, xin, F, coeff) Uses parallel proc~~mixing_coeff~~UsesGraph proc~mixing_coeff mixing_coeff module~parallel parallel proc~mixing_coeff->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Calculate the mixing coefficients for the\n current mixer @param[in] mix the current mixer\n @param[in] n the number of elements used to calculate\n           the coefficients\n @param[in] xin the input value\n @param[in] F xout - xin, (residual)\n @param[out] coeff the coefficients Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: coeff (:) Calls proc~~mixing_coeff~~CallsGraph proc~mixing_coeff mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_coeff~~CalledByGraph proc~mixing_coeff mixing_coeff proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_coeff interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_coeff Source Code subroutine mixing_coeff ( mix , n , xin , F , coeff ) use parallel , only : IONode type ( tMixer ), intent ( inout ) :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ) real ( dp ), intent ( out ) :: coeff (:) integer :: ncoeff ncoeff = size ( coeff ) if ( ncoeff < mixing_ncoeff ( mix ) ) then write ( * , '(a)' ) 'mix: Error in calculating coefficients' ! Do not allow this... return end if select case ( mix % m ) case ( MIX_LINEAR ) call linear_coeff () case ( MIX_PULAY ) call pulay_coeff () case ( MIX_BROYDEN ) call broyden_coeff () end select contains subroutine linear_coeff () integer :: i do i = 1 , ncoeff coeff ( i ) = 0._dp end do end subroutine linear_coeff subroutine pulay_coeff () integer :: ns , nh , nmax integer :: i , j , info logical :: lreturn ! Calculation quantities real ( dp ) :: dnorm , G real ( dp ), pointer :: res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) lreturn = . false . ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) return ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! Allocate arrays for calculating the ! coefficients allocate ( A ( nh , nh ), Ainv ( nh , nh )) ! Calculate A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = norm(RRes[i],RRes[j]) A ( i , j ) = norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Diagonal A ( i , i ) = norm ( n , rres1 , rres1 ) end do #ifdef MPI ! Global operations, but only for the non-extended entries call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) ! copy over reduced arrays A = Ainv #endif ! Get inverse of matrix select case ( mix % v ) case ( 0 , 1 ) call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if case ( 2 , 3 ) ! We forcefully use the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end select ! NOTE, although mix%stack(1) contains ! the x[i] - x[i-1], the tip of the stack ! contains F[i]! ! res == F[i] res => getstackval ( mix , 1 ) ! Initialize the coefficients do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients on all processors do j = 1 , nh ! res  == F[i] ! rres == F[j+1] - F[j] rres => getstackval ( mix , 2 , j ) dnorm = norm ( n , rres , res ) do i = 1 , nh coeff ( i ) = coeff ( i ) - Ainv ( i , j ) * dnorm end do end do #ifdef MPI ! Reduce the coefficients call MPI_AllReduce ( coeff ( 1 ), A ( 1 , 1 ), nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh coeff ( i ) = A ( i , 1 ) end do #endif else info = 0 ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Pulay -- inversion failed, SVD failed, > linear' end if ! Clean up memory deallocate ( A , Ainv ) end subroutine pulay_coeff subroutine broyden_coeff () integer :: ns , nh , nmax integer :: i , j , k , info ! Calculation quantities real ( dp ) :: dnorm , dtmp real ( dp ), pointer :: w (:), res (:), rres (:), rres1 (:), rres2 (:) real ( dp ), allocatable :: c (:), A (:,:), Ainv (:,:) ns = n_items ( mix % stack ( 1 )) nmax = max_size ( mix % stack ( 1 )) nh = n_items ( mix % stack ( 2 )) ! Easy check for initial step... if ( ns == 1 ) then ! reset coeff = 0._dp return end if ! Print out number of currently used history steps if ( debug_mix ) & write ( * , '(a,2(a,i0))' ) trim ( debug_msg ), & ' n_hist = ' , ns , ' / ' , nmax ! This is the modified Broyden algorithm... ! Retrieve the previous weights w => mix % rv ( 2 : 1 + nh ) select case ( mix % v ) case ( 2 ) ! Unity Broyden w ( nh ) = 1._dp case ( 1 ) ! RMS Broyden dnorm = norm ( n , F , F ) #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif w (:) = 1._dp / sqrt ( dnorm ) if ( debug_mix ) & write ( * , '(2(a,e10.4))' ) & trim ( debug_msg ) // ' weight = ' , w ( 1 ), & ' , norm = ' , dnorm case ( 0 ) ! Varying weight dnorm = 0._dp !$OMP parallel do default(shared), private(i), & !$OMP& reduction(max:dnorm) do i = 1 , n dnorm = max ( dnorm , abs ( F ( i )) ) end do !$OMP end parallel do #ifdef MPI call MPI_AllReduce ( dnorm , dtmp , 1 , & MPI_Double_Precision , MPI_Max , & mix % Comm , i ) dnorm = dtmp #endif ! Problay 0.2 should be changed to user-defined w ( nh ) = exp ( 1._dp / ( dnorm + 0.2_dp ) ) if ( debug_mix ) & write ( * , '(2a,1000(tr1,e10.4))' ) & trim ( debug_msg ), ' weights = ' , w ( 1 : nh ) end select ! Allocate arrays used allocate ( c ( nh )) allocate ( A ( nh , nh ), Ainv ( nh , nh )) !  < RRes[i] | Res[n] > do i = 1 , nh rres => getstackval ( mix , 2 , i ) c ( i ) = norm ( n , rres , F ) end do #ifdef MPI call MPI_AllReduce ( c ( 1 ), A ( 1 , 1 ), nh , & MPI_Double_Precision , MPI_Sum , & mix % Comm , i ) do i = 1 , nh c ( i ) = A ( i , 1 ) end do #endif ! Create A_ij coefficients for inversion do i = 1 , nh ! Get RRes[i] array rres1 => getstackval ( mix , 2 , i ) do j = 1 , i - 1 ! Get RRes[j] array rres2 => getstackval ( mix , 2 , j ) ! A(i,j) = A(j,i) = dot_product(RRes[i],RRes[j]) A ( i , j ) = w ( i ) * w ( j ) * norm ( n , rres1 , rres2 ) A ( j , i ) = A ( i , j ) end do ! Do the diagonal term A ( i , i ) = w ( i ) * w ( i ) * norm ( n , rres1 , rres1 ) end do #ifdef MPI call MPI_AllReduce ( A ( 1 , 1 ), Ainv ( 1 , 1 ), nh * nh , & MPI_double_precision , MPI_Sum , & mix % Comm , i ) A = Ainv #endif ! Add the diagonal term ! This should also prevent it from being ! singular (unless mix%w == 0) do i = 1 , nh A ( i , i ) = mix % rv ( 1 ) ** 2 + A ( i , i ) end do ! Calculate the inverse call inverse ( nh , A , Ainv , info ) if ( info /= 0 ) then ! only inform if we should not use SVD per default if ( IONode ) & write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, > SVD' ! We will first try the SVD routine call svd ( nh , A , Ainv , mix % rv ( I_SVD_COND ), info ) end if do i = 1 , nh coeff ( i ) = 0._dp end do if ( info == 0 ) then ! Calculate the coefficients... do i = 1 , nh do j = 1 , nh ! Ainv should be symmetric (A is) coeff ( i ) = coeff ( i ) + w ( j ) * c ( j ) * Ainv ( j , i ) end do ! Calculate correct weight... coeff ( i ) = - w ( i ) * coeff ( i ) end do else ! reset to linear mixing write ( * , '(2a)' ) trim ( debug_msg ), & ' Broyden -- inversion failed, SVD failed, > linear' end if deallocate ( A , Ainv ) end subroutine broyden_coeff end subroutine mixing_coeff","tags":"","loc":"proc/mixing_coeff.html","title":"mixing_coeff – SIESTA"},{"text":"private subroutine mixing_calc_next(mix, n, xin, F, xnext, coeff) Calculate the guess for the next iteration Note this gets passed the coefficients. Hence,\n they may be calculated from another set of history\n steps.\n This may be useful in certain situations. @param[in] mix the current mixer\n @param[in] n the number of elements used to calculate\n           the coefficients\n @param[in] xin the input value\n @param[in] F the xin residual\n @param[out] xnext the input for the following iteration\n @param[in] coeff the coefficients Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: xnext (n) real(kind=dp), intent(in) :: coeff (:) Called by proc~~mixing_calc_next~~CalledByGraph proc~mixing_calc_next mixing_calc_next proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_calc_next interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_calc_next Source Code subroutine mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! The current mixing method type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ) real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( out ) :: xnext ( n ) real ( dp ), intent ( in ) :: coeff (:) select case ( mix % m ) case ( MIX_LINEAR ) call mixing_linear () case ( MIX_PULAY ) call mixing_pulay () case ( MIX_BROYDEN ) call mixing_broyden () end select contains subroutine mixing_linear () integer :: i real ( dp ) :: w w = mix % w if ( debug_mix ) write ( * , '(2a,e10.4)' ) & trim ( debug_msg ), ' alpha = ' , w !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + w * F ( i ) end do !$OMP end parallel do end subroutine mixing_linear subroutine mixing_pulay () integer :: ns , nh integer :: i , j logical :: lreturn real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Pulay' xnext = 0._dp return end if ! Easy check for initial step... select case ( mix % v ) case ( 0 , 2 ) ! Stable Pulay lreturn = ns == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Pulay (initial), alpha = ' , mix % rv ( 1 ) case ( 1 , 3 ) ! Guaranteed Pulay lreturn = mod ( current_itt ( mix ), 2 ) == 1 if ( lreturn . and . debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Direct mixing, alpha = ' , mix % rv ( 1 ) end select ! In case we return we are actually doing ! linear mixing if ( lreturn ) then ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ),& ' G = ' , G , ', sum(alpha) = ' , sum ( coeff ), & ', alpha = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_pulay subroutine mixing_broyden () integer :: ns , nh integer :: i , j real ( dp ) :: G real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) nh = size ( coeff ) if ( nh /= n_items ( mix % stack ( 2 )) ) then write ( * , '(a)' ) 'mix: Error in mixing of Broyden' xnext = 0._dp return end if if ( ns == 1 ) then if ( debug_mix ) & write ( * , '(2a,e10.4)' ) trim ( debug_msg ), & ' Broyden (initial), alpha = ' , mix % rv ( 1 ) ! We are doing a linear mixing !$OMP parallel do default(shared), private(i) do i = 1 , n xnext ( i ) = xin ( i ) + F ( i ) * mix % rv ( 1 ) end do !$OMP end parallel do return end if ! Get the linear mixing term... G = mix % w ! if debugging print out the different variables if ( debug_mix ) then write ( * , '(2a,f10.6,a,e10.4,a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' G = ' , G , & ', sum(coeff) = ' , sum ( coeff ), & ', coeff = ' , coeff end if !$OMP parallel default(shared), private(i, j, res, rres) !  x&#94;opt[i+1] = x[i] + G F[i] !$OMP do do i = 1 , n xnext ( i ) = xin ( i ) + G * F ( i ) end do !$OMP end do do j = 1 , nh !  res == x[j] - x[j-1] ! rres == F[j+1] - F[j] res => getstackval ( mix , 1 , j ) rres => getstackval ( mix , 2 , j ) !  x&#94;opt[i+1] = x&#94;opt[i+1] + !       alpha_j ( x[j] - x[j-1] + G (F[j+1] - F[j]) ) !$OMP do do i = 1 , n xnext ( i ) = xnext ( i ) + coeff ( j ) * ( res ( i ) + G * rres ( i ) ) end do !$OMP end do end do !$OMP end parallel end subroutine mixing_broyden end subroutine mixing_calc_next","tags":"","loc":"proc/mixing_calc_next.html","title":"mixing_calc_next – SIESTA"},{"text":"private subroutine mixing_finalize(mix, n, xin, F, xnext) Uses parallel proc~~mixing_finalize~~UsesGraph proc~mixing_finalize mixing_finalize module~parallel parallel proc~mixing_finalize->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finalize the mixing algorithm @param[inout] mix mixer to be finalized\n @param[in] n size of the input arrays\n @param[in] xin the input for this iteration\n @param[in] F the residual for this iteration\n @param[in] xnext the optimized input for the next iteration Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in) :: xnext (n) Calls proc~~mixing_finalize~~CallsGraph proc~mixing_finalize mixing_finalize reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step n_items n_items proc~mixing_finalize->n_items proc~current_itt current_itt proc~mixing_finalize->proc~current_itt proc~mixing_step->reset proc~mixing_step->n_items push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_finalize~~CalledByGraph proc~mixing_finalize mixing_finalize proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_finalize Source Code subroutine mixing_finalize ( mix , n , xin , F , xnext ) use parallel , only : IONode type ( tMixer ), pointer :: mix integer , intent ( in ) :: n real ( dp ), intent ( in ) :: xin ( n ), F ( n ), xnext ( n ) integer :: rsave select case ( mix % m ) case ( MIX_LINEAR ) call fin_linear () case ( MIX_PULAY ) call fin_pulay () case ( MIX_BROYDEN ) call fin_broyden () end select ! Fix the action to finalize it.. if ( mix % restart > 0 . and . & mod ( current_itt ( mix ), mix % restart ) == 0 ) then mix % action = IOR ( mix % action , ACTION_RESTART ) end if ! Check the actual finalization... ! First check whether we should restart history if ( IAND ( mix % action , ACTION_RESTART ) == ACTION_RESTART ) then ! The user has requested to restart the ! mixing scheme now rsave = mix % restart_save select case ( mix % m ) case ( MIX_PULAY ) if ( IONode ) then write ( * , '(a)' ) 'mix: Pulay -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if case ( MIX_BROYDEN ) if ( IONode ) then write ( * , '(a)' ) 'mix: Broyden -- resetting history' end if if ( rsave == 0 ) then call reset ( mix % stack ( 1 )) call reset ( mix % stack ( 2 )) call reset ( mix % stack ( 3 )) else call reset ( mix % stack ( 1 ), - rsave ) call reset ( mix % stack ( 2 ), - rsave + 1 ) end if end select if ( allocated ( mix % stack ) ) then if ( debug_mix ) & write ( * , '(a,a,i0)' ) trim ( debug_msg ), & ' saved hist = ' , n_items ( mix % stack ( 1 )) end if end if ! check whether we should change the mixer if ( IAND ( mix % action , ACTION_NEXT ) == ACTION_NEXT ) then call mixing_step ( mix ) end if contains subroutine fin_linear () ! do nothing... end subroutine fin_linear subroutine fin_pulay () integer :: ns integer :: i logical :: GR_linear real ( dp ), pointer :: res (:), rres (:) ns = n_items ( mix % stack ( 1 )) select case ( mix % v ) case ( 0 , 2 ) ! stable Pulay if ( n_items ( mix % stack ( 3 )) == 0 ) then call push_stack_data ( mix % stack ( 3 ), n ) end if res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do case ( 1 , 3 ) ! GR Pulay GR_linear = mod ( current_itt ( mix ), 2 ) == 1 if ( n_items ( mix % stack ( 2 )) > 0 . and . & . not . GR_linear ) then res => getstackval ( mix , 1 ) rres => getstackval ( mix , 2 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  rres == F[i] - F[i-1] rres ( i ) = rres ( i ) - res ( i ) ! Output: !  rres == - F[i-1] end do !$OMP end parallel do call pop ( mix % stack ( 1 )) ! Note that this is Res[i-1] = (F&#94;i-1_out - F&#94;i-1_in) res => getstackval ( mix , 1 ) !$OMP parallel do default(shared), private(i) do i = 1 , n res ( i ) = res ( i ) - xin ( i ) + xnext ( i ) end do !$OMP end parallel do end if end select end subroutine fin_pulay subroutine fin_broyden () integer :: ns , nh integer :: i real ( dp ), pointer :: res (:), rres (:) ns = current_itt ( mix ) nh = n_items ( mix % stack ( 2 )) if ( ns >= 2 . and . n_items ( mix % stack ( 3 )) > 0 ) then ! Update the residual to reflect the input residual res => getstackval ( mix , 3 ) !$OMP parallel do default(shared), private(i) do i = 1 , n ! Input: !  res == x[i-1] + F[i-1] res ( i ) = xin ( i ) + F ( i ) ! Output: !  res == x[i] + F[i] end do !$OMP end parallel do end if ! Update weights (if necessary) if ( nh > 1 ) then do i = 2 , nh mix % rv ( i ) = mix % rv ( i + 1 ) end do end if end subroutine fin_broyden end subroutine mixing_finalize","tags":"","loc":"proc/mixing_finalize.html","title":"mixing_finalize – SIESTA"},{"text":"private subroutine mixing_1d(mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub Calls proc~~mixing_1d~~CallsGraph proc~mixing_1d mixing_1d proc~mixing_calc_next mixing_calc_next proc~mixing_1d->proc~mixing_calc_next proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d->proc~mixing_ncoeff proc~mixing_init mixing_init proc~mixing_1d->proc~mixing_init proc~mixing_finalize mixing_finalize proc~mixing_1d->proc~mixing_finalize proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step proc~mixing_finalize->n_items proc~mixing_finalize->proc~current_itt proc~mixing_step->reset proc~mixing_step->n_items push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_1d~~CalledByGraph proc~mixing_1d mixing_1d interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_1d Source Code subroutine mixing_1d ( mix , n , xin , F , xnext , nsub ) ! The current mixing method type ( tMixer ), pointer :: mix ! The current step in the SCF and size of arrays integer , intent ( in ) :: n ! x1 == Input function, ! F1 == Residual from x1 real ( dp ), intent ( in ) :: xin ( n ), F ( n ) ! x2 == Next input function real ( dp ), intent ( inout ) :: xnext ( n ) ! Number of elements used for calculating the mixing ! coefficients integer , intent ( in ), optional :: nsub ! Coefficients integer :: ncoeff real ( dp ), allocatable :: coeff (:) call mixing_init ( mix , n , xin , F ) ncoeff = mixing_ncoeff ( mix ) allocate ( coeff ( ncoeff )) ! Calculate coefficients if ( present ( nsub ) ) then call mixing_coeff ( mix , nsub , xin , F , coeff ) else call mixing_coeff ( mix , n , xin , F , coeff ) end if ! Calculate the following output call mixing_calc_next ( mix , n , xin , F , xnext , coeff ) ! Coefficients are not needed anymore... deallocate ( coeff ) ! Finalize the mixer call mixing_finalize ( mix , n , xin , F , xnext ) end subroutine mixing_1d","tags":"","loc":"proc/mixing_1d.html","title":"mixing_1d – SIESTA"},{"text":"private subroutine mixing_2d(mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub Called by proc~~mixing_2d~~CalledByGraph proc~mixing_2d mixing_2d interface~mixing mixing interface~mixing->proc~mixing_2d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_2d Source Code subroutine mixing_2d ( mix , n1 , n2 , xin , F , xnext , nsub ) type ( tMixer ), pointer :: mix integer , intent ( in ) :: n1 , n2 real ( dp ), intent ( in ) :: xin ( n1 , n2 ), F ( n1 , n2 ) real ( dp ), intent ( inout ) :: xnext ( n1 , n2 ) integer , intent ( in ), optional :: nsub ! Simple wrapper for 1D if ( present ( nsub ) ) then call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 ) ,& nsub = n1 * nsub ) else call mixing_1d ( mix , n1 * n2 , xin ( 1 , 1 ), F ( 1 , 1 ), xnext ( 1 , 1 )) end if end subroutine mixing_2d","tags":"","loc":"proc/mixing_2d.html","title":"mixing_2d – SIESTA"},{"text":"private subroutine mixing_step(mix) Uses parallel proc~~mixing_step~~UsesGraph proc~mixing_step mixing_step module~parallel parallel proc~mixing_step->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix Calls proc~~mixing_step~~CallsGraph proc~mixing_step mixing_step push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer n_items n_items proc~mixing_step->n_items reset reset proc~mixing_step->reset Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_step~~CalledByGraph proc~mixing_step mixing_step proc~mixing_finalize mixing_finalize proc~mixing_finalize->proc~mixing_step proc~mixing_1d mixing_1d proc~mixing_1d->proc~mixing_finalize interface~mixing mixing interface~mixing->proc~mixing_1d Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_step Source Code subroutine mixing_step ( mix ) use parallel , only : IONode type ( tMixer ), pointer :: mix type ( tMixer ), pointer :: next => null () type ( dData1D ), pointer :: d1D integer :: i , is , n , init_itt logical :: reset_stack , copy_stack ! First try and next => mix % next if ( associated ( next ) ) then ! Whether or not the two methods are allowed ! to share history copy_stack = mix % m == next % m select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) copy_stack = . true . end select end select copy_stack = copy_stack . and . allocated ( mix % stack ) ! If the two methods are similar if ( copy_stack ) then ! They are similar, copy over the history stack do is = 1 , size ( mix % stack ) ! Get maximum size of the current stack, n = n_items ( mix % stack ( is )) ! Note that this will automatically take care of ! wrap-arounds and delete the unneccesry elements do i = 1 , n d1D => get_pointer ( mix % stack ( is ), i ) call push ( next % stack ( is ), d1D ) end do ! nullify nullify ( d1D ) end do end if end if reset_stack = . true . if ( associated ( next ) ) then if ( associated ( next % next , mix ) . and . & next % n_itt > 0 ) then ! if this is a circular mixing routine ! we should not reset the history... reset_stack = . false . end if end if if ( reset_stack ) then select case ( mix % m ) case ( MIX_PULAY , MIX_BROYDEN ) n = size ( mix % stack ) do is = 1 , n call reset ( mix % stack ( is )) end do end select end if if ( associated ( next ) ) then init_itt = 0 ! Set-up the next mixer select case ( next % m ) case ( MIX_PULAY , MIX_BROYDEN ) init_itt = n_items ( next % stack ( 1 )) end select next % start_itt = init_itt next % cur_itt = init_itt if ( IONode ) then write ( * , '(3a)' ) trim ( debug_msg ), ' switching mixer --> ' , & trim ( next % name ) end if mix => mix % next end if end subroutine mixing_step","tags":"","loc":"proc/mixing_step.html","title":"mixing_step – SIESTA"},{"text":"private subroutine inverse(n, A, B, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) integer, intent(out) :: info Contents Source Code inverse Source Code subroutine inverse ( n , A , B , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) integer , intent ( out ) :: info integer :: i , j ! Local arrays real ( dp ) :: pm ( n , n ), work ( n * 4 ), err ! Relative tolerance dependent on the magnitude ! For now we retain the old tolerance real ( dp ), parameter :: etol = 1.e-4_dp integer :: ipiv ( n ) ! initialize info info = 0 ! simple check and fast return if ( n == 1 ) then B ( 1 , 1 ) = 1._dp / A ( 1 , 1 ) return end if call lapack_inv () if ( info /= 0 ) call simple_inv () contains subroutine lapack_inv () B = A call dgetrf ( n , n , B , n , ipiv , info ) if ( info /= 0 ) return call dgetri ( n , B , n , ipiv , work , n * 4 , info ) if ( info /= 0 ) return ! This sets info appropriately call check_inv () end subroutine lapack_inv subroutine simple_inv () real ( dp ) :: x integer :: k ! Copy over A B = A do i = 1 , n if ( B ( i , i ) == 0._dp ) then info = - n return end if x = 1._dp / B ( i , i ) B ( i , i ) = 1._dp do j = 1 , n B ( j , i ) = B ( j , i ) * x end do do k = 1 , n if ( ( k - i ) /= 0 ) then x = B ( i , k ) B ( i , k ) = 0._dp do j = 1 , n B ( j , k ) = B ( j , k ) - B ( j , i ) * x end do end if end do end do ! This sets info appropriately call check_inv () end subroutine simple_inv subroutine check_inv () ! Check correcteness pm = matmul ( A , B ) do j = 1 , n do i = 1 , n if ( i == j ) then err = pm ( i , j ) - 1._dp else err = pm ( i , j ) end if ! This is pretty strict tolerance! if ( abs ( err ) > etol ) then ! Signal failure in inversion info = - n - 1 return end if end do end do end subroutine check_inv end subroutine inverse","tags":"","loc":"proc/inverse.html","title":"inverse – SIESTA"},{"text":"private subroutine svd(n, A, B, cond, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) real(kind=dp), intent(in) :: cond integer, intent(out) :: info Calls proc~~svd~~CallsGraph proc~svd svd dgelss dgelss proc~svd->dgelss Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code svd Source Code subroutine svd ( n , A , B , cond , info ) integer , intent ( in ) :: n real ( dp ), intent ( in ) :: A ( n , n ) real ( dp ), intent ( out ) :: B ( n , n ) real ( dp ), intent ( in ) :: cond integer , intent ( out ) :: info ! Local arrays integer :: rank , i character ( len = 50 ) :: fmt real ( dp ) :: AA ( n , n ), S ( n ), work ( n * 5 ) ! Copy A matrix AA = A ! setup pseudo inverse solution for minimizing ! constraints B = 0._dp do i = 1 , n B ( i , i ) = 1._dp end do call dgelss ( n , n , n , AA , n , B , n , S , cond , rank , work , n * 5 , info ) ! if debugging print out the different variables if ( debug_mix ) then ! also mark the rank if ( rank == n ) then ! complete rank write ( * , '(2a,100(tr1,e10.4))' ) & trim ( debug_msg ), ' SVD singular = ' , S else ! this prints the location of the SVD rank, if not full write ( fmt , '(i0,2a)' ) rank , '(tr1,e10.4),'' >'',100(tr1,e10.4)' write ( * , '(2a,' // trim ( fmt ) // ')' ) & trim ( debug_msg ), ' SVD singular = ' , S end if end if end subroutine svd","tags":"","loc":"proc/svd.html","title":"svd – SIESTA"},{"text":"private subroutine push_stack_data(s_F, n) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n Calls proc~~push_stack_data~~CallsGraph proc~push_stack_data push_stack_data push push proc~push_stack_data->push die die proc~push_stack_data->die newddata1d newddata1d proc~push_stack_data->newddata1d proc~stack_check stack_check proc~push_stack_data->proc~stack_check delete delete proc~push_stack_data->delete get_pointer get_pointer proc~stack_check->get_pointer n_items n_items proc~stack_check->n_items Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_stack_data Source Code subroutine push_stack_data ( s_F , n ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n type ( dData1D ) :: dD1 if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if call newdData1D ( dD1 , n , '(F)' ) ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_stack_data","tags":"","loc":"proc/push_stack_data.html","title":"push_stack_data – SIESTA"},{"text":"private subroutine push_F(s_F, n, F, fact) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in), optional :: fact Calls proc~~push_f~~CallsGraph proc~push_f push_F proc~stack_check stack_check proc~push_f->proc~stack_check val val proc~push_f->val get get proc~push_f->get die die proc~push_f->die newddata1d newddata1d proc~push_f->newddata1d max_size max_size proc~push_f->max_size push push proc~push_f->push n_items n_items proc~push_f->n_items dcopy dcopy proc~push_f->dcopy delete delete proc~push_f->delete proc~stack_check->n_items get_pointer get_pointer proc~stack_check->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~push_f~~CalledByGraph proc~push_f push_F proc~update_f update_F proc~update_f->proc~push_f Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_F Source Code subroutine push_F ( s_F , n , F , fact ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) real ( dp ), intent ( in ), optional :: fact type ( dData1D ) :: dD1 real ( dp ), pointer :: sF (:) integer :: in , ns integer :: i if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) ns = max_size ( s_F ) if ( in == ns ) then ! we have to cycle the storage call get ( s_F , 1 , dD1 ) else call newdData1D ( dD1 , n , '(F)' ) end if sF => val ( dD1 ) if ( present ( fact ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n sF ( i ) = F ( i ) * fact end do !$OMP end parallel do else call dcopy ( n , F , 1 , sF , 1 ) end if ! Push the data to the stack call push ( s_F , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_F","tags":"","loc":"proc/push_f.html","title":"push_F – SIESTA"},{"text":"private subroutine update_F(s_F, n, F) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) Calls proc~~update_f~~CallsGraph proc~update_f update_F proc~stack_check stack_check proc~update_f->proc~stack_check val val proc~update_f->val die die proc~update_f->die proc~push_f push_F proc~update_f->proc~push_f get_pointer get_pointer proc~update_f->get_pointer n_items n_items proc~update_f->n_items dcopy dcopy proc~update_f->dcopy proc~stack_check->get_pointer proc~stack_check->n_items proc~push_f->proc~stack_check proc~push_f->val proc~push_f->die proc~push_f->n_items proc~push_f->dcopy push push proc~push_f->push delete delete proc~push_f->delete newddata1d newddata1d proc~push_f->newddata1d max_size max_size proc~push_f->max_size get get proc~push_f->get Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code update_F Source Code subroutine update_F ( s_F , n , F ) type ( Fstack_dData1D ), intent ( inout ) :: s_F integer , intent ( in ) :: n real ( dp ), intent ( in ) :: F ( n ) type ( dData1D ), pointer :: dD1 real ( dp ), pointer :: FF (:) integer :: in if ( . not . stack_check ( s_F , n ) ) then call die ( 'mixing: history has changed size...' ) end if in = n_items ( s_F ) if ( in == 0 ) then ! We need to add it as it does not exist call push_F ( s_F , n , F ) else ! we have an entry, update the latest dD1 => get_pointer ( s_F , in ) FF => val ( dD1 ) call dcopy ( n , F , 1 , FF , 1 ) end if end subroutine update_F","tags":"","loc":"proc/update_f.html","title":"update_F – SIESTA"},{"text":"private subroutine push_diff(s_rres, s_res, alpha) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_rres type(Fstack_dData1D), intent(in) :: s_res real(kind=dp), intent(in), optional :: alpha Calls proc~~push_diff~~CallsGraph proc~push_diff push_diff val val proc~push_diff->val get get proc~push_diff->get die die proc~push_diff->die get_pointer get_pointer proc~push_diff->get_pointer push push proc~push_diff->push n_items n_items proc~push_diff->n_items max_size max_size proc~push_diff->max_size delete delete proc~push_diff->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code push_diff Source Code subroutine push_diff ( s_rres , s_res , alpha ) type ( Fstack_dData1D ), intent ( inout ) :: s_rres type ( Fstack_dData1D ), intent ( in ) :: s_res real ( dp ), intent ( in ), optional :: alpha type ( dData1D ) :: dD1 type ( dData1D ), pointer :: pD1 real ( dp ), pointer :: res1 (:), res2 (:), rres (:) integer :: in , ns , i , n if ( n_items ( s_res ) < 2 ) then call die ( 'mixing: Residual residuals cannot be calculated, & &inferior residual size.' ) end if in = n_items ( s_res ) ! First get the value of in pD1 => get_pointer ( s_res , in - 1 ) res1 => val ( pD1 ) ! get the value of in pD1 => get_pointer ( s_res , in ) res2 => val ( pD1 ) in = n_items ( s_rres ) ns = max_size ( s_rres ) if ( in == ns ) then ! we have to cycle the storage call get ( s_rres , 1 , dD1 ) else call newdData1D ( dD1 , size ( res1 ), '(res)' ) end if ! Get the residual of the residual rres => val ( dD1 ) n = size ( rres ) if ( present ( alpha ) ) then !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = ( res2 ( i ) - res1 ( i )) * alpha end do !$OMP end parallel do else !$OMP parallel do default(shared), private(i) do i = 1 , n rres ( i ) = res2 ( i ) - res1 ( i ) end do !$OMP end parallel do end if ! Push the data to the stack call push ( s_rres , dD1 ) ! Delete double reference call delete ( dD1 ) end subroutine push_diff","tags":"","loc":"proc/push_diff.html","title":"push_diff – SIESTA"},{"text":"public interface mixing Calls interface~~mixing~~CallsGraph interface~mixing mixing proc~mixing_1d mixing_1d interface~mixing->proc~mixing_1d proc~mixing_2d mixing_2d interface~mixing->proc~mixing_2d proc~mixing_calc_next mixing_calc_next proc~mixing_1d->proc~mixing_calc_next proc~mixing_coeff mixing_coeff proc~mixing_1d->proc~mixing_coeff proc~mixing_ncoeff mixing_ncoeff proc~mixing_1d->proc~mixing_ncoeff proc~mixing_init mixing_init proc~mixing_1d->proc~mixing_init proc~mixing_finalize mixing_finalize proc~mixing_1d->proc~mixing_finalize proc~mixing_coeff->proc~mixing_ncoeff n_items n_items proc~mixing_ncoeff->n_items proc~current_itt current_itt proc~mixing_init->proc~current_itt proc~norm norm proc~mixing_init->proc~norm reset reset proc~mixing_finalize->reset proc~mixing_step mixing_step proc~mixing_finalize->proc~mixing_step proc~mixing_finalize->n_items proc~mixing_finalize->proc~current_itt proc~mixing_step->reset proc~mixing_step->n_items push push proc~mixing_step->push get_pointer get_pointer proc~mixing_step->get_pointer Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures mixing_1d mixing_2d Module Procedures private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub","tags":"","loc":"interface/mixing.html","title":"mixing – SIESTA"},{"text":"public subroutine compute_max_diff_2d(X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff Calls proc~~compute_max_diff_2d~~CallsGraph proc~compute_max_diff_2d compute_max_diff_2d die die proc~compute_max_diff_2d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_max_diff_2d~~CalledByGraph proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff compute_max_diff interface~compute_max_diff->proc~compute_max_diff_2d proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_max_diff_2d Source Code subroutine compute_max_diff_2d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:,:), X2 (:,:) real ( dp ), intent ( out ) :: max_diff integer :: n1 , n2 integer :: i1 , i2 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) n2 = size ( X1 , 2 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if if ( size ( X2 , 2 ) /= n2 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (2-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i2,i1), & !$OMP& reduction(max:max_diff), collapse(2) do i2 = 1 , n2 do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 , i2 ) - X2 ( i1 , i2 )) ) end do end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_2d","tags":"","loc":"proc/compute_max_diff_2d.html","title":"compute_max_diff_2d – SIESTA"},{"text":"public subroutine compute_max_diff_1d(X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff Calls proc~~compute_max_diff_1d~~CallsGraph proc~compute_max_diff_1d compute_max_diff_1d die die proc~compute_max_diff_1d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_max_diff_1d~~CalledByGraph proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff compute_max_diff interface~compute_max_diff->proc~compute_max_diff_1d proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_max_diff_1d Source Code subroutine compute_max_diff_1d ( X1 , X2 , max_diff ) #ifdef MPI use m_mpi_utils , only : globalize_max #endif real ( dp ), intent ( in ) :: X1 (:), X2 (:) real ( dp ), intent ( out ) :: max_diff integer :: n1 integer :: i1 #ifdef MPI real ( dp ) :: buffer1 #endif n1 = size ( X1 , 1 ) if ( size ( X2 , 1 ) /= n1 ) then call die ( 'compute_max_diff: Sizes of the arrays are not & &conforming (1-D)' ) end if max_diff = 0.0_dp !$OMP parallel do default(shared), private(i1), reduction(max:max_diff) do i1 = 1 , n1 max_diff = max ( max_diff , abs ( X1 ( i1 ) - X2 ( i1 )) ) end do !$OMP end parallel do #ifdef MPI ! Ensure that max_diff is the same on all nodes call globalize_max ( max_diff , buffer1 ) max_diff = buffer1 #endif dDmax_current = max_diff end subroutine compute_max_diff_1d","tags":"","loc":"proc/compute_max_diff_1d.html","title":"compute_max_diff_1d – SIESTA"},{"text":"public interface compute_max_diff Calls interface~~compute_max_diff~~CallsGraph interface~compute_max_diff compute_max_diff proc~compute_max_diff_1d compute_max_diff_1d interface~compute_max_diff->proc~compute_max_diff_1d proc~compute_max_diff_2d compute_max_diff_2d interface~compute_max_diff->proc~compute_max_diff_2d die die proc~compute_max_diff_1d->die proc~compute_max_diff_2d->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by interface~~compute_max_diff~~CalledByGraph interface~compute_max_diff compute_max_diff proc~siesta_forces siesta_forces proc~siesta_forces->interface~compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Module Procedures compute_max_diff_1d compute_max_diff_2d Module Procedures public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff","tags":"","loc":"interface/compute_max_diff.html","title":"compute_max_diff – SIESTA"},{"text":"public subroutine setup_hamiltonian(iscf) Uses siesta_options sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices class_dSpData1D class_dSpData2D class_zSpData2D siesta_geom atmfuncs atomlist metaforce molecularmechanics ldau_specs m_ldau m_dhscf m_stress m_energies parallel m_steps m_ntm m_spin m_dipol alloc m_gamma m_hsx sys m_partial_charges files m_rhog m_mpi_utils proc~~setup_hamiltonian~~UsesGraph proc~setup_hamiltonian setup_hamiltonian m_partial_charges m_partial_charges proc~setup_hamiltonian->m_partial_charges ldau_specs ldau_specs proc~setup_hamiltonian->ldau_specs module~parallel parallel proc~setup_hamiltonian->module~parallel m_energies m_energies proc~setup_hamiltonian->m_energies module~sys sys proc~setup_hamiltonian->module~sys m_ldau m_ldau proc~setup_hamiltonian->m_ldau m_steps m_steps proc~setup_hamiltonian->m_steps class_dSpData2D class_dSpData2D proc~setup_hamiltonian->class_dSpData2D siesta_geom siesta_geom proc~setup_hamiltonian->siesta_geom m_stress m_stress proc~setup_hamiltonian->m_stress m_ntm m_ntm proc~setup_hamiltonian->m_ntm m_mpi_utils m_mpi_utils proc~setup_hamiltonian->m_mpi_utils molecularmechanics molecularmechanics proc~setup_hamiltonian->molecularmechanics files files proc~setup_hamiltonian->files m_hsx m_hsx proc~setup_hamiltonian->m_hsx module~siesta_options siesta_options proc~setup_hamiltonian->module~siesta_options module~m_dhscf m_dhscf proc~setup_hamiltonian->module~m_dhscf m_spin m_spin proc~setup_hamiltonian->m_spin m_gamma m_gamma proc~setup_hamiltonian->m_gamma class_dSpData1D class_dSpData1D proc~setup_hamiltonian->class_dSpData1D alloc alloc proc~setup_hamiltonian->alloc module~atmfuncs atmfuncs proc~setup_hamiltonian->module~atmfuncs metaforce metaforce proc~setup_hamiltonian->metaforce sparse_matrices sparse_matrices proc~setup_hamiltonian->sparse_matrices atomlist atomlist proc~setup_hamiltonian->atomlist module~m_rhog m_rhog proc~setup_hamiltonian->module~m_rhog m_dipol m_dipol proc~setup_hamiltonian->m_dipol class_zSpData2D class_zSpData2D proc~setup_hamiltonian->class_zSpData2D module~precision precision module~m_dhscf->module~precision module~m_dfscf m_dfscf module~m_dhscf->module~m_dfscf module~atmfuncs->module~sys module~atmfuncs->module~precision spher_harm spher_harm module~atmfuncs->spher_harm module~atm_types atm_types module~atmfuncs->module~atm_types module~radial radial module~atmfuncs->module~radial module~m_rhog->m_spin module~m_rhog->module~precision class_Pair_dData1D class_Pair_dData1D module~m_rhog->class_Pair_dData1D class_dData1D class_dData1D module~m_rhog->class_dData1D class_Fstack_Pair_dData1D class_Fstack_Pair_dData1D module~m_rhog->class_Fstack_Pair_dData1D module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~setup_hamiltonian~~CallsGraph proc~setup_hamiltonian setup_hamiltonian proc~dhscf dhscf proc~setup_hamiltonian->proc~dhscf globalize_sum globalize_sum proc~setup_hamiltonian->globalize_sum proc~bye bye proc~setup_hamiltonian->proc~bye val val proc~setup_hamiltonian->val h h proc~setup_hamiltonian->h timer timer proc~setup_hamiltonian->timer write_hsx write_hsx proc~setup_hamiltonian->write_hsx de_alloc de_alloc proc~setup_hamiltonian->de_alloc re_alloc re_alloc proc~setup_hamiltonian->re_alloc dimag dimag proc~setup_hamiltonian->dimag dscf dscf proc~setup_hamiltonian->dscf update_e0 update_e0 proc~setup_hamiltonian->update_e0 hold hold proc~setup_hamiltonian->hold proc~die die proc~setup_hamiltonian->proc~die hubbard_term hubbard_term proc~setup_hamiltonian->hubbard_term proc~dhscf->proc~bye proc~dhscf->timer proc~dhscf->de_alloc proc~dhscf->re_alloc proc~dhscf->proc~die elecs elecs proc~dhscf->elecs proc~forhar forhar proc~dhscf->proc~forhar ts_voltage ts_voltage proc~dhscf->ts_voltage mpi_barrier mpi_barrier proc~dhscf->mpi_barrier ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix volcel volcel proc~dhscf->volcel ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofdsp rhoofdsp proc~dhscf->rhoofdsp get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field hartree_add hartree_add proc~dhscf->hartree_add proc~dfscf dfscf proc~dhscf->proc~dfscf proc~reord reord proc~dhscf->proc~reord compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata proc~vmat vmat proc~dhscf->proc~vmat mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce vacuum_level vacuum_level proc~dhscf->vacuum_level localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh write_debug write_debug proc~dhscf->write_debug proc~rhoofd rhoofd proc~dhscf->proc~rhoofd proc~write_rho write_rho proc~dhscf->proc~write_rho cmlfinishfile cmlfinishfile proc~bye->cmlfinishfile pxfflush pxfflush proc~bye->pxfflush mpi_finalize mpi_finalize proc~bye->mpi_finalize mpi_abort mpi_abort proc~die->mpi_abort proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign proc~die->pxfflush io_close io_close proc~die->io_close pxfabort pxfabort proc~die->pxfabort proc~forhar->de_alloc proc~forhar->re_alloc proc~forhar->jms_setmeshdistr proc~forhar->bsc_cellxc proc~forhar->proc~setmeshdistr proc~forhar->mymeshbox proc~forhar->interface~distmeshdata proc~dfscf->timer proc~dfscf->de_alloc proc~dfscf->re_alloc proc~dfscf->proc~die indxuo indxuo proc~dfscf->indxuo listsc listsc proc~dfscf->listsc endpht endpht proc~dfscf->endpht listp2 listp2 proc~dfscf->listp2 needdscfl needdscfl proc~dfscf->needdscfl listdlptr listdlptr proc~dfscf->listdlptr matrixotom matrixotom proc~dfscf->matrixotom lstpht lstpht proc~dfscf->lstpht globaltolocalorb globaltolocalorb proc~dfscf->globaltolocalorb numdl numdl proc~dfscf->numdl proc~rcut rcut proc~dfscf->proc~rcut alloc_default alloc_default proc~dfscf->alloc_default dscfl dscfl proc~dfscf->dscfl listdl listdl proc~dfscf->listdl proc~reord->timer proc~reord->de_alloc proc~reord->re_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~vmat->timer proc~vmat->de_alloc proc~vmat->re_alloc matrixmtoo matrixmtoo proc~vmat->matrixmtoo phi phi proc~vmat->phi proc~vmat->indxuo proc~vmat->listsc proc~vmat->endpht proc~vmat->listp2 proc~vmat->needdscfl proc~vmat->listdlptr proc~vmat->lstpht proc~vmat->globaltolocalorb proc~vmat->numdl proc~vmat->proc~rcut proc~vmat->listdl proc~rhoofd->timer proc~rhoofd->de_alloc proc~rhoofd->re_alloc proc~rhoofd->proc~die proc~rhoofd->phi proc~rhoofd->indxuo proc~rhoofd->listsc proc~rhoofd->endpht proc~rhoofd->listp2 proc~rhoofd->needdscfl proc~rhoofd->listdlptr proc~rhoofd->matrixotom proc~rhoofd->lstpht proc~rhoofd->globaltolocalorb proc~rhoofd->numdl proc~rhoofd->proc~rcut proc~rhoofd->dscfl proc~rhoofd->listdl proc~write_rho->de_alloc proc~write_rho->mpi_barrier proc~write_rho->io_assign proc~write_rho->io_close proc~write_rho->write_debug mpi_wait mpi_wait proc~write_rho->mpi_wait mpi_irecv mpi_irecv proc~write_rho->mpi_irecv proc~distmeshdata_rea->timer proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->proc~die proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->de_alloc proc~distmeshdata_int->re_alloc proc~distmeshdata_int->proc~die proc~distmeshdata_int->proc~boxintersection proc~chk chk proc~rcut->proc~chk proc~chk->proc~die var panprocsetup_hamiltonianCallsGraph = svgPanZoom('#procsetup_hamiltonianCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~setup_hamiltonian~~CalledByGraph proc~setup_hamiltonian setup_hamiltonian proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code setup_hamiltonian Source Code subroutine setup_hamiltonian ( iscf ) USE siesta_options use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H , S , Hold use sparse_matrices , only : Dscf , Escf , xijo use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val use siesta_geom use atmfuncs , only : uion use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , . lastkb , no_s , rmaxv , indxua , iphorb , lasto , . rmaxo , no_l use metaforce , only : lMetaForce , meta use molecularmechanics , only : twobody use ldau_specs , only : switch_ldau ! This variable determines whether !   the subroutine to compute the !   Hubbard terms should be called !   or not use m_ldau , only : hubbard_term ! Subroutine that compute the !   Hubbard terms use m_dhscf , only : dhscf use m_stress use m_energies use parallel , only : Node use m_steps , only : istp use m_ntm use m_spin , only : spin use m_dipol use alloc , only : re_alloc , de_alloc use m_gamma use m_hsx , only : write_hsx use sys , only : die , bye use m_partial_charges , only : want_partial_charges use files , only : filesOut_t ! derived type for output file names use m_rhog , only : rhog_in , rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif implicit none integer , intent ( in ) :: iscf real ( dp ) :: stressl ( 3 , 3 ) real ( dp ), pointer :: fal (:,:) ! Local-node part of atomic F #ifdef MPI real ( dp ) :: buffer1 #endif integer :: io , is , ispin integer :: ifa ! Calc. forces?      0=>no, 1=>yes integer :: istr ! Calc. stress?      0=>no, 1=>yes integer :: ihmat ! Calc. hamiltonian? 0=>no, 1=>yes real ( dp ) :: g2max type ( filesOut_t ) :: filesOut ! blank output file names logical :: use_rhog_in real ( dp ), pointer :: H_vkb (:), H_kin (:), H_ldau (:,:) real ( dp ), pointer :: H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) complex ( dp ) :: Dc integer :: ind , i , j !------------------------------------------------------------------------- BEGIN call timer ( 'setup_H' , 1 ) ! Nullify pointers nullify ( fal ) !$OMP parallel default(shared), private(ispin,io) !     Save present H matrix !$OMP do collapse(2) do ispin = 1 , spin % H do io = 1 , maxnh Hold ( io , ispin ) = H ( io , ispin ) end do end do !$OMP end do !$OMP single H_kin => val ( H_kin_1D ) H_vkb => val ( H_vkb_1D ) if ( spin % SO_onsite ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so_on => val ( H_so_on_2D ) else if ( spin % SO_offsite ) then H_so_off => val ( H_so_off_2D ) end if !$OMP end single ! keep wait ! Initialize diagonal Hamiltonian do ispin = 1 , spin % spinor !$OMP do do io = 1 , maxnh H ( io , ispin ) = H_kin ( io ) + H_vkb ( io ) end do !$OMP end do nowait end do if ( spin % SO_onsite ) then !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = H_so_on ( io , ispin - 2 ) end do end do !$OMP end do nowait else !$OMP do collapse(2) do ispin = 3 , spin % H do io = 1 , maxnh H ( io , ispin ) = 0._dp end do end do !$OMP end do nowait end if ! .................. ! Non-SCF part of total energy ....................................... ! Note that these will be \"impure\" for a mixed Dscf ! If mixing the charge, Dscf is the previous step's DM_out. Since ! the \"scf\" components of the energy are computed with the (mixed) ! charge, this introduces an inconsistency. In this case the energies ! coming out of this routine need to be corrected. ! !$OMP single Ekin = 0.0_dp Enl = 0.0_dp Eso = 0.0_dp !$OMP end single ! keep wait !$OMP do collapse(2), reduction(+:Ekin,Enl) do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) end do end do !$OMP end do nowait ! !  Dc IS NOT the dense matrix, it is just a complex number ! (per each io index) used as an artifact to multiply the ! elements of the H_SO times the corresponding elements of ! DM in a such way that the result gives Re{Tr[H_SO*DM]}. ! if ( spin % SO_offsite ) then do io = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ), dp ) Eso = Eso + real ( H_so_off ( io , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ), dp ) Eso = Eso + real ( H_so_off ( io , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( io , 3 ), Dscf ( io , 4 ), dp ) Eso = Eso + real ( H_so_off ( io , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( io , 7 ), - Dscf ( io , 8 ), dp ) Eso = Eso + real ( H_so_off ( io , 3 ) * Dc , dp ) end do else if ( spin % SO_onsite ) then !$OMP do reduction(+:Eso) do io = 1 , maxnh Eso = Eso + H_so_on ( io , 1 ) * Dscf ( io , 7 ) + & H_so_on ( io , 2 ) * Dscf ( io , 8 ) + H_so_on ( io , 5 ) * Dscf ( io , 3 ) + & H_so_on ( io , 6 ) * Dscf ( io , 4 ) - H_so_on ( io , 3 ) * Dscf ( io , 5 ) - & H_so_on ( io , 4 ) * Dscf ( io , 6 ) end do !$OMP end do nowait end if !$OMP end parallel #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 end if #endif !     Non-SCF part of total energy call update_E0 () ! Hubbard term for LDA+U: energy, forces, stress and matrix elements .... if ( switch_ldau ) then if ( spin % NCol ) then call die ( 'LDA+U cannot be used with non-collinear spin.' ) end if if ( spin % SO ) then call die ( 'LDA+U cannot be used with spin-orbit coupling.' ) end if call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) H_ldau => val ( H_ldau_2D ) call hubbard_term ( scell , na_u , na_s , isa , xa , indxua , . maxnh , maxnh , lasto , iphorb , no_u , no_l , . numh , listhptr , listh , numh , listhptr , listh , . spin % spinor , Dscf , Eldau , DEldau , H_ldau , . fal , stressl , H , iscf , . matrix_elements_only = . true .) #ifdef MPI ! Global reduction of energy terms call globalize_sum ( Eldau , buffer1 ) Eldau = buffer1 ! DEldau should not be globalized ! as it is based on globalized occupations #endif Eldau = Eldau + DEldau call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) endif ! .................. ! Add SCF contribution to energy and matrix elements .................. g2max = g2cut call re_alloc ( fal , 1 , 3 , 1 , na_u , 'fal' , 'setup_hamiltonian' ) ifa = 0 istr = 0 ihmat = 1 if (( hirshpop . or . voropop ) $ . and . partial_charges_at_every_scf_step ) then want_partial_charges = . true . endif use_rhog_in = ( mix_charge . and . iscf > 1 ) call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa , indxua , . ntm , ifa , istr , ihmat , filesOut , . maxnh , numh , listhptr , listh , Dscf , Datm , . maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , . Exc , Dxc , dipol , stress , fal , stressl , . use_rhog_in ) if ( spin % SO_offsite ) then ! H(:, [5, 6]) are not updated in dhscf, see vmat for details. !------- H(u,u) H (:, 1 ) = H (:, 1 ) + real ( H_so_off (:, 1 ), dp ) H (:, 5 ) = dimag ( H_so_off (:, 1 )) !------- H(d,d) H (:, 2 ) = H (:, 2 ) + real ( H_so_off (:, 2 ), dp ) H (:, 6 ) = dimag ( H_so_off (:, 2 )) !------- H(u,d) H (:, 3 ) = H (:, 3 ) + real ( H_so_off (:, 3 ), dp ) H (:, 4 ) = H (:, 4 ) + dimag ( H_so_off (:, 3 )) !------- H(d,u) H (:, 7 ) = H (:, 7 ) + real ( H_so_off (:, 4 ), dp ) H (:, 8 ) = H (:, 8 ) - dimag ( H_so_off (:, 4 )) endif ! This statement will apply to iscf = 1, for example, when ! we do not use rhog_in. Rhog here is always the charge used to ! build H, that is, rhog_in. if ( mix_charge ) rhog_in = rhog want_partial_charges = . false . call de_alloc ( fal , 'fal' , 'setup_hamiltonian' ) !  It is wasteful to write over and over H and S, as there are !  no different files. ! Save Hamiltonian and overlap matrices ............................ ! Only in HSX format now.  Use Util/HSX/hsx2hs to generate an HS file if ( savehs . or . write_coop ) then call write_hsx ( gamma , no_u , no_s , spin % H , indxuo , & maxnh , numh , listhptr , listh , H , S , qtot , & temp , xijo ) endif call timer ( 'setup_H' , 2 ) #ifdef SIESTA__PEXSI if ( node == 0 ) call memory_snapshot ( \"after setup_H\" ) #endif if ( h_setup_only ) then call timer ( 'all' , 2 ) ! New call to close the tree call timer ( 'all' , 3 ) call bye ( \"H-Setup-Only requested\" ) STOP endif !------------------------------------------------------------------------- END END subroutine setup_hamiltonian","tags":"","loc":"proc/setup_hamiltonian.html","title":"setup_hamiltonian – SIESTA"},{"text":"public subroutine compute_dm(iscf) Uses precision units siesta_options class_dSpData1D sparse_matrices siesta_geom atomlist sys kpoint_scf_m m_energies m_energies m_rmaxh m_eo m_spin m_diagon m_gamma parallel parallel m_compute_ebs_shift m_pexsi_solver m_hsx mpi_siesta iodmhs_netcdf m_dminim m_zminim m_ordern m_steps m_normalize_dm m_chess m_energies m_ts_global_vars m_transiesta proc~~compute_dm~~UsesGraph proc~compute_dm compute_dm m_diagon m_diagon proc~compute_dm->m_diagon m_eo m_eo proc~compute_dm->m_eo module~units units proc~compute_dm->module~units m_ts_global_vars m_ts_global_vars proc~compute_dm->m_ts_global_vars module~parallel parallel proc~compute_dm->module~parallel m_energies m_energies proc~compute_dm->m_energies m_ordern m_ordern proc~compute_dm->m_ordern module~sys sys proc~compute_dm->module~sys kpoint_scf_m kpoint_scf_m proc~compute_dm->kpoint_scf_m atomlist atomlist proc~compute_dm->atomlist m_steps m_steps proc~compute_dm->m_steps m_normalize_dm m_normalize_dm proc~compute_dm->m_normalize_dm m_compute_ebs_shift m_compute_ebs_shift proc~compute_dm->m_compute_ebs_shift siesta_geom siesta_geom proc~compute_dm->siesta_geom iodmhs_netcdf iodmhs_netcdf proc~compute_dm->iodmhs_netcdf m_pexsi_solver m_pexsi_solver proc~compute_dm->m_pexsi_solver m_hsx m_hsx proc~compute_dm->m_hsx module~siesta_options siesta_options proc~compute_dm->module~siesta_options m_dminim m_dminim proc~compute_dm->m_dminim m_chess m_chess proc~compute_dm->m_chess mpi_siesta mpi_siesta proc~compute_dm->mpi_siesta m_spin m_spin proc~compute_dm->m_spin m_transiesta m_transiesta proc~compute_dm->m_transiesta class_dSpData1D class_dSpData1D proc~compute_dm->class_dSpData1D module~precision precision proc~compute_dm->module~precision m_gamma m_gamma proc~compute_dm->m_gamma sparse_matrices sparse_matrices proc~compute_dm->sparse_matrices m_rmaxh m_rmaxh proc~compute_dm->m_rmaxh m_zminim m_zminim proc~compute_dm->m_zminim module~units->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~compute_dm~~CallsGraph proc~compute_dm compute_dm ordern ordern proc~compute_dm->ordern write_hs_formatted write_hs_formatted proc~compute_dm->write_hs_formatted zminim zminim proc~compute_dm->zminim eold eold proc~compute_dm->eold proc~bye bye proc~compute_dm->proc~bye val val proc~compute_dm->val escf escf proc~compute_dm->escf transiesta transiesta proc~compute_dm->transiesta chess_wrapper chess_wrapper proc~compute_dm->chess_wrapper pexsi_solver pexsi_solver proc~compute_dm->pexsi_solver write_dmh_netcdf write_dmh_netcdf proc~compute_dm->write_dmh_netcdf timer timer proc~compute_dm->timer dold dold proc~compute_dm->dold diagon diagon proc~compute_dm->diagon write_orb_indx write_orb_indx proc~compute_dm->write_orb_indx mpi_bcast mpi_bcast proc~compute_dm->mpi_bcast dscf dscf proc~compute_dm->dscf normalize_dm normalize_dm proc~compute_dm->normalize_dm dminim dminim proc~compute_dm->dminim compute_ebs_shift compute_ebs_shift proc~compute_dm->compute_ebs_shift pxfflush pxfflush proc~bye->pxfflush mpi_finalize mpi_finalize proc~bye->mpi_finalize cmlfinishfile cmlfinishfile proc~bye->cmlfinishfile Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_dm~~CalledByGraph proc~compute_dm compute_dm proc~siesta_forces siesta_forces proc~siesta_forces->proc~compute_dm Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_dm Source Code subroutine compute_dm ( iscf ) use precision use units , only : eV USE siesta_options use class_dSpData1D , only : val use sparse_matrices use siesta_geom use atomlist , only : qa , lasto , iphorb , iaorb , no_u , no_s , indxuo , & qtot , Qtots , no_l use sys , only : die , bye use kpoint_scf_m , only : kpoints_scf use m_energies , only : Ebs , Ecorrec , Entropy , DE_NEGF use m_energies , only : Ef , Efs use m_rmaxh use m_eo use m_spin , only : spin use m_diagon , only : diagon use m_gamma use parallel , only : IONode use parallel , only : SIESTA_worker use m_compute_ebs_shift , only : compute_ebs_shift #ifdef SIESTA__PEXSI use m_pexsi_solver , only : pexsi_solver #endif use m_hsx , only : write_hs_formatted #ifdef MPI use mpi_siesta #endif #ifdef CDF use iodmhs_netcdf , only : write_dmh_netcdf #endif use m_dminim , only : dminim use m_zminim , only : zminim use m_ordern , only : ordern use m_steps , only : istp use m_normalize_dm , only : normalize_dm #ifdef SIESTA__CHESS use m_chess , only : CheSS_wrapper #endif use m_energies , only : DE_NEGF use m_ts_global_vars , only : TSmode , TSinit , TSrun use m_transiesta , only : transiesta implicit none !     Input variables integer , intent ( in ) :: iscf real ( dp ) :: delta_Ebs , delta_Ef logical :: CallDiagon integer :: nnz real ( dp ), pointer :: H_kin (:) ! e1>e2 to signal that we do not want DOS weights real ( dp ), parameter :: e1 = 1.0_dp , e2 = - 1.0_dp real ( dp ) :: buffer1 integer :: mpierr !       character(15)            :: filename, indexstr !       character(15), parameter :: fnameform = '(A,A,A)' !-------------------------------------------------------------------- BEGIN if ( SIESTA_worker ) call timer ( 'compute_dm' , 1 ) #ifdef MPI call MPI_Bcast ( isolve , 1 , MPI_integer , 0 , true_MPI_Comm_World , mpierr ) #endif if ( SIESTA_worker ) then ! Save present density matrix !$OMP parallel default(shared) if ( converge_EDM ) then !$OMP workshare Eold (:,:) = Escf (:,:) Dold (:,:) = Dscf (:,:) !$OMP end workshare else !$OMP workshare Dold (:,:) = Dscf (:,:) !$OMP end workshare end if !$OMP end parallel end if ! Compute shift in Tr(H*DM) for fermi-level bracketting ! Use the current H, the previous iteration H, and the ! previous iteration DM if ( SIESTA_worker ) then if ( iscf > 1 ) then call compute_Ebs_shift ( Dscf , H , Hold , delta_Ebs ) delta_Ef = delta_Ebs / qtot if ( ionode . and . isolve . eq . SOLVE_PEXSI ) then write ( 6 , \"(a,f16.5)\" ) $ \"Estimated change in band-structure energy:\" , $ delta_Ebs / eV , \"Estimated shift in E_fermi: \" , $ delta_Ef / eV endif else delta_Ebs = 0.0_dp delta_Ef = 0.0_dp endif endif #ifdef SIESTA__PEXSI if ( isolve . eq . SOLVE_PEXSI ) then ! This test done in node 0 since NonCol and SpOrb ! are not set for PEXSI-solver-only processes if ( ionode ) then if ( spin % NCol . or . spin % SO ) call die ( $ \"The PEXSI solver does not implement \" // $ \"non-coll spins or Spin-orbit yet\" ) endif call pexsi_solver ( iscf , no_u , no_l , spin % spinor , $ maxnh , numh , listhptr , listh , $ H , S , qtot , Dscf , Escf , $ ef , Entropy , temp , delta_Ef ) endif if (. not . SIESTA_worker ) RETURN #endif ! Here we decide if we want to calculate one or more SCF steps by ! diagonalization before proceeding with the OMM routine CallDiagon = . false . if ( isolve . eq . SOLVE_MINIM ) then if ( istp . eq . 1 ) then if (( iscf . le . call_diagon_first_step ) . or . & ( call_diagon_first_step < 0 )) CallDiagon = . true . else if (( iscf . le . call_diagon_default ) . or . & ( call_diagon_default < 0 )) CallDiagon = . true . endif endif if ( isolve . eq . MATRIX_WRITE ) then !             write(indexstr,'(I15)') iscf !             write(filename,fnameform) 'H_', trim(adjustl(indexstr)), !      &                                '.matrix' !             call write_global_matrix( no_s, no_l, maxnh, numh, listh, !      &           H(1:maxnh,1), filename ) ! !             write(filename,fnameform) 'S_', trim(adjustl(indexstr)), !      &                                '.matrix' !        Note: only one-shot for now call write_hs_formatted ( no_u , spin % H , $ maxnh , numh , listhptr , listh , H , S ) call bye ( \"End of run after writing H.matrix and S.matrix\" ) c$ call write_global_matrix_singlenodewrite ( c$ & no_u , no_s , maxnh , numh , listhptr , listh , c$ & H (:, 1 ), 'H.matrix' ) c$ c$ call write_global_matrix_singlenodewrite ( c$ & no_u , no_s , maxnh , numh , listhptr , listh , c$ & S , 'S.matrix' ) elseif (( isolve . eq . SOLVE_DIAGON ) . or . ( CallDiagon )) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0.0_dp PreviousCallDiagon = . true . elseif ( isolve . eq . SOLVE_ORDERN ) then if (. not . gamma ) call die ( \"Cannot do O(N) with k-points.\" ) if ( spin % NCol . or . spin % SO ) . call die ( \"Cannot do O(N) with non-coll spins or Spin-orbit\" ) call ordern ( usesavelwf , ioptlwf , na_u , no_u , no_l , lasto , & isa , qa , rcoor , rmaxh , ucell , xa , iscf , & istp , ncgmax , etol , eta , qtot , maxnh , numh , & listhptr , listh , H , S , chebef , noeta , rcoorcp , & beta , pmax , Dscf , Escf , Ecorrec , spin % H , qtots ) Entropy = 0.0_dp elseif ( isolve . eq . SOLVE_MINIM ) then if ( spin % NCol . or . spin % SO ) & call die ( 'ERROR: Non-collinear spin calculations &                       not yet implemented with OMM!' ) H_kin => val ( H_kin_1D ) if ( gamma ) then call dminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , H , S , H_kin ) else call zminim (. false ., PreviousCallDiagon , iscf , istp , no_l , & spin % H , no_u , maxnh , numh , listhptr , listh , Dscf , & eta , qtots , no_s , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & H , S , H_kin ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #ifdef SIESTA__CHESS elseif ( isolve . eq . SOLVE_CHESS ) then ! FOE solver from the CheSS library if ( gamma ) then call CheSS_wrapper (. false ., PreviousCallDiagon , & iscf , istp , no_l , & spin % spinor , no_u , maxnh , numh , listhptr , listh , & qs , h , s , & Dscf , Escf , Ef ) end if Ecorrec = 0.0_dp Entropy = 0.0_dp PreviousCallDiagon = . false . #endif elseif ( TSmode . and . TSinit ) then call diagon ( no_s , spin % spinor , & no_l , maxnh , maxnh , no_u , & numh , listhptr , listh , numh , listhptr , listh , & H , S , qtot , fixspin , qtots , temp , e1 , e2 , & gamma , xijo , indxuo , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w , & eo , qo , Dscf , Escf , ef , efs , Entropy , no_u , & occtol , iscf , neigwanted ) Ecorrec = 0._dp else if ( TSrun ) then call transiesta ( iscf , spin % H , block_dist , sparse_pattern , & Gamma , ucell , nsc , isc_off , no_u , na_u , lasto , xa , maxnh , & H , S , Dscf , Escf , Ef , Qtot , . false ., DE_NEGF ) Ecorrec = 0._dp Entropy = 0.0_dp else !call die('siesta: ERROR: wrong solution method') endif #ifdef CDF if ( writedmhs_cdf_history ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf ) else if ( writedmhs_cdf ) then call write_dmh_netcdf ( no_l , maxnh , spin % H , Dold , H , Dscf , & overwrite = . true . ) endif #endif ! Write orbital indexes. JMS Dec.2009 if ( IOnode . and . iscf == 1 ) then call write_orb_indx ( na_u , na_s , no_u , no_s , isa , xa , . iaorb , iphorb , indxuo , nsc , ucell ) endif !     Normalize density matrix to exact charge !     Placed here for now to avoid disturbing EHarris if ( . not . TSrun ) then call normalize_dm ( first = . false . ) end if call timer ( 'compute_dm' , 2 ) #ifdef SIESTA__PEXSI if ( ionode ) call memory_snapshot ( \"after compute_DM\" ) #endif !-----------------------------------------------------------------------END END subroutine compute_dm","tags":"","loc":"proc/compute_dm.html","title":"compute_dm – SIESTA"},{"text":"public subroutine mixers_scf_init(nspin, Comm) Uses fdf precision m_mixing m_mixing m_mixing m_mixing proc~~mixers_scf_init~~UsesGraph proc~mixers_scf_init mixers_scf_init module~precision precision proc~mixers_scf_init->module~precision fdf fdf proc~mixers_scf_init->fdf module~m_mixing m_mixing proc~mixers_scf_init->module~m_mixing module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in), optional :: Comm Calls proc~~mixers_scf_init~~CallsGraph proc~mixers_scf_init mixers_scf_init proc~mixers_init mixers_init proc~mixers_scf_init->proc~mixers_init die die proc~mixers_scf_init->die proc~mix_method mix_method proc~mixers_scf_init->proc~mix_method proc~mixers_history_init mixers_history_init proc~mixers_scf_init->proc~mixers_history_init proc~mixers_reset mixers_reset proc~mixers_scf_init->proc~mixers_reset fdf_get fdf_get proc~mixers_scf_init->fdf_get proc~mix_method_variant mix_method_variant proc~mixers_scf_init->proc~mix_method_variant leqi leqi proc~mixers_scf_init->leqi proc~mixers_init->die proc~mixers_init->proc~mixers_history_init proc~mixers_init->proc~mixers_reset proc~mixers_init->fdf_get fdf_block fdf_block proc~mixers_init->fdf_block fdf_bnnames fdf_bnnames proc~mixers_init->fdf_bnnames fdf_bline fdf_bline proc~mixers_init->fdf_bline fdf_bnames fdf_bnames proc~mixers_init->fdf_bnames fdf_brewind fdf_brewind proc~mixers_init->fdf_brewind proc~mix_method->die proc~mix_method->leqi proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new delete delete proc~mixers_history_init->delete proc~mixers_reset->delete proc~mix_method_variant->leqi Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_init Source Code subroutine mixers_scf_init ( nspin , Comm ) use fdf use precision , only : dp #ifdef MPI use mpi_siesta , only : MPI_Comm_World #endif use m_mixing , only : mixers_reset , mixers_init use m_mixing , only : mix_method , mix_method_variant use m_mixing , only : mixer_init use m_mixing , only : mixers_history_init ! The number of spin-components integer , intent ( in ) :: nspin ! The communicator used for the mixer integer , intent ( in ), optional :: Comm ! Block constructs type ( block_fdf ) :: bfdf ! Get number of history steps integer :: n_hist , n_kick , n_restart , n_save real ( dp ) :: w , w_kick integer :: n_lin_after real ( dp ) :: w_lin_after logical :: lin_after ! number of history steps saved type ( tMixer ), pointer :: m integer :: nm , im , im2 , tmp logical :: is_broyden character ( len = 70 ) :: method , variant , opt ! If the mixers are denoted by a block, then ! the entire logic *MUST* be defined in the blocks opt = fdf_get ( 'SCF.Mix.Spin' , 'all' ) if ( leqi ( opt , 'all' ) ) then mix_spin = MIX_SPIN_ALL else if ( leqi ( opt , 'spinor' ) ) then mix_spin = MIX_SPIN_SPINOR else if ( leqi ( opt , 'sum' ) ) then mix_spin = MIX_SPIN_SUM else if ( leqi ( opt , 'sum+diff' ) ) then mix_spin = MIX_SPIN_SUM_DIFF else call die ( \"Unknown option given for SCF.Mix.Spin & &all|spinor|sum|sum+diff\" ) end if ! If there is only one spinor we should mix all... if ( nspin == 1 ) mix_spin = MIX_SPIN_ALL ! Initialize to ensure debug stuff read call mixers_init ( 'SCF' , scf_mixs , Comm = Comm ) ! Check for existance of the SCF.Mix block if ( associated ( scf_mixs ) ) then if ( size ( scf_mixs ) > 0 ) then return end if ! Something has gone wrong... ! The user has supplied a block, but ! haven't added any content to the block... ! However, we fall-back to the default mechanism end if ! ensure nullification call mixers_reset ( scf_mixs ) ! >>>*** FIRST ***<<< ! Read in compatibility options ! Figure out if we are dealing with ! Broyden or Pulay n_hist = fdf_get ( 'DM.NumberPulay' , 2 ) tmp = fdf_get ( 'DM.NumberBroyden' , 0 ) is_broyden = tmp > 0 if ( is_broyden ) then n_hist = tmp end if ! Define default mixing weight (used for ! Pulay, Broyden and linear mixing) w = fdf_get ( 'DM.MixingWeight' , 0.25_dp ) ! Default kick-options n_kick = fdf_get ( 'DM.NumberKick' , 0 ) w_kick = fdf_get ( 'DM.KickMixingWeight' , 0.5_dp ) lin_after = fdf_get ( 'SCF.LinearMixingAfterPulay' , . false .) w_lin_after = fdf_get ( 'SCF.MixingWeightAfterPulay' , w ) ! >>>*** END ***<<< ! Read options in new format ! Get history length n_hist = fdf_get ( 'SCF.Mixer.History' , n_hist ) ! update mixing weight and kick mixing weight w = fdf_get ( 'SCF.Mixer.Weight' , w ) n_kick = fdf_get ( 'SCF.Mixer.Kick' , n_kick ) w_kick = fdf_get ( 'SCF.Mixer.Kick.Weight' , w_kick ) ! Restart after this number of iterations n_restart = fdf_get ( 'SCF.Mixer.Restart' , 0 ) n_save = fdf_get ( 'SCF.Mixer.Restart.Save' , 1 ) ! negative savings are not allowed n_save = max ( 0 , n_save ) ! Get the variant of the mixing method if ( is_broyden ) then method = 'Broyden' else if ( n_hist > 0 ) then method = 'Pulay' else method = 'Linear' end if method = fdf_get ( 'SCF.Mixer.Method' , trim ( method )) variant = fdf_get ( 'SCF.Mixer.Variant' , 'original' ) ! Determine whether linear mixing should be ! performed after the \"advanced\" mixing n_lin_after = fdf_get ( 'SCF.Mixer.Linear.After' , - 1 ) w_lin_after = fdf_get ( 'SCF.Mixer.Linear.After.Weight' , w_lin_after ) ! Determine total number of mixers nm = 1 if ( n_lin_after >= 0 . or . lin_after ) nm = nm + 1 if ( n_kick > 0 ) nm = nm + 1 ! Initiailaze all mixers allocate ( scf_mixs ( nm )) scf_mixs (:)% w = w scf_mixs (:)% n_hist = n_hist scf_mixs (:)% restart = n_restart scf_mixs (:)% restart_save = n_save ! 1. Current mixing index im = 1 ! Store the advanced mixer index (for references to ! later mixers) im2 = im m => scf_mixs ( im ) m % name = method m % m = mix_method ( method ) m % v = mix_method_variant ( m % m , variant ) ! 2. Setup the linear mixing after the actual mixing if ( n_lin_after > 0 . or . lin_after ) then im = im + 1 m => scf_mixs ( im ) ! Signal to switch to this mixer after ! convergence scf_mixs ( im2 )% next_conv => m m % name = 'Linear-After' m % m = mix_method ( 'linear' ) m % w = w_lin_after m % n_itt = n_lin_after ! jump back to previous after having run a ! few iterations m % next => scf_mixs ( im2 ) end if ! In case we have a kick, apply the kick here ! This overrides the \"linear.after\" option if ( n_kick > 0 ) then im = im + 1 m => scf_mixs ( im ) m % name = 'Linear-Kick' m % n_itt = 1 m % n_hist = 0 m % m = mix_method ( 'linear' ) m % w = w_kick m % next => scf_mixs ( im2 ) ! set the default mixer to kick scf_mixs ( im2 )% n_itt = n_kick - 1 scf_mixs ( im2 )% next => m scf_mixs ( im2 )% restart = n_kick - 1 end if ! Correct the input do im = 1 , nm call mixer_init ( scf_mixs ( im ) ) end do ! Initialize the allocation of each mixer call mixers_history_init ( scf_mixs ) #ifdef MPI if ( present ( Comm ) ) then scf_mixs (:)% Comm = Comm else scf_mixs (:)% Comm = MPI_Comm_World end if #endif end subroutine mixers_scf_init","tags":"","loc":"proc/mixers_scf_init.html","title":"mixers_scf_init – SIESTA"},{"text":"public subroutine mixers_scf_print(nspin) Uses parallel m_mixing proc~~mixers_scf_print~~UsesGraph proc~mixers_scf_print mixers_scf_print module~parallel parallel proc~mixers_scf_print->module~parallel module~m_mixing m_mixing proc~mixers_scf_print->module~m_mixing module~precision precision module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Calls proc~~mixers_scf_print~~CallsGraph proc~mixers_scf_print mixers_scf_print proc~mixers_print mixers_print proc~mixers_scf_print->proc~mixers_print die die proc~mixers_scf_print->die proc~mixers_print->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_print Source Code subroutine mixers_scf_print ( nspin ) use parallel , only : IONode use m_mixing , only : mixers_print integer , intent ( in ) :: nspin ! Print mixing options call mixers_print ( 'SCF' , scf_mixs ) if ( IONode . and . nspin > 1 ) then select case ( mix_spin ) case ( MIX_SPIN_ALL ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'all' case ( MIX_SPIN_SPINOR ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'spinor' if ( nspin <= 2 ) then call die ( \"SCF.Mixer.Spin spinor option only valid for & &non-collinear and spin-orbit calculations\" ) end if case ( MIX_SPIN_SUM ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum' case ( MIX_SPIN_SUM_DIFF ) write ( * , '(a,t50,a)' ) 'mix.SCF: Spin-component mixing' , 'sum and diff' end select end if end subroutine mixers_scf_print","tags":"","loc":"proc/mixers_scf_print.html","title":"mixers_scf_print – SIESTA"},{"text":"public subroutine mixers_scf_print_block() Uses m_mixing proc~~mixers_scf_print_block~~UsesGraph proc~mixers_scf_print_block mixers_scf_print_block module~m_mixing m_mixing proc~mixers_scf_print_block->module~m_mixing module~precision precision module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_print_block~~CallsGraph proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_print_block mixers_print_block proc~mixers_scf_print_block->proc~mixers_print_block die die proc~mixers_print_block->die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_print_block Source Code subroutine mixers_scf_print_block ( ) use m_mixing , only : mixers_print_block ! Print mixing options call mixers_print_block ( 'SCF' , scf_mixs ) end subroutine mixers_scf_print_block","tags":"","loc":"proc/mixers_scf_print_block.html","title":"mixers_scf_print_block – SIESTA"},{"text":"public subroutine mixing_scf_converged(SCFconverged) Uses parallel proc~~mixing_scf_converged~~UsesGraph proc~mixing_scf_converged mixing_scf_converged module~parallel parallel proc~mixing_scf_converged->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name logical, intent(inout) :: SCFconverged Calls proc~~mixing_scf_converged~~CallsGraph proc~mixing_scf_converged mixing_scf_converged reset reset proc~mixing_scf_converged->reset Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixing_scf_converged~~CalledByGraph proc~mixing_scf_converged mixing_scf_converged proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixing_scf_converged Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixing_scf_converged Source Code subroutine mixing_scf_converged ( SCFconverged ) use parallel , only : IONode logical , intent ( inout ) :: SCFconverged integer :: i ! Return if no convergence if ( . not . SCFconverged ) return if ( associated ( scf_mix % next_conv ) ) then ! this means that we skip to the ! following algorithm scf_mix => scf_mix % next_conv SCFconverged = . false . if ( allocated ( scf_mix % stack ) ) then do i = 1 , size ( scf_mix % stack ) ! delete all but one history ! This should be fine call reset ( scf_mix % stack ( i ), - 1 ) end do end if if ( IONode ) then write ( * , '(/,2a)' ) ':!: SCF cycle continuation mixer: ' , & trim ( scf_mix % name ) end if end if end subroutine mixing_scf_converged","tags":"","loc":"proc/mixing_scf_converged.html","title":"mixing_scf_converged – SIESTA"},{"text":"public subroutine mixers_scf_reset() Uses m_mixing proc~~mixers_scf_reset~~UsesGraph proc~mixers_scf_reset mixers_scf_reset module~m_mixing m_mixing proc~mixers_scf_reset->module~m_mixing module~precision precision module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_reset~~CallsGraph proc~mixers_scf_reset mixers_scf_reset proc~mixers_reset mixers_reset proc~mixers_scf_reset->proc~mixers_reset delete delete proc~mixers_reset->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_reset Source Code subroutine mixers_scf_reset () use m_mixing , only : mixers_reset nullify ( scf_mix ) call mixers_reset ( scf_mixs ) end subroutine mixers_scf_reset","tags":"","loc":"proc/mixers_scf_reset.html","title":"mixers_scf_reset – SIESTA"},{"text":"public subroutine mixers_scf_history_init() Uses m_mixing proc~~mixers_scf_history_init~~UsesGraph proc~mixers_scf_history_init mixers_scf_history_init module~m_mixing m_mixing proc~mixers_scf_history_init->module~m_mixing module~precision precision module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~mixers_scf_history_init~~CallsGraph proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_history_init mixers_history_init proc~mixers_scf_history_init->proc~mixers_history_init new new proc~mixers_history_init->new proc~current_itt current_itt proc~mixers_history_init->proc~current_itt delete delete proc~mixers_history_init->delete Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mixers_scf_history_init~~CalledByGraph proc~mixers_scf_history_init mixers_scf_history_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~mixers_scf_history_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code mixers_scf_history_init Source Code subroutine mixers_scf_history_init ( ) use m_mixing , only : mixers_history_init call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) end subroutine mixers_scf_history_init","tags":"","loc":"proc/mixers_scf_history_init.html","title":"mixers_scf_history_init – SIESTA"},{"text":"public subroutine vmat(no, np, dvol, spin, V, nvmax, numVs, listVsptr, listVs, Vs, nuo, nuotot, iaorb, iphorb, isa) Uses precision atmfuncs atm_types atomlist t_spin listsc_module mesh meshdscf meshdscf meshphi parallel alloc parallelsubs proc~~vmat~~UsesGraph proc~vmat vmat listsc_module listsc_module proc~vmat->listsc_module module~precision precision proc~vmat->module~precision module~atmfuncs atmfuncs proc~vmat->module~atmfuncs meshphi meshphi proc~vmat->meshphi parallelsubs parallelsubs proc~vmat->parallelsubs alloc alloc proc~vmat->alloc atomlist atomlist proc~vmat->atomlist module~parallel parallel proc~vmat->module~parallel t_spin t_spin proc~vmat->t_spin module~mesh mesh proc~vmat->module~mesh meshdscf meshdscf proc~vmat->meshdscf module~atm_types atm_types proc~vmat->module~atm_types module~atmfuncs->module~precision module~atmfuncs->module~atm_types spher_harm spher_harm module~atmfuncs->spher_harm module~radial radial module~atmfuncs->module~radial module~sys sys module~atmfuncs->module~sys module~mesh->module~precision module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finds the matrix elements of the potential. First version written by P.\n Name and interface modified by J.M.Soler. May'95. Re-ordered so that mesh is the outer loop and the orbitals are\n handled as lower-half triangular. J.D.Gale and J.M.Soler, Feb'99 Version of vmat that uses a direct algorithm to save memory.\n Modified by J.D.Gale, November'99 Arguments Type Intent Optional Attributes Name integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of columns in C (local) real(kind=dp), intent(in) :: dvol Volume per mesh point type(tSpin), intent(in) :: spin Spin configuration real(kind=grid_p), intent(in) :: V (nsp,np,spin%Grid) Value of the potential at the mesh points integer, intent(in) :: nvmax First dimension of listV and Vs , and maximum\n number of nonzero elements in any row of Vs integer, intent(in) :: numVs (nuo) Number of non-zero elements in a row of Vs integer, intent(in) :: listVsptr (nuo) Pointer to the start of rows in listVs integer, intent(in) :: listVs (nvmax) List of non-zero elements of Vs real(kind=dp), target :: Vs (nvmax,spin%H) Value of nonzero elements in each row\n of Vs to which the potential matrix\n elements are summed up integer, intent(in) :: nuo integer, intent(in) :: nuotot integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms Calls proc~~vmat~~CallsGraph proc~vmat vmat phi phi proc~vmat->phi indxuo indxuo proc~vmat->indxuo listsc listsc proc~vmat->listsc endpht endpht proc~vmat->endpht re_alloc re_alloc proc~vmat->re_alloc listp2 listp2 proc~vmat->listp2 needdscfl needdscfl proc~vmat->needdscfl timer timer proc~vmat->timer listdlptr listdlptr proc~vmat->listdlptr matrixmtoo matrixmtoo proc~vmat->matrixmtoo lstpht lstpht proc~vmat->lstpht globaltolocalorb globaltolocalorb proc~vmat->globaltolocalorb de_alloc de_alloc proc~vmat->de_alloc numdl numdl proc~vmat->numdl proc~rcut rcut proc~vmat->proc~rcut listdl listdl proc~vmat->listdl proc~chk chk proc~rcut->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~vmat~~CalledByGraph proc~vmat vmat proc~dhscf dhscf proc~dhscf->proc~vmat proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code vmat Source Code subroutine vmat ( no , np , dvol , spin , V , nvmax , & numVs , listVsptr , listVs , Vs , & nuo , nuotot , iaorb , iphorb , isa ) !! author: P.Ordejon !! !! Finds the matrix elements of the potential. !! !! First version written by P. !! Name and interface modified by J.M.Soler. May'95. !! !! Re-ordered so that mesh is the outer loop and the orbitals are !! handled as lower-half triangular. J.D.Gale and J.M.Soler, Feb'99 !! !! Version of vmat that uses a direct algorithm to save memory. !! Modified by J.D.Gale, November'99 !  Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo use t_spin , only : tSpin use listsc_module , only : LISTSC use mesh , only : dxa , nsp , xdop , xdsp , meshLim use meshdscf , only : matrixMtoO use meshdscf , only : needdscfl , listdl , numdl , nrowsdscfl , listdlptr use meshphi , only : directphi , endpht , lstpht , listp2 , phi use parallel , only : Nodes , Node use alloc , only : re_alloc , de_alloc use parallelsubs , only : GlobalToLocalOrb #ifdef MPI use mpi_siesta #endif #ifdef _OPENMP use omp_lib #endif ! Argument types and dimensions integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of columns in C (local) integer , intent ( in ) :: nvmax !! First dimension of `listV` and `Vs`, and maximum !! number of nonzero elements in any row of `Vs` integer , intent ( in ) :: nuo integer , intent ( in ) :: nuotot integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs type ( tSpin ), intent ( in ) :: spin !! Spin configuration integer , intent ( in ) :: numVs ( nuo ) !! Number of non-zero elements in a row of `Vs` integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms integer , intent ( in ) :: listVsptr ( nuo ) !! Pointer to the start of rows in `listVs` integer , intent ( in ) :: listVs ( nvmax ) !! List of non-zero elements of `Vs` real ( grid_p ), intent ( in ) :: V ( nsp , np , spin % Grid ) !! Value of the potential at the mesh points real ( dp ), intent ( in ) :: dvol !! Volume per mesh point real ( dp ), target :: Vs ( nvmax , spin % H ) !! Value of nonzero elements in each row !! of `Vs` to which the potential matrix !! elements are summed up ! Internal variables and arrays integer , parameter :: minloc = 1000 ! Min buffer size integer , parameter :: maxoa = 100 ! Max # of orb/atom integer :: i , ia , ic , ii , ijl , il , imp , ind , iop integer :: ip , iphi , io , is , isp , ispin , iu , iul integer :: j , jc , jl , last , lasta , lastop integer :: maxloc , maxloc2 , nc , nlocal , nphiloc integer :: nvmaxl , triang , lenx , leny , lenz , lenxy ! Size of Hamiltonian logical :: ParallelLocal real ( dp ) :: Vij , r2sp , dxsp ( 3 ), VClocal ( nsp ) integer , pointer :: ilc (:), ilocal (:), iorb (:) real ( dp ), pointer :: DscfL (:,:), t_DscfL (:,:,:) real ( dp ), pointer :: Vss (:,:), t_Vss (:,:,:), Clocal (:,:) real ( dp ), pointer :: Vlocal (:,:), phia (:,:), r2cut (:) integer :: NTH , TID #ifdef _TRACE_ integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE vmat' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 4 ) #endif !   Start time counter call timer ( 'vmat' , 1 ) !   Find atomic cutoff radii nullify ( r2cut ) call re_alloc ( r2cut , 1 , nsmax , 'r2cut' , 'vmat' ) r2cut = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) end do !   Set algorithm logical ParallelLocal = ( Nodes > 1 ) lenx = meshLim ( 2 , 1 ) - meshLim ( 1 , 1 ) + 1 leny = meshLim ( 2 , 2 ) - meshLim ( 1 , 2 ) + 1 lenz = meshLim ( 2 , 3 ) - meshLim ( 1 , 3 ) + 1 lenxy = lenx * leny !   Find value of maxloc maxloc2 = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxloc = maxloc2 + minloc maxloc = min ( maxloc , no ) triang = ( maxloc + 1 ) * ( maxloc + 2 ) / 2 if ( ParallelLocal ) then if ( nrowsDscfL > 0 ) then nvmaxl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else nvmaxl = 1 end if end if !   Allocate local memory !$OMP parallel default(shared), & !$OMP&shared(NTH,t_DscfL,t_Vss,spin), & !$OMP&private(TID,last), & !$OMP&private(ip,nc,nlocal,ic,imp,i,il,iu,iul,ii,ind,j,ijl,ispin), & !$OMP&private(lasta,lastop,ia,is,iop,isp,dxsp,r2sp,nphiloc,iphi,jc,jl), & !$OMP&private(Vij,VClocal,DscfL,Vss,ilocal,ilc,iorb,Vlocal,Clocal,phia) !$OMP single #ifdef _OPENMP NTH = omp_get_num_threads ( ) #else NTH = 1 #endif !$OMP end single ! implicit barrier, IMPORTANT #ifdef _OPENMP TID = omp_get_thread_num ( ) + 1 #else TID = 1 #endif nullify ( Clocal , phia , ilocal , ilc , iorb , Vlocal ) !$OMP critical ! Perhaps the critical section is not needed, ! however it \"tells\" the OS to allocate per ! thread, possibly waiting for each thread to ! place the memory in the best position. allocate ( Clocal ( nsp , maxloc2 ) ) allocate ( ilocal ( no ) , ilc ( maxloc2 ) , iorb ( maxloc ) ) allocate ( Vlocal ( triang , spin % Grid ) ) if ( DirectPhi ) allocate ( phia ( maxoa , nsp ) ) !$OMP end critical !$OMP single if ( ParallelLocal ) then nullify ( t_DscfL ) call re_alloc ( t_DscfL , 1 , nvmaxl , 1 , spin % H , 1 , NTH , & 'DscfL' , 'vmat' ) else if ( NTH > 1 ) then nullify ( t_Vss ) call re_alloc ( t_Vss , 1 , nvmax , 1 , spin % H , 2 , NTH , & 'Vss' , 'vmat' ) end if end if !$OMP end single ! implicit barrier if ( ParallelLocal ) then DscfL => t_DscfL ( 1 : nvmaxl ,:, TID ) DscfL ( 1 : nvmaxl ,:) = 0._dp else if ( NTH > 1 ) then if ( TID == 1 ) then Vss => Vs else Vss => t_Vss ( 1 : nvmax ,:, TID ) Vss ( 1 : nvmax ,:) = 0._dp end if else Vss => Vs end if end if !   Full initializations done only once ilocal ( 1 : no ) = 0 iorb ( 1 : maxloc ) = 0 Vlocal ( 1 : triang ,:) = 0._dp last = 0 !   Loop over grid points !$OMP do do ip = 1 , np !      Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !      Find new required size of Vlocal nlocal = last do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) nlocal = nlocal + 1 end do !      If overflooded, add Vlocal to Vs and reinitialize it if ( nlocal > maxloc . and . last > 0 ) then if ( ParallelLocal ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + & Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = LISTSC ( i , iu , listdl ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + & Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do else do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) call GlobalToLocalOrb ( iu , Node , Nodes , iul ) if ( i == iu ) then do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = listVs ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = LISTSC ( i , iu , listVs ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do end if !         Reset local arrays do i = 1 , last ilocal ( iorb ( i )) = 0 end do iorb ( 1 : last ) = 0 ijl = ( last + 1 ) * ( last + 2 ) / 2 do ispin = 1 , spin % Grid do i = 1 , ijl Vlocal ( i , ispin ) = 0._dp end do end do last = 0 end if !      Look for required orbitals not yet in Vlocal if ( nlocal > last ) then do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) then last = last + 1 ilocal ( i ) = last iorb ( last ) = i end if end do end if !      Check algorithm if ( DirectPhi ) then lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !            Generate or retrieve phi values if ( ia /= lasta . or . iop /= lastop ) then lasta = ia lastop = iop is = isa ( ia ) do isp = 1 , nsp dxsp (:) = xdsp (:, isp ) + xdop (:, iop ) - dxa (:, ia ) r2sp = sum ( dxsp ** 2 ) if ( r2sp < r2cut ( is ) ) then !$OMP critical call all_phi ( is , + 1 , dxsp , nphiloc , phia (:, isp ) ) !$OMP end critical else phia (:, isp ) = 0.0_dp end if end do end if iphi = iphorb ( i ) Clocal (:, ic ) = phia ( iphi ,:) do ispin = 1 , spin % Grid !               Create VClocal for the first orbital of mesh point Vij = 0._dp do isp = 1 , nsp VClocal ( isp ) = V ( isp , ip , ispin ) * Clocal ( isp , ic ) !                  This is the jc == ic value Vij = Vij + VClocal ( isp ) * Clocal ( isp , ic ) end do !               ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij !               Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !                  Calculate ic * jc Vij = 0._dp do isp = 1 , nsp Vij = Vij + VClocal ( isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij * 2._dp else Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij end if end do end do end do else do ic = 1 , nc imp = endpht ( ip - 1 ) + ic il = ilocal ( lstpht ( imp )) ilc ( ic ) = il Clocal (:, ic ) = phi (:, imp ) do ispin = 1 , spin % Grid !               Create VClocal for the first orbital of mesh point Vij = 0._dp do isp = 1 , nsp VClocal ( isp ) = V ( isp , ip , ispin ) * Clocal ( isp , ic ) !                  This is the jc == ic value Vij = Vij + VClocal ( isp ) * Clocal ( isp , ic ) end do !               ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij !               Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !                  Calculate ic * jc Vij = 0._dp do isp = 1 , nsp Vij = Vij + VClocal ( isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij * 2._dp else Vlocal ( ijl , ispin ) = Vlocal ( ijl , ispin ) + Vij end if end do end do end do end if end do !$OMP end do nowait ! Note that this is already performed in parallel! !   Add final Vlocal to Vs if ( ParallelLocal . and . last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = LISTSC ( i , iu , listdl ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid DscfL ( ind , ispin ) = DscfL ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then DscfL ( ind , 7 : 8 ) = DscfL ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do else if ( last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) if ( i == iu ) then do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = listVs ( ind ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do else do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = LISTSC ( i , iu , listVs ( ind ) ) ijl = idx_ijl ( il , ilocal ( j )) do ispin = 1 , spin % Grid Vss ( ind , ispin ) = Vss ( ind , ispin ) + Vlocal ( ijl , ispin ) * dVol end do if ( spin % SO ) then Vss ( ind , 7 : 8 ) = Vss ( ind , 7 : 8 ) + Vlocal ( ijl , 3 : 4 ) * dVol end if end do end if end do end if !$OMP barrier if ( ParallelLocal . and . NTH > 1 ) then !$OMP do collapse(2) do ispin = 1 , spin % H do ind = 1 , nvmaxl do ii = 2 , NTH t_DscfL ( ind , ispin , 1 ) = t_DscfL ( ind , ispin , 1 ) + & t_DscfL ( ind , ispin , ii ) end do end do end do !$OMP end do else if ( NTH > 1 ) then !$OMP do collapse(2) do ispin = 1 , spin % H do ind = 1 , nvmax do ii = 2 , NTH Vs ( ind , ispin ) = Vs ( ind , ispin ) + t_Vss ( ind , ispin , ii ) end do end do end do !$OMP end do end if !   Free memory deallocate ( Clocal , ilocal , ilc , iorb , Vlocal ) if ( DirectPhi ) deallocate ( phia ) !$OMP master if ( ParallelLocal ) then !      Redistribute Hamiltonian from mesh to orbital based distribution DscfL => t_DscfL ( 1 : nvmaxl , 1 : spin % H , 1 ) call matrixMtoO ( nvmaxl , nvmax , numVs , listVsptr , nuo , & spin % H , DscfL , Vs ) call de_alloc ( t_DscfL , 'DscfL' , 'vmat' ) else if ( NTH > 1 ) then call de_alloc ( t_Vss , 'Vss' , 'vmat' ) end if !$OMP end master !$OMP end parallel call de_alloc ( r2cut , 'r2cut' , 'vmat' ) #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'vmat' , 2 ) #ifdef DEBUG call write_debug ( '    POS vmat' ) #endif contains ! In any case will the compiler most likely inline this ! small routine. So it should not pose any problem. pure function idx_ijl ( i , j ) result ( ij ) integer , intent ( in ) :: i , j integer :: ij if ( i > j ) then ij = i * ( i + 1 ) / 2 + j + 1 else ij = j * ( j + 1 ) / 2 + i + 1 end if end function idx_ijl end subroutine vmat","tags":"","loc":"proc/vmat.html","title":"vmat – SIESTA"},{"text":"public subroutine write_rho(fname, cell, mesh, nsm, maxp, nspin, rho) Writes the electron density or potential at the mesh points. Parallel modifications added, while maintaining independence\n of density matrix on disk from parallel distribution. Uses a\n block distribution for density matrix. It is important to\n remember that the density matrix is divided so that all\n sub-points reside on the same node. Modified by J.D.Gale March 1999. Note In order to achieve a consistent format of the disk file\n each record in the unformatted file corresponds to one pair of\n Y and Z values. Hence there will be a total of mesh(2) x mesh(3)\n records. Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname File name real(kind=dp), intent(in) :: cell (3,3) integer, intent(in) :: mesh (3) Number of mesh divisions of each lattice vector integer, intent(in) :: nsm Number of sub-mesh points per mesh point (not used in this version) integer, intent(in) :: maxp integer, intent(in) :: nspin real(kind=grid_p), intent(in) :: rho (maxp,nspin) Calls proc~~write_rho~~CallsGraph proc~write_rho write_rho io_close io_close proc~write_rho->io_close io_assign io_assign proc~write_rho->io_assign mpi_wait mpi_wait proc~write_rho->mpi_wait mpi_barrier mpi_barrier proc~write_rho->mpi_barrier de_alloc de_alloc proc~write_rho->de_alloc write_debug write_debug proc~write_rho->write_debug mpi_irecv mpi_irecv proc~write_rho->mpi_irecv Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~write_rho~~CalledByGraph proc~write_rho write_rho proc~dhscf dhscf proc~dhscf->proc~write_rho proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code write_rho Source Code subroutine write_rho ( fname , cell , mesh , nsm , maxp , nspin , rho ) !! author: J.Soler !! date: July 1997 !! !! Writes the electron density or potential at the mesh points. !! !! Parallel modifications added, while maintaining independence !! of density matrix on disk from parallel distribution. Uses a !! block distribution for density matrix. It is important to !! remember that the density matrix is divided so that all !! sub-points reside on the same node. !! !! Modified by J.D.Gale March 1999. !! !!@note !! In order to achieve a consistent format of the disk file !! each record in the unformatted file corresponds to one pair of !! Y and Z values. Hence there will be a total of mesh(2) x mesh(3) !! records. !!@endnote character ( len =* ), intent ( in ) :: fname !! File name integer , intent ( in ) :: nsm !! Number of sub-mesh points per mesh point !! (not used in this version) integer , intent ( in ) :: mesh ( 3 ) !! Number of mesh divisions of each lattice vector integer , intent ( in ) :: maxp integer , intent ( in ) :: nspin real ( grid_p ), intent ( in ) :: rho ( maxp , nspin ) real ( dp ), intent ( in ) :: cell ( 3 , 3 ) external io_assign , io_close , memory ! Internal variables and arrays integer i , ip , iu , is , j , np , . BlockSizeY , BlockSizeZ , ProcessorZ , . meshnsm ( 3 ), NRemY , NRemZ , . iy , iz , izm , Ind , ir #ifdef MPI integer Ind2 , MPIerror , Request , . Status ( MPI_Status_Size ), BNode , NBlock real ( grid_p ), pointer :: bdens (:) => null () #endif real ( sp ), pointer :: temp (:) => null () #ifdef MPI #ifdef DEBUG call write_debug ( 'ERROR: write_rho not ready yet' ) call write_debug ( fname ) #endif ! Work out density block dimensions if ( mod ( Nodes , ProcessorY ). gt . 0 ) $ call die ( 'ERROR: ProcessorY must be a factor of the' // $ ' number of processors!' ) ProcessorZ = Nodes / ProcessorY BlockSizeY = (((( mesh ( 2 ) / nsm ) - 1 ) / ProcessorY ) + 1 ) * nsm call re_alloc ( bdens , 1 , BlockSizeY * mesh ( 1 ), 'bdens' , 'write_rho' ) #else ProcessorZ = 1 #endif call re_alloc ( temp , 1 , mesh ( 1 ), 'temp' , 'write_rho' ) ! Open file if ( Node . eq . 0 ) then call io_assign ( iu ) open ( iu , file = fname , form = fform , status = 'unknown' ) endif np = mesh ( 1 ) * mesh ( 2 ) * mesh ( 3 ) meshnsm ( 1 ) = mesh ( 1 ) / nsm meshnsm ( 2 ) = mesh ( 2 ) / nsm meshnsm ( 3 ) = mesh ( 3 ) / nsm ! Write data if ( Node . eq . 0 ) then if ( fform . eq . 'formatted' ) then do i = 1 , 3 write ( iu , * ) ( cell ( j , i ), j = 1 , 3 ) enddo write ( iu , * ) mesh , nspin else write ( iu ) cell write ( iu ) mesh , nspin endif endif ! Outer loop over spins do is = 1 , nspin Ind = 0 ! Loop over Z dimension of processor grid do iz = 1 , ProcessorZ BlockSizeZ = ( meshnsm ( 3 ) / ProcessorZ ) NRemZ = meshnsm ( 3 ) - BlockSizeZ * ProcessorZ if ( iz - 1. lt . NRemZ ) BlockSizeZ = BlockSizeZ + 1 BlockSizeZ = BlockSizeZ * nsm ! Loop over local Z mesh points do izm = 1 , BlockSizeZ ! Loop over blocks in Y mesh direction do iy = 1 , ProcessorY ! Work out size of density sub-matrix to be transfered BlockSizeY = ( meshnsm ( 2 ) / ProcessorY ) NRemY = meshnsm ( 2 ) - BlockSizeY * ProcessorY if ( iy - 1. lt . NRemY ) BlockSizeY = BlockSizeY + 1 BlockSizeY = BlockSizeY * nsm #ifdef MPI NBlock = BlockSizeY * mesh ( 1 ) ! Work out which node block is stored on BNode = ( iy - 1 ) * ProcessorZ + iz - 1 if ( BNode . eq . 0. and . Node . eq . BNode ) then #endif ! If density sub-matrix is local Node 0 then just write it out if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY write ( iu , '(e15.6)' ) ( rho ( Ind + ip , is ), . ip = 1 , mesh ( 1 )) Ind = Ind + mesh ( 1 ) enddo else !AG** do ir = 1 , BlockSizeY temp ( 1 : mesh ( 1 )) = $ real ( rho ( Ind + 1 : Ind + mesh ( 1 ), is ), kind = sp ) write ( iu ) temp ( 1 : mesh ( 1 )) Ind = Ind + mesh ( 1 ) enddo endif #ifdef MPI elseif ( Node . eq . 0 ) then ! If this is Node 0 then recv and write density sub-matrix call MPI_IRecv ( bdens , NBlock , MPI_grid_real , BNode , 1 , . MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) elseif ( Node . eq . BNode ) then ! If this is the Node where the density sub-matrix is, then send call MPI_ISend ( rho ( Ind + 1 , is ), NBlock , MPI_grid_real , 0 , 1 , . MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) Ind = Ind + NBlock endif if ( BNode . ne . 0 ) then call MPI_Barrier ( MPI_Comm_World , MPIerror ) if ( Node . eq . 0 ) then Ind2 = 0 if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY write ( iu , '(e15.6)' ) ( bdens ( Ind2 + ip ), ip = 1 , . mesh ( 1 )) Ind2 = Ind2 + mesh ( 1 ) enddo else !AG** do ir = 1 , BlockSizeY temp ( 1 : mesh ( 1 )) = $ real ( bdens ( Ind2 + 1 : Ind2 + mesh ( 1 )), kind = sp ) write ( iu ) temp ( 1 : mesh ( 1 )) Ind2 = Ind2 + mesh ( 1 ) enddo endif endif endif #endif enddo enddo enddo enddo if ( Node . eq . 0 ) then ! Close file call io_close ( iu ) endif #ifdef MPI C Deallocate density buffer memory call de_alloc ( bdens , 'bdens' , 'write_rho' ) #endif call de_alloc ( temp , 'temp' , 'write_rho' ) end subroutine write_rho","tags":"","loc":"proc/write_rho.html","title":"write_rho – SIESTA"},{"text":"public subroutine read_rho(fname, cell, mesh, nsm, maxp, nspin, rho) Reads the electron density or potential at the mesh points. If the values of maxp or nspin on input are less than\n those required to copy the array rho from the file, the subrout      !ine\n stops. Use subroutine check_rho to find the right maxp and nspin . Note AG: The magnitudes are saved in SINGLE PRECISION (sp), regardless\n     of the internal precision used in the program. Historically,\n     the internal precision has been \"single\". Parallel modifications added, while maintaining independence\n of density matrix on disk from parallel distribution. Uses a\n block distribution for density matrix. It is important to\n remember that the density matrix is divided so that all\n sub-points reside on the same node. Modified by J.D.Gale March 1999. Note In order to achieve a consistent format of the disk file\n each record in the unformatted file corresponds to one pair of\n Y and Z values. Hence there will be a total of mesh(2) x mesh(3)\n records. Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname File name for input or output real(kind=dp), intent(out) :: cell (3,3) Lattice vectors integer, intent(out) :: mesh (3) Number of mesh divisions of each lattice vector integer, intent(in) :: nsm Number of sub-mesh points per mesh point (not used in this version) integer, intent(in) :: maxp First dimension of array rho integer, intent(in) :: nspin Second dimension of array rho real(kind=grid_p), intent(out) :: rho (maxp,nspin) Electron density Calls proc~~read_rho~~CallsGraph proc~read_rho read_rho mpi_isend mpi_isend proc~read_rho->mpi_isend io_close io_close proc~read_rho->io_close mpi_bcast mpi_bcast proc~read_rho->mpi_bcast io_assign io_assign proc~read_rho->io_assign mpi_wait mpi_wait proc~read_rho->mpi_wait howmanymeshpernode howmanymeshpernode proc~read_rho->howmanymeshpernode de_alloc de_alloc proc~read_rho->de_alloc mpi_barrier mpi_barrier proc~read_rho->mpi_barrier proc~die die proc~read_rho->proc~die mpi_allreduce mpi_allreduce proc~read_rho->mpi_allreduce proc~die->io_close proc~die->io_assign pxfflush pxfflush proc~die->pxfflush pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort cmlfinishfile cmlfinishfile proc~die->cmlfinishfile Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code read_rho Source Code subroutine read_rho ( fname , cell , mesh , nsm , maxp , nspin , rho ) !! author: J.Soler !! date: July 1997 !! !! Reads the electron density or potential at the mesh points. !! !! If the values of maxp or nspin on input are less than !! those required to copy the array rho from the file, the subroutine !! stops. Use subroutine [[check_rho(proc)]] !! to find the right `maxp` and `nspin`. !! !!@note !! AG: The magnitudes are saved in SINGLE PRECISION (sp), regardless !!     of the internal precision used in the program. Historically, !!     the internal precision has been \"single\". !!@endnote !! !! Parallel modifications added, while maintaining independence !! of density matrix on disk from parallel distribution. Uses a !! block distribution for density matrix. It is important to !! remember that the density matrix is divided so that all !! sub-points reside on the same node. !! !! Modified by J.D.Gale March 1999. !! !!@note !! In order to achieve a consistent format of the disk file !! each record in the unformatted file corresponds to one pair of !! Y and Z values. Hence there will be a total of mesh(2) x mesh(3) !! records. !!@endnote character ( len =* ), intent ( in ) :: fname !! File name for input or output integer , intent ( in ) :: nsm !! Number of sub-mesh points per mesh point !! (not used in this version) integer , intent ( in ) :: maxp !! First dimension of array rho integer , intent ( in ) :: nspin !! Second dimension of array rho integer , intent ( out ) :: mesh ( 3 ) !! Number of mesh divisions of each lattice vector real ( grid_p ), intent ( out ) :: rho ( maxp , nspin ) !! Electron density real ( dp ), intent ( out ) :: cell ( 3 , 3 ) !! Lattice vectors external io_assign , io_close , memory ! Internal variables and arrays integer ip , iu , is , np , ns , . BlockSizeY , BlockSizeZ , ProcessorZ , . meshnsm ( 3 ), npl , NRemY , NRemZ , . iy , iz , izm , Ind , ir #ifdef MPI integer Ind2 , MPIerror , Request , meshl ( 3 ), . Status ( MPI_Status_Size ), BNode , NBlock logical ltmp real ( grid_p ), pointer :: bdens (:) => null () #endif real ( sp ), pointer :: temp (:) => null () logical baddim , found #ifdef MPI ! Work out density block dimensions if ( mod ( Nodes , ProcessorY ). gt . 0 ) $ call die ( 'ERROR: ProcessorY must be a factor of the' // $ ' number of processors!' ) ProcessorZ = Nodes / ProcessorY BlockSizeY = (((( mesh ( 2 ) / nsm ) - 1 ) / ProcessorY ) + 1 ) * nsm call re_alloc ( bdens , 1 , BlockSizeY * mesh ( 1 ), 'bdens' , 'read_rho' ) #else ProcessorZ = 1 #endif ! Check if input file exists if ( Node . eq . 0 ) then inquire ( file = fname , exist = found ) if (. not . found ) call die ( \"Cannot find file \" // trim ( fname )) endif ! Open file if ( Node . eq . 0 ) then call io_assign ( iu ) open ( iu , file = fname , form = fform , status = 'old' ) ! Read cell vectors and number of points if ( fform . eq . 'formatted' ) then read ( iu , * ) cell read ( iu , * ) mesh , ns else read ( iu ) cell read ( iu ) mesh , ns endif endif call re_alloc ( temp , 1 , mesh ( 1 ), 'temp' , 'read_rho' ) #ifdef MPI call MPI_Bcast ( cell ( 1 , 1 ), 9 , MPI_double_precision , 0 , . MPI_Comm_World , MPIerror ) call MPI_Bcast ( mesh , 3 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) call MPI_Bcast ( ns , 1 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) #endif np = mesh ( 1 ) * mesh ( 2 ) * mesh ( 3 ) ! Get local dimensions meshnsm ( 1 ) = mesh ( 1 ) / nsm meshnsm ( 2 ) = mesh ( 2 ) / nsm meshnsm ( 3 ) = mesh ( 3 ) / nsm #ifdef MPI call HowManyMeshPerNode ( meshnsm , Node , Nodes , npl , meshl ) #else npl = np #endif ! Check dimensions baddim = . false . if ( ns . gt . nspin ) baddim = . true . if ( npl . gt . maxp ) baddim = . true . #ifdef MPI ! Globalise dimension check call MPI_AllReduce ( baddim , ltmp , 1 , MPI_logical , MPI_Lor , . MPI_Comm_World , MPIerror ) baddim = ltmp #endif if ( baddim ) then call die ( \"Dimensions of array rho too small in read_rho\" ) endif ! Outer loop over spins do is = 1 , ns Ind = 0 ! Loop over Z mesh direction do iz = 1 , ProcessorZ ! Work out number of mesh points in Z direction BlockSizeZ = ( meshnsm ( 3 ) / ProcessorZ ) NRemZ = meshnsm ( 3 ) - BlockSizeZ * ProcessorZ if ( iz - 1. lt . NRemZ ) BlockSizeZ = BlockSizeZ + 1 BlockSizeZ = BlockSizeZ * nsm ! Loop over local Z mesh points do izm = 1 , BlockSizeZ ! Loop over blocks in Y mesh direction do iy = 1 , ProcessorY ! Work out size of density sub-matrix to be transfered BlockSizeY = ( meshnsm ( 2 ) / ProcessorY ) NRemY = meshnsm ( 2 ) - BlockSizeY * ProcessorY if ( iy - 1. lt . NRemY ) BlockSizeY = BlockSizeY + 1 BlockSizeY = BlockSizeY * nsm #ifdef MPI NBlock = BlockSizeY * mesh ( 1 ) ! Work out which node block is stored on BNode = ( iy - 1 ) * ProcessorZ + iz - 1 if ( BNode . eq . 0. and . Node . eq . BNode ) then #endif ! If density sub-matrix is local Node 0 then just read it in if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY read ( iu , * ) ( rho ( Ind + ip , is ), ip = 1 , mesh ( 1 )) Ind = Ind + mesh ( 1 ) enddo else ! ! AG**: Check sizes of records here -- grid-precision issues !       Either agree on a \"single\" format for file storage, !       or put in some intelligence in all post-processors do ir = 1 , BlockSizeY read ( iu ) temp ( 1 : mesh ( 1 )) rho ( Ind + 1 : Ind + mesh ( 1 ), is ) = $ real ( temp ( 1 : mesh ( 1 )), kind = grid_p ) Ind = Ind + mesh ( 1 ) enddo endif #ifdef MPI elseif ( Node . eq . 0 ) then C If this is Node 0 then read and send density sub - matrix Ind2 = 0 if ( fform . eq . 'formatted' ) then do ir = 1 , BlockSizeY read ( iu , * ) ( bdens ( Ind2 + ip ), ip = 1 , mesh ( 1 )) Ind2 = Ind2 + mesh ( 1 ) enddo else !AG** do ir = 1 , BlockSizeY read ( iu ) temp ( 1 : mesh ( 1 )) bdens ( Ind2 + 1 : Ind2 + mesh ( 1 )) = $ real ( temp ( 1 : mesh ( 1 )), kind = grid_p ) Ind2 = Ind2 + mesh ( 1 ) enddo endif call MPI_ISend ( bdens , NBlock , MPI_grid_real , BNode , 1 , . MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) elseif ( Node . eq . BNode ) then ! If this is the Node where the density sub-matrix is, then receive call MPI_IRecv ( rho ( Ind + 1 , is ), NBlock , MPI_grid_real , . 0 , 1 , MPI_Comm_World , Request , MPIerror ) call MPI_Wait ( Request , Status , MPIerror ) Ind = Ind + NBlock endif if ( BNode . ne . 0 ) then call MPI_Barrier ( MPI_Comm_World , MPIerror ) endif #endif enddo enddo enddo enddo ! Close file if ( Node . eq . 0 ) then call io_close ( iu ) endif #ifdef MPI ! Deallocate density buffer memory call de_alloc ( bdens , 'bdens' , 'read_rho' ) #endif call de_alloc ( temp , 'temp' , 'read_rho' ) end subroutine read_rho","tags":"","loc":"proc/read_rho.html","title":"read_rho – SIESTA"},{"text":"public subroutine check_rho(fname, maxp, nspin, nsm, found, overflow) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname integer, intent(inout) :: maxp Required first dimension of array rho, equal to mesh(1)*mesh(2)*mesh(3) integer, intent(inout) :: nspin Number of spin polarizations (1 or 2) integer, intent(in) :: nsm Number of sub-mesh points per mesh point (not used in this version) logical, intent(out) :: found Were data found? logical, intent(out) :: overflow True if maxp or nspin were changed Calls proc~~check_rho~~CallsGraph proc~check_rho check_rho io_assign io_assign proc~check_rho->io_assign mpi_allreduce mpi_allreduce proc~check_rho->mpi_allreduce mpi_bcast mpi_bcast proc~check_rho->mpi_bcast howmanymeshpernode howmanymeshpernode proc~check_rho->howmanymeshpernode Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code check_rho Source Code subroutine check_rho ( fname , maxp , nspin , nsm , found , overflow ) character ( len =* ), intent ( in ) :: fname integer , intent ( in ) :: nsm !! Number of sub-mesh points per mesh point !! (not used in this version) integer , intent ( inout ) :: maxp !! Required first dimension of array rho, !! equal to `mesh(1)*mesh(2)*mesh(3)` integer , intent ( inout ) :: nspin !! Number of spin polarizations (1 or 2) logical , intent ( out ) :: found !! Were data found? logical , intent ( out ) :: overflow !! True if `maxp` or `nspin` were changed real ( dp ) :: cell ( 3 , 3 ) integer :: mesh ( 3 ) integer :: meshnsm ( 3 ), npl , ns , iu , np , npmax #ifdef MPI integer :: meshl ( 3 ) logical :: ltmp integer :: MPIerror #endif ! Check if input file exists if ( Node . eq . 0 ) then inquire ( file = fname , exist = found ) endif #ifdef MPI call MPI_Bcast ( found , 1 , MPI_logical , 0 , MPI_Comm_World , MPIerror ) #endif if (. not . found ) return ! Open file if ( Node . eq . 0 ) then call io_assign ( iu ) open ( iu , file = fname , form = fform , status = 'old' ) ! Read cell vectors and number of points if ( fform . eq . 'formatted' ) then read ( iu , * ) cell read ( iu , * ) mesh , ns else read ( iu ) cell read ( iu ) mesh , ns endif endif #ifdef MPI call MPI_Bcast ( cell ( 1 , 1 ), 9 , MPI_double_precision , 0 , . MPI_Comm_World , MPIerror ) call MPI_Bcast ( mesh , 3 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) call MPI_Bcast ( ns , 1 , MPI_integer , 0 , MPI_Comm_World , MPIerror ) #endif np = mesh ( 1 ) * mesh ( 2 ) * mesh ( 3 ) ! Get local dimensions meshnsm ( 1 ) = mesh ( 1 ) / nsm meshnsm ( 2 ) = mesh ( 2 ) / nsm meshnsm ( 3 ) = mesh ( 3 ) / nsm #ifdef MPI call HowManyMeshPerNode ( meshnsm , Node , Nodes , npl , meshl ) #else npl = np #endif ! Check dimensions overflow = . false . if ( ns . gt . nspin ) overflow = . true . if ( npl . gt . maxp ) overflow = . true . #ifdef MPI ! Globalise dimension check call MPI_AllReduce ( overflow , ltmp , 1 , MPI_logical , MPI_Lor , . MPI_Comm_World , MPIerror ) overflow = ltmp #endif if ( overflow ) then ! Some processor has npl > maxp ... #ifdef MPI ! Find largest value of npl call MPI_AllReduce ( npl , npmax , 1 , MPI_integer , MPI_Max , . MPI_Comm_World , MPIerror ) #else npmax = np #endif maxp = npmax nspin = ns endif if ( Node . eq . 0 ) call io_close ( iu ) end subroutine check_rho","tags":"","loc":"proc/check_rho.html","title":"check_rho – SIESTA"},{"text":"public subroutine siesta_analysis(relaxd) Uses band writewave writewave m_ksvinit m_ksv m_projected_DOS m_local_DOS m_pexsi_local_DOS m_pexsi_dos siesta_options units sparse_matrices sparse_matrices siesta_geom m_dhscf atomlist atomlist fdf writewave siesta_cml files files zmatrix kpoint_scf_m parallel parallel files m_energies m_steps m_ntm m_spin m_spin m_dipol m_eo m_forces m_gamma alloc basis_enthalpy m_partial_charges m_iodm_old m_siesta2wannier90 m_mpi_utils flook_siesta proc~~siesta_analysis~~UsesGraph proc~siesta_analysis siesta_analysis flook_siesta flook_siesta proc~siesta_analysis->flook_siesta m_partial_charges m_partial_charges proc~siesta_analysis->m_partial_charges sparse_matrices sparse_matrices proc~siesta_analysis->sparse_matrices module~units units proc~siesta_analysis->module~units m_projected_DOS m_projected_DOS proc~siesta_analysis->m_projected_DOS siesta_cml siesta_cml proc~siesta_analysis->siesta_cml module~parallel parallel proc~siesta_analysis->module~parallel m_energies m_energies proc~siesta_analysis->m_energies kpoint_scf_m kpoint_scf_m proc~siesta_analysis->kpoint_scf_m atomlist atomlist proc~siesta_analysis->atomlist m_steps m_steps proc~siesta_analysis->m_steps basis_enthalpy basis_enthalpy proc~siesta_analysis->basis_enthalpy m_iodm_old m_iodm_old proc~siesta_analysis->m_iodm_old siesta_geom siesta_geom proc~siesta_analysis->siesta_geom m_ksvinit m_ksvinit proc~siesta_analysis->m_ksvinit m_ntm m_ntm proc~siesta_analysis->m_ntm m_ksv m_ksv proc~siesta_analysis->m_ksv files files proc~siesta_analysis->files module~siesta_options siesta_options proc~siesta_analysis->module~siesta_options m_pexsi_dos m_pexsi_dos proc~siesta_analysis->m_pexsi_dos module~m_dhscf m_dhscf proc~siesta_analysis->module~m_dhscf band band proc~siesta_analysis->band m_local_DOS m_local_DOS proc~siesta_analysis->m_local_DOS m_spin m_spin proc~siesta_analysis->m_spin writewave writewave proc~siesta_analysis->writewave m_siesta2wannier90 m_siesta2wannier90 proc~siesta_analysis->m_siesta2wannier90 m_forces m_forces proc~siesta_analysis->m_forces alloc alloc proc~siesta_analysis->alloc m_gamma m_gamma proc~siesta_analysis->m_gamma fdf fdf proc~siesta_analysis->fdf m_eo m_eo proc~siesta_analysis->m_eo m_mpi_utils m_mpi_utils proc~siesta_analysis->m_mpi_utils m_dipol m_dipol proc~siesta_analysis->m_dipol zmatrix zmatrix proc~siesta_analysis->zmatrix m_pexsi_local_DOS m_pexsi_local_DOS proc~siesta_analysis->m_pexsi_local_DOS module~precision precision module~units->module~precision module~m_dhscf->module~precision module~m_dfscf m_dfscf module~m_dhscf->module~m_dfscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Check that we are converged in geometry,\n if strictly required,\n before carrying out any analysis. @code\n@endcode Arguments Type Intent Optional Attributes Name logical :: relaxd Calls proc~~siesta_analysis~~CallsGraph proc~siesta_analysis siesta_analysis eo eo proc~siesta_analysis->eo re_alloc re_alloc proc~siesta_analysis->re_alloc xa xa proc~siesta_analysis->xa scell scell proc~siesta_analysis->scell proc~dhscf dhscf proc~siesta_analysis->proc~dhscf siesta_write_energies siesta_write_energies proc~siesta_analysis->siesta_write_energies de_alloc de_alloc proc~siesta_analysis->de_alloc timer timer proc~siesta_analysis->timer siesta_write_forces siesta_write_forces proc~siesta_analysis->siesta_write_forces message message proc~siesta_analysis->message fdf_boolean fdf_boolean proc~siesta_analysis->fdf_boolean dipol dipol proc~siesta_analysis->dipol xa_last xa_last proc~siesta_analysis->xa_last print_spin print_spin proc~siesta_analysis->print_spin local_dos local_dos proc~siesta_analysis->local_dos scell_last scell_last proc~siesta_analysis->scell_last siesta_write_stress_pressure siesta_write_stress_pressure proc~siesta_analysis->siesta_write_stress_pressure pexsi_local_dos pexsi_local_dos proc~siesta_analysis->pexsi_local_dos projected_dos projected_dos proc~siesta_analysis->projected_dos slua_call slua_call proc~siesta_analysis->slua_call write_basis_enthalpy write_basis_enthalpy proc~siesta_analysis->write_basis_enthalpy barrier barrier proc~siesta_analysis->barrier bands bands proc~siesta_analysis->bands bk bk proc~siesta_analysis->bk optical optical proc~siesta_analysis->optical pexsi_dos pexsi_dos proc~siesta_analysis->pexsi_dos cmladdproperty cmladdproperty proc~siesta_analysis->cmladdproperty cmlstartmodule cmlstartmodule proc~siesta_analysis->cmlstartmodule outcoor outcoor proc~siesta_analysis->outcoor die die proc~siesta_analysis->die fdf_get fdf_get proc~siesta_analysis->fdf_get setup_wfs_list setup_wfs_list proc~siesta_analysis->setup_wfs_list siesta_write_positions siesta_write_positions proc~siesta_analysis->siesta_write_positions ksv_pol ksv_pol proc~siesta_analysis->ksv_pol ucell ucell proc~siesta_analysis->ucell ucell_last ucell_last proc~siesta_analysis->ucell_last read_spmatrix read_spmatrix proc~siesta_analysis->read_spmatrix wwave wwave proc~siesta_analysis->wwave proc~dhscf->re_alloc proc~dhscf->de_alloc proc~dhscf->timer elecs elecs proc~dhscf->elecs proc~forhar forhar proc~dhscf->proc~forhar ts_voltage ts_voltage proc~dhscf->ts_voltage mpi_barrier mpi_barrier proc~dhscf->mpi_barrier ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix volcel volcel proc~dhscf->volcel ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~bye bye proc~dhscf->proc~bye proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofdsp rhoofdsp proc~dhscf->rhoofdsp proc~die die proc~dhscf->proc~die get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field hartree_add hartree_add proc~dhscf->hartree_add proc~dfscf dfscf proc~dhscf->proc~dfscf proc~reord reord proc~dhscf->proc~reord compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata proc~vmat vmat proc~dhscf->proc~vmat mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce vacuum_level vacuum_level proc~dhscf->vacuum_level localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh write_debug write_debug proc~dhscf->write_debug proc~rhoofd rhoofd proc~dhscf->proc~rhoofd proc~write_rho write_rho proc~dhscf->proc~write_rho proc~forhar->re_alloc proc~forhar->de_alloc proc~forhar->jms_setmeshdistr proc~forhar->bsc_cellxc proc~forhar->proc~setmeshdistr proc~forhar->mymeshbox proc~forhar->interface~distmeshdata pxfflush pxfflush proc~bye->pxfflush cmlfinishfile cmlfinishfile proc~bye->cmlfinishfile mpi_finalize mpi_finalize proc~bye->mpi_finalize io_close io_close proc~die->io_close io_assign io_assign proc~die->io_assign proc~die->pxfflush proc~die->cmlfinishfile mpi_abort mpi_abort proc~die->mpi_abort pxfabort pxfabort proc~die->pxfabort proc~dfscf->re_alloc proc~dfscf->de_alloc proc~dfscf->timer proc~dfscf->proc~die endpht endpht proc~dfscf->endpht listp2 listp2 proc~dfscf->listp2 needdscfl needdscfl proc~dfscf->needdscfl numdl numdl proc~dfscf->numdl alloc_default alloc_default proc~dfscf->alloc_default indxuo indxuo proc~dfscf->indxuo listsc listsc proc~dfscf->listsc listdlptr listdlptr proc~dfscf->listdlptr lstpht lstpht proc~dfscf->lstpht globaltolocalorb globaltolocalorb proc~dfscf->globaltolocalorb matrixotom matrixotom proc~dfscf->matrixotom proc~rcut rcut proc~dfscf->proc~rcut dscfl dscfl proc~dfscf->dscfl listdl listdl proc~dfscf->listdl proc~reord->re_alloc proc~reord->de_alloc proc~reord->timer proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~vmat->re_alloc proc~vmat->de_alloc proc~vmat->timer proc~vmat->endpht proc~vmat->listp2 proc~vmat->needdscfl proc~vmat->numdl matrixmtoo matrixmtoo proc~vmat->matrixmtoo phi phi proc~vmat->phi proc~vmat->indxuo proc~vmat->listsc proc~vmat->listdlptr proc~vmat->lstpht proc~vmat->globaltolocalorb proc~vmat->proc~rcut proc~vmat->listdl proc~rhoofd->re_alloc proc~rhoofd->de_alloc proc~rhoofd->timer proc~rhoofd->proc~die proc~rhoofd->endpht proc~rhoofd->listp2 proc~rhoofd->needdscfl proc~rhoofd->numdl proc~rhoofd->phi proc~rhoofd->indxuo proc~rhoofd->listsc proc~rhoofd->listdlptr proc~rhoofd->lstpht proc~rhoofd->globaltolocalorb proc~rhoofd->matrixotom proc~rhoofd->proc~rcut proc~rhoofd->dscfl proc~rhoofd->listdl proc~write_rho->de_alloc proc~write_rho->mpi_barrier proc~write_rho->write_debug mpi_wait mpi_wait proc~write_rho->mpi_wait proc~write_rho->io_close proc~write_rho->io_assign mpi_irecv mpi_irecv proc~write_rho->mpi_irecv proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->timer proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->proc~die proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~die proc~distmeshdata_int->proc~boxintersection proc~chk chk proc~rcut->proc~chk proc~chk->proc~die var panprocsiesta_analysisCallsGraph = svgPanZoom('#procsiesta_analysisCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code siesta_analysis Source Code subroutine siesta_analysis ( relaxd ) USE band , only : nbk , bk , maxbk , bands USE writewave , only : nwk , wfk , wwave USE writewave , only : setup_wfs_list , wfs_filename USE m_ksvinit , only : nkpol , kpol , wgthpol use m_ksv USE m_projected_DOS , only : projected_DOS USE m_local_DOS , only : local_DOS #ifdef SIESTA__PEXSI USE m_pexsi_local_DOS , only : pexsi_local_DOS USE m_pexsi_dos , only : pexsi_dos #endif USE siesta_options use units , only : Debye , eV use sparse_matrices , only : maxnh , listh , listhptr , numh use sparse_matrices , only : H , S , Dscf , xijo use siesta_geom use m_dhscf , only : dhscf use atomlist , only : indxuo , iaorb , lastkb , lasto , datm , no_l , & iphkb , no_u , no_s , iza , iphorb , rmaxo , indxua use atomlist , only : qtot use fdf use writewave , only : wwave use siesta_cml use files , only : slabel use files , only : filesOut_t ! derived type for output file names use zmatrix , only : lUseZmatrix , write_zmatrix use kpoint_scf_m , only : kpoints_scf use parallel , only : IOnode use parallel , only : SIESTA_worker use files , only : label_length use m_energies use m_steps , only : final use m_ntm use m_spin , only : nspin , spinor_dim , h_spin_dim use m_spin , only : SpOrb , NonCol , SPpol , NoMagn use m_dipol use m_eo use m_forces , only : fa use m_gamma use alloc , only : re_alloc , de_alloc use basis_enthalpy , only : write_basis_enthalpy use m_partial_charges , only : want_partial_charges use m_iodm_old , only : read_spmatrix use m_siesta2wannier90 , only : siesta2wannier90 use m_mpi_utils , only : barrier #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_ANALYSIS #endif implicit none logical :: relaxd , getPSI , quenched_MD , found real ( dp ) :: dummy_str ( 3 , 3 ) real ( dp ) :: dummy_strl ( 3 , 3 ) real ( dp ) :: qspin ( 4 ) ! Local real ( dp ) :: polxyz ( 3 , nspin ) ! Autom., small real ( dp ) :: polR ( 3 , nspin ) ! Autom., small real ( dp ) :: qaux real ( dp ), pointer :: ebk (:,:,:) ! Band energies integer :: j , ix , ind , ik , io , ispin integer :: wfs_band_min , wfs_band_max real ( dp ) :: g2max , current_ef type ( filesOut_t ) :: filesOut ! blank output file names !-----------------------------------------------------------------------BEGIN if ( SIESTA_worker ) call timer ( \"Analysis\" , 1 ) !! Check that we are converged in geometry, !! if strictly required, !! before carrying out any analysis. !!@code quenched_MD = ( ( iquench > 0 ) . and . $ (( idyn . eq . 1 ) . or . ( idyn . eq . 3 )) ) if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( SIESTA_worker ) then ! For timing ops and associated barrier if ( GeometryMustConverge . and . (. not . relaxd )) then call message ( \"FATAL\" , $ \"GEOM_NOT_CONV: Geometry relaxation not converged\" ) call timer ( 'all' , 2 ) call timer ( 'all' , 3 ) call barrier () call die ( \"ABNORMAL_TERMINATION\" ) endif endif endif !!@endcode !     All the comments below assume that this compatibility option !     is not used. !     Note also that full compatibility cannot be guaranteed if (. not . compat_pre_v4_dynamics ) then !     This is a sanity safeguard: we reset the geometry (which might !     have been moved by the relaxation or MD routines) to the one used !     in the last computation of the electronic structure. !     See the comments below for explanation !$OMP parallel workshare default(shared) xa ( 1 : 3 , 1 : na_s ) = xa_last ( 1 : 3 , 1 : na_s ) ucell ( 1 : 3 , 1 : 3 ) = ucell_last ( 1 : 3 , 1 : 3 ) scell ( 1 : 3 , 1 : 3 ) = scell_last ( 1 : 3 , 1 : 3 ) !$OMP end parallel workshare endif ! zmatrix info reset?? if ( fdf_get ( \"Read-H-from-file\" ,. false .)) then if ( SIESTA_worker ) then call read_spmatrix ( maxnh , no_l , h_spin_dim , numh , . listhptr , listh , H , found , userfile = \"H_IN\" ) if (. not . found ) call die ( \"Could not find H_IN\" ) current_ef = ef ef = fdf_get ( \"Manual-Fermi-Level\" , current_ef , \"Ry\" ) endif endif #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.DOS\" ,. false .)) then call pexsi_dos ( no_u , no_l , spinor_dim , $ maxnh , numh , listhptr , listh , H , S , qtot , ef ) endif #endif ! section done by Siesta subset of nodes if ( SIESTA_worker ) then final = . true . if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'Finalization' ) endif #ifdef SIESTA__FLOOK ! Call lua right before doing the analysis, ! possibly changing some of the variables call slua_call ( LUA , LUA_ANALYSIS ) #endif ! !     NOTE that the geometry output by the following sections !     used to be that \"predicted\" for the next MD or relaxation step. !     This is now changed ! if ( IOnode ) then ! Print atomic coordinates ! This covers CG and MD-quench (verlet, pr), instances in which ! \"relaxd\" is meaningful if (( nmove . ne . 0 ) . or . quenched_MD ) then if ( relaxd ) then ! xa = xa_last ! The \"relaxation\" routines do not update ! the coordinates if relaxed, so this behavior is unchanged call outcoor ( ucell , xa , na_u , 'Relaxed' , . true . ) else ! Since xa = xa_last now, this will just repeat the ! last set of coordinates used, not the predicted ones. call outcoor ( ucell , xa , na_u , 'Final (unrelaxed)' , . true . ) endif endif ! This call will write xa_last to the .STRUCT_OUT file ! (again, since it has already been written by state_init), ! CML records of the latest processed structure, and ! possibly zmatrix info.  *** unmoved?? how? ! Note that the .STRUCT_NEXT_ITER file is produced ! in siesta_move for checkpointing of relaxations and MD runs. ! If all we want are the CML records (to satisfy some expectation ! of appearance in the \"finalization\" section, we might put the ! cml call explicitly and forget about the rest. if ( compat_pre_v4_dynamics ) then call siesta_write_positions ( moved = . true .) else call siesta_write_positions ( moved = . false .) endif ! ??  Clarify Zmatrix behavior **** if ( lUseZmatrix ) call write_Zmatrix ! Print unit cell (cell_last) for variable cell and server operation if ( varcel . or . ( idyn . eq . 8 )) call outcell ( ucell ) !------------------------------------------------------------------ ! It can be argued that these needed the xa_last coordinates ! all along !       Print coordinates in xmol format in a separate file if ( fdf_boolean ( 'WriteCoorXmol' ,. false .)) & call coxmol ( iza , xa , na_u ) !       Print coordinates in cerius format in a separate file if ( fdf_boolean ( 'WriteCoorCerius' ,. false .)) & call coceri ( iza , xa , ucell , na_u , sname ) !       Find interatomic distances (output in file BONDS_FINAL) call bonds ( ucell , na_u , isa , xa , & rmax_bonds , trim ( slabel ) // \".BONDS_FINAL\" ) endif ! IONode !--- end output of geometry information ! ! ! NOTE: In the following sections, wavefunction generation, computation !       of band structure, etc, are carried out using the last Hamiltonian !       generated in the SCF run for the last geometry considered. !   But, if xa /= xa_last, the computation of, say, bands, will use !      H phases which are not the same as those producing the final !      ground-state electronic structure. ! !    Also, since we have removed the replication (superx call) !      of \"moved\" coordinates !      into the auxiliary supercell from 'siesta_move' (recall that it is !      done always in state_init for every new geometry), the \"moved unit !      cell coordinates\" could coexist here with \"unmoved non-unit cell SC coords\", !      which is wrong. !      For all of the above, we should put here a sanity safeguard !        (if we have not done so already at the top of this routine) !        xa(1:3,1:na_s) = xa_last(1:3,1:na_s) !        ucell(1:3,1:3) = ucell_last(1:3,1:3) !        scell(1:3,1:3) = scell_last(1:3,1:3) !        DM and H issues ! !        Some of the routines that follow use H and S, and some use the DM. !        Those which use the DM should work with the final \"out\" DM for !        consistency. !        Those which use H,S should work with the latest diagonalized H,S pair. ! !      If mixing the DM during the scf loop we should avoid mixing it one more time !        after convergence (or restoring Dold) !        If mixing H, we should avoid mixing it one more time !        after convergence (and restoring Hold to have the exact H that generated the !        latest DM, although this is probably too much). !        See the logic in siesta_forces. !     Find and print wavefunctions at selected k-points !   This uses H,S, and xa if ( nwk . gt . 0 ) then wfs_filename = trim ( slabel ) // \".selected.WFSX\" if ( IONode ) print \"(a)\" , $ \"Writing WFSX for selected k-points in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , & nwk , & numh , listhptr , listh , H , S , Ef , xijo , indxuo , & nwk , wfk , no_u , gamma , occtol ) endif !   This uses H,S, and xa if ( write_coop ) then ! Output the wavefunctions for the kpoints in the SCF set ! Note that we allow both a band number and an energy range ! The user is responsible for using appropriate values. wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( kpoints_scf % N , no_u , & wfs_band_min , wfs_band_max , $ use_scf_weights = . true ., $ use_energy_window = . true .) wfs_filename = trim ( slabel ) // \".fullBZ.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for COOP/COHP in \" $ // trim ( wfs_filename ) call wwave ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . kpoints_scf % N , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . kpoints_scf % N , kpoints_scf % k , no_u , gamma , occtol ) endif !     Compute bands !   This uses H,S, and xa nullify ( ebk ) call re_alloc ( ebk , 1 , no_u , 1 , spinor_dim , 1 , maxbk , & 'ebk' , 'siesta_analysis' ) if ( nbk . gt . 0 ) then if ( IONode ) print \"(a)\" , \"Computing bands...\" getPSI = fdf_get ( 'WFS.Write.For.Bands' , . false .) if ( getPSI ) then wfs_filename = trim ( slabel ) // \".bands.WFSX\" if ( IONode ) print \"(a)\" , \"Writing WFSX for bands in \" $ // trim ( wfs_filename ) wfs_band_min = fdf_get ( \"WFS.BandMin\" , 1 ) wfs_band_max = fdf_get ( \"WFS.BandMax\" , no_u ) call setup_wfs_list ( nbk , no_u , wfs_band_min , wfs_band_max , $ use_scf_weights = . false ., $ use_energy_window = . false .) endif call bands ( no_s , h_spin_dim , spinor_dim , no_u , no_l , maxnh , . maxbk , . numh , listhptr , listh , H , S , Ef , xijo , indxuo , . . true ., nbk , bk , ebk , occtol , getPSI ) if ( IOnode ) then if ( writbk ) then write ( 6 , '(/,a,/,a4,a12)' ) & 'siesta: Band k vectors (Bohr**-1):' , 'ik' , 'k' do ik = 1 , nbk write ( 6 , '(i4,3f12.6)' ) ik , ( bk ( ix , ik ), ix = 1 , 3 ) enddo endif if ( writb ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Band energies (eV):' , 'ik' , 'is' , 'eps' do ispin = 1 , spinor_dim do ik = 1 , nbk write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin , ( ebk ( io , ispin , ik ) / eV , io = 1 , min ( 10 , no_u )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( ebk ( io , ispin , ik ) / eV , io = 11 , no_u ) enddo enddo endif endif endif !     Print eigenvalues if ( IOnode . and . writeig ) then if (( isolve . eq . SOLVE_DIAGON . or . & (( isolve . eq . SOLVE_MINIM ) . and . minim_calc_eigenvalues )) & . and . no_l . lt . 1000 ) then if ( h_spin_dim <= 2 ) then write ( 6 , '(/,a,/,a4,a3,a7)' ) & 'siesta: Eigenvalues (eV):' , 'ik' , 'is' , 'eps' do ik = 1 , kpoints_scf % N do ispin = 1 , spinor_dim write ( 6 , '(i4,i3,10f7.2)' ) & ik , ispin ,( eo ( io , ispin , ik ) / eV , io = 1 , min ( 10 , neigwanted )) if ( no_u . gt . 10 ) write ( 6 , '(7x,10f7.2)' ) & ( eo ( io , ispin , ik ) / eV , io = 11 , neigwanted ) enddo enddo else write ( 6 , '(/,a)' ) 'siesta: Eigenvalues (eV):' do ik = 1 , kpoints_scf % N write ( 6 , '(a,i6)' ) 'ik =' , ik write ( 6 , '(10f7.2)' ) & (( eo ( io , ispin , ik ) / eV , io = 1 , neigwanted ), ispin = 1 , 2 ) enddo endif write ( 6 , '(a,f15.6,a)' ) 'siesta: Fermi energy =' , ef / eV , ' eV' endif endif if ((( isolve . eq . SOLVE_DIAGON ). or . & (( isolve . eq . SOLVE_MINIM ). and . minim_calc_eigenvalues )) & . and . IOnode ) & call ioeig ( eo , ef , neigwanted , nspin , kpoints_scf % N , & no_u , spinor_dim , & kpoints_scf % N , kpoints_scf % k , kpoints_scf % w ) !   This uses H,S, and xa, as it diagonalizes them again call projected_DOS () !     Print program's energy decomposition and final forces if ( IOnode ) then call siesta_write_energies ( iscf = 0 , dDmax = 0._dp , dHmax = 0._dp ) ! final == .true. which makes the step counter irrelevant call siesta_write_forces ( - 1 ) call siesta_write_stress_pressure () call write_basis_enthalpy ( FreeE , FreeEHarris ) endif ! NOTE: Here, the spin polarization is computed using Dscf, which is !       a density matrix obtained after mixing the \"in\" and \"out\" !       DMs of the SCF run for the last geometry considered. !       This can be considered a feature or a bug. call print_spin ( qspin ) ! qspin returned for use below !     This uses the last computed dipole in dhscf during the scf cycle, !     which is in fact derived from the \"in\" DM. !     Perhaps this section should be moved after the call to dhscf below !     AND use the DM_out of the last step (but there might not be a call !     to dhscf if there are no files to output, and the computation of the !     charge density is expensive... !     Print electric dipole if ( shape . ne . 'bulk' ) then if ( IOnode ) then write ( 6 , '(/,a,3f12.6)' ) & 'siesta: Electric dipole (a.u.)  =' , dipol write ( 6 , '(a,3f12.6)' ) & 'siesta: Electric dipole (Debye) =' , & ( dipol ( ix ) / Debye , ix = 1 , 3 ) endif if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = dipol / Debye , & title = 'Electric dipole' , dictref = 'siesta:dipol' , . units = 'siestaUnits:Debye' ) endif !cml_p endif ! NOTE: The use of *_last geometries in the following sections !       guarantees that the analysis of the electronic structure !       is done for the geometry for which it was computed. !  BUT these routines need H,S, so H should not be mixed after !       convergence. !     Calculation of the bulk polarization using the Berry phase !     formulas by King-Smith and Vanderbilt if ( nkpol . gt . 0 . and . . not . bornz ) then if ( NonCol . or . SpOrb ) then if ( IOnode ) then write ( 6 , '(/a)' ) . 'siesta_analysis: bulk polarization implemented only for' write ( 6 , '(/a)' ) . 'siesta_analysis: paramagnetic or collinear spin runs' endif else call KSV_pol ( na_u , na_s , xa_last , rmaxo , scell_last , & ucell_last , no_u , no_l , no_s , nspin , qspin , & maxnh , nkpol , numh , listhptr , listh , & H , S , xijo , indxuo , isa , iphorb , & iaorb , lasto , shape , & nkpol , kpol , wgthpol , polR , polxyz ) endif endif !     Calculation of the optical conductivity call optical ( na_u , na_s , xa_last , scell_last , ucell_last , & no_u , no_l , no_s , nspin , qspin , & maxnh , numh , listhptr , listh , H , S , xijo , $ indxuo , ebk , ef , temp , & isa , iphorb , iphKB , lasto , lastkb , shape ) call de_alloc ( ebk , 'ebk' , 'siesta_analysis' ) !................................... ! !  NOTE: Dscf here might be the mixed one (see above). ! want_partial_charges = ( hirshpop . or . voropop ) . AND . $ (. not . partial_charges_at_every_geometry ) !     Save electron density and potential if ( saverho . or . savedrho . or . saverhoxc . or . & savevh . or . savevt . or . savevna . or . & savepsch . or . savetoch . or . & save_ebs_dens . or . & want_partial_charges ) then if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( save_ebs_dens ) filesOut % ebs_dens = trim ( slabel ) // '.EBS' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' g2max = g2cut dummy_str = 0.0 dummy_strl = 0.0 call dhscf ( nspin , no_s , iaorb , iphorb , no_l , . no_u , na_u , na_s , isa , xa_last , indxua , & ntm , 0 , 0 , 0 , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_str , fa , dummy_strl ) ! next to last argument is dummy here, ! as no forces are calculated ! todo: make all these optional endif C C Call the wannier90 interface here , as local_DOS destroys the DM ... C if ( w90_processing ) call siesta2wannier90 () C Find local density of states !  It needs H,S, and xa, as it diagonalizes them again !  NOTE: This call will obliterate Dscf !  It is better to put a explicit out argument for the partial DM computed. call local_DOS () ! In summary, it is better to: ! !   -- Avoid (or warn the user about) doing any analysis if the calculation is not converged !   -- Avoid mixing DM or H after scf convergence !   -- Set xa to xa_last at the top of this file. Write any \"next iter\" coordinate file !      in 'siesta_move' endif ! SIESTA_worker #ifdef SIESTA__PEXSI if ( fdf_get ( \"PEXSI.LDOS\" ,. false .)) then call pexsi_local_DOS () endif #endif if ( SIESTA_worker ) call timer ( \"Analysis\" , 2 ) !------------------------------------------------------------------------- END END subroutine siesta_analysis","tags":"","loc":"proc/siesta_analysis.html","title":"siesta_analysis – SIESTA"},{"text":"public subroutine compute_energies(iscf) Uses precision fdf siesta_options siesta_options sparse_matrices sparse_matrices sparse_matrices m_dhscf m_energies atomlist m_ntm m_spin parallel m_dipol siesta_geom m_rhog proc~~compute_energies~~UsesGraph proc~compute_energies compute_energies module~m_dhscf m_dhscf proc~compute_energies->module~m_dhscf module~precision precision proc~compute_energies->module~precision module~siesta_options siesta_options proc~compute_energies->module~siesta_options siesta_geom siesta_geom proc~compute_energies->siesta_geom fdf fdf proc~compute_energies->fdf sparse_matrices sparse_matrices proc~compute_energies->sparse_matrices atomlist atomlist proc~compute_energies->atomlist module~parallel parallel proc~compute_energies->module~parallel m_energies m_energies proc~compute_energies->m_energies module~m_rhog m_rhog proc~compute_energies->module~m_rhog m_dipol m_dipol proc~compute_energies->m_dipol m_spin m_spin proc~compute_energies->m_spin m_ntm m_ntm proc~compute_energies->m_ntm module~m_dhscf->module~precision module~m_dfscf m_dfscf module~m_dhscf->module~m_dfscf module~m_rhog->module~precision module~m_rhog->m_spin class_Fstack_Pair_dData1D class_Fstack_Pair_dData1D module~m_rhog->class_Fstack_Pair_dData1D class_dData1D class_dData1D module~m_rhog->class_dData1D class_Pair_dData1D class_Pair_dData1D module~m_rhog->class_Pair_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. This routine computes the Harris energy from E_KS(DM_in): Todo Edit formulas E_Harris = E_KS(DM_in) + Tr[H_in*(DM_out-DM_in)] and possibly E_KS(DM_out) as E_KS(DM_out) = Tr[H_in*DM_out] + E_HXC(DM_out) Note that E_KS(DM_in), as computed in setup_hamiltonian, is not\n variational if mixing the DM, as the kinetic energy term is\n using a DM (DM_in) which is not derived from wave-functions. It\n is also not consistent if we are using the charge density as\n primary variable for mixing. In this case the Enl and Ekin parts\n are computed with the previous step's DM, whereas the \"scf\" part\n comes from the (mixed) rho. E_KS(DM_out) computed in setup_hamiltonian with DM_out is\n variational. Rather than calling again setup_hamiltonian, it is\n enough to compute the \"scf\" part of the energy by calling dhscf\n with DM_out. The routine checks the mixing type and proceeds accordingly Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf Calls proc~~compute_energies~~CallsGraph proc~compute_energies compute_energies update_freee update_freee proc~compute_energies->update_freee update_dena update_dena proc~compute_energies->update_dena fdf_get fdf_get proc~compute_energies->fdf_get update_etot update_etot proc~compute_energies->update_etot Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~compute_energies~~CalledByGraph proc~compute_energies compute_energies proc~siesta_forces siesta_forces proc~siesta_forces->proc~compute_energies Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code compute_energies Source Code subroutine compute_energies ( iscf ) !! This routine computes the Harris energy from E_KS(DM_in): !! !!@todo !! Edit formulas !!@endtodo !! !!    E_Harris = E_KS(DM_in) + Tr[H_in*(DM_out-DM_in)] !! !! and possibly E_KS(DM_out) as !! !!    E_KS(DM_out) = Tr[H_in*DM_out] + E_HXC(DM_out) !! !! Note that E_KS(DM_in), as computed in setup_hamiltonian, is not !! variational if mixing the DM, as the kinetic energy term is !! using a DM (DM_in) which is not derived from wave-functions. It !! is also not consistent if we are using the charge density as !! primary variable for mixing. In this case the Enl and Ekin parts !! are computed with the previous step's DM, whereas the \"scf\" part !! comes from the (mixed) rho. !! !! E_KS(DM_out) computed in setup_hamiltonian with DM_out is !! variational. Rather than calling again setup_hamiltonian, it is !! enough to compute the \"scf\" part of the energy by calling dhscf !! with DM_out. !! !! The routine checks the mixing type and proceeds accordingly use precision , only : dp use fdf , only : fdf_get use siesta_options , only : g2cut , Temp use siesta_options , only : mix_charge , mixH use sparse_matrices , only : listh , listhptr , numh , maxnh use sparse_matrices , only : H use sparse_matrices , only : Dscf , Dold use m_dhscf , only : dhscf use m_energies use atomlist , only : no_u , iaorb , iphkb , qtot , indxuo , datm , & lastkb , no_s , rmaxv , indxua , iphorb , lasto , & rmaxo , no_l use m_ntm , only : ntm use m_spin , only : spin use parallel , only : IONode use m_dipol , only : dipol use siesta_geom , only : na_u , na_s , xa , isa use m_rhog , only : rhog #ifdef MPI use m_mpi_utils , only : globalize_sum #endif integer , intent ( in ) :: iscf integer :: ispin , io #ifdef MPI real ( dp ) :: buffer1 #endif logical :: mixDM mixDM = (. not . ( mixH . or . mix_charge )) !     Compute the band-structure energy call compute_EBS () ! These energies were calculated in the latest call to ! setup_hamiltonian, using as ingredient D_in ! Ecorrec comes from O(N)... ! DUext is the energy of the charge from DM_in in a field. ! Emad, Emm, Emeta are extra terms that are added for ! consistency of the total energy. ! E0 = Ena + Ekin + Enl + Eso - Eions call update_DEna () call update_Etot () ! Harris energy ! It does not make sense if mixing the charge density, as there is no DM_in ! If mixing the Hamiltonian its usefulness is questionable also. if ( mix_charge ) then ! possibly add mixH here EHarrs = 0.0_dp else call compute_DEharr () Eharrs = Etot + DEharr endif ! Possible correction to Etot if mixing the DM. This is purely ! cosmetic, to show uniformly converged values during the scf ! cycle. The final energy will be completely correct if the DM is not ! mixed after scf convergence. ! The correction is mandatory if mixing the charge density. In this ! case the call to dhscf is needed to generate rho(G)_out ! If mixing H the KS energy is already variational, as it is ! computed with DM_out if ( mixDM ) then if ( fdf_get ( \"SCF.Want.Variational.EKS\" ,. false .)) then call compute_correct_EKS () endif else if ( mixH ) then ! not needed else if ( mix_charge ) then call compute_correct_EKS () endif call update_FreeE ( Temp ) CONTAINS subroutine compute_EBS () real ( dp ) :: Ebs_SO ( 4 ) complex ( dp ) :: Ebs_Daux ( 2 , 2 ), Ebs_Haux ( 2 , 2 ) integer :: i , j , ind Ebs = 0.0_dp ! Modifed for Off-Site Spin-orbit coupling by R. Cuadrado, Feb. 2018 ! !***************************************************************************** !  Note about Ebs and E_Harris calculation for the Spin-Orbit: !***************************************************************************** ! !  E_bs and E_Harris are calculated by means of the following ! complex matrix multiplication: ! !                 E_bs = Re { Tr[ H * DM ] } !             E_Harris = Re { Tr[ H * (DM-DM_old) ] } ! !  In the following: DM/H(1,1) --> up/up     <--> uu !                    DM/H(2,2) --> down/down <--> dd !                    DM/H(1,2) --> up/down   <--> ud !                    DM/H(2,1) --> down/up   <--> du ! !  Using DM/H components, E_bs would be sum(E_bs(1:4)), where ! !   E_bs(1)=Re{sum_ij(H_ij(1,1)*D_ji(1,1))}=Re{sum_ij(H_ij&#94;uu*(DM_ij&#94;uu)&#94;*)} !   E_bs(2)=Re{sum_ij(H_ij(2,2)*D_ji(2,2))}=Re{sum_ij(H_ij&#94;dd*(DM_ij&#94;dd)&#94;*)} !   E_bs(3)=Re{sum_ij(H_ij(1,2)*D_ji(2,1))}=Re{sum_ij(H_ij&#94;ud*(DM_ij&#94;ud)&#94;*)} !   E_bs(4)=Re{sum_ij(H_ij(2,1)*D_ji(1,2))}=Re{sum_ij(H_ij&#94;du*(DM_ij&#94;du)&#94;*)} ! !         since, due to overall hermiticity,  DM_ij&#94;ab = (DM_ji&#94;ba)&#94;* ! !   The trace operation is then an extended dot product over the \"ij\" !   sparse index, which can also be conveniently done in parallel, as !   each processor handles the same indexes in H and the DM. Only a !   global reduction is needed at the end. !  Same comments are valid for the E_Harris calculation. ! !***************************************************************************** ! if ( spin % SO ) then Ebs_SO = 0.0_dp Ebs_Daux = dcmplx ( 0.0_dp , 0.0_dp ) Ebs_Haux = dcmplx ( 0.0_dp , 0.0_dp ) do io = 1 , maxnh Ebs_Haux ( 1 , 1 ) = dcmplx ( H ( io , 1 ), H ( io , 5 )) Ebs_Haux ( 2 , 2 ) = dcmplx ( H ( io , 2 ), H ( io , 6 )) Ebs_Haux ( 1 , 2 ) = dcmplx ( H ( io , 3 ), - H ( io , 4 )) Ebs_Haux ( 2 , 1 ) = dcmplx ( H ( io , 7 ), H ( io , 8 )) Ebs_Daux ( 1 , 1 ) = dcmplx ( Dscf ( io , 1 ), Dscf ( io , 5 )) Ebs_Daux ( 2 , 2 ) = dcmplx ( Dscf ( io , 2 ), Dscf ( io , 6 )) Ebs_Daux ( 1 , 2 ) = dcmplx ( Dscf ( io , 3 ), - Dscf ( io , 4 )) Ebs_Daux ( 2 , 1 ) = dcmplx ( Dscf ( io , 7 ), Dscf ( io , 8 )) Ebs_SO ( 1 ) = Ebs_SO ( 1 ) + real ( Ebs_Haux ( 1 , 1 ) * dconjg ( Ebs_Daux ( 1 , 1 )) ) Ebs_SO ( 2 ) = Ebs_SO ( 2 ) + real ( Ebs_Haux ( 2 , 2 ) * dconjg ( Ebs_Daux ( 2 , 2 )) ) Ebs_SO ( 3 ) = Ebs_SO ( 3 ) + real ( Ebs_Haux ( 1 , 2 ) * dconjg ( Ebs_Daux ( 1 , 2 )) ) Ebs_SO ( 4 ) = Ebs_SO ( 4 ) + real ( Ebs_Haux ( 2 , 1 ) * dconjg ( Ebs_Daux ( 2 , 1 )) ) enddo Ebs = sum ( Ebs_SO ) else if ( spin % NCol ) then do io = 1 , maxnh Ebs = Ebs + H ( io , 1 ) * ( Dscf ( io , 1 ) ) & + H ( io , 2 ) * ( Dscf ( io , 2 ) ) & + 2.0_dp * H ( io , 3 ) * ( Dscf ( io , 3 ) ) & + 2.0_dp * H ( io , 4 ) * ( Dscf ( io , 4 ) ) enddo else if ( spin % Col ) then do io = 1 , maxnh Ebs = Ebs + H ( io , 1 ) * Dscf ( io , 1 ) & + H ( io , 2 ) * Dscf ( io , 2 ) enddo else if ( spin % none ) then do io = 1 , maxnh Ebs = Ebs + H ( io , 1 ) * Dscf ( io , 1 ) enddo endif #ifdef MPI !     Global reduction call globalize_sum ( Ebs , buffer1 ) Ebs = buffer1 #endif end subroutine compute_EBS subroutine compute_DEharr () real ( dp ) :: DEharr_SO ( 4 ) complex ( dp ) :: DEharr_Daux ( 2 , 2 ), DEharr_Haux ( 2 , 2 ), DEharr_Daux_old ( 2 , 2 ) DEharr = 0.0_dp if ( spin % SO ) then DEharr_SO = 0.0_dp DEharr_Daux = dcmplx ( 0.0_dp , 0.0_dp ) DEharr_Daux_old = dcmplx ( 0.0_dp , 0.0_dp ) DEharr_Haux = dcmplx ( 0.0_dp , 0.0_dp ) do io = 1 , maxnh DEharr_Haux ( 1 , 1 ) = dcmplx ( H ( io , 1 ), H ( io , 5 ) ) DEharr_Haux ( 2 , 2 ) = dcmplx ( H ( io , 2 ), H ( io , 6 ) ) DEharr_Haux ( 1 , 2 ) = dcmplx ( H ( io , 3 ), - H ( io , 4 ) ) DEharr_Haux ( 2 , 1 ) = dcmplx ( H ( io , 7 ), H ( io , 8 ) ) DEharr_Daux ( 1 , 1 ) = dcmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ) ) DEharr_Daux ( 2 , 2 ) = dcmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ) ) DEharr_Daux ( 1 , 2 ) = dcmplx ( Dscf ( io , 3 ), - Dscf ( io , 4 ) ) DEharr_Daux ( 2 , 1 ) = dcmplx ( Dscf ( io , 7 ), Dscf ( io , 8 ) ) DEharr_Daux_old ( 1 , 1 ) = dcmplx ( Dold ( io , 1 ), Dold ( io , 5 ) ) DEharr_Daux_old ( 2 , 2 ) = dcmplx ( Dold ( io , 2 ), Dold ( io , 6 ) ) DEharr_Daux_old ( 1 , 2 ) = dcmplx ( Dold ( io , 3 ), - Dold ( io , 4 ) ) DEharr_Daux_old ( 2 , 1 ) = dcmplx ( Dold ( io , 7 ), Dold ( io , 8 ) ) DEharr_SO ( 1 ) = DEharr_SO ( 1 ) & + real ( DEharr_Haux ( 1 , 1 ) * dconjg ( DEharr_Daux ( 1 , 1 ) - DEharr_Daux_old ( 1 , 1 )) ) DEharr_SO ( 2 ) = DEharr_SO ( 2 ) & + real ( DEharr_Haux ( 2 , 2 ) * dconjg ( DEharr_Daux ( 2 , 2 ) - DEharr_Daux_old ( 2 , 2 )) ) DEharr_SO ( 3 ) = DEharr_SO ( 3 ) & + real ( DEharr_Haux ( 1 , 2 ) * dconjg ( DEharr_Daux ( 1 , 2 ) - DEharr_Daux_old ( 1 , 2 )) ) DEharr_SO ( 4 ) = DEharr_SO ( 4 ) & + real ( DEharr_Haux ( 2 , 1 ) * dconjg ( DEharr_Daux ( 2 , 1 ) - DEharr_Daux_old ( 2 , 1 )) ) enddo DEharr = sum ( DEharr_SO ) else if ( spin % NCol ) then do io = 1 , maxnh DEharr = DEharr + H ( io , 1 ) * ( Dscf ( io , 1 ) - Dold ( io , 1 ) ) & + H ( io , 2 ) * ( Dscf ( io , 2 ) - Dold ( io , 2 ) ) & + 2.0_dp * H ( io , 3 ) * ( Dscf ( io , 3 ) - Dold ( io , 3 ) ) & + 2.0_dp * H ( io , 4 ) * ( Dscf ( io , 4 ) - Dold ( io , 4 ) ) enddo elseif ( spin % Col ) then do io = 1 , maxnh DEharr = DEharr + H ( io , 1 ) * ( Dscf ( io , 1 ) - Dold ( io , 1 ) ) & + H ( io , 2 ) * ( Dscf ( io , 2 ) - Dold ( io , 2 ) ) enddo elseif ( spin % none ) then do io = 1 , maxnh DEharr = DEharr + H ( io , 1 ) * ( Dscf ( io , 1 ) - Dold ( io , 1 ) ) enddo endif #ifdef MPI !     Global reduction of DEharr call globalize_sum ( DEharr , buffer1 ) DEharr = buffer1 #endif end subroutine compute_DEharr subroutine compute_correct_EKS () use files , only : filesOut_t ! derived type for output file names use class_dSpData1D , only : val use class_dSpData2D , only : val use class_zSpData2D , only : val use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_so_on_2D , H_so_off_2D type ( filesOut_t ) :: filesOut ! blank output file names real ( dp ), pointer :: H_vkb (:), H_kin (:), H_so_on (:,:) complex ( dp ), pointer :: H_so_off (:,:) complex ( dp ) :: Hc , Dc real ( dp ) :: dummy_stress ( 3 , 3 ), dummy_fa ( 1 , 1 ) real ( dp ) :: dummy_E , g2max , dummy_H ( 1 , 1 ) integer :: ihmat , ifa , istr ! Compute E_KS(DM_out) g2max = g2cut ifa = 0 istr = 0 ihmat = 0 ! Pass DM_out to compute E_HXC(out) ! Remove unwanted arguments... call dhscf ( spin % Grid , no_s , iaorb , iphorb , no_l , & no_u , na_u , na_s , isa , xa , indxua , & ntm , ifa , istr , ihmat , filesOut , & maxnh , numh , listhptr , listh , Dscf , Datm , & maxnh , dummy_H , Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , dummy_stress , dummy_fa , dummy_stress ) ! (when mix_charge is true, rhog will contain the output rho(G)) call update_DEna () !     Compute Tr[H_0*DM_out] = Ekin + Enl + Eso with DM_out H_vkb => val ( H_vkb_1D ) H_kin => val ( H_kin_1D ) Ekin = 0.0_dp Enl = 0.0_dp do ispin = 1 , spin % spinor do io = 1 , maxnh Ekin = Ekin + H_kin ( io ) * Dscf ( io , ispin ) Enl = Enl + H_vkb ( io ) * Dscf ( io , ispin ) enddo enddo #ifdef MPI ! Global reduction of Ekin, Enl call globalize_sum ( Ekin , buffer1 ) Ekin = buffer1 call globalize_sum ( Enl , buffer1 ) Enl = buffer1 #endif Eso = 0._dp if ( spin % SO_offsite ) then H_so_off => val ( H_so_off_2D ) ! The computation of the trace is different here, as H_so_off has ! a different structure from H and the DM. do io = 1 , maxnh !-------- Eso(u,u) Dc = cmplx ( Dscf ( io , 1 ), Dscf ( io , 5 ), dp ) Eso = Eso + real ( H_so_off ( io , 1 ) * Dc , dp ) !-------- Eso(d,d) Dc = cmplx ( Dscf ( io , 2 ), Dscf ( io , 6 ), dp ) Eso = Eso + real ( H_so_off ( io , 2 ) * Dc , dp ) !-------- Eso(u,d) Dc = cmplx ( Dscf ( io , 3 ), Dscf ( io , 4 ), dp ) Eso = Eso + real ( H_so_off ( io , 4 ) * Dc , dp ) !-------- Eso(d,u) Dc = cmplx ( Dscf ( io , 7 ), - Dscf ( io , 8 ), dp ) Eso = Eso + real ( H_so_off ( io , 3 ) * Dc , dp ) end do else if ( spin % SO_onsite ) then ! Sadly some compilers (g95), does ! not allow bounds for pointer assignments :( H_so_on => val ( H_so_on_2D ) do io = 1 , maxnh Eso = Eso + H_so_on ( io , 1 ) * Dscf ( io , 7 ) + H_so_on ( io , 2 ) * Dscf ( io , 8 ) & + H_so_on ( io , 5 ) * Dscf ( io , 3 ) + H_so_on ( io , 6 ) * Dscf ( io , 4 ) & - H_so_on ( io , 3 ) * Dscf ( io , 5 ) - H_so_on ( io , 4 ) * Dscf ( io , 6 ) end do end if #ifdef MPI if ( spin % SO ) then ! Global reduction of Eso call globalize_sum ( Eso , buffer1 ) Eso = buffer1 endif #endif ! E0 = Ena + Ekin + Enl + Eso - Eions ! Clarify: Ecorrec (from O(N)) call update_Etot () end subroutine compute_correct_EKS end subroutine compute_energies","tags":"","loc":"proc/compute_energies.html","title":"compute_energies – SIESTA"},{"text":"public subroutine message(level, str) Uses parallel proc~~message~~UsesGraph proc~message message module~parallel parallel proc~message->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Prints a message string if node==0 Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: level One of INFO, WARNING, FATAL character(len=*), intent(in) :: str Calls proc~~message~~CallsGraph proc~message message io_assign io_assign proc~message->io_assign pxfflush pxfflush proc~message->pxfflush io_close io_close proc~message->io_close Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~message~~CalledByGraph proc~message message proc~state_init state_init proc~state_init->proc~message proc~check_cohp check_cohp proc~state_init->proc~check_cohp proc~check_cohp->proc~message proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code message Source Code subroutine message ( level , str ) !! Prints a message string if `node==0` use parallel , only : Node character ( len =* ), intent ( in ) :: level !! One of INFO, WARNING, FATAL character ( len =* ), intent ( in ) :: str external :: io_assign , io_close integer :: lun if ( Node . eq . 0 ) then write ( 6 , '(a)' ) trim ( str ) write ( 0 , '(a)' ) trim ( str ) call io_assign ( lun ) open ( lun , file = \"MESSAGES\" , status = \"unknown\" , $ position = \"append\" , action = \"write\" ) write ( lun , \"(a)\" ) trim ( level ) // \": \" // trim ( str ) call io_close ( lun ) call pxfflush ( 6 ) call pxfflush ( 0 ) endif end subroutine message","tags":"","loc":"proc/message.html","title":"message – SIESTA"},{"text":"public subroutine die(str) Uses parallel siesta_cml mpi_siesta proc~~die~~UsesGraph proc~die die module~parallel parallel proc~die->module~parallel mpi_siesta mpi_siesta proc~die->mpi_siesta siesta_cml siesta_cml proc~die->siesta_cml Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Prints an error message and calls MPI_Abort Arguments Type Intent Optional Attributes Name character(len=*), intent(in), optional :: str Calls proc~~die~~CallsGraph proc~die die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~die~~CalledByGraph proc~die die proc~dhscf_init dhscf_init proc~dhscf_init->proc~die proc~rcore RCORE proc~dhscf_init->proc~rcore interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata proc~rcut rcut proc~dhscf_init->proc~rcut proc~rhooda rhooda proc~dhscf_init->proc~rhooda proc~state_init state_init proc~state_init->proc~die proc~chk chk proc~chk->proc~die proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->proc~die proc~dfscf dfscf proc~dfscf->proc~die proc~dfscf->proc~rcut proc~distextmeshdata distExtMeshData proc~distextmeshdata->proc~die proc~zetafio ZETAFIO proc~zetafio->proc~die proc~zetafio->proc~chk proc~kbproj_gindex kbproj_gindex proc~kbproj_gindex->proc~die proc~kbproj_gindex->proc~chk proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->proc~die proc~dhscf dhscf proc~dhscf->proc~die proc~dhscf->proc~dfscf proc~rhoofd rhoofd proc~dhscf->proc~rhoofd proc~dhscf->interface~distmeshdata proc~vmat vmat proc~dhscf->proc~vmat proc~forhar forhar proc~dhscf->proc~forhar proc~initmeshdistr initMeshDistr proc~initmeshdistr->proc~die proc~distmeshdata_int distMeshData_int proc~distmeshdata_int->proc~die proc~rhoofd->proc~die proc~rhoofd->proc~rcut proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~die proc~setup_hamiltonian->proc~dhscf proc~read_rho read_rho proc~read_rho->proc~die proc~all_phi all_phi proc~all_phi->proc~die proc~all_phi->proc~chk proc~floating floating proc~all_phi->proc~floating proc~nkbfis NKBFIS proc~nkbfis->proc~chk proc~psch psch proc~psch->proc~chk proc~psch->proc~floating proc~uion UION proc~uion->proc~chk proc~labelfis LABELFIS proc~labelfis->proc~chk proc~zvalfis ZVALFIS proc~zvalfis->proc~chk proc~nkbl_func NKBL_FUNC proc~nkbl_func->proc~chk proc~atmpopfio ATMPOPFIO proc~atmpopfio->proc~chk proc~rcore->proc~chk proc~lmxkbfis LMXKBFIS proc~lmxkbfis->proc~chk proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~setup_h0->proc~uion proc~cnfigfio CNFIGFIO proc~cnfigfio->proc~chk proc~massfis MASSFIS proc~massfis->proc~chk proc~lofio LOFIO proc~lofio->proc~chk proc~vna_gindex vna_gindex proc~vna_gindex->proc~chk proc~lomaxfis LOMAXFIS proc~lomaxfis->proc~chk proc~nztfl NZTFL proc~nztfl->proc~chk proc~vna_sub vna_sub proc~vna_sub->proc~chk proc~vna_sub->proc~floating proc~ldau_gindex ldau_gindex proc~ldau_gindex->proc~chk proc~chcore_sub chcore_sub proc~chcore_sub->proc~chk proc~chcore_sub->proc~floating proc~mofio MOFIO proc~mofio->proc~chk proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf interface~distmeshdata->proc~distmeshdata_rea interface~distmeshdata->proc~distmeshdata_int proc~psover psover proc~psover->proc~chk proc~psover->proc~floating proc~nofis NOFIS proc~nofis->proc~chk proc~orb_gindex orb_gindex proc~orb_gindex->proc~chk proc~izofis IZOFIS proc~izofis->proc~chk proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init proc~siesta_forces->proc~setup_hamiltonian proc~symfio SYMFIO proc~symfio->proc~chk proc~symfio->proc~lofio proc~symfio->proc~mofio proc~rcut->proc~chk proc~rchlocal RCHLOCAL proc~rchlocal->proc~chk proc~delk delk proc~delk->proc~rcut proc~vmat->proc~rcut proc~rhooda->proc~rcut proc~phiatm phiatm proc~rhooda->proc~phiatm proc~floating->proc~izofis proc~forhar->interface~distmeshdata proc~phiatm->proc~floating proc~delk_wrapper delk_wrapper proc~delk_wrapper->proc~delk proc~rphiatm rphiatm proc~rphiatm->proc~floating var panprocdieCalledByGraph = svgPanZoom('#procdieCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code die Source Code subroutine die ( str ) !! Prints an error message and calls MPI_Abort use parallel , only : Node use siesta_cml #ifdef MPI use mpi_siesta #endif character ( len =* ), intent ( in ), optional :: str external :: io_assign , io_close integer :: lun #ifdef MPI integer MPIerror #endif ! Even though formally (in MPI 1.X), only the master node ! can do I/O, in those systems that allow it having each ! node state its complaint can be useful. !                                        if (Node.eq.0) then if ( present ( str )) then write ( 6 , '(a)' ) trim ( str ) write ( 0 , '(a)' ) trim ( str ) endif write ( 6 , '(a,i4)' ) 'Stopping Program from Node: ' , Node write ( 0 , '(a,i4)' ) 'Stopping Program from Node: ' , Node !                                        endif if ( Node . eq . 0 ) then call io_assign ( lun ) open ( lun , file = \"MESSAGES\" , status = \"unknown\" , $ position = \"append\" , action = \"write\" ) write ( lun , \"(a)\" ) 'FATAL: ' // trim ( str ) call io_close ( lun ) call pxfflush ( 6 ) call pxfflush ( 0 ) If ( cml_p ) Then Call cmlFinishFile ( mainXML ) Endif !cml_p endif #ifdef MPI call MPI_Abort ( MPI_Comm_World , 1 , MPIerror ) stop #else call pxfabort () #endif end subroutine die","tags":"","loc":"proc/die.html","title":"die – SIESTA"},{"text":"public subroutine bye(str) Uses parallel siesta_cml mpi_siesta proc~~bye~~UsesGraph proc~bye bye module~parallel parallel proc~bye->module~parallel mpi_siesta mpi_siesta proc~bye->mpi_siesta siesta_cml siesta_cml proc~bye->siesta_cml Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Prints an error message and calls MPI_Finalize Arguments Type Intent Optional Attributes Name character(len=*), intent(in), optional :: str Calls proc~~bye~~CallsGraph proc~bye bye pxfflush pxfflush proc~bye->pxfflush mpi_finalize mpi_finalize proc~bye->mpi_finalize cmlfinishfile cmlfinishfile proc~bye->cmlfinishfile Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~bye~~CalledByGraph proc~bye bye proc~dhscf dhscf proc~dhscf->proc~bye proc~state_init state_init proc~state_init->proc~bye proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~bye proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~bye proc~siesta_forces->proc~state_init proc~siesta_forces->proc~setup_hamiltonian proc~compute_dm compute_dm proc~siesta_forces->proc~compute_dm proc~compute_dm->proc~bye proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code bye Source Code subroutine bye ( str ) !! Prints an error message and calls MPI_Finalize use parallel , only : Node use siesta_cml #ifdef MPI use mpi_siesta #endif character ( len =* ), intent ( in ), optional :: str #ifdef MPI integer rc #endif if ( Node . eq . 0 ) then if ( present ( str )) then write ( 6 , '(a)' ) trim ( str ) endif write ( 6 , '(a)' ) 'Requested End of Run. Bye!!' call pxfflush ( 6 ) If ( cml_p ) Then Call cmlFinishFile ( mainXML ) Endif !cml_p endif #ifdef MPI call MPI_Finalize ( rc ) stop #else stop #endif end subroutine bye","tags":"","loc":"proc/bye.html","title":"bye – SIESTA"},{"text":"private function rad_rvals(func) result(r) Uses alloc proc~~rad_rvals~~UsesGraph proc~rad_rvals rad_rvals alloc alloc proc~rad_rvals->alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name type( rad_func ), intent(in) :: func Return Value real(kind=dp),\n  dimension(:),pointer Calls proc~~rad_rvals~~CallsGraph proc~rad_rvals rad_rvals re_alloc re_alloc proc~rad_rvals->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rad_rvals Source Code function rad_rvals ( func ) result ( r ) use alloc , only : re_alloc implicit none real ( dp ), dimension (:), pointer :: r type ( rad_func ), intent ( in ) :: func integer i nullify ( r ) if ( func % n . eq . 0 ) return !      allocate(r(func%n)) call re_alloc ( r , 1 , func % n , 'r' , 'rad_rvals' ) do i = 1 , func % n r ( i ) = func % delta * ( i - 1 ) enddo end function rad_rvals","tags":"","loc":"proc/rad_rvals.html","title":"rad_rvals – SIESTA"},{"text":"public subroutine reset_rad_func(func) Arguments Type Intent Optional Attributes Name type( rad_func ) :: func Contents Source Code reset_rad_func Source Code subroutine reset_rad_func ( func ) implicit none type ( rad_func ) :: func func % n = 0 nullify ( func % f ) nullify ( func % d2 ) end subroutine reset_rad_func","tags":"","loc":"proc/reset_rad_func.html","title":"reset_rad_func – SIESTA"},{"text":"public subroutine rad_alloc(func, n) Uses alloc proc~~rad_alloc~~UsesGraph proc~rad_alloc rad_alloc alloc alloc proc~rad_alloc->alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Sets the 'size' n of the arrays and allocates f and d2. Arguments Type Intent Optional Attributes Name type( rad_func ), intent(inout) :: func integer, intent(in) :: n Calls proc~~rad_alloc~~CallsGraph proc~rad_alloc rad_alloc re_alloc re_alloc proc~rad_alloc->re_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rad_alloc~~CalledByGraph proc~rad_alloc rad_alloc proc~radial_read_ascii radial_read_ascii proc~radial_read_ascii->proc~rad_alloc Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rad_alloc Source Code subroutine rad_alloc ( func , n ) !! Sets the 'size' n of the arrays and allocates f and d2. use alloc , only : re_alloc implicit none type ( rad_func ), intent ( inout ) :: func integer , intent ( in ) :: n func % n = n nullify ( func % f , func % d2 ) call re_alloc ( func % f , 1 , n , 'func%f' , 'rad_alloc' ) call re_alloc ( func % d2 , 1 , n , 'func%d2' , 'rad_alloc' ) !      allocate(func%f(n),func%d2(n)) end subroutine rad_alloc","tags":"","loc":"proc/rad_alloc.html","title":"rad_alloc – SIESTA"},{"text":"public subroutine rad_get(func, r, fr, dfdr) Arguments Type Intent Optional Attributes Name type( rad_func ), intent(in) :: func real(kind=dp), intent(in) :: r real(kind=dp), intent(out) :: fr real(kind=dp), intent(out) :: dfdr Calls proc~~rad_get~~CallsGraph proc~rad_get rad_get splint splint proc~rad_get->splint Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rad_get~~CalledByGraph proc~rad_get rad_get proc~psch psch proc~psch->proc~rad_get proc~rphiatm rphiatm proc~rphiatm->proc~rad_get proc~vna_sub vna_sub proc~vna_sub->proc~rad_get proc~psover psover proc~psover->proc~rad_get proc~chcore_sub chcore_sub proc~chcore_sub->proc~rad_get proc~phiatm phiatm proc~phiatm->proc~rad_get proc~all_phi all_phi proc~all_phi->proc~rad_get proc~rhooda rhooda proc~rhooda->proc~phiatm proc~dhscf_init dhscf_init proc~dhscf_init->proc~rhooda proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rad_get Source Code subroutine rad_get ( func , r , fr , dfdr ) type ( rad_func ), intent ( in ) :: func real ( dp ), intent ( in ) :: r real ( dp ), intent ( out ) :: fr real ( dp ), intent ( out ) :: dfdr if ( func % n . eq . 0 ) then fr = 0._dp dfdr = 0._dp else call splint ( func % delta , func % f , func % d2 , func % n , r , fr , dfdr ) endif end subroutine rad_get","tags":"","loc":"proc/rad_get.html","title":"rad_get – SIESTA"},{"text":"public subroutine rad_setup_d2(func, yp1, ypn) Set up second derivative in a radial function Arguments Type Intent Optional Attributes Name type( rad_func ), intent(inout) :: func real(kind=dp), intent(in) :: yp1 real(kind=dp), intent(in) :: ypn Calls proc~~rad_setup_d2~~CallsGraph proc~rad_setup_d2 rad_setup_d2 spline spline proc~rad_setup_d2->spline Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rad_setup_d2~~CalledByGraph proc~rad_setup_d2 rad_setup_d2 proc~radial_read_ascii radial_read_ascii proc~radial_read_ascii->proc~rad_setup_d2 Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rad_setup_d2 Source Code subroutine rad_setup_d2 ( func , yp1 , ypn ) !! Set up second derivative in a radial function type ( rad_func ), intent ( inout ) :: func real ( dp ), intent ( in ) :: yp1 , ypn if ( func % n . eq . 0 ) return call spline ( func % delta , func % f , func % n , yp1 , ypn , func % d2 ) end subroutine rad_setup_d2","tags":"","loc":"proc/rad_setup_d2.html","title":"rad_setup_d2 – SIESTA"},{"text":"public subroutine rad_zero(func) Arguments Type Intent Optional Attributes Name type( rad_func ), intent(inout) :: func Contents Source Code rad_zero Source Code subroutine rad_zero ( func ) type ( rad_func ), intent ( inout ) :: func func % n = 0 end subroutine rad_zero","tags":"","loc":"proc/rad_zero.html","title":"rad_zero – SIESTA"},{"text":"public subroutine radial_read_ascii(op, lun, yp1, ypn) Arguments Type Intent Optional Attributes Name type( rad_func ) :: op integer :: lun real(kind=dp), intent(in) :: yp1 real(kind=dp), intent(in) :: ypn Calls proc~~radial_read_ascii~~CallsGraph proc~radial_read_ascii radial_read_ascii proc~rad_alloc rad_alloc proc~radial_read_ascii->proc~rad_alloc proc~rad_setup_d2 rad_setup_d2 proc~radial_read_ascii->proc~rad_setup_d2 re_alloc re_alloc proc~rad_alloc->re_alloc spline spline proc~rad_setup_d2->spline Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code radial_read_ascii Source Code subroutine radial_read_ascii ( op , lun , yp1 , ypn ) type ( rad_func ) :: op real ( dp ), intent ( in ) :: yp1 , ypn integer lun integer j , npts real ( dp ) dummy read ( lun , * ) npts , op % delta , op % cutoff call rad_alloc ( op , npts ) do j = 1 , npts read ( lun , * ) dummy , op % f ( j ) enddo call rad_setup_d2 ( op , yp1 , ypn ) end subroutine radial_read_ascii","tags":"","loc":"proc/radial_read_ascii.html","title":"radial_read_ascii – SIESTA"},{"text":"public subroutine radial_dump_ascii(op, lun, header) Arguments Type Intent Optional Attributes Name type( rad_func ) :: op integer :: lun logical, intent(in), optional :: header Contents Source Code radial_dump_ascii Source Code subroutine radial_dump_ascii ( op , lun , header ) type ( rad_func ) :: op integer :: lun logical , intent ( in ), optional :: header integer :: j logical :: print_header ! !     The standard dump is to unit \"lun\" !     and includes a header with npts, delta, and cutoff ! print_header = . true . if ( present ( header )) then print_header = header endif ! if ( print_header ) then write ( lun , '(i4,2g22.12,a)' ) op % n , $ op % delta , op % cutoff , \" # npts, delta, cutoff\" endif do j = 1 , op % n write ( lun , '(2g22.12)' ) ( j - 1 ) * op % delta , op % f ( j ) enddo end subroutine radial_dump_ascii","tags":"","loc":"proc/radial_dump_ascii.html","title":"radial_dump_ascii – SIESTA"},{"text":"public subroutine radial_dump_xml(op, lun) Arguments Type Intent Optional Attributes Name type( rad_func ) :: op integer :: lun Calls proc~~radial_dump_xml~~CallsGraph proc~radial_dump_xml radial_dump_xml str str proc~radial_dump_xml->str Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code radial_dump_xml Source Code subroutine radial_dump_xml ( op , lun ) type ( rad_func ) :: op integer lun integer j write ( lun , '(a)' ) '<radfunc>' call xml_dump_element ( lun , 'npts' , str ( op % n )) call xml_dump_element ( lun , 'delta' , str ( op % delta )) call xml_dump_element ( lun , 'cutoff' , str ( op % cutoff )) write ( lun , '(a)' ) '<data>' do j = 1 , op % n write ( lun , '(2g22.12)' ) ( j - 1 ) * op % delta , op % f ( j ) enddo write ( lun , '(a)' ) '</data>' write ( lun , '(a)' ) '</radfunc>' end subroutine radial_dump_xml","tags":"","loc":"proc/radial_dump_xml.html","title":"radial_dump_xml – SIESTA"},{"text":"public subroutine dfscf(ifa, istr, na, no, nuo, nuotot, np, nspin, indxua, isa, iaorb, iphorb, maxnd, numd, listdptr, listd, Dscf, Datm, Vscf, Vatm, dvol, VolCel, Fal, Stressl) Uses precision atmfuncs atm_types atomlist listsc_module mesh meshphi meshdscf meshdscf meshdscf alloc parallel sys parallelsubs proc~~dfscf~~UsesGraph proc~dfscf dfscf listsc_module listsc_module proc~dfscf->listsc_module module~precision precision proc~dfscf->module~precision module~atmfuncs atmfuncs proc~dfscf->module~atmfuncs meshphi meshphi proc~dfscf->meshphi parallelsubs parallelsubs proc~dfscf->parallelsubs alloc alloc proc~dfscf->alloc atomlist atomlist proc~dfscf->atomlist module~parallel parallel proc~dfscf->module~parallel module~mesh mesh proc~dfscf->module~mesh meshdscf meshdscf proc~dfscf->meshdscf module~sys sys proc~dfscf->module~sys module~atm_types atm_types proc~dfscf->module~atm_types module~atmfuncs->module~precision module~atmfuncs->module~sys module~atmfuncs->module~atm_types spher_harm spher_harm module~atmfuncs->spher_harm module~radial radial module~atmfuncs->module~radial module~mesh->module~precision module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Adds the SCF contribution to atomic forces and stress. Written by P.Ordejon, J.M.Soler, and J.Gale. Last modification by J.M.Soler, October 2000. Modifed for Off-Site Spin-orbit coupling by R. Cuadrado, Feb. 2018 Arguments Type Intent Optional Attributes Name integer, intent(in) :: ifa Are forces required? ( 1 =yes, 0 =no) integer, intent(in) :: istr Is stress required? ( 1 =yes, 0 =no) integer, intent(in) :: na Number of atoms integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: nuo Number of orbitals in unit cell (local) integer, intent(in) :: nuotot Number of orbitals in unit cell (global) integer, intent(in) :: np Number of mesh points (total is nsp*np) integer, intent(in) :: nspin spin%Grid (i.e., min(nspin,4) ) nspin=1 => Unpolarized, nspin=2 => polarized nspin=4 => Noncollinear spin/SOC integer, intent(in) :: indxua (na) Index of equivalent atom in unit cell integer, intent(in) :: isa (na) Species index of each atom integer, intent(in) :: iaorb (no) Atom to which orbitals belong integer, intent(in) :: iphorb (no) Index of orbital within its atom integer, intent(in) :: maxnd First dimension of Dscf integer, intent(in) :: numd (nuo) Number of nonzero elemts in each row of Dscf integer, intent(in) :: listdptr (nuo) Pointer to start of row in listd integer, intent(in) :: listd (maxnd) List of nonzero elements of Dscf real(kind=dp), intent(in), target :: Dscf (:,:) Dscf(maxnd,*) : Value of nonzero elemens of density matrix.\n It is used in \"hermitified\" here. real(kind=dp), intent(in) :: Datm (nuotot) real(kind=grid_p), intent(in) :: Vscf (nsp,np,nspin) Value of SCF potential at the mesh points real(kind=grid_p), intent(in) :: Vatm (nsp,np) Value of Harris potential (Hartree potential\n of sum of atomic desities) at mesh points. Notice single precision of Vscf and Vatm. real(kind=dp), intent(in) :: dvol Volume per mesh point real(kind=dp), intent(in) :: VolCel Unit cell volume real(kind=dp), intent(inout) :: Fal (3,*) Atomic forces (contribution added on output) real(kind=dp), intent(inout) :: Stressl (9) Stress tensor Calls proc~~dfscf~~CallsGraph proc~dfscf dfscf indxuo indxuo proc~dfscf->indxuo listsc listsc proc~dfscf->listsc endpht endpht proc~dfscf->endpht re_alloc re_alloc proc~dfscf->re_alloc alloc_default alloc_default proc~dfscf->alloc_default listp2 listp2 proc~dfscf->listp2 needdscfl needdscfl proc~dfscf->needdscfl timer timer proc~dfscf->timer listdlptr listdlptr proc~dfscf->listdlptr matrixotom matrixotom proc~dfscf->matrixotom lstpht lstpht proc~dfscf->lstpht globaltolocalorb globaltolocalorb proc~dfscf->globaltolocalorb de_alloc de_alloc proc~dfscf->de_alloc numdl numdl proc~dfscf->numdl proc~rcut rcut proc~dfscf->proc~rcut proc~die die proc~dfscf->proc~die dscfl dscfl proc~dfscf->dscfl listdl listdl proc~dfscf->listdl proc~chk chk proc~rcut->proc~chk io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~chk->proc~die Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~dfscf~~CalledByGraph proc~dfscf dfscf proc~dhscf dhscf proc~dhscf->proc~dfscf proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code dfscf Source Code subroutine dfscf ( ifa , istr , na , no , nuo , nuotot , np , nspin , . indxua , isa , iaorb , iphorb , . maxnd , numd , listdptr , listd , Dscf , Datm , . Vscf , Vatm , dvol , VolCel , Fal , Stressl ) !! author: P.Ordejon, J.M.Soler, and J.Gale !! !! Adds the SCF contribution to atomic forces and stress. !! !! Written by P.Ordejon, J.M.Soler, and J.Gale. !! Last modification by J.M.Soler, October 2000. !! Modifed for Off-Site Spin-orbit coupling by R. Cuadrado, Feb. 2018 !  Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo use listsc_module , only : listsc use mesh , only : dxa , nsp , xdop , xdsp use meshphi , only : endpht , lstpht , listp2 use meshdscf , only : matrixOtoM use meshdscf , only : DscfL , nrowsDscfL , needDscfL use meshdscf , only : listDl , listDlPtr , numdL use alloc , only : re_alloc , de_alloc , alloc_default , $ allocDefaults use parallel , only : Nodes , Node use sys , only : die use parallelsubs , only : GlobalToLocalOrb implicit none !  Passed arguments integer , intent ( in ) :: ifa !! Are forces required? (`1`=yes,`0`=no) integer , intent ( in ) :: istr !! Is stress required? (`1`=yes, `0`=no) integer , intent ( in ) :: na !! Number of atoms integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: nuo !! Number of orbitals in unit cell (local) integer , intent ( in ) :: nuotot !! Number of orbitals in unit cell (global) integer , intent ( in ) :: np !! Number of mesh points (total is nsp*np) integer , intent ( in ) :: nspin !! spin%Grid (i.e., `min(nspin,4)`) !! `nspin=1` => Unpolarized, !! `nspin=2` => polarized !! `nspin=4` => Noncollinear spin/SOC integer , intent ( in ) :: indxua ( na ) !! Index of equivalent atom in unit cell integer , intent ( in ) :: isa ( na ) !! Species index of each atom integer , intent ( in ) :: iaorb ( no ) !! Atom to which orbitals belong integer , intent ( in ) :: iphorb ( no ) !! Index of orbital within its atom integer , intent ( in ) :: maxnd !! First dimension of Dscf integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero elemts in each row of Dscf integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of row in listd integer , intent ( in ) :: listd ( maxnd ) !! List of nonzero elements of Dscf real ( grid_p ), intent ( in ) :: Vscf ( nsp , np , nspin ) !! Value of SCF potential at the mesh points real ( grid_p ), intent ( in ) :: Vatm ( nsp , np ) !! Value of Harris potential (Hartree potential !! of sum of atomic desities) at mesh points. !! Notice single precision of Vscf and Vatm. real ( dp ), intent ( in ) :: dvol !! Volume per mesh point real ( dp ), intent ( in ) :: VolCel !! Unit cell volume real ( dp ), intent ( in ) :: Datm ( nuotot ) real ( dp ), intent ( in ), target :: Dscf (:,:) !! `Dscf(maxnd,*)`: Value of nonzero elemens of density matrix. !! It is used in \"hermitified\" here. real ( dp ), intent ( inout ) :: Fal ( 3 , * ) !! Atomic forces (contribution added on output) real ( dp ), intent ( inout ) :: Stressl ( 9 ) !! Stress tensor ! Internal variables integer , parameter :: . minb = 100 , ! Min buffer size for local copy of Dscf . maxoa = 100 ! Max # of orbitals per atom integer . i , ia , ib , ibuff ( no ), ic , ii , imp , ind , iop , ip , iphi , io , . is , isp , ispin , iu , iua , iul , ix , ix1 , ix2 , iy , . j , jb , jc , last , lasta , lastop , maxb , maxc , maxndl , . nc , nphiloc real ( dp ) . CD ( nsp ), CDV ( nsp ), DF ( 12 ), Dji , dxsp ( 3 , nsp ), . gCi ( 12 , nsp ), grada ( 3 , maxoa , nsp ), . phia ( maxoa , nsp ), rvol , r2sp , r2cut ( nsmax ) ! Allocate real ( dp ), pointer :: V (:,:) => null () real ( dp ), pointer :: DM_spbherm (:,:) => null () ! integer , pointer , save :: ibc (:), iob (:) real ( dp ), pointer , save :: C (:,:), D (:,:,:), . gC (:,:,:), xgC (:,:,:) logical :: Parallel_Run , nullified = . false . type ( allocDefaults ) oldDefaults !  Start time counter call timer ( 'dfscf' , 1 ) !  Nullify pointers if (. not . nullified ) then nullify ( C , D , gC , ibc , iob , xgC ) nullified = . true . end if !  Get old allocation defaults and set new ones call alloc_default ( old = oldDefaults , . copy = . false ., shrink = . false ., . imin = 1 , routine = 'dfscf' ) if ( size ( Dscf , 2 ) == 8 ) then if ( nspin /= 4 ) call die ( \"Spin size inconsistency in dfscf\" ) !     Prepare \"spin-box hermitian\" form of DM for work below !     We could re-use similar work in rhoofd !     This is the relevant part of the DM in view of the structure !     of the potential Vscf. call re_alloc ( DM_spbherm , 1 , size ( Dscf , 1 ), 1 , 4 , \"DM_spbherm\" ) DM_spbherm (:, 1 ) = Dscf (:, 1 ) DM_spbherm (:, 2 ) = Dscf (:, 2 ) DM_spbherm (:, 3 ) = 0.5_dp * ( Dscf (:, 3 ) + Dscf (:, 7 )) DM_spbherm (:, 4 ) = 0.5_dp * ( Dscf (:, 4 ) + Dscf (:, 8 )) else DM_spbherm => Dscf ! Just use passed Dscf end if !  Allocate buffers to store partial copies of Dscf and C maxc = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxb = maxc + minb maxb = min ( maxb , no ) call re_alloc ( C , 1 , nsp , 1 , maxc , 'C' , 'dfscf' ) call re_alloc ( D , 0 , maxb , 0 , maxb , 1 , nspin , 'D' , 'dfscf' ) call re_alloc ( gC , 1 , 3 , 1 , nsp , 1 , maxc , 'gC' , 'dfscf' ) call re_alloc ( ibc , 1 , maxc , 'ibc' , 'dfscf' ) call re_alloc ( iob , 0 , maxb , 'iob' , 'dfscf' ) call re_alloc ( xgC , 1 , 9 , 1 , nsp , 1 , maxc , 'xgC' , 'dfscf' ) call re_alloc ( V , 1 , nsp , 1 , nspin , 'V' , 'dfscf' ) V = 0._dp !  Set logical that determines whether we need to use parallel or serial mode Parallel_Run = ( Nodes . gt . 1 ) !  If parallel, allocate temporary storage for Local Dscf if ( Parallel_Run ) then if ( nrowsDscfL . gt . 0 ) then maxndl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else maxndl = 1 endif call re_alloc ( DscfL , 1 , maxndl , 1 , nspin , . 'DscfL' , 'dfscf' ) ! Redistribute Dscf to DscfL form call matrixOtoM ( maxnd , numd , listdptr , maxndl , nuo , . nspin , DM_spbherm , DscfL ) endif !     Note: Since this routine is only called to get forces AND stresses, !     ifa = 1 and istr = 1 always. We could get rid of ix1 and ix2 and !     unroll the relevant loops below for more efficiency. !     Also, it might be worth unrolling some of the nsp loops below !     if nsp is always 8. !  Find range of a single array to hold force and stress derivatives !  Range 1-3 for forces if ( ifa . eq . 1 ) then ix1 = 1 else ix1 = 4 end if !  Range 4-12 for stress if ( istr . eq . 1 ) then ix2 = 12 else ix2 = 3 end if !  Initialise variables D (:,:,:) = 0.0_dp ibuff (:) = 0 iob (:) = 0 last = 0 !  Find atomic cutoff radii r2cut (:) = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) enddo !  Evaluate constants rvol = 1.0_dp / VolCel !  Loop over grid points do ip = 1 , np !  Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !  iob(ib)>0 means that row ib of D must not be overwritten !  iob(ib)=0 means that row ib of D is empty !  iob(ib)<0 means that row ib of D contains a valid row of !             Dscf, but which is not required at this point do imp = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( imp ) ib = ibuff ( i ) if ( ib . gt . 0 ) iob ( ib ) = i enddo !  Look for required rows of Dscf not yet stored in D do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ibuff ( i ) . eq . 0 ) then !  Look for an available row in D do ib = 1 , maxb !  last runs circularly over rows of D last = last + 1 if ( last . gt . maxb ) last = 1 if ( iob ( last ) . le . 0 ) goto 10 enddo call die ( 'dfscf: no slot available in D' ) 10 continue !  Copy row i of Dscf into row last of D j = abs ( iob ( last )) if ( j . ne . 0 ) ibuff ( j ) = 0 ibuff ( i ) = last iob ( last ) = i ib = last iu = indxuo ( i ) if ( Parallel_Run ) then iul = NeedDscfL ( iu ) do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) if ( i . ne . iu ) j = listsc ( i , iu , j ) jb = ibuff ( j ) ! i-j symmetry due to the spin-box hermiticity D ( ib , jb ,:) = DscfL ( ind ,:) D ( jb , ib ,:) = DscfL ( ind ,:) enddo else call GlobalToLocalOrb ( iu , Node , Nodes , iul ) do ii = 1 , numd ( iul ) ind = listdptr ( iul ) + ii j = listd ( ind ) if ( i . ne . iu ) j = listsc ( i , iu , j ) jb = ibuff ( j ) D ( ib , jb ,:) = DM_spbherm ( ind ,:) D ( jb , ib ,:) = DM_spbherm ( ind ,:) enddo endif endif ibc ( ic ) = ibuff ( i ) enddo !  Restore iob for next point do imp = 1 + endpht ( ip - 1 ), endpht ( ip ) i = lstpht ( imp ) ib = ibuff ( i ) iob ( ib ) = - i enddo !  Calculate all phi values and derivatives at all subpoints lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) iu = indxuo ( i ) ia = iaorb ( i ) iphi = iphorb ( i ) is = isa ( ia ) iua = indxua ( ia ) iop = listp2 ( imp ) if ( ia . ne . lasta . or . iop . ne . lastop ) then lasta = ia lastop = iop do isp = 1 , nsp dxsp ( 1 : 3 , isp ) = xdop ( 1 : 3 , iop ) + xdsp ( 1 : 3 , isp ) - dxa ( 1 : 3 , ia ) r2sp = dxsp ( 1 , isp ) ** 2 + dxsp ( 2 , isp ) ** 2 + dxsp ( 3 , isp ) ** 2 if ( r2sp . lt . r2cut ( is )) then call all_phi ( is , + 1 , dxsp (:, isp ), nphiloc , . phia (:, isp ), grada (:,:, isp )) else phia (:, isp ) = 0.0_dp grada ( 1 : 3 ,:, isp ) = 0.0_dp endif enddo endif C ( 1 : nsp , ic ) = phia ( iphi , 1 : nsp ) gC ( 1 : 3 , 1 : nsp , ic ) = grada ( 1 : 3 , iphi , 1 : nsp ) !  If stress required. Generate stress derivatives if ( istr . eq . 1 ) then do isp = 1 , nsp ii = 0 do ix = 1 , 3 do iy = 1 , 3 ii = ii + 1 xgC ( ii , isp , ic ) = dxsp ( iy , isp ) * gC ( ix , isp , ic ) * rvol enddo enddo enddo endif enddo !     Copy potential to a double precision array V ( 1 : nsp , 1 : nspin ) = Vscf ( 1 : nsp , ip , 1 : nspin ) !     Factor two for nondiagonal elements for non-collinear spin (and SO) if ( nspin == 4 ) then V ( 1 : nsp , 3 : 4 ) = 2.0_dp * V ( 1 : nsp , 3 : 4 ) end if !     Loop on first orbital of mesh point do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) iu = indxuo ( i ) ia = iaorb ( i ) iua = indxua ( ia ) ib = ibc ( ic ) !  Copy force and stress gradients to a single array gCi ( 1 : 3 , 1 : nsp ) = gC ( 1 : 3 , 1 : nsp , ic ) gCi ( 4 : 12 , 1 : nsp ) = xgC ( 1 : 9 , 1 : nsp , ic ) !  Find Sum_{j,spin}(Vscf*Dij*Cj) for every subpoint !  Some loops are not done using f90 form as this !  leads to much slower execution on machines with stupid f90 !  compilers at the moment CDV ( 1 : nsp ) = 0.0_dp do ispin = 1 , nspin !  Loop on second orbital of mesh point CD ( 1 : nsp ) = 0.0_dp do jc = 1 , nc jb = ibc ( jc ) Dji = D ( jb , ib , ispin ) do isp = 1 , nsp CD ( isp ) = CD ( isp ) + C ( isp , jc ) * Dji end do end do do isp = 1 , nsp CDV ( isp ) = CDV ( isp ) + CD ( isp ) * V ( isp , ispin ) end do enddo do isp = 1 , nsp CDV ( isp ) = CDV ( isp ) - . C ( isp , ic ) * Datm ( iu ) * Vatm ( isp , ip ) CDV ( isp ) = 2.0_dp * dVol * CDV ( isp ) end do !  Add 2*Dscf_ij*<Cj|Vscf|gCi> to forces do ix = ix1 , ix2 DF ( ix ) = 0.0_dp do isp = 1 , nsp DF ( ix ) = DF ( ix ) + gCi ( ix , isp ) * CDV ( isp ) enddo enddo !  Add force and stress to output arrays if ( ifa . eq . 1 ) then Fal ( 1 : 3 , iua ) = Fal ( 1 : 3 , iua ) + DF ( 1 : 3 ) endif if ( istr . eq . 1 ) then Stressl ( 1 : 9 ) = Stressl ( 1 : 9 ) + DF ( 4 : 12 ) endif !  End of first orbital loop enddo !  End of mesh point loop enddo !  Deallocate local memory call de_alloc ( xgC , 'xgC' , 'dfscf' ) call de_alloc ( iob , 'iob' , 'dfscf' ) call de_alloc ( ibc , 'ibc' , 'dfscf' ) call de_alloc ( gC , 'gC' , 'dfscf' ) call de_alloc ( D , 'D' , 'dfscf' ) call de_alloc ( C , 'C' , 'dfscf' ) call de_alloc ( V , 'V' , 'dfscf' ) if ( Parallel_Run ) then call de_alloc ( DscfL , 'DscfL' , 'dfscf' ) endif if ( size ( Dscf , 2 ) == 8 ) then call de_alloc ( DM_spbherm , 'DM_spbherm' , 'dfscf' ) endif !  Restore old allocation defaults call alloc_default ( restore = oldDefaults ) call timer ( 'dfscf' , 2 ) end subroutine dfscf","tags":"","loc":"proc/dfscf.html","title":"dfscf – SIESTA"},{"text":"public function floating(is) Returns .true. if the species is really a \"fake\" one,\n intended to provide some floating orbitals. Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value logical Calls proc~~floating~~CallsGraph proc~floating floating proc~izofis IZOFIS proc~floating->proc~izofis proc~chk chk proc~izofis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~floating~~CalledByGraph proc~floating floating proc~psch psch proc~psch->proc~floating proc~rphiatm rphiatm proc~rphiatm->proc~floating proc~vna_sub vna_sub proc~vna_sub->proc~floating proc~psover psover proc~psover->proc~floating proc~chcore_sub chcore_sub proc~chcore_sub->proc~floating proc~phiatm phiatm proc~phiatm->proc~floating proc~all_phi all_phi proc~all_phi->proc~floating proc~rhooda rhooda proc~rhooda->proc~phiatm proc~dhscf_init dhscf_init proc~dhscf_init->proc~rhooda proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code floating Source Code function floating ( is ) !! Returns `.true.` if the species is really a \"fake\" one, !! intended to provide some floating orbitals. logical floating integer , intent ( in ) :: is floating = izofis ( is ) . lt . 0 end function floating","tags":"","loc":"proc/floating.html","title":"floating – SIESTA"},{"text":"public function IZOFIS(is) result(izofis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value integer Atomic number Calls proc~~izofis~~CallsGraph proc~izofis IZOFIS proc~chk chk proc~izofis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~izofis~~CalledByGraph proc~izofis IZOFIS proc~floating floating proc~floating->proc~izofis proc~psch psch proc~psch->proc~floating proc~rphiatm rphiatm proc~rphiatm->proc~floating proc~vna_sub vna_sub proc~vna_sub->proc~floating proc~psover psover proc~psover->proc~floating proc~chcore_sub chcore_sub proc~chcore_sub->proc~floating proc~phiatm phiatm proc~phiatm->proc~floating proc~all_phi all_phi proc~all_phi->proc~floating proc~rhooda rhooda proc~rhooda->proc~phiatm proc~dhscf_init dhscf_init proc~dhscf_init->proc~rhooda proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code IZOFIS Source Code FUNCTION IZOFIS ( IS ) integer :: izofis !! Atomic number integer , intent ( in ) :: is !! Species index call chk ( 'izofis' , is ) izofis = species ( is )% z end function izofis","tags":"","loc":"proc/izofis.html","title":"IZOFIS – SIESTA"},{"text":"public function ZVALFIS(is) result(zvalfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value real(kind=dp) Valence charge Calls proc~~zvalfis~~CallsGraph proc~zvalfis ZVALFIS proc~chk chk proc~zvalfis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code ZVALFIS Source Code FUNCTION ZVALFIS ( IS ) real ( dp ) :: zvalfis !! Valence charge integer , intent ( in ) :: is !! Species index call chk ( 'zvalfis' , is ) zvalfis = species ( is )% zval end function zvalfis","tags":"","loc":"proc/zvalfis.html","title":"ZVALFIS – SIESTA"},{"text":"public function LABELFIS(is) result(labelfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value character(len=20) Atomic label Calls proc~~labelfis~~CallsGraph proc~labelfis LABELFIS proc~chk chk proc~labelfis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code LABELFIS Source Code FUNCTION LABELFIS ( IS ) character ( len = 20 ) :: labelfis !! Atomic label integer , intent ( in ) :: is !! Species index call chk ( 'labelfis' , is ) labelfis = species ( is )% label end function labelfis","tags":"","loc":"proc/labelfis.html","title":"LABELFIS – SIESTA"},{"text":"public function LMXKBFIS(is) result(lmxkbfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value integer Maximum ang mom of the KB projectors Calls proc~~lmxkbfis~~CallsGraph proc~lmxkbfis LMXKBFIS proc~chk chk proc~lmxkbfis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code LMXKBFIS Source Code FUNCTION LMXKBFIS ( IS ) integer :: lmxkbfis !! Maximum ang mom of the KB projectors integer , intent ( in ) :: is !! Species index call chk ( 'lmxkbfis' , is ) lmxkbfis = species ( is )% lmax_projs end function lmxkbfis","tags":"","loc":"proc/lmxkbfis.html","title":"LMXKBFIS – SIESTA"},{"text":"public function LOMAXFIS(is) result(lomaxfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer Calls proc~~lomaxfis~~CallsGraph proc~lomaxfis LOMAXFIS proc~chk chk proc~lomaxfis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code LOMAXFIS Source Code FUNCTION LOMAXFIS ( IS ) integer :: lomaxfis ! Maximum ang mom of the Basis Functions integer , intent ( in ) :: is ! Species index call chk ( 'lomaxfis' , is ) lomaxfis = species ( is )% lmax_basis end function lomaxfis","tags":"","loc":"proc/lomaxfis.html","title":"LOMAXFIS – SIESTA"},{"text":"public function MASSFIS(is) result(massfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) Calls proc~~massfis~~CallsGraph proc~massfis MASSFIS proc~chk chk proc~massfis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code MASSFIS Source Code FUNCTION MASSFIS ( IS ) real ( dp ) :: massfis ! Mass integer , intent ( in ) :: is ! Species index call chk ( 'massfis' , is ) massfis = species ( is )% mass end function massfis","tags":"","loc":"proc/massfis.html","title":"MASSFIS – SIESTA"},{"text":"public function NKBFIS(is) result(nkbfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer Calls proc~~nkbfis~~CallsGraph proc~nkbfis NKBFIS proc~chk chk proc~nkbfis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code NKBFIS Source Code FUNCTION NKBFIS ( IS ) integer :: nkbfis ! Total number of KB projectors integer , intent ( in ) :: is ! Species index call chk ( 'nkbfis' , is ) nkbfis = species ( is )% nprojs end function nkbfis","tags":"","loc":"proc/nkbfis.html","title":"NKBFIS – SIESTA"},{"text":"public function NOFIS(is) result(nofis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer Calls proc~~nofis~~CallsGraph proc~nofis NOFIS proc~chk chk proc~nofis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code NOFIS Source Code FUNCTION NOFIS ( IS ) integer :: nofis ! Total number of Basis functions integer , intent ( in ) :: is ! Species index call chk ( 'nofis' , is ) nofis = species ( is )% norbs end function nofis","tags":"","loc":"proc/nofis.html","title":"NOFIS – SIESTA"},{"text":"public function UION(is) result(uion) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) Calls proc~~uion~~CallsGraph proc~uion UION proc~chk chk proc~uion->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~uion~~CalledByGraph proc~uion UION proc~setup_h0 setup_H0 proc~setup_h0->proc~uion Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code UION Source Code FUNCTION UION ( IS ) real ( dp ) uion integer , intent ( in ) :: is ! Species index call chk ( 'uion' , is ) uion = species ( is )% self_energy end function uion","tags":"","loc":"proc/uion.html","title":"UION – SIESTA"},{"text":"public function RCORE(is) result(rcore) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) Calls proc~~rcore~~CallsGraph proc~rcore RCORE proc~chk chk proc~rcore->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rcore~~CalledByGraph proc~rcore RCORE proc~dhscf_init dhscf_init proc~dhscf_init->proc~rcore proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code RCORE Source Code FUNCTION RCORE ( is ) real ( dp ) rcore integer , intent ( in ) :: is ! Species index C Returns cutoff radius of the pseudo - core charge density for the non - linear C core corrections for xc potential . C Distances in Bohr call chk ( 'rcore' , is ) rcore = species ( is )% core % cutoff end function rcore","tags":"","loc":"proc/rcore.html","title":"RCORE – SIESTA"},{"text":"public function RCHLOCAL(is) result(rchlocal) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) Calls proc~~rchlocal~~CallsGraph proc~rchlocal RCHLOCAL proc~chk chk proc~rchlocal->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code RCHLOCAL Source Code FUNCTION RCHLOCAL ( is ) real ( dp ) rchlocal integer , intent ( in ) :: is ! Species index C Returns cutoff radius of the Vlocal charge density C Distances in Bohr call chk ( 'rchlocal' , is ) rchlocal = species ( is )% Chlocal % cutoff end function rchlocal","tags":"","loc":"proc/rchlocal.html","title":"RCHLOCAL – SIESTA"},{"text":"public function orb_gindex(is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer Calls proc~~orb_gindex~~CallsGraph proc~orb_gindex orb_gindex proc~chk chk proc~orb_gindex->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code orb_gindex Source Code FUNCTION orb_gindex ( IS , IO ) integer orb_gindex integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the global index of a basis orbital call chk ( 'orb_gindex' , is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . lt . 1 )) call die ( \"orb_gindex: Wrong io\" ) orb_gindex = species ( is )% orb_gindex ( io ) end function orb_gindex","tags":"","loc":"proc/orb_gindex.html","title":"orb_gindex – SIESTA"},{"text":"public function kbproj_gindex(is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer Calls proc~~kbproj_gindex~~CallsGraph proc~kbproj_gindex kbproj_gindex proc~chk chk proc~kbproj_gindex->proc~chk proc~die die proc~kbproj_gindex->proc~die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code kbproj_gindex Source Code FUNCTION kbproj_gindex ( IS , IO ) integer kbproj_gindex integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! KBproj index ! (within atom, <0, for compatibility) C Returns the global index of a KB projector integer :: ko call chk ( 'kbproj_gindex' , is ) ko = - io if ( ( ko . gt . species ( is )% nprojs ) . or . $ ( ko . lt . 1 )) then call die ( \"kbproj_gindex: Wrong io\" ) endif kbproj_gindex = species ( is )% pj_gindex ( ko ) end function kbproj_gindex","tags":"","loc":"proc/kbproj_gindex.html","title":"kbproj_gindex – SIESTA"},{"text":"public function ldau_gindex(is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer Calls proc~~ldau_gindex~~CallsGraph proc~ldau_gindex ldau_gindex proc~chk chk proc~ldau_gindex->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code ldau_gindex Source Code FUNCTION ldau_gindex ( IS , IO ) integer ldau_gindex integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the global index of a LDA + U projector call chk ( 'ldau_gindex' , is ) if ( ( io . gt . species ( is )% nprojsldau ) . or . $ ( io . lt . 1 )) call die ( \"ldau_gindex: Wrong io\" ) ldau_gindex = species ( is )% pjldau_gindex ( io ) end function ldau_gindex","tags":"","loc":"proc/ldau_gindex.html","title":"ldau_gindex – SIESTA"},{"text":"public function vna_gindex(is) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer Calls proc~~vna_gindex~~CallsGraph proc~vna_gindex vna_gindex proc~chk chk proc~vna_gindex->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code vna_gindex Source Code FUNCTION vna_gindex ( IS ) integer vna_gindex integer , intent ( in ) :: is ! Species index C Returns the global index for a Vna function call chk ( 'vna_gindex' , is ) vna_gindex = species ( is )% vna_gindex end function vna_gindex","tags":"","loc":"proc/vna_gindex.html","title":"vna_gindex – SIESTA"},{"text":"public function ATMPOPFIO(is, io) result(atmpopfio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value real(kind=dp) Calls proc~~atmpopfio~~CallsGraph proc~atmpopfio ATMPOPFIO proc~chk chk proc~atmpopfio->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code ATMPOPFIO Source Code FUNCTION ATMPOPFIO ( IS , IO ) real ( dp ) atmpopfio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the population of the atomic basis orbitals in the atomic C ground state configuration . call chk ( 'atmpopfio' , is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . lt . 1 )) call die ( \"atmpopfio: Wrong io\" ) atmpopfio = species ( is )% orb_pop ( io ) end function atmpopfio","tags":"","loc":"proc/atmpopfio.html","title":"ATMPOPFIO – SIESTA"},{"text":"public function CNFIGFIO(is, io) result(cnfigfio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer Calls proc~~cnfigfio~~CallsGraph proc~cnfigfio CNFIGFIO proc~chk chk proc~cnfigfio->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code CNFIGFIO Source Code FUNCTION CNFIGFIO ( IS , IO ) integer cnfigfio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns the valence - shell configuration in the atomic ground state C ( i . e . the principal quatum number for orbitals of angular momentum l ) C INTEGER CNFIGFIO : Principal quantum number of the shell to what C the orbital belongs ( for polarization orbitals C the quantum number corresponds to the shell which C is polarized by the orbital io ) call chk ( 'cnfigfio' , is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . lt . 1 )) call die ( \"cnfigfio: Wrong io\" ) cnfigfio = species ( is )% orb_n ( io ) end function cnfigfio","tags":"","loc":"proc/cnfigfio.html","title":"CNFIGFIO – SIESTA"},{"text":"public function LOFIO(is, io) result(lofio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io * * * OUTPUT * * * * * ****\n   INTEGER LOFIO  : Quantum number L of orbital or KB projector Return Value integer Calls proc~~lofio~~CallsGraph proc~lofio LOFIO proc~chk chk proc~lofio->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~lofio~~CalledByGraph proc~lofio LOFIO proc~symfio SYMFIO proc~symfio->proc~lofio Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code LOFIO Source Code FUNCTION LOFIO ( IS , IO ) integer lofio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns total angular momentum quantum number of a given atomic basis C basis orbital or Kleynman - Bylander projector . C INTEGER IO : Orbital index ( within atom ) C IO > 0 => Basis orbitals C IO < 0 => Kleynman - Bylander projectors C IO = 0 => Local pseudopotential C ************************ OUTPUT ***************************************** C INTEGER LOFIO : Quantum number L of orbital or KB projector call chk ( 'lofio' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"lofio: No such orbital\" ) lofio = spp % orb_l ( io ) else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"lofio: No such projector\" ) lofio = spp % pj_l ( - io ) else lofio = 0 endif end function lofio","tags":"","loc":"proc/lofio.html","title":"LOFIO – SIESTA"},{"text":"public function MOFIO(is, io) result(mofio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io * * * OUTPUT * * * * * ****\n   INTEGER MOFIO  : Quantum number m of orbital or KB projector Return Value integer Calls proc~~mofio~~CallsGraph proc~mofio MOFIO proc~chk chk proc~mofio->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~mofio~~CalledByGraph proc~mofio MOFIO proc~symfio SYMFIO proc~symfio->proc~mofio Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code MOFIO Source Code FUNCTION MOFIO ( IS , IO ) integer mofio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns m quantum number of a given atomic basis C basis orbital or Kleynman - Bylander projector . C INTEGER IO : Orbital index ( within atom ) C IO > 0 => Basis orbitals C IO < 0 => Kleynman - Bylander projectors C IO = 0 => Local pseudopotential C ************************ OUTPUT ***************************************** C INTEGER MOFIO : Quantum number m of orbital or KB projector call chk ( 'mofio' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"mofio: No such orbital\" ) mofio = spp % orb_m ( io ) else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"mofio: No such projector\" ) mofio = spp % pj_m ( - io ) else mofio = 0 endif end function mofio","tags":"","loc":"proc/mofio.html","title":"MOFIO – SIESTA"},{"text":"public function ZETAFIO(is, io) result(zetafio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io * * * OUTPUT * * * * * ****\n   INTEGER ZETAFIO  : Zeta number of orbital Return Value integer Calls proc~~zetafio~~CallsGraph proc~zetafio ZETAFIO proc~chk chk proc~zetafio->proc~chk proc~die die proc~zetafio->proc~die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code ZETAFIO Source Code FUNCTION ZETAFIO ( IS , IO ) integer zetafio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns zeta number of a C basis orbital C INTEGER IO : Orbital index ( within atom ) C IO > 0 => Basis orbitals C ************************ OUTPUT ***************************************** C INTEGER ZETAFIO : Zeta number of orbital call chk ( 'mofio' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"zetafio: No such orbital\" ) zetafio = spp % orbnl_z ( spp % orb_index ( io )) else call die ( 'zetafio only deals with orbitals' ) endif end function zetafio","tags":"","loc":"proc/zetafio.html","title":"ZETAFIO – SIESTA"},{"text":"public function rcut(is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value real(kind=dp) Calls proc~~rcut~~CallsGraph proc~rcut rcut proc~chk chk proc~rcut->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~rcut~~CalledByGraph proc~rcut rcut proc~dhscf_init dhscf_init proc~dhscf_init->proc~rcut proc~rhooda rhooda proc~dhscf_init->proc~rhooda proc~dfscf dfscf proc~dfscf->proc~rcut proc~vmat vmat proc~vmat->proc~rcut proc~rhooda->proc~rcut proc~rhoofd rhoofd proc~rhoofd->proc~rcut proc~delk delk proc~delk->proc~rcut proc~dhscf dhscf proc~dhscf->proc~dfscf proc~dhscf->proc~vmat proc~dhscf->proc~rhoofd proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init proc~delk_wrapper delk_wrapper proc~delk_wrapper->proc~delk proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rcut Source Code function rcut ( is , io ) real ( dp ) rcut integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) ! io> => basis orbitals ! io<0  => KB projectors ! io=0 : Local screened pseudopotential C Returns cutoff radius of Kleynman - Bylander projectors and C atomic basis orbitals . C Distances in Bohr call chk ( 'rcut' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"rcut: No such orbital\" ) op => spp % orbnl ( spp % orb_index ( io )) rcut = op % cutoff else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"rcut: No such projector\" ) pp => spp % pjnl ( spp % pj_index ( - io )) rcut = pp % cutoff else rcut = spp % vna % cutoff endif end function rcut","tags":"","loc":"proc/rcut.html","title":"rcut – SIESTA"},{"text":"public function SYMFIO(is, io) result(symfio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value character(len=20) Calls proc~~symfio~~CallsGraph proc~symfio SYMFIO proc~chk chk proc~symfio->proc~chk proc~lofio LOFIO proc~symfio->proc~lofio proc~mofio MOFIO proc~symfio->proc~mofio proc~pol POL proc~symfio->proc~pol proc~die die proc~chk->proc~die proc~lofio->proc~chk proc~mofio->proc~chk io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code SYMFIO Source Code FUNCTION SYMFIO ( IS , IO ) character ( len = 20 ) symfio integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) C Returns a label describing the symmetry of the C basis orbital or Kleynman - Bylander projector . C INTEGER IO : Orbital index ( within atom ) C IO > 0 => Basis orbitals C IO < 0 => Kleynman - Bylander projectors C INTEGER SYMFIO : Symmetry of the orbital or KB projector C 2 ) Returns 's' for IO = 0 integer ilm , i , lorb , morb integer , parameter :: lmax_sym = 4 character ( len = 11 ) sym_label (( lmax_sym + 1 ) * ( lmax_sym + 1 )) data sym_label ( 1 ) . / 's' / data ( sym_label ( i ), i = 2 , 4 ) . / 'py' , 'pz' , 'px' / data ( sym_label ( i ), i = 5 , 9 ) . / 'dxy' , 'dyz' , 'dz2' , 'dxz' , 'dx2-y2' / data ( sym_label ( i ), i = 10 , 16 ) . / 'fy(3x2-y2)' , 'fxyz' , 'fz2y' , 'fz3' , . 'fz2x' , 'fz(x2-y2)' , 'fx(x2-3y2)' / data ( sym_label ( i ), i = 17 , 25 ) . / 'gxy(x2-y2)' , 'gzy(3x2-y2)' , 'gz2xy' , 'gz3y' , 'gz4' , . 'gz3x' , 'gz2(x2-y2)' , 'gzx(x2-3y2)' , 'gx4+y4' / call chk ( 'rcut' , is ) spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"symfio: No such orbital\" ) else if ( io . lt . 0 ) then if ( - io . gt . spp % nprojs ) call die ( \"symfio: No such projector\" ) else symfio = 's' endif lorb = lofio ( is , io ) morb = mofio ( is , io ) if ( lorb . gt . lmax_sym ) then symfio = ' ' else ilm = lorb * lorb + lorb + morb + 1 if ( pol ( is , io )) then symfio = 'P' // sym_label ( ilm ) else symfio = sym_label ( ilm ) endif endif end function symfio","tags":"","loc":"proc/symfio.html","title":"SYMFIO – SIESTA"},{"text":"public function POL(is, io) result(pol) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value logical Called by proc~~pol~~CalledByGraph proc~pol POL proc~symfio SYMFIO proc~symfio->proc~pol Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code POL Source Code FUNCTION POL ( IS , IO ) logical pol integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) ! io>0 => basis orbitals C If true , the orbital IO is a perturbative polarization orbital spp => species ( is ) if ( ( io . gt . species ( is )% norbs ) . or . $ ( io . le . 0 )) call die ( \"pol: Wrong io\" ) spp => species ( is ) pol = spp % orbnl_ispol ( spp % orb_index ( io )) end function pol","tags":"","loc":"proc/pol.html","title":"POL – SIESTA"},{"text":"public function EPSKB(is, io) result(epskb) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value real(kind=dp) Contents Source Code EPSKB Source Code FUNCTION EPSKB ( IS , IO ) real ( dp ) epskb integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! KB proyector index (within atom) ! May be positive or negative ! (only ABS(IO) is used). C Returns the energies epsKB_l of the Kleynman - Bylander projectors : C < Psi | V_KB | Psi '> = <Psi|V_local|Psi' > + C Sum_lm ( epsKB_l * < Psi | Phi_lm > * < Phi_lm | Psi ' > ) C where Phi_lm is returned by subroutine PHIATM . C Energy in Rydbergs . integer ik spp => species ( is ) ik = abs ( io ) if (( ik . gt . spp % nprojs ) . or . $ ( ik . lt . 1 ) ) call die ( \"epskb: No such projector\" ) epskb = spp % pjnl_ekb ( spp % pj_index ( ik )) end function epskb","tags":"","loc":"proc/epskb.html","title":"EPSKB – SIESTA"},{"text":"public function NZTFL(is, l) result(nztfl) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: l Return Value integer Calls proc~~nztfl~~CallsGraph proc~nztfl NZTFL proc~chk chk proc~nztfl->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code NZTFL Source Code FUNCTION NZTFL ( IS , L ) integer nztfl integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: l ! Angular momentum of the basis funcs C Returns the number of different basis functions C with the same angular momentum and for a given species integer i call chk ( 'nztfl' , is ) spp => species ( is ) nztfl = 0 do i = 1 , spp % norbs if ( spp % orb_l ( i ). eq . l ) nztfl = nztfl + 1 enddo end function nztfl","tags":"","loc":"proc/nztfl.html","title":"NZTFL – SIESTA"},{"text":"private function NKBL_FUNC(is, l) result(nkbl_func) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: l Return Value integer Calls proc~~nkbl_func~~CallsGraph proc~nkbl_func NKBL_FUNC proc~chk chk proc~nkbl_func->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code NKBL_FUNC Source Code FUNCTION NKBL_FUNC ( IS , L ) integer nkbl_func integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: l ! Angular momentum of the basis funcs C Returns the number of different KB projectors C with the same angular momentum and for a given species integer i call chk ( 'nkbl_func' , is ) spp => species ( is ) nkbl_func = 0 do i = 1 , spp % nprojs if ( spp % pj_l ( i ). eq . l ) nkbl_func = nkbl_func + 1 enddo end function nkbl_func","tags":"","loc":"proc/nkbl_func.html","title":"NKBL_FUNC – SIESTA"},{"text":"private subroutine chk(name, is) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: name integer, intent(in) :: is Calls proc~~chk~~CallsGraph proc~chk chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~chk~~CalledByGraph proc~chk chk proc~nkbfis NKBFIS proc~nkbfis->proc~chk proc~psch psch proc~psch->proc~chk proc~floating floating proc~psch->proc~floating proc~uion UION proc~uion->proc~chk proc~labelfis LABELFIS proc~labelfis->proc~chk proc~zetafio ZETAFIO proc~zetafio->proc~chk proc~kbproj_gindex kbproj_gindex proc~kbproj_gindex->proc~chk proc~zvalfis ZVALFIS proc~zvalfis->proc~chk proc~nkbl_func NKBL_FUNC proc~nkbl_func->proc~chk proc~atmpopfio ATMPOPFIO proc~atmpopfio->proc~chk proc~rcore RCORE proc~rcore->proc~chk proc~lmxkbfis LMXKBFIS proc~lmxkbfis->proc~chk proc~cnfigfio CNFIGFIO proc~cnfigfio->proc~chk proc~massfis MASSFIS proc~massfis->proc~chk proc~lofio LOFIO proc~lofio->proc~chk proc~lomaxfis LOMAXFIS proc~lomaxfis->proc~chk proc~nztfl NZTFL proc~nztfl->proc~chk proc~vna_sub vna_sub proc~vna_sub->proc~chk proc~vna_sub->proc~floating proc~ldau_gindex ldau_gindex proc~ldau_gindex->proc~chk proc~chcore_sub chcore_sub proc~chcore_sub->proc~chk proc~chcore_sub->proc~floating proc~mofio MOFIO proc~mofio->proc~chk proc~psover psover proc~psover->proc~chk proc~psover->proc~floating proc~nofis NOFIS proc~nofis->proc~chk proc~orb_gindex orb_gindex proc~orb_gindex->proc~chk proc~izofis IZOFIS proc~izofis->proc~chk proc~vna_gindex vna_gindex proc~vna_gindex->proc~chk proc~symfio SYMFIO proc~symfio->proc~chk proc~symfio->proc~lofio proc~symfio->proc~mofio proc~rcut rcut proc~rcut->proc~chk proc~rchlocal RCHLOCAL proc~rchlocal->proc~chk proc~all_phi all_phi proc~all_phi->proc~chk proc~all_phi->proc~floating proc~dhscf_init dhscf_init proc~dhscf_init->proc~rcore proc~dhscf_init->proc~rcut proc~rhooda rhooda proc~dhscf_init->proc~rhooda proc~setup_h0 setup_H0 proc~setup_h0->proc~uion proc~setup_h0->proc~dhscf_init proc~dfscf dfscf proc~dfscf->proc~rcut proc~vmat vmat proc~vmat->proc~rcut proc~rhooda->proc~rcut proc~phiatm phiatm proc~rhooda->proc~phiatm proc~rhoofd rhoofd proc~rhoofd->proc~rcut proc~delk delk proc~delk->proc~rcut proc~floating->proc~izofis proc~dhscf dhscf proc~dhscf->proc~dfscf proc~dhscf->proc~vmat proc~dhscf->proc~rhoofd proc~phiatm->proc~floating proc~delk_wrapper delk_wrapper proc~delk_wrapper->proc~delk proc~rphiatm rphiatm proc~rphiatm->proc~floating proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian var panprocchkCalledByGraph = svgPanZoom('#procchkCalledByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code chk Source Code subroutine chk ( name , is ) character ( len =* ), intent ( in ) :: name integer , intent ( in ) :: is if (( is . lt . 1 ). or .( is . gt . nspecies )) then write ( message , '(2a,i3,a,i3)' ) $ name , \": Wrong species\" , is , \". Have\" , nspecies call die ( message ) endif end subroutine chk","tags":"","loc":"proc/chk.html","title":"chk – SIESTA"},{"text":"private subroutine vna_sub(is, r, v, grv) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: v real(kind=dp), intent(out) :: grv (3) Calls proc~~vna_sub~~CallsGraph proc~vna_sub vna_sub proc~chk chk proc~vna_sub->proc~chk proc~floating floating proc~vna_sub->proc~floating proc~rad_get rad_get proc~vna_sub->proc~rad_get proc~die die proc~chk->proc~die proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~izofis->proc~chk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code vna_sub Source Code subroutine vna_sub ( is , r , v , grv ) integer , intent ( in ) :: is ! Species index real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: v ! Value of local pseudopotential real ( dp ), intent ( out ) :: grv ( 3 ) ! Gradient of local pseudopotential C Returns local part of neutral - atom Kleynman - Bylander pseudopotential . C Distances in Bohr , Energies in Rydbergs C 2 ) Returns exactly zero when | R | > RCUT ( IS , 0 ) real ( dp ) rmod , dvdr call chk ( 'vna_sub' , is ) v = 0.0_dp grv ( 1 : 3 ) = 0.0_dp if ( floating ( is )) return func => species ( is )% vna rmod = sqrt ( sum ( r * r )) if ( rmod . gt . func % cutoff ) return call rad_get ( func , rmod , v , dvdr ) rmod = rmod + tiny20 grv ( 1 : 3 ) = dvdr * r ( 1 : 3 ) / rmod end subroutine vna_sub","tags":"","loc":"proc/vna_sub.html","title":"vna_sub – SIESTA"},{"text":"public subroutine psch(is, r, ch, grch) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: ch real(kind=dp), intent(out) :: grch (3) Calls proc~~psch~~CallsGraph proc~psch psch proc~chk chk proc~psch->proc~chk proc~floating floating proc~psch->proc~floating proc~rad_get rad_get proc~psch->proc~rad_get proc~die die proc~chk->proc~die proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~izofis->proc~chk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code psch Source Code subroutine psch ( is , r , ch , grch ) integer , intent ( in ) :: is ! Species index real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: ch ! Local pseudopot. charge dens. real ( dp ), intent ( out ) :: grch ( 3 ) ! Gradient of local ps. ch. dens. C Returns 'local-pseudotential charge density' . C Distances in Bohr , Energies in Rydbergs C Density in electrons / Bohr ** 3 C 2 ) Returns exactly zero when | R | > Rchloc real ( dp ) :: rmod , dchdr call chk ( 'psch' , is ) ch = 0.0_dp grch ( 1 : 3 ) = 0.0_dp if ( floating ( is )) return func => species ( is )% chlocal rmod = sqrt ( sum ( r * r )) if ( rmod . gt . func % cutoff ) return call rad_get ( func , rmod , ch , dchdr ) rmod = rmod + tiny20 grch ( 1 : 3 ) = dchdr * r ( 1 : 3 ) / rmod end subroutine psch","tags":"","loc":"proc/psch.html","title":"psch – SIESTA"},{"text":"public subroutine chcore_sub(is, r, ch, grch) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: ch real(kind=dp), intent(out) :: grch (3) Calls proc~~chcore_sub~~CallsGraph proc~chcore_sub chcore_sub proc~chk chk proc~chcore_sub->proc~chk proc~floating floating proc~chcore_sub->proc~floating proc~rad_get rad_get proc~chcore_sub->proc~rad_get proc~die die proc~chk->proc~die proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~izofis->proc~chk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code chcore_sub Source Code subroutine chcore_sub ( is , r , ch , grch ) integer , intent ( in ) :: is ! Species index real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: ch ! Value of pseudo-core charge dens. real ( dp ), intent ( out ) :: grch ( 3 ) ! Gradient of pseudo-core ch. dens. C Returns returns pseudo - core charge density for non - linear core correction C in the xc potential . C Distances in Bohr , Energies in Rydbergs , Density in electrons / Bohr ** 3 C 2 ) Returns exactly zero when | R | > Rcore real ( dp ) rmod , dchdr call chk ( 'chcore_sub' , is ) ch = 0.0_dp grch ( 1 : 3 ) = 0.0_dp if ( floating ( is )) return func => species ( is )% core rmod = sqrt ( sum ( r * r )) rmod = rmod + tiny20 ! Moved here. JMS, Dec.2012 if ( rmod . gt . func % cutoff ) return call rad_get ( func , rmod , ch , dchdr ) !      rmod=rmod+tiny20                   ! Removed. JMS, Dec.2012 grch ( 1 : 3 ) = dchdr * r ( 1 : 3 ) / rmod end subroutine chcore_sub","tags":"","loc":"proc/chcore_sub.html","title":"chcore_sub – SIESTA"},{"text":"public subroutine phiatm(is, io, r, phi, grphi) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: phi real(kind=dp), intent(out) :: grphi (3) Calls proc~~phiatm~~CallsGraph proc~phiatm phiatm proc~rad_get rad_get proc~phiatm->proc~rad_get rlylm rlylm proc~phiatm->rlylm proc~floating floating proc~phiatm->proc~floating splint splint proc~rad_get->splint proc~izofis IZOFIS proc~floating->proc~izofis proc~chk chk proc~izofis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~phiatm~~CalledByGraph proc~phiatm phiatm proc~rhooda rhooda proc~rhooda->proc~phiatm proc~dhscf_init dhscf_init proc~dhscf_init->proc~rhooda proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code phiatm Source Code subroutine phiatm ( is , io , r , phi , grphi ) integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) !              IO > 0 =>  Basis orbitals !              IO = 0 =>  Local screened pseudopotential !              IO < 0 =>  Kleynman-Bylander projectors real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom real ( dp ), intent ( out ) :: phi ! Basis orbital, KB projector, or !  local pseudopotential real ( dp ), intent ( out ) :: grphi ( 3 ) ! Gradient of BO, KB proj, or Loc ps C Returns Kleynman - Bylander local pseudopotential , nonlocal projectors , C and atomic basis orbitals ( and their gradients ). C Distances in Bohr C 1 ) Each projector and basis function has a well defined total C angular momentum ( quantum number l ). C 2 ) Basis functions are normalized and mutually orthogonal C 3 ) Projection functions are normalized and mutually orthogonal C 4 ) Normalization of KB projectors | Phi_lm > is such that C < Psi | V_KB | Psi '> = <Psi|V_local|Psi' > + C Sum_lm ( epsKB_l * < Psi | Phi_lm > * < Phi_lm | Psi ' > ) C where epsKB_l is returned by function EPSKB C 5 ) Prints a message and stops when no data exits for IS and / or IO C 6 ) Returns exactly zero when | R | > RCUT ( IS , IO ) C 7 ) PHIATM with IO = 0 is strictly equivalent to VNA_SUB real ( dp ) rmod , phir , dphidr real ( dp ) rly ( max_ilm ), grly ( 3 , max_ilm ) integer i , l , m , ik , ilm phi = 0.0_dp grphi ( 1 : 3 ) = 0.0_dp spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"phiatm: No such orbital\" ) func => spp % orbnl ( spp % orb_index ( io )) l = spp % orb_l ( io ) m = spp % orb_m ( io ) else if ( io . lt . 0 ) then if ( floating ( is )) return ik = - io if ( ik . gt . spp % nprojs ) call die ( \"phiatm: No such projector\" ) func => spp % pjnl ( spp % pj_index ( ik )) l = spp % pj_l ( ik ) m = spp % pj_m ( ik ) else ! io=0 if ( floating ( is )) return func => spp % vna l = 0 m = 0 endif rmod = sqrt ( sum ( r * r )) + tiny20 if ( rmod . gt . func % cutoff - tiny12 ) return call rad_get ( func , rmod , phir , dphidr ) if ( io . eq . 0 ) then phi = phir grphi ( 1 : 3 ) = dphidr * r ( 1 : 3 ) / rmod else ilm = l * l + l + m + 1 call rlylm ( l , r , rly , grly ) phi = phir * rly ( ilm ) do i = 1 , 3 grphi ( i ) = dphidr * rly ( ilm ) * r ( i ) / rmod + phir * grly ( i , ilm ) enddo endif end subroutine phiatm","tags":"","loc":"proc/phiatm.html","title":"phiatm – SIESTA"},{"text":"public subroutine rphiatm(is, io, r, phi, dphidr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io real(kind=dp), intent(in) :: r real(kind=dp), intent(out) :: phi real(kind=dp), intent(out) :: dphidr Calls proc~~rphiatm~~CallsGraph proc~rphiatm rphiatm proc~rad_get rad_get proc~rphiatm->proc~rad_get proc~floating floating proc~rphiatm->proc~floating splint splint proc~rad_get->splint proc~izofis IZOFIS proc~floating->proc~izofis proc~chk chk proc~izofis->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code rphiatm Source Code subroutine rphiatm ( is , io , r , phi , dphidr ) integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: io ! Orbital index (within atom) !              IO > 0 =>  Basis orbitals !              IO = 0 =>  Local screened pseudopotential !              IO < 0 =>  Kleynman-Bylander projectors real ( dp ), intent ( in ) :: r ! Radial distance, relative to atom real ( dp ), intent ( out ) :: phi ! Basis orbital, KB projector, or !  local pseudopotential real ( dp ), intent ( out ) :: dphidr ! Radial derivative of BO, !  KB proj, or Loc pseudopot. C Returns the radial component of C Kleynman - Bylander local pseudopotential , nonlocal projectors , C and atomic basis orbitals ( and their radial drivatives ) C Distances in Bohr C 1 ) Each projector and basis function has a well defined total C angular momentum ( quantum number l ). C 2 ) Basis functions are normalized and mutually orthogonal C 3 ) Projection functions are normalized and mutually orthogonal C 4 ) Normalization of KB projectors | Phi_lm > is such that C < Psi | V_KB | Psi '> = <Psi|V_local|Psi' > + C Sum_lm ( epsKB_l * < Psi | Phi_lm > * < Phi_lm | Psi ' > ) C where epsKB_l is returned by function EPSKB C 6 ) Returns exactly zero when | R | > RCUT ( IS , IO ) C 7 ) RPHIATM with ITYPE = 0 is strictly equivalent to VNA_SUB real ( dp ) rmod , phir integer l , m , ik phi = 0.0_dp dphidr = 0._dp spp => species ( is ) if ( io . gt . 0 ) then if ( io . gt . spp % norbs ) call die ( \"rphiatm: No such orbital\" ) func => spp % orbnl ( spp % orb_index ( io )) l = spp % orb_l ( io ) m = spp % orb_m ( io ) else if ( io . lt . 0 ) then if ( floating ( is )) return ik = - io if ( ik . gt . spp % nprojs ) call die ( \"rphiatm: No such projector\" ) func => spp % pjnl ( spp % pj_index ( ik )) l = spp % pj_l ( ik ) m = spp % pj_m ( ik ) else if ( floating ( is )) return func => spp % vna l = 0 m = 0 endif rmod = r + tiny20 if ( rmod . gt . func % cutoff - tiny12 ) return call rad_get ( func , rmod , phir , dphidr ) if ( l . eq . 0 ) then phi = phir elseif ( l . eq . 1 ) then phi = phir * r dphidr = dphidr * r dphidr = dphidr + phir else phi = phir * r ** l dphidr = dphidr * r ** l dphidr = dphidr + l * phir * r ** ( l - 1 ) endif end subroutine rphiatm","tags":"","loc":"proc/rphiatm.html","title":"rphiatm – SIESTA"},{"text":"public subroutine all_phi(is, it, r, nphi, phi, grphi) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: it real(kind=dp), intent(in) :: r (3) integer, intent(out) :: nphi real(kind=dp), intent(out) :: phi (:) real(kind=dp), intent(out), optional :: grphi (:,:) Calls proc~~all_phi~~CallsGraph proc~all_phi all_phi proc~chk chk proc~all_phi->proc~chk rlylm rlylm proc~all_phi->rlylm proc~die die proc~all_phi->proc~die proc~floating floating proc~all_phi->proc~floating proc~rad_get rad_get proc~all_phi->proc~rad_get proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint proc~izofis->proc~chk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code all_phi Source Code subroutine all_phi ( is , it , r , nphi , phi , grphi ) integer , intent ( in ) :: is ! Species index integer , intent ( in ) :: it ! Orbital-type switch: ! IT > 0 => Basis orbitals ! IT < 0 => KB projectors real ( dp ), intent ( in ) :: r ( 3 ) ! Point vector, relative to atom integer , intent ( out ) :: nphi ! Number of phi's real ( dp ), intent ( out ) :: phi (:) ! Basis orbital, KB projector, or !  local pseudopotential real ( dp ), optional , intent ( out ) :: grphi (:,:) ! Gradient of phi C Returns Kleynman - Bylander local pseudopotential , nonlocal projectors , C and atomic basis orbitals ( and their gradients ). C Same as phiatm but returns all orbitals or KB projectors of the atom C Written by D . Sanchez - Portal and J . M . Soler . Jan . 2000 C Distances in Bohr C 1 ) Each projector and basis function has a well defined total C angular momentum ( quantum number l ). C 2 ) Basis functions are normalized and mutually orthogonal C 3 ) Projection functions are normalized and mutually orthogonal C 4 ) Normalization of KB projectors | Phi_lm > is such that C < Psi | V_KB | Psi '> = <Psi|V_local|Psi' > + C Sum_lm ( epsKB_l * < Psi | Phi_lm > * < Phi_lm | Psi '> ) C    where epsKB_l is returned by function EPSKB C 5) Prints a message and stops when no data exits for IS C 6) Returns exactly zero when |R| > RCUT(IS,IO) C 8) If arrays phi or grphi are too small, returns with the required C    value of nphi integer i, jlm, l, lmax, m, maxlm, n double precision  rmod, phir, dphidr real(dp) rly(max_ilm), grly(3,max_ilm) integer, parameter :: maxphi=100 integer :: ilm(maxphi) double precision :: rmax(maxphi) logical :: within(maxphi) call chk(' all_phi ',is) spp => species(is) !     Find number of orbitals if (it.gt.0) then nphi=spp%norbs elseif (it.lt.0) then nphi=spp%nprojs else call die(\"all_phi: Please use phiatm to get Vna...\") endif if (nphi.gt.maxphi) call die(' all_phi : maxphi too small ') if (it.gt.0) then do i = 1, nphi l = spp%orb_l(i) m = spp%orb_m(i) ilm(i) = l*(l+1)+m+1 op => spp%orbnl(spp%orb_index(i)) rmax(i) = op%cutoff enddo else do i = 1, nphi pp => spp%pjnl(spp%pj_index(i)) rmax(i) = pp%cutoff l = spp%pj_l(i) m = spp%pj_m(i) ilm(i) = l*(l+1)+m+1 enddo endif !     Check size of output arrays if (present(grphi)) then if (size(grphi,1).ne.3) .    call die(' all_phi : incorrect first dimension of grphi ' ) n = min ( size ( phi ), size ( grphi , 2 ) ) else n = size ( phi ) endif !     Return if the caller did not provide arrays large enough... if ( n . lt . nphi ) return !     Initialize orbital values phi ( 1 : nphi ) = 0._dp if ( present ( grphi )) grphi (:, 1 : nphi ) = 0._dp if (( it . lt . 0 ) . and . floating ( is )) return !     Find for which orbitals rmod < rmax and test for quick return rmod = sqrt ( sum ( r * r )) + tiny20 within ( 1 : nphi ) = ( rmax ( 1 : nphi ) > rmod ) if (. not . any ( within ( 1 : nphi ))) return !     Find spherical harmonics maxlm = maxval ( ilm ( 1 : nphi ), mask = within ( 1 : nphi ) ) lmax = nint ( sqrt ( real ( maxlm , dp ))) - 1 call rlylm ( lmax , r , rly , grly ) !     Find values i_loop : do i = 1 , nphi !       Check if rmod > rmax if (. not . within ( i )) cycle i_loop !       Find radial part if ( it . gt . 0 ) then func => spp % orbnl ( spp % orb_index ( i )) else func => spp % pjnl ( spp % pj_index ( i )) endif call rad_get ( func , rmod , phir , dphidr ) !       Multiply radial and angular parts jlm = ilm ( i ) phi ( i ) = phir * rly ( jlm ) if ( present ( grphi )) . grphi (:, i ) = dphidr * rly ( jlm ) * r (:) / rmod + . phir * grly (:, jlm ) enddo i_loop end subroutine all_phi","tags":"","loc":"proc/all_phi.html","title":"all_phi – SIESTA"},{"text":"public subroutine psover(is1, is2, r, energ, dedr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is1 integer, intent(in) :: is2 real(kind=dp), intent(in) :: r real(kind=dp), intent(out) :: energ real(kind=dp), intent(out) :: dedr Calls proc~~psover~~CallsGraph proc~psover psover proc~chk chk proc~psover->proc~chk proc~floating floating proc~psover->proc~floating proc~rad_get rad_get proc~psover->proc~rad_get proc~die die proc~chk->proc~die proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~izofis->proc~chk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code psover Source Code subroutine psover ( is1 , is2 , r , energ , dedr ) integer , intent ( in ) :: is1 , is2 ! Species indexes real ( dp ), intent ( in ) :: r ! Distance between atoms real ( dp ), intent ( out ) :: energ ! Value of the correction !  interaction energy real ( dp ), intent ( out ) :: dedr ! Radial derivative of the correction C Returns electrostatic correction to the ions interaction energy C due to the overlap of the two 'local pseudopotential charge densities' C Distances in Bohr , Energies in Rydbergs C 2 ) Returns exactly zero when | R | > Rchloc integer ismx , ismn , indx real ( dp ) r_local call chk ( 'psover' , is1 ) call chk ( 'psover' , is2 ) energ = 0.0_dp dedr = 0.0_dp if ( floating ( is1 ) . or . floating ( is2 )) return ismx = max ( is1 , is2 ) ismn = min ( is1 , is2 ) indx = (( ismx - 1 ) * ismx ) / 2 + ismn func => elec_corr ( indx ) if ( r . gt . func % cutoff - tiny12 ) return call rad_get ( func , r , energ , dedr ) r_local = r + tiny20 energ = 2.0_dp * energ / r_local dedr = ( - energ + 2.0_dp * dedr ) / r_local end subroutine psover","tags":"","loc":"proc/psover.html","title":"psover – SIESTA"},{"text":"public subroutine state_init(istep) Uses kpoint_scf_m kpoint_t_m m_os m_new_dm m_proximity_check siesta_options units sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices sparse_matrices create_Sparsity_SC m_sparsity_handling m_sparsity_handling m_pivot_methods siesta_geom atomlist alloc m_hsparse m_overlap m_supercell siesta_cml siesta_cml siesta_cml siesta_cml zmatrix m_energies write_subs m_ioxv m_iotdxv m_steps parallel m_spin m_rmaxh m_mixing m_mixing_scf m_normalize_dm m_eo m_gamma files m_mpi_utils m_mpi_utils domain_decom ldau_specs fdf sys m_sparse ts_kpoint_scf_m m_ts_charge m_ts_options m_ts_options m_ts_options m_ts_electype m_ts_global_vars sys m_ts_io m_ts_sparse m_ts_tri_init files m_chess iodm_netcdf iodmhs_netcdf class_Sparsity class_dSpData1D class_dSpData2D class_zSpData2D class_dData2D m_test_io siesta_dicts proc~~state_init~~UsesGraph proc~state_init state_init m_sparse m_sparse proc~state_init->m_sparse m_os m_os proc~state_init->m_os m_test_io m_test_io proc~state_init->m_test_io ts_kpoint_scf_m ts_kpoint_scf_m proc~state_init->ts_kpoint_scf_m ldau_specs ldau_specs proc~state_init->ldau_specs siesta_dicts siesta_dicts proc~state_init->siesta_dicts m_ts_options m_ts_options proc~state_init->m_ts_options class_dData2D class_dData2D proc~state_init->class_dData2D m_supercell m_supercell proc~state_init->m_supercell m_proximity_check m_proximity_check proc~state_init->m_proximity_check module~parallel parallel proc~state_init->module~parallel m_overlap m_overlap proc~state_init->m_overlap m_energies m_energies proc~state_init->m_energies module~sys sys proc~state_init->module~sys domain_decom domain_decom proc~state_init->domain_decom kpoint_scf_m kpoint_scf_m proc~state_init->kpoint_scf_m atomlist atomlist proc~state_init->atomlist m_steps m_steps proc~state_init->m_steps m_ts_electype m_ts_electype proc~state_init->m_ts_electype m_normalize_dm m_normalize_dm proc~state_init->m_normalize_dm class_dSpData2D class_dSpData2D proc~state_init->class_dSpData2D siesta_geom siesta_geom proc~state_init->siesta_geom m_sparsity_handling m_sparsity_handling proc~state_init->m_sparsity_handling m_ts_charge m_ts_charge proc~state_init->m_ts_charge m_iotdxv m_iotdxv proc~state_init->m_iotdxv m_ts_tri_init m_ts_tri_init proc~state_init->m_ts_tri_init module~units units proc~state_init->module~units iodmhs_netcdf iodmhs_netcdf proc~state_init->iodmhs_netcdf kpoint_t_m kpoint_t_m proc~state_init->kpoint_t_m m_mpi_utils m_mpi_utils proc~state_init->m_mpi_utils class_zSpData2D class_zSpData2D proc~state_init->class_zSpData2D files files proc~state_init->files m_pivot_methods m_pivot_methods proc~state_init->m_pivot_methods write_subs write_subs proc~state_init->write_subs module~siesta_options siesta_options proc~state_init->module~siesta_options class_Sparsity class_Sparsity proc~state_init->class_Sparsity m_ts_global_vars m_ts_global_vars proc~state_init->m_ts_global_vars m_ts_io m_ts_io proc~state_init->m_ts_io m_chess m_chess proc~state_init->m_chess m_ioxv m_ioxv proc~state_init->m_ioxv siesta_cml siesta_cml proc~state_init->siesta_cml m_spin m_spin proc~state_init->m_spin module~m_mixing m_mixing proc~state_init->module~m_mixing m_new_dm m_new_dm proc~state_init->m_new_dm m_hsparse m_hsparse proc~state_init->m_hsparse class_dSpData1D class_dSpData1D proc~state_init->class_dSpData1D sparse_matrices sparse_matrices proc~state_init->sparse_matrices alloc alloc proc~state_init->alloc m_gamma m_gamma proc~state_init->m_gamma fdf fdf proc~state_init->fdf create_Sparsity_SC create_Sparsity_SC proc~state_init->create_Sparsity_SC m_eo m_eo proc~state_init->m_eo m_rmaxh m_rmaxh proc~state_init->m_rmaxh m_ts_sparse m_ts_sparse proc~state_init->m_ts_sparse module~m_mixing_scf m_mixing_scf proc~state_init->module~m_mixing_scf iodm_netcdf iodm_netcdf proc~state_init->iodm_netcdf zmatrix zmatrix proc~state_init->zmatrix module~precision precision module~units->module~precision module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D module~m_mixing_scf->module~m_mixing module~m_mixing_scf->class_Fstack_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: istep Calls proc~~state_init~~CallsGraph proc~state_init state_init volcel volcel proc~state_init->volcel kpoint_delete kpoint_delete proc~state_init->kpoint_delete mscell mscell proc~state_init->mscell escf escf proc~state_init->escf re_alloc re_alloc proc~state_init->re_alloc newddata2d newddata2d proc~state_init->newddata2d nsc nsc proc~state_init->nsc proc~mixers_history_init mixers_history_init proc~state_init->proc~mixers_history_init globalize_or globalize_or proc~state_init->globalize_or iotdxv iotdxv proc~state_init->iotdxv timer timer proc~state_init->timer time_io time_io proc~state_init->time_io proc~check_cohp check_cohp proc~state_init->proc~check_cohp delete delete proc~state_init->delete file_exist file_exist proc~state_init->file_exist listhptr listhptr proc~state_init->listhptr crtsparsity_sc crtsparsity_sc proc~state_init->crtsparsity_sc setup_kpoint_scf setup_kpoint_scf proc~state_init->setup_kpoint_scf proximity_check proximity_check proc~state_init->proximity_check siesta_write_positions siesta_write_positions proc~state_init->siesta_write_positions superc superc proc~state_init->superc attach attach proc~state_init->attach cmlendpropertylist cmlendpropertylist proc~state_init->cmlendpropertylist newdspdata1d newdspdata1d proc~state_init->newdspdata1d ts_sparse_init ts_sparse_init proc~state_init->ts_sparse_init setup_dm_netcdf_file setup_dm_netcdf_file proc~state_init->setup_dm_netcdf_file sp_to_spglobal sp_to_spglobal proc~state_init->sp_to_spglobal proc~die die proc~state_init->proc~die ioxv ioxv proc~state_init->ioxv setup_ts_kpoint_scf setup_ts_kpoint_scf proc~state_init->setup_ts_kpoint_scf init_val init_val proc~state_init->init_val newsparsity newsparsity proc~state_init->newsparsity setup_ordern_indexes setup_ordern_indexes proc~state_init->setup_ordern_indexes setup_dmhs_netcdf_file setup_dmhs_netcdf_file proc~state_init->setup_dmhs_netcdf_file cmlstartpropertylist cmlstartpropertylist proc~state_init->cmlstartpropertylist newdspdata2d newdspdata2d proc~state_init->newdspdata2d new_dm new_dm proc~state_init->new_dm proc~message message proc~state_init->proc~message madelung madelung proc~state_init->madelung chess_init chess_init proc~state_init->chess_init exact_sc_ag exact_sc_ag proc~state_init->exact_sc_ag normalize_dm normalize_dm proc~state_init->normalize_dm get_chess_parameter get_chess_parameter proc~state_init->get_chess_parameter numh numh proc~state_init->numh write_zmatrix write_zmatrix proc~state_init->write_zmatrix hsparse hsparse proc~state_init->hsparse dict_repopulate_md dict_repopulate_md proc~state_init->dict_repopulate_md cmladdproperty cmladdproperty proc~state_init->cmladdproperty proc~bye bye proc~state_init->proc~bye outcoor outcoor proc~state_init->outcoor globalize_sum globalize_sum proc~state_init->globalize_sum xij_offset xij_offset proc~state_init->xij_offset fdf_get fdf_get proc~state_init->fdf_get newzspdata2d newzspdata2d proc~state_init->newzspdata2d ts_tri_analyze ts_tri_analyze proc~state_init->ts_tri_analyze val val proc~state_init->val ucell ucell proc~state_init->ucell kpoint_nullify kpoint_nullify proc~state_init->kpoint_nullify write_debug write_debug proc~state_init->write_debug ts_write_tshs ts_write_tshs proc~state_init->ts_write_tshs fname_tshs fname_tshs proc~state_init->fname_tshs nscold nscold proc~state_init->nscold isc_off isc_off proc~state_init->isc_off overlap overlap proc~state_init->overlap sporb_to_spatom sporb_to_spatom proc~state_init->sporb_to_spatom domaindecom domaindecom proc~state_init->domaindecom proc~mixers_history_init->delete proc~current_itt current_itt proc~mixers_history_init->proc~current_itt new new proc~mixers_history_init->new proc~check_cohp->proc~message io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~message->io_close proc~message->io_assign proc~message->pxfflush proc~bye->cmlfinishfile mpi_finalize mpi_finalize proc~bye->mpi_finalize proc~bye->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~state_init~~CalledByGraph proc~state_init state_init proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code state_init Source Code subroutine state_init ( istep ) use kpoint_scf_m , only : setup_kpoint_scf , kpoints_scf use kpoint_t_m , only : kpoint_delete , kpoint_nullify use m_os , only : file_exist use m_new_dm , only : new_dm use m_proximity_check , only : proximity_check use siesta_options use units , only : Ang use sparse_matrices , only : maxnh , numh , listh , listhptr use sparse_matrices , only : Dold , Dscf , DM_2D use sparse_matrices , only : Eold , Escf , EDM_2D use sparse_matrices , only : Hold , H , H_2D use sparse_matrices , only : xijo , xij_2D use sparse_matrices , only : S , S_1D use sparse_matrices , only : H_kin_1D , H_vkb_1D use sparse_matrices , only : H_ldau_2D use sparse_matrices , only : H_so_on_2D , H_so_off_2D use sparse_matrices , only : sparse_pattern use sparse_matrices , only : block_dist , single_dist use sparse_matrices , only : DM_history use create_Sparsity_SC , only : crtSparsity_SC use m_sparsity_handling , only : SpOrb_to_SpAtom use m_sparsity_handling , only : Sp_to_Spglobal use m_pivot_methods , only : sp2graphviz use siesta_geom use atomlist , only : iphorb , iphkb , indxua , & rmaxo , rmaxkb , rmaxv , rmaxldau , & lastkb , lasto , superc , indxuo , & no_u , no_s , no_l , iza , qtots use alloc , only : re_alloc , de_alloc , alloc_report use m_hsparse , only : hsparse use m_overlap , only : overlap use m_supercell , only : exact_sc_ag use siesta_cml , only : cml_p , cmlStartStep , mainXML use siesta_cml , only : cmlStartPropertyList use siesta_cml , only : cmlEndPropertyList use siesta_cml , only : cmlAddProperty use zmatrix , only : lUseZmatrix , write_zmatrix use m_energies , only : Emad use write_subs use m_ioxv , only : ioxv use m_iotdxv , only : iotdxv use m_steps use parallel , only : IOnode , node , nodes , BlockSize use m_spin , only : spin use m_rmaxh use m_mixing , only : mixers_history_init use m_mixing_scf , only : scf_mixs , scf_mix use m_normalize_dm , only : normalize_dm use m_eo use m_gamma use files , only : slabel use m_mpi_utils , only : globalize_or use m_mpi_utils , only : globalize_sum use domain_decom , only : domainDecom , use_dd , use_dd_perm use ldau_specs , only : switch_ldau , ldau_init use fdf , only : fdf_get use sys , only : message , die use m_sparse , only : xij_offset use ts_kpoint_scf_m , only : setup_ts_kpoint_scf , ts_kpoints_scf use m_ts_charge , only : TS_RHOCORR_METHOD , TS_RHOCORR_FERMI use m_ts_options , only : BTD_method use m_ts_options , only : TS_Analyze use m_ts_options , only : N_Elec , Elecs , IsVolt use m_ts_electype use m_ts_global_vars , only : TSrun , TSmode , onlyS use sys , only : bye use m_ts_io , only : fname_TSHS , ts_write_tshs use m_ts_sparse , only : ts_sparse_init use m_ts_tri_init , only : ts_tri_init , ts_tri_analyze use files , only : slabel , label_length #ifdef SIESTA__CHESS use m_chess , only : CheSS_init , get_CheSS_parameter #endif #ifdef CDF use iodm_netcdf , only : setup_dm_netcdf_file use iodmhs_netcdf , only : setup_dmhs_netcdf_file #endif use class_Sparsity use class_dSpData1D use class_dSpData2D use class_zSpData2D use class_dData2D #ifdef TEST_IO use m_test_io #endif #ifdef SIESTA__FLOOK use siesta_dicts , only : dict_repopulate_MD #endif implicit none integer :: istep , nnz real ( dp ) :: veclen ! Length of a unit-cell vector real ( dp ) :: rmax logical :: cell_can_change integer :: i , ix , iadispl , ixdispl logical :: auxchanged ! Auxiliary supercell changed? logical :: folding , folding1 logical :: diag_folding , diag_folding1 logical :: foundxv ! dummy for call to ioxv external :: madelung , timer real ( dp ), external :: volcel integer :: ts_kscell_file ( 3 , 3 ) = 0 real ( dp ) :: ts_kdispl_file ( 3 ) = 0.0 logical :: ts_Gamma_file = . true . character ( len = label_length + 6 ) :: fname real ( dp ) :: dummyef = 0.0 , dummyqtot = 0.0 #ifdef SIESTA__CHESS integer :: maxnh_kernel , maxnh_mult , no_l_kernel , no_l_mult integer , dimension (:), allocatable :: listh_kernel , listh_mult integer , dimension (:), allocatable :: numh_kernel , numh_mult real ( dp ) :: chess_value #endif type ( Sparsity ) :: g_Sp character ( len = 256 ) :: oname type ( dData2D ) :: tmp_2D real ( dp ) :: dummy_qspin ( 8 ) !------------------------------------------------------------------- BEGIN call timer ( 'IterGeom' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_init' ) #endif call timer ( 'state_init' , 1 ) istp = istp + 1 if ( IOnode ) then write ( 6 , '(/,t22,a)' ) repeat ( '=' , 36 ) select case ( idyn ) case ( 0 ) if ( nmove == 0 ) then write ( 6 , '(t25,a)' ) 'Single-point calculation' if ( cml_p ) call cmlStartStep ( mainXML , type = 'Single-Point' , $ index = istp ) else if ( broyden_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin Broyden opt. move = ' , $ istep else if ( fire_optim ) then write ( 6 , '(t25,a,i6)' ) 'Begin FIRE opt. move = ' , $ istep else write ( 6 , '(t25,a,i6)' ) 'Begin CG opt. move = ' , $ istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'Geom. Optim' , $ index = istp ) endif !        Print Z-matrix coordinates if ( lUseZmatrix ) then call write_Zmatrix () endif case ( 1 , 3 ) if ( iquench > 0 ) then write ( 6 , '(t25,a,i6)' ) 'Begin MD quenched step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD-quenched' , $ index = istep ) else write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , $ istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , $ index = istep ) endif case ( 2 , 4 , 5 ) write ( 6 , '(t25,a,i6)' ) 'Begin MD step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'MD' , index = istep ) case ( 6 ) write ( 6 , '(t25,a,i6)' ) 'Begin FC step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FC' , index = istep ) if ( istep . eq . 0 ) then write ( 6 , '(t25,a)' ) 'Undisplaced coordinates' else iadispl = ( istep - mod ( istep - 1 , 6 )) / 6 + ia1 ix = mod ( istep - 1 , 6 ) + 1 ixdispl = ( ix - mod ( ix - 1 , 2 ) + 1 ) / 2 write ( 6 , '(t26,a,i0,/,t26,a,i1,a,f10.6,a)' ) 'displace atom ' , & iadispl , 'in direction ' , ixdispl , ' by' , dx / Ang , ' Ang' endif case ( 8 ) write ( 6 , '(t25,a,i6)' ) 'Begin Server step = ' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'FS' , index = istep ) case ( 9 ) if ( istep == 0 ) then write ( 6 , '(t25,a,i7)' ) 'Explicit coord. initialization' else write ( 6 , '(t25,a,i7)' ) 'Explicit coord. step =' , istep end if if ( cml_p ) call cmlStartStep ( mainXML , type = 'ECS' , index = istep ) case ( 10 ) write ( 6 , '(t25,a,i7)' ) 'LUA coord. step =' , istep if ( cml_p ) call cmlStartStep ( mainXML , type = 'LUA' , index = istep ) end select write ( 6 , '(t22,a)' ) repeat ( '=' , 36 ) !     Print atomic coordinates call outcoor ( ucell , xa , na_u , ' ' , writec ) !     Save structural information in crystallographic format !     (in file SystemLabel.STRUCT_OUT), !     canonical Zmatrix (if applicable), and CML record call siesta_write_positions ( moved = . false .) endif ! IONode ! Write the XV file for single-point calculations, so that ! it is there at the end for those users who rely on it call ioxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , & foundxv ) ! Write TDXV file for TDDFT restart. if ( writetdwf . or . td_elec_dyn ) then call iotdxv ( 'write' , ucell , vcell , na_u , isa , iza , xa , va , foundxv ) end if !     Actualize things if variable cell auxchanged = . false . cell_can_change = ( varcel . or . & ( idyn . eq . 8 ) ! Force/stress evaluation & ) if ( change_kgrid_in_md ) then cell_can_change = cell_can_change . or . & ( idyn . eq . 3 ) . or . ! Parrinello-Rahman & ( idyn . eq . 4 ) . or . ! Nose-Parrinello-Rahman & ( idyn . eq . 5 ) ! Anneal endif if ( cell_can_change . and . & ( istep . ne . inicoor ) . and . (. not . gamma ) ) then !       Will print k-points also call kpoint_delete ( kpoints_scf ) call setup_kpoint_scf ( ucell ) if ( TSmode ) then call kpoint_delete ( ts_kpoints_scf ) else call kpoint_nullify ( ts_kpoints_scf ) end if call setup_ts_kpoint_scf ( ucell , kpoints_scf ) call re_alloc ( eo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'eo' , 'state_init' ) call re_alloc ( qo , 1 , no_u , 1 , spin % spinor , 1 , kpoints_scf % N , & 'qo' , 'state_init' ) !       Find required supercell if ( gamma ) then nsc ( 1 : 3 ) = 1 else do i = 1 , 3 veclen = sqrt ( ucell ( 1 , i ) ** 2 + ucell ( 2 , i ) ** 2 + ucell ( 3 , i ) ** 2 ) nsc ( i ) = 1 + 2 * ceiling ( rmaxh / veclen ) end do ! The above is kept for historical reasons, ! but a tight supercell can be found from the atom-graph info: call exact_sc_ag ( negl , ucell , na_u , isa , xa , nsc ) endif mscell = 0.0_dp do i = 1 , 3 mscell ( i , i ) = nsc ( i ) if ( nsc ( i ) /= nscold ( i )) auxchanged = . true . nscold ( i ) = nsc ( i ) enddo !       Madelung correction for charged systems if ( charnet . ne . 0.0_dp ) then call madelung ( ucell , shape , charnet , Emad ) endif endif !     End variable cell actualization !     Auxiliary supercell !     Do not move from here, as the coordinates might have changed !     even if not the unit cell call superc ( ucell , scell , nsc ) #ifdef SIESTA__FLOOK call dict_repopulate_MD () #endif !     Print unit cell and compute cell volume !     Possible BUG: !     Note that this volume is later used in write_subs and the md output !     routines, even if the cell later changes. if ( IOnode ) call outcell ( ucell ) volume_of_some_cell = volcel ( ucell ) !     Use largest possible range in program, except hsparse... !     2 * rmaxv: Vna overlap !     rmaxo + rmaxkb: Non-local KB action !     2 * (rmaxo + rmaxldau): Interaction through LDAU projector !     2.0_dp * (rmaxo+rmaxkb) : Orbital interaction through KB projectors rmax = max ( 2._dp * rmaxv , 2._dp * ( rmaxo + rmaxldau ), rmaxo + rmaxkb ) if ( . not . negl ) then rmax = max ( rmax , 2.0_dp * ( rmaxo + rmaxkb ) ) endif !     Check if any two atoms are unreasonably close call proximity_check ( rmax ) ! Clear history of mixing parameters call mixers_history_init ( scf_mixs ) scf_mix => scf_mixs ( 1 ) ! Ensure sparsity pattern is empty call delete ( sparse_pattern ) ! sadly deleting the sparse pattern does not necessarily ! mean that the arrays are de-associated. ! Remember that the reference counter could (in MD) ! be higher than 1, hence we need to create \"fake\" ! containers and let the new<class> delete the old ! sparsity pattern nullify ( numh , listhptr , listh ) allocate ( numh ( no_l ), listhptr ( no_l )) ! We do not need to allocate listh ! that will be allocated in hsparse #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then !         Calculate a sparsity pattern with some buffers... Only required !         for CheSS chess_value = get_chess_parameter ( 'chess_buffer_kernel' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_kernel = maxnh no_l_kernel = no_l allocate ( listh_kernel ( maxnh_kernel )) allocate ( numh_kernel ( no_l_kernel )) listh_kernel = listh numh_kernel = numh chess_value = get_chess_parameter ( 'chess_buffer_mult' ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , & set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ buffer = chess_value ) maxnh_mult = maxnh no_l_mult = no_l allocate ( listh_mult ( maxnh_mult )) allocate ( numh_mult ( no_l_mult )) listh_mult = listh numh_mult = numh end if #endif /* CHESS */ !     List of nonzero Hamiltonian matrix elements !     and, if applicable,  vectors between orbital centers !     Listh and xijo are allocated inside hsparse !     Note: We always generate xijo now, for COOP and other !           analyses. call delete ( xij_2D ) ! as xijo will be reallocated nullify ( xijo ) call hsparse ( negl , scell , nsc , na_s , isa , xa , lasto , & lastkb , iphorb , iphKB , maxnh , gamma , $ set_xijo = . true ., folding = folding1 , $ diagonal_folding = diag_folding1 , $ debug_folding = fdf_get ( 'debug-folding' ,. false .)) ! call globalize_or ( diag_folding1 , diag_folding ) call globalize_or ( folding1 , folding ) if ( diag_folding . and . gamma ) then call message ( \"WARNING\" , \"Gamma-point calculation \" // $ \"with interaction between periodic images\" ) call message ( \"WARNING\" , $ \"Some features might not work optimally:\" ) call message ( \"WARNING\" , $ \"e.g. DM initialization from atomic data\" ) if ( harrisfun ) call die ( \"Harris functional run needs \" // $ \"'force-aux-cell T'\" ) else if ( folding ) then if ( gamma ) then call message ( \"INFO\" , \"Gamma-point calculation \" // $ \"with multiply-connected orbital pairs\" ) call message ( \"INFO\" , $ \"Folding of H and S implicitly performed\" ) call check_cohp () else write ( 6 , \"(a,/,a)\" ) \"Non Gamma-point calculation \" // $ \"with multiply-connected orbital pairs \" // $ \"in auxiliary supercell.\" , $ \"Possible internal error. \" // $ \"Use 'debug-folding T' to debug.\" call die ( \"Inadequate auxiliary supercell\" ) endif endif ! call globalize_sum ( maxnh , nnz ) if ( cml_p ) then call cmlStartPropertyList ( mainXML , title = 'Orbital info' ) call cmlAddProperty ( xf = mainXML , value = no_u , $ title = 'Number of orbitals in unit cell' , $ dictref = 'siesta:no_u' , units = \"cmlUnits:countable\" ) call cmlAddProperty ( xf = mainXML , value = nnz , $ title = 'Number of non-zeros' , $ dictref = 'siesta:nnz' , units = \"cmlUnits:countable\" ) call cmlEndPropertyList ( mainXML ) endif ! #ifdef SIESTA__CHESS if ( isolve == SOLVE_CHESS ) then call CheSS_init ( node , nodes , maxnh , maxnh_kernel , maxnh_mult , & no_u , no_l , no_l_kernel , no_l_mult , BlockSize , & spin % spinor , qtots , listh , listh_kernel , listh_mult , & numh , numh_kernel , numh_mult ) deallocate ( listh_kernel ) deallocate ( numh_kernel ) deallocate ( listh_mult ) deallocate ( numh_mult ) end if #endif /* CHESS */ ! ! If using domain decomposition, redistribute orbitals ! for this geometry, based on the hsparse info. ! The first time round, the initial distribution is a ! simple block one (given by preSetOrbitLimits). ! ! Any DM, etc, read from file will be redistributed according ! to the new pattern. ! Inherited DMs from a previous geometry cannot be used if the ! orbital distribution changes. For now, we avoid changing the ! distribution (the variable use_dd_perm is .true. if domain ! decomposition is in effect). Names should be changed... if ( use_dd . and . (. not . use_dd_perm )) then call domainDecom ( no_u , no_l , maxnh ) ! maxnh intent(in) here maxnh = sum ( numh ( 1 : no_l )) ! We still need to re-create Julian Gale's ! indexing for O(N) in parallel. print \"(a5,i3,a20,3i8)\" , $ \"Node: \" , Node , \"no_u, no_l, maxnh: \" , no_u , no_l , maxnh call setup_ordern_indexes ( no_l , no_u , Nodes ) endif ! I would like to skip this alloc/move/dealloc/attach ! by allowing sparsity to have pointer targets. ! However, this poses a problem with intel compilers, ! as it apparently errors out when de-allocating a target pointer write ( oname , \"(a,i0)\" ) \"sparsity for geom step \" , istep call newSparsity ( sparse_pattern , no_l , no_u , maxnh , & numh , listhptr , listh , name = oname ) deallocate ( numh , listhptr , listh ) call attach ( sparse_pattern , & n_col = numh , list_ptr = listhptr , list_col = listh ) ! In case the user requests to create the connectivity graph if ( write_GRAPHVIZ > 0 ) then ! first create the unit-cell sparsity pattern call crtSparsity_SC ( sparse_pattern , g_Sp , UC = . true .) ! next move to global sparsity pattern call Sp_to_Spglobal ( block_dist , g_Sp , g_Sp ) if ( IONode ) then if ( write_GRAPHVIZ /= 2 ) & call sp2graphviz ( trim ( slabel ) // '.ORB.gv' , g_Sp ) ! Convert to atomic if ( write_GRAPHVIZ /= 1 ) then call SpOrb_to_SpAtom ( single_dist , g_Sp , na_u , lasto , g_Sp ) call sp2graphviz ( trim ( slabel ) // '.ATOM.gv' , g_Sp ) end if end if call delete ( g_Sp ) end if ! Copy over xijo array (we can first do it here... :( ) call newdData2D ( tmp_2D , xijo , 'xijo' ) deallocate ( xijo ) write ( oname , \"(a,i0)\" ) \"xijo at geom step \" , istep call newdSpData2D ( sparse_pattern , tmp_2D , block_dist , xij_2D , & name = oname ) call delete ( tmp_2D ) ! decrement container... xijo => val ( xij_2D ) ! Calculate the super-cell offsets... if ( Gamma ) then ! Here we create the super-cell offsets call re_alloc ( isc_off , 1 , 3 , 1 , 1 ) isc_off (:,:) = 0 else call xij_offset ( ucell , nsc , na_u , xa , lasto , & xij_2D , isc_off , & Bcast = . true .) end if ! When the user requests to only do an analyzation, we can call ! appropriate routines and quit if ( TS_Analyze ) then ! Force the creation of the full sparsity pattern call ts_sparse_init ( slabel , IsVolt , N_Elec , Elecs , & ucell , nsc , na_u , xa , lasto , block_dist , sparse_pattern , & Gamma , isc_off ) ! create the tri-diagonal matrix call ts_tri_analyze ( block_dist , sparse_pattern , N_Elec , & Elecs , ucell , na_u , lasto , nsc , isc_off , & BTD_method ) ! Print-out timers call timer ( 'TS-rgn2tri' , 3 ) ! Bye also waits for all processors call bye ( 'transiesta analyzation performed' ) end if write ( oname , \"(a,i0)\" ) \"EDM at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % EDM , block_dist , EDM_2D , & name = oname ) !if (ionode) call print_type(EDM_2D) Escf => val ( EDM_2D ) call re_alloc ( Dold , 1 , maxnh , 1 , spin % DM , name = 'Dold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) call re_alloc ( Hold , 1 , maxnh , 1 , spin % H , name = 'Hold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) if ( converge_EDM ) then call re_alloc ( Eold , 1 , maxnh , 1 , spin % EDM , name = 'Eold' , . routine = 'sparseMat' , copy = . false ., shrink = . true .) end if !     Allocate/reallocate storage associated with Hamiltonian/Overlap matrix write ( oname , \"(a,i0)\" ) \"H at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H , block_dist , H_2D , & name = oname ) !if (ionode) call print_type(H_2D) H => val ( H_2D ) write ( oname , \"(a,i0)\" ) \"H_vkb at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_vkb_1D , name = oname ) !if (ionode) call print_type(H_vkb_1D) write ( oname , \"(a,i0)\" ) \"H_kin at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , H_kin_1D , name = oname ) !if (ionode) call print_type(H_kin_1D) if ( switch_ldau ) then write ( oname , \"(a,i0)\" ) \"H_ldau at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % spinor , & block_dist , H_ldau_2D , name = oname ) ! Initialize to 0, LDA+U may re-calculate !   this matrix sporadically doing the SCF. ! Hence initialization MUST be performed upon ! re-allocation. call init_val ( H_ldau_2D ) if ( inicoor /= istep ) then ! Force initialization of the LDA+U ! when changing geometry ! For the first geometry this is controlled ! by the user via an fdf-key ldau_init = . true . end if end if if ( spin % SO_onsite ) then write ( oname , \"(a,i0)\" ) \"H_so (onsite) at geom step \" , istep call newdSpData2D ( sparse_pattern , spin % H - 2 , & block_dist , H_so_on_2D , name = oname ) else if ( spin % SO_offsite ) then write ( oname , \"(a,i0)\" ) \"H_so (offsite) at geom step \" , istep call newzSpData2D ( sparse_pattern , 4 , & block_dist , H_so_off_2D , name = oname ) endif write ( oname , \"(a,i0)\" ) \"S at geom step \" , istep call newdSpData1D ( sparse_pattern , block_dist , S_1D , name = oname ) if ( ionode ) call print_type ( S_1D ) S => val ( S_1D ) !     Find overlap matrix call overlap ( na_u , na_s , no_s , scell , xa , indxua , rmaxo , maxnh , & lasto , iphorb , isa , numh , listhptr , listh , S ) ! !     Here we could also read a Hamiltonian, either to proceed to !     the analysis section (with nscf=0) or to start a mix-H scf cycle. ! !     Initialize density matrix ! The resizing of Dscf is done inside new_dm call new_DM ( auxchanged , DM_history , DM_2D , EDM_2D ) Dscf => val ( DM_2D ) Escf => val ( EDM_2D ) if ( spin % H > 1 ) call print_spin ( dummy_qspin ) ! Initialize energy-density matrix to zero for first call to overfsm ! Only part of Escf is updated in TS, so if it is put as zero here ! a continuation run gives bad forces. if ( . not . TSrun ) then call normalize_DM ( first = . true . ) !$OMP parallel workshare default(shared) Escf (:,:) = 0.0_dp !$OMP end parallel workshare end if #ifdef TEST_IO ! We test the io-performance here call time_io ( spin % H , H_2D ) #endif !     If onlyS, Save overlap matrix and exit if ( onlyS ) then fname = fname_TSHS ( slabel , onlyS = . true . ) ! We include H as S, well-knowing that we only write one of ! them, there is no need to allocate space for no reason! call ts_write_tshs ( fname , & . true ., Gamma , ts_Gamma_file , & ucell , nsc , isc_off , na_u , no_s , spin % H , & ts_kscell_file , ts_kdispl_file , & xa , lasto , & H_2D , S_1D , indxuo , & dummyEf , dummyQtot , Temp , 0 , 0 ) call bye ( 'Save overlap matrix and exit' ) ! Exit siesta endif ! In case the user is requesting a Fermi-correction ! we need to delete the TS_FERMI file after each iteration if ( TSmode . and . TS_RHOCORR_METHOD == TS_RHOCORR_FERMI & . and . IONode ) then ! Delete the TS_FERMI file (enables ! reading it in and improve on the convergence) if ( file_exist ( 'TS_FERMI' ) ) then i = 23455 ! this should just not be used any were... ! Delete the file... open ( unit = i , file = 'TS_FERMI' ) close ( i , status = 'delete' ) end if end if #ifdef CDF if ( writedm_cdf ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh ) endif if ( writedm_cdf_history ) then call setup_dm_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & istep ) endif if ( writedmhs_cdf ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s ) endif if ( writedmhs_cdf_history ) then call setup_dmhs_netcdf_file ( maxnh , no_l , spin % H , & no_s , indxuo , & numh , listhptr , listh , & s , & istep ) endif #endif call timer ( 'state_init' , 2 ) END subroutine state_init","tags":"","loc":"proc/state_init.html","title":"state_init – SIESTA"},{"text":"private subroutine check_cohp() Uses siesta_options sys proc~~check_cohp~~UsesGraph proc~check_cohp check_cohp module~siesta_options siesta_options proc~check_cohp->module~siesta_options module~sys sys proc~check_cohp->module~sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments None Calls proc~~check_cohp~~CallsGraph proc~check_cohp check_cohp proc~message message proc~check_cohp->proc~message io_assign io_assign proc~message->io_assign pxfflush pxfflush proc~message->pxfflush io_close io_close proc~message->io_close Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~check_cohp~~CalledByGraph proc~check_cohp check_cohp proc~state_init state_init proc~state_init->proc~check_cohp proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code check_cohp Source Code subroutine check_cohp () use siesta_options , only : write_coop use sys , only : message if ( write_coop ) then call message ( \"WARNING\" , \"There are multiply-connected \" // $ \"orbitals.\" ) call message ( \"WARNING\" , \"Your COOP/COHP analysis might \" // $ \"be affected by folding.\" ) call message ( \"WARNING\" , 'Use \"force-aux-cell T \"' // $ 'or k-point sampling' ) endif end subroutine check_cohp","tags":"","loc":"proc/check_cohp.html","title":"check_cohp – SIESTA"},{"text":"public subroutine state_analysis(istep) Uses siesta_cml m_born_charge parallel m_wallclock zmatrix atomlist atomlist m_spin m_fixed sparse_matrices siesta_geom siesta_options units m_stress m_energies m_energies m_ntm m_forces m_energies m_intramol_pressure flook_siesta proc~~state_analysis~~UsesGraph proc~state_analysis state_analysis m_fixed m_fixed proc~state_analysis->m_fixed flook_siesta flook_siesta proc~state_analysis->flook_siesta module~siesta_options siesta_options proc~state_analysis->module~siesta_options siesta_geom siesta_geom proc~state_analysis->siesta_geom zmatrix zmatrix proc~state_analysis->zmatrix module~units units proc~state_analysis->module~units sparse_matrices sparse_matrices proc~state_analysis->sparse_matrices siesta_cml siesta_cml proc~state_analysis->siesta_cml atomlist atomlist proc~state_analysis->atomlist m_forces m_forces proc~state_analysis->m_forces m_born_charge m_born_charge proc~state_analysis->m_born_charge m_energies m_energies proc~state_analysis->m_energies m_stress m_stress proc~state_analysis->m_stress m_wallclock m_wallclock proc~state_analysis->m_wallclock m_spin m_spin proc~state_analysis->m_spin m_intramol_pressure m_intramol_pressure proc~state_analysis->m_intramol_pressure m_ntm m_ntm proc~state_analysis->m_ntm module~parallel parallel proc~state_analysis->module~parallel module~precision precision module~units->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer :: istep Calls proc~~state_analysis~~CallsGraph proc~state_analysis state_analysis amass amass proc~state_analysis->amass va va proc~state_analysis->va volcel volcel proc~state_analysis->volcel moments moments proc~state_analysis->moments kin_stress kin_stress proc~state_analysis->kin_stress siesta_write_forces siesta_write_forces proc~state_analysis->siesta_write_forces cartesianforce_to_zmatforce cartesianforce_to_zmatforce proc~state_analysis->cartesianforce_to_zmatforce print_spin print_spin proc~state_analysis->print_spin wallclock wallclock proc~state_analysis->wallclock siesta_write_stress_pressure siesta_write_stress_pressure proc~state_analysis->siesta_write_stress_pressure remove_intramol_pressure remove_intramol_pressure proc~state_analysis->remove_intramol_pressure slua_call slua_call proc~state_analysis->slua_call update_freeeharris update_freeeharris proc~state_analysis->update_freeeharris update_freee update_freee proc~state_analysis->update_freee born_charge born_charge proc~state_analysis->born_charge cmladdproperty cmladdproperty proc~state_analysis->cmladdproperty cmlstartmodule cmlstartmodule proc~state_analysis->cmlstartmodule mulliken mulliken proc~state_analysis->mulliken eggbox eggbox proc~state_analysis->eggbox timer timer proc~state_analysis->timer cmlendmodule cmlendmodule proc~state_analysis->cmlendmodule write_debug write_debug proc~state_analysis->write_debug fixed fixed proc~state_analysis->fixed Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~state_analysis~~CalledByGraph proc~state_analysis state_analysis proc~siesta_forces siesta_forces proc~siesta_forces->proc~state_analysis Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code state_analysis Source Code subroutine state_analysis ( istep ) use siesta_cml use m_born_charge , only : born_charge use parallel , only : IOnode use m_wallclock , only : wallclock use zmatrix , only : lUseZmatrix , iofaZmat , & CartesianForce_to_ZmatForce use atomlist , only : iaorb , iphorb , amass , no_u , lasto use atomlist , only : indxuo use m_spin , only : spin use m_fixed , only : fixed use sparse_matrices use siesta_geom USE siesta_options use units , only : amu , eV use m_stress use m_energies , only : Etot , FreeE , Eharrs , FreeEHarris , Entropy use m_energies , only : Ebs , Ef use m_ntm use m_forces use m_energies , only : update_FreeE , update_FreeEHarris use m_intramol_pressure , only : remove_intramol_pressure #ifdef SIESTA__FLOOK use flook_siesta , only : slua_call , LUA_FORCES #endif implicit none integer :: istep integer :: ia , jx , ix real ( dp ) :: volume logical :: eggbox_block = . true . ! Read eggbox info from data file? real ( dp ) :: qspin external :: eggbox , mulliken , moments real ( dp ), external :: volcel !------------------------------------------------------------------------- BEGIN call timer ( 'state_analysis' , 1 ) #ifdef DEBUG call write_debug ( '  PRE state_analysis' ) #endif if ( cml_p ) then call cmlStartModule ( xf = mainXML , title = 'SCF Finalization' ) endif !     Write final Kohn-Sham and Free Energy FreeE = Etot - Temp * Entropy FreeEHarris = Eharrs - Temp * Entropy if ( cml_p ) call cmlStartPropertyList ( mainXML , & title = 'Energies and spin' ) if ( IOnode ) then if ( . not . harrisfun ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS(eV) =        ' , Etot / eV if ( cml_p ) then call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = FreeE / eV , & dictref = 'siesta:FreeE' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ebs / eV , & dictref = 'siesta:Ebs' , units = 'siestaUnits:eV' , . fmt = 'r6' ) call cmlAddProperty ( xf = mainXML , value = Ef / eV , & dictref = 'siesta:E_Fermi' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif endif !     Substract egg box effect from energy if ( eggbox_block ) then call eggbox ( 'energy' , ucell , na_u , isa , ntm , xa , fa , Etot , & eggbox_block ) FreeE = Etot - Temp * Entropy if ( IOnode ) & write ( 6 , \"(/a,f14.4)\" ) 'siesta: E_KS - E_eggbox = ' , Etot / eV if ( cml_p ) call cmlAddProperty ( xf = mainXML , value = Etot / eV , & dictref = 'siesta:E_KS_egg' , units = 'siestaUnits:eV' , . fmt = 'r6' ) endif call update_FreeE ( Temp ) call update_FreeEHarris ( Temp ) call print_spin ( qspin ) if ( cml_p ) call cmlEndPropertyList ( mainXML ) !     Substract egg box effect from the forces if ( eggbox_block ) then call eggbox ( 'forces' , ucell , na_u , isa , ntm , xa , fa , Etot , eggbox_block ) endif if ( IOnode ) call write_raw_efs ( stress , na_u , fa , FreeE ) !     Compute stress without internal molecular pressure call remove_intramol_pressure ( ucell , stress , na_u , xa , fa , mstress ) !     Impose constraints to atomic movements by changing forces ........... if ( RemoveIntraMolecularPressure ) then !        Consider intramolecular pressure-removal as another !        kind of constraint call fixed ( ucell , mstress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) else call fixed ( ucell , stress , na_u , isa , amass , xa , fa , & cstress , cfa , ntcon , & magnitude_usage = idyn == 0 ) endif #ifdef SIESTA__FLOOK ! We call it right after using the ! geometry constraints. ! In that way we can use both methods on top ! of each other! ! The easy, already implemented methods in fixed, ! and custom ones in Lua :) call slua_call ( LUA , LUA_FORCES ) #endif !     Calculate and output Zmatrix forces if ( lUseZmatrix . and . ( idyn . eq . 0 )) then call CartesianForce_to_ZmatForce ( na_u , xa , fa ) if ( IOnode ) call iofaZmat () endif !     Compute kinetic contribution to stress kin_stress ( 1 : 3 , 1 : 3 ) = 0.0_dp volume = volcel ( ucell ) do ia = 1 , na_u do jx = 1 , 3 do ix = 1 , 3 kin_stress ( ix , jx ) = kin_stress ( ix , jx ) - & amu * amass ( ia ) * va ( ix , ia ) * va ( jx , ia ) / volume enddo enddo enddo !     Add kinetic term to stress tensor tstress = stress + kin_stress !     Force output if ( IOnode ) then call siesta_write_forces ( istep ) call siesta_write_stress_pressure () call wallclock ( '--- end of geometry step' ) endif !     Population and moment analysis if ( spin % SO . and . orbmoms ) then call moments ( 1 , na_u , no_u , maxnh , numh , listhptr , . listh , S , Dscf , isa , lasto , iaorb , iphorb , . indxuo ) endif ! Call this unconditionally call mulliken ( mullipop , na_u , no_u , maxnh , & numh , listhptr , listh , S , Dscf , isa , & lasto , iaorb , iphorb ) ! !     Call the born effective charge routine only in those steps (even) !     in which the dx  is positive. if ( bornz . and . ( mod ( istep , 2 ) . eq . 0 )) then call born_charge () endif !     End the xml module corresponding to the analysis if ( cml_p ) then call cmlEndModule ( mainXML ) endif call timer ( 'state_analysis' , 2 ) !--------------------------------------------------------------------------- END END subroutine state_analysis","tags":"","loc":"proc/state_analysis.html","title":"state_analysis – SIESTA"},{"text":"public subroutine dhscf_init(nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ucell, mscell, g2max, ntm, maxnd, numd, listdptr, listd, datm, Fal, stressl) Uses precision parallel atmfuncs fdf sys mesh parsing siestaXC bsc_xcmod alloc siesta_options meshsubs meshsubs meshsubs meshsubs meshsubs meshsubs meshsubs moreMeshSubs moreMeshSubs moreMeshSubs meshdscf iogrid_netcdf m_ncdf_io cellxc_mod m_efield m_efield m_efield m_efield m_doping_uniform m_doping_uniform m_rhog m_rhog siesta_options mpi_siesta m_mesh_node m_charge_add m_hartree_add m_ts_global_vars m_ts_options m_ts_voltage m_ts_hartree proc~~dhscf_init~~UsesGraph proc~dhscf_init dhscf_init m_charge_add m_charge_add proc~dhscf_init->m_charge_add m_ncdf_io m_ncdf_io proc~dhscf_init->m_ncdf_io module~sys sys proc~dhscf_init->module~sys m_ts_options m_ts_options proc~dhscf_init->m_ts_options parsing parsing proc~dhscf_init->parsing module~parallel parallel proc~dhscf_init->module~parallel meshdscf meshdscf proc~dhscf_init->meshdscf m_ts_voltage m_ts_voltage proc~dhscf_init->m_ts_voltage siestaXC siestaXC proc~dhscf_init->siestaXC bsc_xcmod bsc_xcmod proc~dhscf_init->bsc_xcmod module~mesh mesh proc~dhscf_init->module~mesh cellxc_mod cellxc_mod proc~dhscf_init->cellxc_mod m_efield m_efield proc~dhscf_init->m_efield m_doping_uniform m_doping_uniform proc~dhscf_init->m_doping_uniform module~siesta_options siesta_options proc~dhscf_init->module~siesta_options m_ts_global_vars m_ts_global_vars proc~dhscf_init->m_ts_global_vars module~moremeshsubs moreMeshSubs proc~dhscf_init->module~moremeshsubs module~precision precision proc~dhscf_init->module~precision meshsubs meshsubs proc~dhscf_init->meshsubs mpi_siesta mpi_siesta proc~dhscf_init->mpi_siesta m_mesh_node m_mesh_node proc~dhscf_init->m_mesh_node m_ts_hartree m_ts_hartree proc~dhscf_init->m_ts_hartree iogrid_netcdf iogrid_netcdf proc~dhscf_init->iogrid_netcdf alloc alloc proc~dhscf_init->alloc module~atmfuncs atmfuncs proc~dhscf_init->module~atmfuncs fdf fdf proc~dhscf_init->fdf module~m_rhog m_rhog proc~dhscf_init->module~m_rhog m_hartree_add m_hartree_add proc~dhscf_init->m_hartree_add module~mesh->module~precision module~moremeshsubs->module~sys module~moremeshsubs->module~parallel module~moremeshsubs->module~precision module~moremeshsubs->alloc module~atmfuncs->module~sys module~atmfuncs->module~precision spher_harm spher_harm module~atmfuncs->spher_harm module~atm_types atm_types module~atmfuncs->module~atm_types module~radial radial module~atmfuncs->module~radial module~m_rhog->module~precision class_Pair_dData1D class_Pair_dData1D module~m_rhog->class_Pair_dData1D class_dData1D class_dData1D module~m_rhog->class_dData1D class_Fstack_Pair_dData1D class_Fstack_Pair_dData1D module~m_rhog->class_Fstack_Pair_dData1D m_spin m_spin module~m_rhog->m_spin module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in) :: norb integer, intent(in) :: iaorb (norb) integer, intent(in) :: iphorb (norb) integer, intent(in) :: nuo integer, intent(in) :: nuotot integer, intent(in) :: nua integer, intent(in) :: na integer, intent(in) :: isa (na) real(kind=dp), intent(in) :: xa (3,na) integer, intent(in) :: indxua (na) real(kind=dp), intent(in) :: ucell (3,3) integer, intent(in) :: mscell (3,3) real(kind=dp), intent(inout) :: g2max integer, intent(inout) :: ntm (3) integer, intent(in) :: maxnd integer, intent(in) :: numd (nuo) integer, intent(in) :: listdptr (nuo) integer, intent(in) :: listd (maxnd) real(kind=dp), intent(in) :: datm (norb) real(kind=dp), intent(inout) :: Fal (3,nua) real(kind=dp), intent(inout) :: stressl (3,3) Calls proc~~dhscf_init~~CallsGraph proc~dhscf_init dhscf_init volcel volcel proc~dhscf_init->volcel ts_init_voltage ts_init_voltage proc~dhscf_init->ts_init_voltage re_alloc re_alloc proc~dhscf_init->re_alloc de_alloc de_alloc proc~dhscf_init->de_alloc fdf_get fdf_get proc~dhscf_init->fdf_get setupextmesh setupextmesh proc~dhscf_init->setupextmesh distriphionmesh distriphionmesh proc~dhscf_init->distriphionmesh proc~rcore RCORE proc~dhscf_init->proc~rcore initmesh initmesh proc~dhscf_init->initmesh leqi leqi proc~dhscf_init->leqi proc~setmeshdistr setMeshDistr proc~dhscf_init->proc~setmeshdistr initatommesh initatommesh proc~dhscf_init->initatommesh fdf_integer fdf_integer proc~dhscf_init->fdf_integer init_hartree_add init_hartree_add proc~dhscf_init->init_hartree_add ts_init_hartree_fix ts_init_hartree_fix proc~dhscf_init->ts_init_hartree_fix partialcoreonmesh partialcoreonmesh proc~dhscf_init->partialcoreonmesh proc~die die proc~dhscf_init->proc~die digcel digcel proc~dhscf_init->digcel proc~reord reord proc~dhscf_init->proc~reord init_mesh_node init_mesh_node proc~dhscf_init->init_mesh_node setgga setgga proc~dhscf_init->setgga phionmesh phionmesh proc~dhscf_init->phionmesh proc~rcut rcut proc~dhscf_init->proc~rcut initialize_efield initialize_efield proc~dhscf_init->initialize_efield createlocaldscfpointers createlocaldscfpointers proc~dhscf_init->createlocaldscfpointers init_charge_add init_charge_add proc~dhscf_init->init_charge_add neutralatomonmesh neutralatomonmesh proc~dhscf_init->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf_init->interface~distmeshdata elecs elecs proc~dhscf_init->elecs timer timer proc~dhscf_init->timer shaper shaper proc~dhscf_init->shaper user_specified_field user_specified_field proc~dhscf_init->user_specified_field proc~rhooda rhooda proc~dhscf_init->proc~rhooda write_debug write_debug proc~dhscf_init->write_debug cdf_init_mesh cdf_init_mesh proc~dhscf_init->cdf_init_mesh set_box_limits set_box_limits proc~dhscf_init->set_box_limits getxc getxc proc~dhscf_init->getxc compute_doping_structs_uniform compute_doping_structs_uniform proc~dhscf_init->compute_doping_structs_uniform ddot ddot proc~dhscf_init->ddot proc~chk chk proc~rcore->proc~chk io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush proc~reord->re_alloc proc~reord->de_alloc proc~reord->timer proc~rcut->proc~chk proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~rhooda->proc~rcut proc~rhooda->write_debug indxuo indxuo proc~rhooda->indxuo endpht endpht proc~rhooda->endpht phi phi proc~rhooda->phi lstpht lstpht proc~rhooda->lstpht proc~phiatm phiatm proc~rhooda->proc~phiatm listp2 listp2 proc~rhooda->listp2 proc~chk->proc~die proc~floating floating proc~phiatm->proc~floating proc~rad_get rad_get proc~phiatm->proc~rad_get rlylm rlylm proc~phiatm->rlylm proc~distmeshdata_int->re_alloc proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~die proc~boxintersection boxIntersection proc~distmeshdata_int->proc~boxintersection proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->proc~die proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->timer proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~distmeshdata_rea->proc~boxintersection mpi_barrier mpi_barrier proc~distmeshdata_rea->mpi_barrier proc~izofis IZOFIS proc~floating->proc~izofis splint splint proc~rad_get->splint proc~izofis->proc~chk var panprocdhscf_initCallsGraph = svgPanZoom('#procdhscf_initCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~dhscf_init~~CalledByGraph proc~dhscf_init dhscf_init proc~setup_h0 setup_H0 proc~setup_h0->proc~dhscf_init Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code dhscf_init Source Code subroutine dhscf_init ( nspin , norb , iaorb , iphorb , & nuo , nuotot , nua , na , & isa , xa , indxua , ucell , & mscell , G2max , ntm , & maxnd , numd , listdptr , listd , datm , & Fal , stressl ) use precision , only : dp , grid_p use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use fdf use sys , only : die use mesh , only : xdsp , nsm , nsp , meshLim use parsing #ifndef BSC_CELLXC use siestaXC , only : getXC ! Returns the XC functional used #else /* BSC_CELLXC */ use bsc_xcmod , only : nXCfunc , XCauth #endif /* BSC_CELLXC */ use alloc , only : re_alloc , de_alloc use siesta_options , only : harrisfun use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use meshsubs , only : PhiOnMesh use meshsubs , only : InitMesh use meshsubs , only : InitAtomMesh use meshsubs , only : setupExtMesh use meshsubs , only : distriPhiOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use meshdscf , only : createLocalDscfPointers use iogrid_netcdf , only : set_box_limits #ifdef NCDF_4 use m_ncdf_io , only : cdf_init_mesh #endif #ifdef BSC_CELLXC use cellxc_mod , only : setGGA #endif /* BSC_CELLXC */ use m_efield , only : initialize_efield , acting_efield use m_efield , only : get_field_from_dipole use m_efield , only : dipole_correction use m_efield , only : user_specified_field use m_doping_uniform , only : initialize_doping_uniform use m_doping_uniform , only : compute_doping_structs_uniform , $ doping_active use m_rhog , only : rhog , rhog_in use m_rhog , only : order_rhog use siesta_options , only : mix_charge #ifdef MPI use mpi_siesta #endif use m_mesh_node , only : init_mesh_node use m_charge_add , only : init_charge_add use m_hartree_add , only : init_hartree_add use m_ts_global_vars , only : TSmode use m_ts_options , only : IsVolt , N_Elec , Elecs use m_ts_voltage , only : ts_init_voltage use m_ts_hartree , only : ts_init_hartree_fix implicit none integer , intent ( in ) :: nspin , norb , iaorb ( norb ), iphorb ( norb ), & nuo , nuotot , nua , na , isa ( na ), & indxua ( na ), mscell ( 3 , 3 ), maxnd , & numd ( nuo ), listdptr ( nuo ), listd ( maxnd ) real ( dp ), intent ( in ) :: xa ( 3 , na ), ucell ( 3 , 3 ), datm ( norb ) real ( dp ), intent ( inout ) :: g2max integer , intent ( inout ) :: ntm ( 3 ) real ( dp ), intent ( inout ) :: Fal ( 3 , nua ), stressl ( 3 , 3 ) real ( dp ), parameter :: tiny = 1.e-12_dp integer :: io , ia , iphi , is , n , i , j integer :: nsc ( 3 ), nbcell , nsd real ( dp ) :: DStres ( 3 , 3 ), volume real ( dp ), external :: volcel , ddot real ( grid_p ) :: dummy_Drho ( 1 , 1 ), dummy_Vaux ( 1 ), & dummy_Vscf ( 1 ) logical , save :: frstme = . true . ! Keeps state real ( grid_p ), pointer :: Vscf (:,:), rhoatm_par (:) integer , pointer :: numphi (:), numphi_par (:) integer :: nm ( 3 ) ! For call to initMesh #ifndef BSC_CELLXC integer :: nXCfunc character ( len = 20 ) :: XCauth ( 10 ), XCfunc ( 10 ) #endif /* ! BSC_CELLXC */ ! Transport direction (unit-cell aligned) integer :: iE real ( dp ) :: ortho , field ( 3 ), field2 ( 3 ) !--------------------------------------------------------------------- BEGIN #ifdef DEBUG call write_debug ( '    PRE dhscf_init' ) #endif ! ---------------------------------------------------------------------- !     General initialisation ! ---------------------------------------------------------------------- !     Start time counter call timer ( 'DHSCF_Init' , 1 ) nsd = min ( nspin , 2 ) nullify ( Vscf , rhoatm_par ) if ( frstme ) then debug_dhscf = fdf_get ( 'Debug.DHSCF' , . false .) nullify ( xdsp , rhopcc , Vna , rhoatm ) !       nsm lives in module m_dhscf now    !! AG** nsm = fdf_integer ( 'MeshSubDivisions' , 2 ) nsm = max ( nsm , 1 ) !       Set mesh sub-division variables & perform one off allocation nsp = nsm * nsm * nsm call re_alloc ( xdsp , 1 , 3 , 1 , nsp , 'xdsp' , 'dhscf_init' ) !       Check spin-spiral wavevector (if defined) if ( spiral . and . nspin . lt . 4 ) & call die ( 'dhscf: ERROR: spiral defined but nspin < 4' ) endif ! First time #ifndef BSC_CELLXC ! Get functional(s) being used call getXC ( nXCfunc , XCfunc , XCauth ) #endif /* ! BSC_CELLXC */ if ( harrisfun ) then do n = 1 , nXCfunc if (. not .( leqi ( XCauth ( n ), 'PZ' ). or . leqi ( XCauth ( n ), 'CA' ))) then call die ( \"** Harris forces not implemented for non-LDA XC\" ) endif enddo endif ! ---------------------------------------------------------------------- !     Orbital initialisation : part 1 ! ---------------------------------------------------------------------- !     Find the maximum orbital radius rmax = 0.0_dp do io = 1 , norb ia = iaorb ( io ) ! Atomic index of each orbital iphi = iphorb ( io ) ! Orbital index of each  orbital in its atom is = isa ( ia ) ! Species index of each atom rmax = max ( rmax , rcut ( is , iphi ) ) enddo !     Start time counter for mesh initialization call timer ( 'DHSCF1' , 1 ) ! ---------------------------------------------------------------------- !     Unit cell handling ! ---------------------------------------------------------------------- !     Find diagonal unit cell and supercell call digcel ( ucell , mscell , cell , scell , nsc , IsDiag ) if (. not . IsDiag ) then if ( Node . eq . 0 ) then write ( 6 , '(/,a,3(/,a,3f12.6,a,i6))' ) & 'DHSCF: WARNING: New shape of unit cell and supercell:' , & ( 'DHSCF:' ,( cell ( i , j ), i = 1 , 3 ), '   x' , nsc ( j ), j = 1 , 3 ) endif endif !     Find the system shape call shaper ( cell , nua , isa , xa , shape , nbcell , bcell ) !     Find system volume volume = volcel ( cell ) ! ---------------------------------------------------------------------- !     Mesh initialization ! ---------------------------------------------------------------------- call InitMesh ( na , cell , norb , iaorb , iphorb , isa , rmax , & G2max , G2mesh , nsc , nmpl , nm , & nml , ntm , ntml , ntpl , dvol ) !     Setup box descriptors for each processor, !     held in module iogrid_netcdf call set_box_limits ( ntm , nsm ) ! Initialize information on local mesh for each node call init_mesh_node ( cell , ntm , meshLim , nsm ) ! Setup charge additions in the mesh call init_charge_add ( cell , ntm ) ! Setup Hartree additions in the mesh call init_hartree_add ( cell , ntm ) #ifdef NCDF_4 ! Initialize the box for each node... call cdf_init_mesh ( ntm , nsm ) #endif !     Stop time counter for mesh initialization call timer ( 'DHSCF1' , 2 ) ! ---------------------------------------------------------------------- !     End of mesh initialization ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     Initialize atomic orbitals, density and potential ! ---------------------------------------------------------------------- !     Start time counter for atomic initializations call timer ( 'DHSCF2' , 1 ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Initialise quantities relating to the atom-mesh positioning call InitAtomMesh ( UNIFORM , na , xa ) #ifdef BSC_CELLXC !     Check if we need extencils in cellxc call setGGA ( ) #endif /* BSC_CELLXC */ !     Compute the number of orbitals on the mesh and recompute the !     partions for every processor in order to have a similar load !     in each of them. nullify ( numphi ) call re_alloc ( numphi , 1 , nmpl , 'numphi' , 'dhscf_init' ) !$OMP parallel do default(shared), private(i) do i = 1 , nmpl numphi ( i ) = 0 enddo !$OMP end parallel do call distriPhiOnMesh ( nm , nmpl , norb , iaorb , iphorb , & isa , numphi ) !     Find if there are partial-core-corrections for any atom npcc = 0 do ia = 1 , na if ( rcore ( isa ( ia )) . gt . tiny ) npcc = 1 enddo !     Find partial-core-correction energy density !     Vscf and Vaux are not used here call re_alloc ( rhopcc , 1 , ntpl * npcc + 1 , 'rhopcc' , 'dhscf_init' ) if ( npcc . eq . 1 ) then call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , & nsd , dvol , volume , dummy_Vscf , dummy_Vaux , Fal , stressl , & . false ., . false . ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhopcc' , sqrt ( sum ( rhopcc ** 2 )) end if endif !     Find neutral-atom potential !     Drho is not used here call re_alloc ( Vna , 1 , ntpl , 'Vna' , 'dhscf_init' ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , dummy_DRho , Fal , stressl , & . false ., . false . ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Vna' , sqrt ( sum ( Vna ** 2 )) end if if ( nodes . gt . 1 ) then if ( node . eq . 0 ) then write ( 6 , \"(a)\" ) \"Setting up quadratic distribution...\" endif call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) !       Create extended mesh arrays for the second data distribution call setupExtMesh ( QUADRATIC , rmax ) !       Compute atom positions for the second data distribution call InitAtomMesh ( QUADRATIC , na , xa ) endif !     Calculate orbital values on mesh !     numphi has already been computed in distriPhiOnMesh !     in the UNIFORM distribution if ( nodes . eq . 1 ) then numphi_par => numphi else nullify ( numphi_par ) call re_alloc ( numphi_par , 1 , nmpl , 'numphi_par' , & 'dhscf_init' ) call distMeshData ( UNIFORM , numphi , QUADRATIC , & numphi_par , KEEP ) endif call PhiOnMesh ( nmpl , norb , iaorb , iphorb , isa , numphi_par ) if ( nodes . gt . 1 ) then call de_alloc ( numphi_par , 'numphi_par' , 'dhscf_init' ) endif call de_alloc ( numphi , 'numphi' , 'dhscf_init' ) ! ---------------------------------------------------------------------- !       Create sparse indexing for Dscf as needed for local mesh !       Note that this is done in the QUADRATIC distribution !       since 'endpht' (computed finally in PhiOnMesh and stored in !       meshphi module) is in that distribution. ! ---------------------------------------------------------------------- if ( Nodes . gt . 1 ) then call CreateLocalDscfPointers ( nmpl , nuotot , numd , listdptr , & listd ) endif ! ---------------------------------------------------------------------- !     Calculate terms relating to the neutral atoms on the mesh ! ---------------------------------------------------------------------- !     Find Harris (sum of atomic) electron density call re_alloc ( rhoatm_par , 1 , ntpl , 'rhoatm_par' , 'dhscf_init' ) call rhooda ( norb , nmpl , datm , rhoatm_par , iaorb , iphorb , isa ) !     rhoatm_par comes out of here in clustered form in QUADRATIC dist !     Routine Poison should use the uniform data distribution if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !     Create Rhoatm using UNIFORM distr, in sequential form call re_alloc ( rhoatm , 1 , ntpl , 'rhoatm' , 'dhscf_init' ) call distMeshData ( QUADRATIC , rhoatm_par , & UNIFORM , rhoatm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'rhoatm' , sqrt ( sum ( rhoatm ** 2 )) end if ! !  AG: The initialization of doping structs could be done here now, !      in the uniform distribution, and with a simple loop over !      rhoatm. if ( frstme ) call initialize_doping_uniform () if ( doping_active ) then call compute_doping_structs_uniform ( ntpl , rhoatm , nsd ) ! Will get the global number of hit points ! Then, the doping density to be added can be simply computed endif !     Allocate Temporal array call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf_init' ) !     Vscf is filled here but not used later !     Uharrs is computed (and saved) !     DStres is computed but not used later call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , rhoatm , & Uharrs , Vscf , DStres , nsm ) call de_alloc ( Vscf , 'Vscf' , 'dhscf_init' ) !     Always deallocate rhoatm_par, as it was used even if nodes=1 call de_alloc ( rhoatm_par , 'rhoatm_par' , 'dhscf_init' ) if ( mix_charge ) then call re_alloc ( rhog , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog' , 'dhscf_init' ) call re_alloc ( rhog_in , 1 , 2 , 1 , ntpl , 1 , nspin , $ 'Rhog_in' , 'dhscf_init' ) call order_rhog ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nsm ) endif !     Stop time counter for atomic initializations call timer ( 'DHSCF2' , 2 ) ! ---------------------------------------------------------------------- !     At the end of initializations: !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution ! ---------------------------------------------------------------------- if ( frstme ) then call initialize_efield () end if ! Check if we need to add the potential ! corresponding to the voltage-drop. if ( TSmode ) then ! These routines are important if there are cell-changes call ts_init_hartree_fix ( cell , nua , xa , ntm , ntml ) if ( IsVolt ) then call ts_init_voltage ( cell , nua , xa , ntm ) end if if ( acting_efield ) then ! We do not allow the electric field for ! transiesta runs with V = 0, either. ! It does not make sense, only for fields perpendicular ! to the applied bias. ! We need to check that the e-field is perpendicular ! to the transport direction, and that the system is ! either a chain, or a slab. ! However, due to the allowance of a dipole correction ! along the transport direction for buffer calculations ! we have to allow all shapes. (atom is not transiesta ! compatible anyway) ! check that we do not violate the periodicity if ( Node . eq . 0 ) then write ( * , '(/,2(2a,/))' ) 'ts-WARNING: ' , & 'E-field/dipole-correction! ' , & 'ts-WARNING: ' , & 'I hope you know what you are doing!' end if ! This is either dipole or user, or both field (:) = user_specified_field (:) do iE = 1 , N_Elec field2 = Elecs ( iE )% cell (:, Elecs ( iE )% t_dir ) ortho = ddot ( 3 , field2 , 1 , field , 1 ) if ( abs ( ortho ) > 1.e-9_dp ) then call die ( 'User defined E-field must be &perpendicular to semi-infinite directions' ) end if end do end if ! acting_efield ! We know that we currently allow people to do more than ! they probably should be allowed. However, there are many ! corner cases that may require dipole corrections, or ! electric fields to \"correct\" an intrinsic dipole. ! For instance, what should we do with a dipole in a transiesta ! calculation? ! Should we apply a field to counter act it in a device ! calculation? end if frstme = . false . call timer ( 'DHSCF_Init' , 2 ) #ifdef DEBUG call write_debug ( '    POS dhscf_init' ) #endif !------------------------------------------------------------------------- END end subroutine dhscf_init","tags":"","loc":"proc/dhscf_init.html","title":"dhscf_init – SIESTA"},{"text":"public subroutine dhscf(nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ntm, ifa, istr, iHmat, filesOut, maxnd, numd, listdptr, listd, Dscf, datm, maxnh, Hmat, Enaatm, Enascf, Uatm, Uscf, DUscf, DUext, Exc, Dxc, dipol, stress, Fal, stressl, use_rhog_in, charge_density_only) Uses precision parallel parallel atmfuncs units fdf sys mesh parsing m_iorho m_forhar alloc files files siesta_options siesta_options meshsubs meshsubs meshsubs moreMeshSubs moreMeshSubs moreMeshSubs m_partial_charges m_partial_charges siestaXC siestaXC siestaXC m_vmat m_rhoofd mpi_siesta iogrid_netcdf iogrid_netcdf siesta_options siesta_options siesta_options siesta_options m_efield m_efield m_efield m_doping_uniform m_charge_add m_hartree_add siesta_options m_ncdf_siesta m_rhofft m_rhog m_spin m_spin m_iotddft m_ts_global_vars m_ts_options m_ts_voltage m_ts_hartree proc~~dhscf~~UsesGraph proc~dhscf dhscf m_charge_add m_charge_add proc~dhscf->m_charge_add m_partial_charges m_partial_charges proc~dhscf->m_partial_charges module~units units proc~dhscf->module~units m_ts_global_vars m_ts_global_vars proc~dhscf->m_ts_global_vars parsing parsing proc~dhscf->parsing module~parallel parallel proc~dhscf->module~parallel module~sys sys proc~dhscf->module~sys m_ts_voltage m_ts_voltage proc~dhscf->m_ts_voltage siestaXC siestaXC proc~dhscf->siestaXC module~mesh mesh proc~dhscf->module~mesh m_iotddft m_iotddft proc~dhscf->m_iotddft m_efield m_efield proc~dhscf->m_efield m_doping_uniform m_doping_uniform proc~dhscf->m_doping_uniform files files proc~dhscf->files m_rhofft m_rhofft proc~dhscf->m_rhofft module~siesta_options siesta_options proc~dhscf->module~siesta_options module~m_forhar m_forhar proc~dhscf->module~m_forhar m_ts_options m_ts_options proc~dhscf->m_ts_options m_ncdf_siesta m_ncdf_siesta proc~dhscf->m_ncdf_siesta module~moremeshsubs moreMeshSubs proc~dhscf->module~moremeshsubs module~precision precision proc~dhscf->module~precision meshsubs meshsubs proc~dhscf->meshsubs mpi_siesta mpi_siesta proc~dhscf->mpi_siesta m_spin m_spin proc~dhscf->m_spin module~m_iorho m_iorho proc~dhscf->module~m_iorho m_ts_hartree m_ts_hartree proc~dhscf->m_ts_hartree iogrid_netcdf iogrid_netcdf proc~dhscf->iogrid_netcdf alloc alloc proc~dhscf->alloc module~m_vmat m_vmat proc~dhscf->module~m_vmat module~atmfuncs atmfuncs proc~dhscf->module~atmfuncs module~m_rhoofd m_rhoofd proc~dhscf->module~m_rhoofd fdf fdf proc~dhscf->fdf module~m_rhog m_rhog proc~dhscf->module~m_rhog m_hartree_add m_hartree_add proc~dhscf->m_hartree_add module~units->module~precision module~mesh->module~precision module~m_forhar->module~parallel module~m_forhar->siestaXC module~m_forhar->module~mesh module~m_forhar->module~moremeshsubs module~m_forhar->module~precision module~m_forhar->alloc module~moremeshsubs->module~parallel module~moremeshsubs->module~sys module~moremeshsubs->module~precision module~moremeshsubs->alloc module~m_iorho->module~parallel module~m_iorho->module~sys module~m_iorho->module~precision module~m_iorho->mpi_siesta module~m_iorho->alloc parallelsubs parallelsubs module~m_iorho->parallelsubs module~atmfuncs->module~sys module~atmfuncs->module~precision spher_harm spher_harm module~atmfuncs->spher_harm module~atm_types atm_types module~atmfuncs->module~atm_types module~radial radial module~atmfuncs->module~radial module~m_rhog->module~precision module~m_rhog->m_spin class_Pair_dData1D class_Pair_dData1D module~m_rhog->class_Pair_dData1D class_dData1D class_dData1D module~m_rhog->class_dData1D class_Fstack_Pair_dData1D class_Fstack_Pair_dData1D module~m_rhog->class_Fstack_Pair_dData1D module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Calculates the self-consistent field contributions to Hamiltonian\n matrix elements, total energy and atomic forces. Coded by J.M. Soler, August 1996. July 1997. Modified by J.D. Gale, February 2000. Units Energies in Rydbergs. Distances in Bohr. Routines called internally cellxc : Finds total exch-corr energy and potential CROSS : Finds the cross product of two vectors dfscf : Finds SCF contribution to atomic forces dipole : Finds electric dipole moment doping : Adds a background charge for doped systems write_rho : Saves electron density on a file poison : Solves Poisson equation reord : Reorders electron density and potential arrays rhooda : Finds Harris electron density in the mesh rhoofd : Finds SCF electron density in the mesh rhoofdsp : Finds SCF electron density in the mesh for\n                    spiral arrangement of spins timer : Finds CPU times vmat : Finds matrix elements of SCF potential vmatsp : Finds matrix elements of SCF potential for\n                         spiral arrangement of spins delk : Finds matrix elements of exp(i \\vec{k} \\cdot \\vec{r}) real*8 volcel( cell ) : Returns volume of unit cell Internal variables and arrays real*8  bcell(3,3) : Bulk lattice vectors real*8  cell(3,3) : Auxiliary lattice vectors (same as ucell) real*8  const : Auxiliary variable (constant within a loop) real*8  DEc : Auxiliary variable to call cellxc real*8  DEx : Auxiliary variable to call cellxc real*8  dvol : Mesh-cell volume real*8  Ec : Correlation energy real*8  Ex : Exchange energy real*8  field(3) : External electric field integer i : General-purpose index integer ia : Atom index integer io : Orbital index integer ip : Point index integer is : Species index logical IsDiag : Is supercell diagonal? integer ispin : Spin index integer j : General-purpose index integer JDGdistr : J.D.Gale's parallel distribution of mesh points integer myBox(2,3) : My processor's mesh box integer nbcell : Number of independent bulk lattice vectors integer npcc : Partial core corrections? (0=no, 1=yes) integer nsd : Number of diagonal spin values (1 or 2) integer ntpl : Number of mesh Total Points in unit cell\n                           (including subpoints) locally real*4  rhoatm(ntpl) : Harris electron density real*4  rhopcc(ntpl) : Partial-core-correction density for xc real*4  DRho(ntpl) : Selfconsistent electron density difference real*8  rhotot : Total density at one point real*8  rmax : Maximum orbital radius real*8  scell(3,3) : Supercell vectors character shape*10 : Name of system shape real*4  Vaux(ntpl) : Auxiliary potential array real*4  Vna(ntpl) : Sum of neutral-atom potentials real*8  volume : Unit cell volume real*4  Vscf(ntpl) : Hartree potential of selfconsistent density real*8  x0(3) : Center of molecule logical harrisfun : Harris functional or Kohn-Sham? Use the functionality in the first block\n of the routine to get charge files and partial charges Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Number of different spin polarisations: nspin=1 => Unpolarized, nspin=2 => polarized nspin=4 => Noncollinear spin or spin-orbit. integer, intent(in) :: norb Total number of basis orbitals in supercell integer, intent(in) :: iaorb (norb) Atom to which each orbital belongs integer, intent(in) :: iphorb (norb) Orbital index (within atom) of each orbital integer, intent(in) :: nuo Number of orbitals in a unit cell in this node integer, intent(in) :: nuotot Number of orbitals in a unit cell integer, intent(in) :: nua Number of atoms in unit cell integer, intent(in) :: na Number of atoms in supercell integer, intent(in) :: isa (na) Species index of all atoms in supercell real(kind=dp), intent(in) :: xa (3,na) Atomic positions of all atoms in supercell integer, intent(in) :: indxua (na) Index of equivalent atom in unit cell integer, intent(inout) :: ntm (3) Number of mesh divisions of each cell\n vector, including subgrid. integer, intent(in) :: ifa Switch which fixes whether the SCF contrib: to atomic forces is calculated and added to fa. integer, intent(in) :: istr Switch which fixes whether the SCF contrib: to stress is calculated and added to stress. integer, intent(in) :: iHmat Switch which fixes whether the Hmat matrix\n elements are calculated or not. type(filesOut_t), intent(inout) :: filesOut Output file names (If blank => not saved) integer, intent(in) :: maxnd First dimension of listd and Dscf integer, intent(in) :: numd (nuo) Number of nonzero density-matrix\n elements for each matrix row integer, intent(in) :: listdptr (nuo) Pointer to start of rows of density-matrix integer, intent(in) :: listd (*) listd(maxnd) : Nonzero-density-matrix-element column\n indexes for each matrix row real(kind=dp), intent(in) :: Dscf (:,:) Dscf(maxnd,h_spin_dim) : SCF density-matrix elements real(kind=dp), intent(in) :: datm (norb) Harris density-matrix diagonal elements (atomic occupation charges of orbitals) integer, intent(in) :: maxnh First dimension of listh and Hmat real(kind=dp), intent(in) :: Hmat (:,:) Hmat(maxnh,h_spin_dim) : Hamiltonian matrix in sparse form, to which are added the matrix elements <ORB_I | DeltaV | ORB_J> , where DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris) real(kind=dp), intent(out) :: Enaatm Integral of Vna * rhoatm real(kind=dp), intent(out) :: Enascf Integral of Vna * rhoscf real(kind=dp), intent(out) :: Uatm Harris hartree electron-interaction energy real(kind=dp), intent(out) :: Uscf SCF hartree electron-interaction energy real(kind=dp), intent(out) :: DUscf Electrostatic (Hartree) energy of (rhoscf - rhoatm) density real(kind=dp), intent(out) :: DUext Interaction energy with external electric field real(kind=dp), intent(out) :: Exc SCF exchange-correlation energy real(kind=dp), intent(out) :: Dxc SCF double-counting correction to Exc Dxc = integral of ( (epsxc - Vxc) * Rho ) All energies in Rydbergs real(kind=dp), intent(out) :: dipol (3) Electric dipole (in a.u.)\n only when the system is a molecule real(kind=dp) :: stress (3,3) real(kind=dp), intent(inout) :: Fal (3,nua) Atomic forces, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative\n of (Enascf - Enaatm + DUscf + Exc) with\n respect to atomic positions, in Ry/Bohr.\n Contributions local to this node. real(kind=dp), intent(inout) :: stressl (3,3) Stress tensor, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative of (Enascf - Enaatm + DUscf + Exc) / volume with respect to the strain tensor, in Ry.\n Contributions local to this node. logical, intent(in), optional :: use_rhog_in logical, intent(in), optional :: charge_density_only Calls proc~~dhscf~~CallsGraph proc~dhscf dhscf volcel volcel proc~dhscf->volcel proc~forhar forhar proc~dhscf->proc~forhar re_alloc re_alloc proc~dhscf->re_alloc ts_voltage ts_voltage proc~dhscf->ts_voltage mpi_barrier mpi_barrier proc~dhscf->mpi_barrier ts_hartree_fix ts_hartree_fix proc~dhscf->ts_hartree_fix elecs elecs proc~dhscf->elecs ddot ddot proc~dhscf->ddot jms_setmeshdistr jms_setmeshdistr proc~dhscf->jms_setmeshdistr mpitrace_restart mpitrace_restart proc~dhscf->mpitrace_restart bsc_cellxc bsc_cellxc proc~dhscf->bsc_cellxc write_grid_netcdf write_grid_netcdf proc~dhscf->write_grid_netcdf proc~bye bye proc~dhscf->proc~bye proc~setmeshdistr setMeshDistr proc~dhscf->proc~setmeshdistr rhoofdsp rhoofdsp proc~dhscf->rhoofdsp proc~die die proc~dhscf->proc~die get_field_from_dipole get_field_from_dipole proc~dhscf->get_field_from_dipole vmatsp vmatsp proc~dhscf->vmatsp add_potential_from_field add_potential_from_field proc~dhscf->add_potential_from_field hartree_add hartree_add proc~dhscf->hartree_add proc~dfscf dfscf proc~dhscf->proc~dfscf proc~reord reord proc~dhscf->proc~reord compute_partial_charges compute_partial_charges proc~dhscf->compute_partial_charges write_tdrho write_tdrho proc~dhscf->write_tdrho partialcoreonmesh partialcoreonmesh proc~dhscf->partialcoreonmesh mymeshbox mymeshbox proc~dhscf->mymeshbox neutralatomonmesh neutralatomonmesh proc~dhscf->neutralatomonmesh interface~distmeshdata distMeshData proc~dhscf->interface~distmeshdata proc~vmat vmat proc~dhscf->proc~vmat timer timer proc~dhscf->timer mpi_allreduce mpi_allreduce proc~dhscf->mpi_allreduce vacuum_level vacuum_level proc~dhscf->vacuum_level de_alloc de_alloc proc~dhscf->de_alloc localchargeonmesh localchargeonmesh proc~dhscf->localchargeonmesh write_debug write_debug proc~dhscf->write_debug proc~rhoofd rhoofd proc~dhscf->proc~rhoofd proc~write_rho write_rho proc~dhscf->proc~write_rho proc~forhar->re_alloc proc~forhar->jms_setmeshdistr proc~forhar->bsc_cellxc proc~forhar->proc~setmeshdistr proc~forhar->mymeshbox proc~forhar->interface~distmeshdata proc~forhar->de_alloc pxfflush pxfflush proc~bye->pxfflush cmlfinishfile cmlfinishfile proc~bye->cmlfinishfile mpi_finalize mpi_finalize proc~bye->mpi_finalize io_close io_close proc~die->io_close io_assign io_assign proc~die->io_assign proc~die->pxfflush proc~die->cmlfinishfile mpi_abort mpi_abort proc~die->mpi_abort pxfabort pxfabort proc~die->pxfabort proc~dfscf->re_alloc proc~dfscf->proc~die proc~dfscf->timer proc~dfscf->de_alloc endpht endpht proc~dfscf->endpht listp2 listp2 proc~dfscf->listp2 needdscfl needdscfl proc~dfscf->needdscfl numdl numdl proc~dfscf->numdl alloc_default alloc_default proc~dfscf->alloc_default indxuo indxuo proc~dfscf->indxuo listsc listsc proc~dfscf->listsc listdlptr listdlptr proc~dfscf->listdlptr lstpht lstpht proc~dfscf->lstpht globaltolocalorb globaltolocalorb proc~dfscf->globaltolocalorb matrixotom matrixotom proc~dfscf->matrixotom proc~rcut rcut proc~dfscf->proc~rcut dscfl dscfl proc~dfscf->dscfl listdl listdl proc~dfscf->listdl proc~reord->re_alloc proc~reord->timer proc~reord->de_alloc proc~distmeshdata_rea distMeshData_rea interface~distmeshdata->proc~distmeshdata_rea proc~distmeshdata_int distMeshData_int interface~distmeshdata->proc~distmeshdata_int proc~vmat->re_alloc proc~vmat->timer proc~vmat->de_alloc proc~vmat->endpht proc~vmat->listp2 proc~vmat->needdscfl proc~vmat->numdl matrixmtoo matrixmtoo proc~vmat->matrixmtoo phi phi proc~vmat->phi proc~vmat->indxuo proc~vmat->listsc proc~vmat->listdlptr proc~vmat->lstpht proc~vmat->globaltolocalorb proc~vmat->proc~rcut proc~vmat->listdl proc~rhoofd->re_alloc proc~rhoofd->proc~die proc~rhoofd->timer proc~rhoofd->de_alloc proc~rhoofd->endpht proc~rhoofd->listp2 proc~rhoofd->needdscfl proc~rhoofd->numdl proc~rhoofd->phi proc~rhoofd->indxuo proc~rhoofd->listsc proc~rhoofd->listdlptr proc~rhoofd->lstpht proc~rhoofd->globaltolocalorb proc~rhoofd->matrixotom proc~rhoofd->proc~rcut proc~rhoofd->dscfl proc~rhoofd->listdl proc~write_rho->mpi_barrier proc~write_rho->de_alloc proc~write_rho->write_debug mpi_wait mpi_wait proc~write_rho->mpi_wait proc~write_rho->io_close proc~write_rho->io_assign mpi_irecv mpi_irecv proc~write_rho->mpi_irecv proc~distmeshdata_rea->re_alloc proc~distmeshdata_rea->mpi_barrier proc~distmeshdata_rea->proc~die proc~distmeshdata_rea->proc~reord proc~distmeshdata_rea->timer proc~distmeshdata_rea->de_alloc proc~distmeshdata_rea->write_debug mpitrace_event mpitrace_event proc~distmeshdata_rea->mpitrace_event proc~boxintersection boxIntersection proc~distmeshdata_rea->proc~boxintersection proc~distmeshdata_int->re_alloc proc~distmeshdata_int->proc~die proc~distmeshdata_int->de_alloc proc~distmeshdata_int->proc~boxintersection proc~chk chk proc~rcut->proc~chk proc~chk->proc~die var panprocdhscfCallsGraph = svgPanZoom('#procdhscfCallsGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~dhscf~~CalledByGraph proc~dhscf dhscf proc~siesta_analysis siesta_analysis proc~siesta_analysis->proc~dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->proc~dhscf proc~siesta_forces siesta_forces proc~siesta_forces->proc~setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code dhscf Source Code subroutine dhscf ( nspin , norb , iaorb , iphorb , nuo , & nuotot , nua , na , isa , xa , indxua , & ntm , ifa , istr , iHmat , & filesOut , maxnd , numd , & listdptr , listd , Dscf , datm , maxnh , Hmat , & Enaatm , Enascf , Uatm , Uscf , DUscf , DUext , & Exc , Dxc , dipol , stress , Fal , stressl , & use_rhog_in , charge_density_only ) !! author: J.M. Soler !! date: August 1996 !! !! Calculates the self-consistent field contributions to Hamiltonian !! matrix elements, total energy and atomic forces. !! !! Coded by J.M. Soler, August 1996. July 1997. !! Modified by J.D. Gale, February 2000. !! !! !!### Units !! Energies in Rydbergs. !! Distances in Bohr. !! !!### Routines called internally !! * [[cellxc(proc)]]    : Finds total exch-corr energy and potential !! * [[cross(proc)]]   : Finds the cross product of two vectors !! * [[dfscf(proc)]]     : Finds SCF contribution to atomic forces !! * [[dipole(proc)]]    : Finds electric dipole moment !! * [[doping(proc)]]    : Adds a background charge for doped systems !! * [[write_rho(proc)]]     : Saves electron density on a file !! * [[poison(proc)]]    : Solves Poisson equation !! * [[reord(proc)]]     : Reorders electron density and potential arrays !! * [[rhooda(proc)]]    : Finds Harris electron density in the mesh !! * [[rhoofd(proc)]]    : Finds SCF electron density in the mesh !! * [[rhoofdsp(proc)]]  : Finds SCF electron density in the mesh for !!                    spiral arrangement of spins !! * [[timer(proc)]]     : Finds CPU times !! * [[vmat(proc)]]      : Finds matrix elements of SCF potential !! * [[vmatsp(proc)]]    : Finds matrix elements of SCF potential for !!                         spiral arrangement of spins !! * [[delk(proc)]] : Finds matrix elements of  exp(i \\vec{k} \\cdot \\vec{r})  !! * real*8 volcel( cell ) : Returns volume of unit cell !! !!### Internal variables and arrays !! * `real*8  bcell(3,3)`    : Bulk lattice vectors !! * `real*8  cell(3,3)`     : Auxiliary lattice vectors (same as ucell) !! * `real*8  const`         : Auxiliary variable (constant within a loop) !! * `real*8  DEc`           : Auxiliary variable to call cellxc !! * `real*8  DEx`           : Auxiliary variable to call cellxc !! * `real*8  dvol`          : Mesh-cell volume !! * `real*8  Ec`            : Correlation energy !! * `real*8  Ex`            : Exchange energy !! * `real*8  field(3)`      : External electric field !! * `integer i`             : General-purpose index !! * `integer ia`            : Atom index !! * `integer io`            : Orbital index !! * `integer ip`            : Point index !! * `integer is`            : Species index !! * `logical IsDiag`        : Is supercell diagonal? !! * `integer ispin`         : Spin index !! * `integer j`             : General-purpose index #ifndef BSC_CELLXC !! * `integer JDGdistr`      : J.D.Gale's parallel distribution of mesh points !! * `integer myBox(2,3)`    : My processor's mesh box #endif /* ! BSC_CELLXC */ !! * `integer nbcell`        : Number of independent bulk lattice vectors !! * `integer npcc`          : Partial core corrections? (0=no, 1=yes) !! * `integer nsd`           : Number of diagonal spin values (1 or 2) !! * `integer ntpl`          : Number of mesh Total Points in unit cell !!                           (including subpoints) locally !! * `real*4  rhoatm(ntpl)`  : Harris electron density !! * `real*4  rhopcc(ntpl)`  : Partial-core-correction density for xc !! * `real*4  DRho(ntpl)`    : Selfconsistent electron density difference !! * `real*8  rhotot`        : Total density at one point !! * `real*8  rmax`          : Maximum orbital radius !! * `real*8  scell(3,3)`    : Supercell vectors !! * `character shape*10`    : Name of system shape !! * `real*4  Vaux(ntpl)`    : Auxiliary potential array !! * `real*4  Vna(ntpl)`     : Sum of neutral-atom potentials !! * `real*8  volume`        : Unit cell volume !! * `real*4  Vscf(ntpl)`    : Hartree potential of selfconsistent density !! * `real*8  x0(3)`         : Center of molecule !! * `logical harrisfun`     : Harris functional or Kohn-Sham? use precision , only : dp , grid_p #ifndef BSC_CELLXC use parallel , only : ProcessorY #endif /* ! BSC_CELLXC */ !     Number of Mesh divisions of each cell vector (global) !     The status of this variable is confusing use parallel , only : Node , Nodes use atmfuncs , only : rcut , rcore use units , only : Debye , eV , Ang use fdf use sys , only : die , bye use mesh , only : nsm , nsp use parsing use m_iorho , only : write_rho use m_forhar , only : forhar use alloc , only : re_alloc , de_alloc use files , only : slabel use files , only : filesOut_t ! derived type for output file names use siesta_options , only : harrisfun , save_initial_charge_density use siesta_options , only : analyze_charge_density_only use meshsubs , only : LocalChargeOnMesh use meshsubs , only : PartialCoreOnMesh use meshsubs , only : NeutralAtomOnMesh use moreMeshSubs , only : setMeshDistr , distMeshData use moreMeshSubs , only : UNIFORM , QUADRATIC , LINEAR use moreMeshSubs , only : TO_SEQUENTIAL , TO_CLUSTER , KEEP use m_partial_charges , only : compute_partial_charges use m_partial_charges , only : want_partial_charges #ifndef BSC_CELLXC use siestaXC , only : cellXC ! Finds xc energy and potential use siestaXC , only : myMeshBox ! Returns my processor mesh box use siestaXC , only : jms_setMeshDistr => setMeshDistr ! Sets a distribution of mesh ! points over parallel processors #endif /* BSC_CELLXC */ use m_vmat , only : vmat use m_rhoofd , only : rhoofd #ifdef MPI use mpi_siesta #endif use iogrid_netcdf , only : write_grid_netcdf use iogrid_netcdf , only : read_grid_netcdf use siesta_options , only : read_charge_cdf use siesta_options , only : savebader use siesta_options , only : read_deformation_charge_cdf use siesta_options , only : mix_charge use m_efield , only : get_field_from_dipole , dipole_correction use m_efield , only : add_potential_from_field use m_efield , only : user_specified_field , acting_efield use m_doping_uniform , only : doping_active , doping_uniform use m_charge_add , only : charge_add use m_hartree_add , only : hartree_add #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif use m_rhofft , only : rhofft , FORWARD , BACKWARD use m_rhog , only : rhog_in , rhog use m_spin , only : spin use m_spin , only : Spiral , qSpiral use m_iotddft , only : write_tdrho use m_ts_global_vars , only : TSmode , TSrun use m_ts_options , only : IsVolt , Elecs , N_elec use m_ts_voltage , only : ts_voltage use m_ts_hartree , only : ts_hartree_fix implicit none integer , intent ( in ) :: nspin !! Number of different spin polarisations: !! nspin=1 => Unpolarized, nspin=2 => polarized !! nspin=4 => Noncollinear spin or spin-orbit. integer , intent ( in ) :: norb !! Total number of basis orbitals in supercell integer , intent ( in ) :: iaorb ( norb ) !! Atom to which each orbital belongs integer , intent ( in ) :: iphorb ( norb ) !! Orbital index (within atom) of each orbital integer , intent ( in ) :: nuo !! Number of orbitals in a unit cell in this node integer , intent ( in ) :: nuotot !! Number of orbitals in a unit cell integer , intent ( in ) :: nua !! Number of atoms in unit cell integer , intent ( in ) :: na !! Number of atoms in supercell integer , intent ( in ) :: isa ( na ) !! Species index of all atoms in supercell integer , intent ( in ) :: indxua ( na ) !! Index of equivalent atom in unit cell integer , intent ( in ) :: ifa !! Switch which fixes whether the SCF contrib: !! to atomic forces is calculated and added to fa. integer , intent ( in ) :: istr !! Switch which fixes whether the SCF contrib: !! to stress is calculated and added to stress. integer , intent ( in ) :: iHmat !! Switch which fixes whether the Hmat matrix !! elements are calculated or not. integer , intent ( in ) :: maxnd !! First dimension of listd and Dscf integer , intent ( in ) :: numd ( nuo ) !! Number of nonzero density-matrix !! elements for each matrix row integer , intent ( in ) :: listdptr ( nuo ) !! Pointer to start of rows of density-matrix integer , intent ( in ) :: listd ( * ) !! `listd(maxnd)`: Nonzero-density-matrix-element column !! indexes for each matrix row integer , intent ( in ) :: maxnh !! First dimension of listh and Hmat real ( dp ), intent ( in ) :: xa ( 3 , na ) !! Atomic positions of all atoms in supercell real ( dp ), intent ( in ) :: Dscf (:,:) !! `Dscf(maxnd,h_spin_dim)`: !! SCF density-matrix elements real ( dp ), intent ( in ) :: datm ( norb ) !! Harris density-matrix diagonal elements !! (atomic occupation charges of orbitals) real ( dp ), intent ( in ) :: Hmat (:,:) !! `Hmat(maxnh,h_spin_dim)`: !! Hamiltonian matrix in sparse form, !! to which are added the matrix elements !! `<ORB_I | DeltaV | ORB_J>`, where !! `DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris)` type ( filesOut_t ), intent ( inout ) :: filesOut !! Output file names (If blank => not saved) integer , intent ( inout ) :: ntm ( 3 ) !! Number of mesh divisions of each cell !! vector, including subgrid. real ( dp ), intent ( inout ) :: Fal ( 3 , nua ) !! Atomic forces, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative !! of `(Enascf - Enaatm + DUscf + Exc)` with !! respect to atomic positions, in Ry/Bohr. !! Contributions local to this node. real ( dp ), intent ( inout ) :: stressl ( 3 , 3 ) !! Stress tensor, to which the SCF contribution !! is added by this routine when `ifa=1`. !! The SCF contribution is minus the derivative of !! `(Enascf - Enaatm + DUscf + Exc) / volume` !! with respect to the strain tensor, in Ry. !! Contributions local to this node. real ( dp ) :: stress ( 3 , 3 ) real ( dp ), intent ( out ) :: Enaatm !! Integral of `Vna * rhoatm` real ( dp ), intent ( out ) :: Enascf !! Integral of `Vna * rhoscf` real ( dp ), intent ( out ) :: Uatm !! Harris hartree electron-interaction energy real ( dp ), intent ( out ) :: Uscf !! SCF hartree electron-interaction energy real ( dp ), intent ( out ) :: DUscf !! Electrostatic (Hartree) energy of !! `(rhoscf - rhoatm)` density real ( dp ), intent ( out ) :: DUext !! Interaction energy with external electric field real ( dp ), intent ( out ) :: Exc !! SCF exchange-correlation energy real ( dp ), intent ( out ) :: Dxc !! SCF double-counting correction to Exc !! `Dxc = integral of ( (epsxc - Vxc) * Rho )` !! All energies in Rydbergs real ( dp ), intent ( out ) :: dipol ( 3 ) !! Electric dipole (in a.u.) !! only when the system is a molecule logical , intent ( in ), optional :: use_rhog_in logical , intent ( in ), optional :: charge_density_only !     Local variables integer :: i , ia , ip , ispin , nsd , np_vac #ifndef BSC_CELLXC !     Interface to JMS's SiestaXC integer :: myBox ( 2 , 3 ) integer , save :: JDGdistr =- 1 real ( dp ) :: stressXC ( 3 , 3 ) #endif /* ! BSC_CELLXC */ real ( dp ) :: b1Xb2 ( 3 ), const , DEc , DEx , DStres ( 3 , 3 ), & Ec , Ex , rhotot , x0 ( 3 ), volume , Vmax_vac , Vmean_vac #ifdef BSC_CELLXC !     Dummy arrays for cellxc call real ( grid_p ) :: aux3 ( 3 , 1 ) real ( grid_p ) :: dummy_DVxcdn ( 1 , 1 , 1 ) #endif /* BSC_CELLXC */ logical :: use_rhog real ( dp ), external :: volcel , ddot external & cross , & dipole , & poison , & reord , rhooda , rhoofdsp , & timer , vmatsp , & readsp #ifdef BSC_CELLXC external bsc_cellxc #endif /* BSC_CELLXC */ !     Work arrays real ( grid_p ), pointer :: Vscf (:,:), Vscf_par (:,:), & DRho (:,:), DRho_par (:,:), & Vaux (:), Vaux_par (:), Chlocal (:), & Totchar (:), fsrc (:), fdst (:), & rhoatm_quad (:) => null (), & DRho_quad (:,:) => null () ! Temporary reciprocal spin quantity real ( grid_p ) :: rnsd #ifdef BSC_CELLXC real ( grid_p ), pointer :: Vscf_gga (:,:), DRho_gga (:,:) #endif /* BSC_CELLXC */ #ifdef MPI integer :: MPIerror real ( dp ) :: sbuffer ( 7 ), rbuffer ( 7 ) #endif #ifdef DEBUG call write_debug ( '    PRE DHSCF' ) #endif if ( spin % H /= size ( Dscf , dim = 2 ) ) then call die ( 'Spin components is not equal to options.' ) end if if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DM' , & ( sqrt ( sum ( Dscf (:, ispin ) ** 2 )), ispin = 1 , spin % H ) write ( * , debug_fmt ) Node , 'H' , & ( sqrt ( sum ( Hmat (:, ispin ) ** 2 )), ispin = 1 , spin % H ) end if !-------------------------------------------------------------------- BEGIN ! ---------------------------------------------------------------------- ! Start of SCF iteration part ! ---------------------------------------------------------------------- ! ---------------------------------------------------------------------- !     At the end of DHSCF_INIT, and also at the end of any previous !     call to dhscf, we were in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna were in UNIFORM dist, sequential form !     The index array endpht was in the QUADRATIC distribution ! ---------------------------------------------------------------------- #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_restart ( ) #endif call timer ( 'DHSCF' , 1 ) call timer ( 'DHSCF3' , 1 ) nullify ( Vscf , Vscf_par , DRho , DRho_par , & Vaux , Vaux_par , Chlocal , Totchar ) #ifdef BSC_CELLXC nullify ( Vscf_gga , DRho_gga ) #endif /* BSC_CELLXC */ volume = volcel ( cell ) !------------------------------------------------------------------------- if ( analyze_charge_density_only ) then !! Use the functionality in the first block !! of the routine to get charge files and partial charges call setup_analysis_options () endif if ( filesOut % vna . ne . ' ' ) then ! Uniform dist, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vna' , 1 , ntml , Vna ) else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) end if #else call write_rho ( filesOut % vna , & cell , ntm , nsm , ntpl , 1 , Vna ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vna , \"Vna\" ) #endif endif !     Allocate memory for DRho using the UNIFORM data distribution call re_alloc ( DRho , 1 , ntpl , 1 , nspin , 'DRho' , 'dhscf' ) ! Find number of diagonal spin values nsd = min ( nspin , 2 ) if ( nsd == 1 ) then rnsd = 1._grid_p else rnsd = 1._grid_p / nsd end if ! ---------------------------------------------------------------------- ! Find SCF electron density at mesh points. Store it in array DRho ! ---------------------------------------------------------------------- ! !     The reading routine works in the uniform distribution, in !     sequential form ! if ( present ( use_rhog_in )) then use_rhog = use_rhog_in else use_rhog = . false . endif if ( use_rhog ) then ! fourier transform back into drho call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog_in , BACKWARD ) else if ( read_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"Rho\" ) read_charge_cdf = . false . else if ( read_deformation_charge_cdf ) then call read_grid_netcdf ( ntm ( 1 : 3 ), nspin , ntpl , DRho , \"DeltaRho\" ) ! Add to diagonal components only do ispin = 1 , nsd do ip = 1 , ntpl !             rhoatm and Drho are in sequential mode DRho ( ip , ispin ) = DRho ( ip , ispin ) + rhoatm ( ip ) * rnsd enddo enddo read_deformation_charge_cdf = . false . else ! Set the QUADRATIC distribution and allocate memory for DRho_par ! since the construction of the density from the DM and orbital ! data needs that distribution if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_par , 1 , ntpl , 1 , nspin , & 'DRho_par' , 'dhscf' ) if ( Spiral ) then call rhoofdsp ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , nuo , nuotot , iaorb , & iphorb , isa , qspiral ) else call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Dscf , DRho_par , & nuo , nuotot , iaorb , iphorb , isa ) endif ! DRHO_par is here in QUADRATIC, clustered form !       Set the UNIFORM distribution again and copy DRho to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => DRho_par (:, ispin ) fdst => DRho (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( DRho_par , 'DRho_par' , 'dhscf' ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'DRho' , & ( sqrt ( sum ( DRho (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if if ( save_initial_charge_density ) then ! This section is to be deprecated in favor ! of \"analyze_charge_density_only\" ! (except for the special name for the .nc file) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoInit' , nspin , & ntml , DRho ) else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) end if #else call write_rho ( \"RHO_INIT\" , cell , ntm , nsm , ntpl , $ nspin , DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , $ \"RhoInit\" ) #endif call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after producing RHO_INIT from input DM\" ) endif endif if ( mix_charge ) then ! Save fourier transform of charge density call rhofft ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , nspin , $ DRho , rhog , FORWARD ) endif ! !     Proper place to integrate Hirshfeld and Voronoi code, !     since we have just computed rhoatm and Rho. if ( want_partial_charges ) then ! The endpht array is in the quadratic distribution, so ! we need to use it for this... if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( DRho_quad , 1 , ntpl , 1 , nspin , & 'DRho_quad' , 'dhscf' ) call re_alloc ( rhoatm_quad , 1 , ntpl , & 'rhoatm_quad' , 'dhscf' ) ! Redistribute grid-density do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_quad (:, ispin ) ! if nodes==1, this call will just reorder call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo call distMeshData ( UNIFORM , rhoatm , & QUADRATIC , rhoatm_quad , TO_CLUSTER ) call compute_partial_charges ( DRho_quad , rhoatm_quad , . nspin , iaorb , iphorb , . isa , nmpl , dvol ) call de_alloc ( rhoatm_quad , 'rhoatm_quad' , 'dhscf' ) call de_alloc ( Drho_quad , 'DRho_quad' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif ! ---------------------------------------------------------------------- ! Save electron density ! ---------------------------------------------------------------------- if ( filesOut % rho . ne . ' ' ) then !  DRho is already using a uniform, sequential form #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Rho' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) end if #else call write_rho ( filesOut % rho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"Rho\" ) #endif endif !----------------------------------------------------------------------- ! Save TD-electron density after every given number of steps- Rafi, Jan 2016 !----------------------------------------------------------------------- call write_tdrho ( filesOut ) if ( filesOut % tdrho . ne . ' ' ) then !  DRho is already using a uniform, sequential form call write_rho ( filesOut % tdrho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , \"TDRho\" ) endif ! ---------------------------------------------------------------------- ! Save the diffuse ionic charge and/or the total (ionic+electronic) charge ! ---------------------------------------------------------------------- if ( filesOut % psch . ne . ' ' . or . filesOut % toch . ne . ' ' ) then !       Find diffuse ionic charge on mesh ! Note that the *OnMesh routines, except PhiOnMesh, ! work with any distribution, thanks to the fact that ! the ipa, idop, and indexp arrays are distro-specific call re_alloc ( Chlocal , 1 , ntpl , 'Chlocal' , 'dhscf' ) call LocalChargeOnMesh ( na , isa , ntpl , Chlocal , indxua ) ! Chlocal comes out in clustered form, so we convert it call reord ( Chlocal , Chlocal , nml , nsm , TO_SEQUENTIAL ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Chlocal' , sqrt ( sum ( Chlocal ** 2 )) end if !       Save diffuse ionic charge if ( filesOut % psch . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Chlocal' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & 'Chlocal' ) end if #else call write_rho ( filesOut % psch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , 'Chlocal' ) #endif endif !       Save total (ionic+electronic) charge if ( filesOut % toch . ne . ' ' ) then ! ***************** ! **  IMPORTANT  ** ! The Chlocal array is re-used to minimize memory ! usage. In the this small snippet the Chlocal ! array will contain the total charge, and ! if the logic should change, (i.e. should Chlocal ! be retained) is the Totchar needed to be re-instantiated. ! ***************** !$OMP parallel default(shared), private(ispin,ip) do ispin = 1 , nsd !$OMP do do ip = 1 , ntpl Chlocal ( ip ) = Chlocal ( ip ) + DRho ( ip , ispin ) end do !$OMP end do end do !$OMP end parallel ! See note above #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoTot' , 1 , ntml , & Chlocal ) else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Chlocal , & \"TotalCharge\" ) end if #else call write_rho ( filesOut % toch , cell , ntm , nsm , ntpl , 1 , & Chlocal ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Chlocal , \"TotalCharge\" ) #endif end if call de_alloc ( Chlocal , 'Chlocal' , 'dhscf' ) endif ! ---------------------------------------------------------------------- ! Save the total charge (model core + valence) for Bader analysis ! ---------------------------------------------------------------------- ! The test for toch guarantees that we are in \"analysis mode\" if ( filesOut % toch . ne . ' ' . and . savebader ) then call save_bader_charge () endif ! Find difference between selfconsistent and atomic densities !Both DRho and rhoatm are using a UNIFORM, sequential form !$OMP parallel do default(shared), private(ispin,ip), !$OMP&collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do ! ---------------------------------------------------------------------- ! Save electron density difference ! ---------------------------------------------------------------------- if ( filesOut % drho . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoDelta' , nspin , ntml , & DRho ) else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"DeltaRho\" ) end if #else call write_rho ( filesOut % drho , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & DRho , \"DeltaRho\" ) #endif endif if ( present ( charge_density_only )) then if ( charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) RETURN endif endif ! End of analysis section ! Can exit now, if requested if ( analyze_charge_density_only ) then call timer ( 'DHSCF3' , 2 ) call timer ( 'DHSCF' , 2 ) call bye ( \"STOP after analyzing charge from input DM\" ) endif !------------------------------------------------------------- !     Transform spin density into sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif ! Add a background charge to neutralize the net charge, to ! model doped systems. It only adds the charge at points ! where there are atoms (i.e., not in vacuum). ! First, call with 'task=0' to add background charge if ( doping_active ) call doping_uniform ( cell , ntpl , 0 , $ DRho (:, 1 ), rhoatm ) ! Add doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '+' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Calculate the dipole moment ! ---------------------------------------------------------------------- dipol ( 1 : 3 ) = 0.0_dp if ( shape . ne . 'bulk' ) then ! Find center of system x0 ( 1 : 3 ) = 0.0_dp do ia = 1 , nua x0 ( 1 : 3 ) = x0 ( 1 : 3 ) + xa ( 1 : 3 , ia ) / nua enddo ! Find dipole ! This routine is distribution-blind ! and will reduce over all processors. call dipole ( cell , ntm , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), nsm , & DRho , x0 , dipol ) ! Orthogonalize dipole to bulk directions if ( shape . eq . 'chain' ) then const = ddot ( 3 , dipol , 1 , bcell , 1 ) / ddot ( 3 , bcell , 1 , bcell , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * bcell ( 1 : 3 , 1 ) else if ( shape . eq . 'slab' ) then call cross ( bcell ( 1 , 1 ), bcell ( 1 , 2 ), b1Xb2 ) const = ddot ( 3 , dipol , 1 , b1Xb2 , 1 ) / ddot ( 3 , b1Xb2 , 1 , b1Xb2 , 1 ) dipol ( 1 : 3 ) = const * b1Xb2 ( 1 : 3 ) end if if ( TSmode ) then if ( N_elec > 1 ) then ! Orthogonalize dipole to electrode transport directions do ia = 1 , N_Elec x0 = Elecs ( ia )% cell (:, Elecs ( ia )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = dipol ( 1 : 3 ) - const * x0 end do else if ( ( shape == 'molecule' ) . or . ( shape == 'chain' ) ) then ! Only allow dipole correction for chains and molecules ! along the semi-infinite direciton. ! Note this is *only* for 1-electrode setups ! Note that since the above removes the periodic directions ! this should not do anything for 'chain' with the same semi-infinite ! direction x0 = Elecs ( 1 )% cell (:, Elecs ( 1 )% t_dir ) const = ddot ( 3 , dipol , 1 , x0 , 1 ) / ddot ( 3 , x0 , 1 , x0 , 1 ) dipol ( 1 : 3 ) = const * x0 end if end if endif ! ---------------------------------------------------------------------- !     Find Hartree potential of DRho = rhoscf-rhoatm. Store it in Vaux ! ---------------------------------------------------------------------- !     Solve Poisson's equation call re_alloc ( Vaux , 1 , ntpl , 'Vaux' , 'dhscf' ) call poison ( cell , ntml ( 1 ), ntml ( 2 ), ntml ( 3 ), ntm , DRho , & DUscf , Vaux , DStres , nsm ) if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'Poisson' , sqrt ( sum ( Vaux (:) ** 2 )) end if ! Vscf is in the UNIFORM, sequential form, and only using ! the first spin index ! We require that even the SIESTA potential is \"fixed\" ! NOTE, this will only do something if !   TS.Hartree.Fix is set call ts_hartree_fix ( ntm , ntml , Vaux ) ! Add contribution to stress from electrostatic energy of rhoscf-rhoatm if ( istr . eq . 1 ) then stressl ( 1 : 3 , 1 : 3 ) = stressl ( 1 : 3 , 1 : 3 ) + DStres ( 1 : 3 , 1 : 3 ) endif ! ---------------------------------------------------------------------- !     Find electrostatic (Hartree) energy of full SCF electron density !     using the original data distribution ! ---------------------------------------------------------------------- Uatm = Uharrs Uscf = 0._dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Uscf) do ip = 1 , ntpl Uscf = Uscf + Vaux ( ip ) * rhoatm ( ip ) enddo !$OMP end parallel do Uscf = Uscf * dVol + Uatm + DUscf ! Call doping with 'task=1' to remove background charge added previously ! The extra charge thus only affects the Hartree energy and potential, ! but not the contribution to Enascf ( = \\Int_{Vna*\\rho}) if ( doping_active ) call doping_uniform ( cell , ntpl , 1 , $ DRho (:, 1 ), rhoatm ) ! Remove doping in cell (from ChargeGeometries/Geometry.Charge) ! Note that this routine will return immediately if no dopant is present call charge_add ( '-' , cell , ntpl , DRho (:, 1 ) ) ! ---------------------------------------------------------------------- ! Add neutral-atom potential to Vaux ! ---------------------------------------------------------------------- Enaatm = 0.0_dp Enascf = 0.0_dp !$OMP parallel do default(shared), private(ip), !$OMP&reduction(+:Enaatm,Enascf) do ip = 1 , ntpl Enaatm = Enaatm + Vna ( ip ) * rhoatm ( ip ) Enascf = Enascf + Vna ( ip ) * DRho ( ip , 1 ) Vaux ( ip ) = Vaux ( ip ) + Vna ( ip ) enddo !$OMP end parallel do Enaatm = Enaatm * dVol Enascf = Enaatm + Enascf * dVol ! ---------------------------------------------------------------------- ! Add potential from external electric field (if present) ! ---------------------------------------------------------------------- if ( acting_efield ) then if ( dipole_correction ) then field = get_field_from_dipole ( dipol , cell ) if ( Node == 0 ) then write ( 6 , '(a,3f12.4,a)' ) $ 'Dipole moment in unit cell   =' , dipol / Debye , ' D' write ( 6 , '(a,3f12.6,a)' ) $ 'Electric field for dipole correction =' , $ field / eV * Ang , ' eV/Ang/e' end if ! The dipole correction energy has an extra factor ! of one half because the field involved is internal. ! See the paper by Bengtsson DOI:10.1103/PhysRevB.59.12301 ! Hence we compute this part separately DUext = - 0.5_dp * ddot ( 3 , field , 1 , dipol , 1 ) else field = 0._dp DUext = 0._dp end if ! Add the external electric field field = field + user_specified_field ! This routine expects a sequential array, ! but it is distribution-blind call add_potential_from_field ( field , cell , nua , isa , xa , & ntm , nsm , Vaux ) ! Add energy of external electric field DUext = DUext - ddot ( 3 , user_specified_field , 1 , dipol , 1 ) endif ! --------------------------------------------------------------------- !     Transiesta: !     add the potential corresponding to the (possible) voltage-drop. !     note that ts_voltage is not sharing the reord wih efield since !     we should not encounter both at the same time. ! --------------------------------------------------------------------- if ( TSmode . and . IsVolt . and . TSrun ) then ! This routine expects a sequential array, ! in whatever distribution #ifdef TRANSIESTA_VOLTAGE_DEBUG !$OMP parallel workshare default(shared) Vaux (:) = 0._dp !$OMP end parallel workshare #endif call ts_voltage ( cell , ntm , ntml , Vaux ) #ifdef TRANSIESTA_VOLTAGE_DEBUG call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"TransiestaHartreePotential\" ) call timer ( 'ts_volt' , 3 ) call bye ( 'transiesta debug for Hartree potential' ) #endif endif ! ---------------------------------------------------------------------- ! Add potential from user defined geometries (if present) ! ---------------------------------------------------------------------- call hartree_add ( cell , ntpl , Vaux ) ! ---------------------------------------------------------------------- !     Save electrostatic potential ! ---------------------------------------------------------------------- if ( filesOut % vh . ne . ' ' ) then ! Note that only the first spin component is used #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vh' , 1 , ntml , & Vaux ) else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , Vaux , & \"ElectrostaticPotential\" ) end if #else call write_rho ( filesOut % vh , cell , ntm , nsm , ntpl , 1 , Vaux ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , & Vaux , \"ElectrostaticPotential\" ) #endif endif !     Get back spin density from sum and difference ! TODO Check for NC/SO: ! Should we diagonalize locally at every point first? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(ip,rhotot) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) DRho ( ip , 1 ) = 0.5_dp * ( rhotot - DRho ( ip , 2 )) DRho ( ip , 2 ) = 0.5_dp * ( rhotot + DRho ( ip , 2 )) enddo !$OMP end parallel do endif ! ---------------------------------------------------------------------- #ifndef BSC_CELLXC ! Set uniform distribution of mesh points and find my processor mesh box ! This is the interface to JM Soler's own cellxc routine, which sets ! up the right distribution internally. ! ---------------------------------------------------------------------- call jms_setMeshDistr ( distrID = JDGdistr , nMesh = ntm , . nNodesX = 1 , nNodesY = ProcessorY , nBlock = nsm ) call myMeshBox ( ntm , JDGdistr , myBox ) ! ---------------------------------------------------------------------- #endif /* ! BSC_CELLXC */ ! Exchange-correlation energy ! ---------------------------------------------------------------------- call re_alloc ( Vscf , 1 , ntpl , 1 , nspin , 'Vscf' , 'dhscf' ) if ( npcc . eq . 1 ) then !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & ( rhopcc ( ip ) + rhoatm ( ip )) * rnsd enddo enddo !$OMP end parallel do else !$OMP parallel do default(shared), private(ip,ispin), collapse(2) do ispin = 1 , nsd do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) + & rhoatm ( ip ) * rnsd enddo enddo !$OMP end parallel do end if ! Write the electron density used by cellxc if ( filesOut % rhoxc . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoXC' , nspin , ntml , & DRho ) else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) end if #else call write_rho ( filesOut % rhoxc , cell , ntm , nsm , ntpl , nspin , & DRho ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , DRho , & \"RhoXC\" ) #endif endif !     Everything now is in UNIFORM, sequential form call timer ( \"CellXC\" , 1 ) #ifdef BSC_CELLXC if ( nodes . gt . 1 ) then call setMeshDistr ( LINEAR , nsm , nsp , nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_gga , 1 , ntpl , 1 , nspin , 'Vscf_gga' , 'dhscf' ) call re_alloc ( DRho_gga , 1 , ntpl , 1 , nspin , 'DRho_gga' , 'dhscf' ) ! Redistribute all spin densities do ispin = 1 , nspin fsrc => DRho (:, ispin ) fdst => DRho_gga (:, ispin ) call distMeshData ( UNIFORM , fsrc , LINEAR , fdst , KEEP ) enddo call bsc_cellxc ( 0 , 0 , cell , ntml , ntml , ntpl , 0 , aux3 , nspin , & DRho_gga , Ex , Ec , DEx , DEc , Vscf_gga , & dummy_DVxcdn , stressl ) #endif /* BSC_CELLXC */ #ifndef BSC_CELLXC call cellXC ( 0 , cell , ntm , myBox ( 1 , 1 ), myBox ( 2 , 1 ), . myBox ( 1 , 2 ), myBox ( 2 , 2 ), . myBox ( 1 , 3 ), myBox ( 2 , 3 ), nspin , . DRho , Ex , Ec , DEx , DEc , stressXC , Vscf ) #else /* BSC_CELLXC */ if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif ! Redistribute to the Vxc array do ispin = 1 , nspin fsrc => Vscf_gga (:, ispin ) fdst => Vscf (:, ispin ) call distMeshData ( LINEAR , fsrc , UNIFORM , fdst , KEEP ) enddo #endif /* BSC_CELLXC */ if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'XC' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nspin ) end if #ifndef BSC_CELLXC !     Vscf is still sequential after the call to JMS's cellxc #else /* BSC_CELLXC */ call de_alloc ( DRho_gga , 'DRho_gga' , 'dhscf' ) call de_alloc ( Vscf_gga , 'Vscf_gga' , 'dhscf' ) #endif /* BSC_CELLXC */ Exc = Ex + Ec Dxc = DEx + DEc call timer ( \"CellXC\" , 2 ) !     Vscf contains only Vxc, and is UNIFORM and sequential !     Now we add up the other contributions to it, at !     the same time that we get DRho back to true DeltaRho form !$OMP parallel default(shared), private(ip,ispin) ! Hartree potential only has diagonal components do ispin = 1 , nsd if ( npcc . eq . 1 ) then !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - & ( rhoatm ( ip ) + rhopcc ( ip )) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do else !$OMP do do ip = 1 , ntpl DRho ( ip , ispin ) = DRho ( ip , ispin ) - rhoatm ( ip ) * rnsd Vscf ( ip , ispin ) = Vscf ( ip , ispin ) + Vaux ( ip ) enddo !$OMP end do endif enddo !$OMP end parallel #ifndef BSC_CELLXC stress = stress + stressXC #endif /* ! BSC_CELLXC */ ! ---------------------------------------------------------------------- !     Save total potential ! ---------------------------------------------------------------------- if ( filesOut % vt . ne . ' ' ) then #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Vt' , nspin , ntml , & Vscf ) else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , & Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Vscf , & \"TotalPotential\" ) end if #else call write_rho ( filesOut % vt , cell , ntm , nsm , ntpl , nspin , Vscf ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Vscf , \"TotalPotential\" ) #endif endif ! ---------------------------------------------------------------------- ! Print vacuum level ! ---------------------------------------------------------------------- if ( filesOut % vt /= ' ' . or . filesOut % vh /= ' ' ) then forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) + rhoatm (:) * rnsd call vacuum_level ( ntpl , nspin , DRho , Vscf , . np_vac , Vmax_vac , Vmean_vac ) forall ( ispin = 1 : nsd ) . DRho (:, ispin ) = DRho (:, ispin ) - rhoatm (:) * rnsd if ( np_vac > 0 . and . Node == 0 ) print '(/,a,2f12.6,a)' , . 'dhscf: Vacuum level (max, mean) =' , . Vmax_vac / eV , Vmean_vac / eV , ' eV' endif if ( filesOut % ebs_dens /= '' ) then call save_ebs_density () endif ! ---------------------------------------------------------------------- !     Find SCF contribution to hamiltonian matrix elements ! ---------------------------------------------------------------------- if ( iHmat . eq . 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif !        This is a work array, to which we copy Vscf call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo if ( Spiral ) then call vmatsp ( norb , nmpl , dvol , nspin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa , qspiral ) else call vmat ( norb , nmpl , dvol , spin , Vscf_par , maxnd , & numd , listdptr , listd , Hmat , nuo , & nuotot , iaorb , iphorb , isa ) endif call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then !          Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif #ifdef MPI !     Global reduction of Uscf/DUscf/Uatm/Enaatm/Enascf #ifndef BSC_CELLXC !     Note that Exc and Dxc are already reduced in the new cellxc #endif /* ! BSC_CELLXC */ sbuffer ( 1 ) = Uscf sbuffer ( 2 ) = DUscf sbuffer ( 3 ) = Uatm sbuffer ( 4 ) = Enaatm sbuffer ( 5 ) = Enascf #ifdef BSC_CELLXC sbuffer ( 6 ) = Exc sbuffer ( 7 ) = Dxc #else sbuffer ( 6 : 7 ) = 0._dp #endif /* BSC_CELLXC */ call MPI_AllReduce ( sbuffer , rbuffer , 7 , MPI_double_precision , & MPI_Sum , MPI_Comm_World , MPIerror ) Uscf = rbuffer ( 1 ) DUscf = rbuffer ( 2 ) Uatm = rbuffer ( 3 ) Enaatm = rbuffer ( 4 ) Enascf = rbuffer ( 5 ) #ifdef BSC_CELLXC Exc = rbuffer ( 6 ) Dxc = rbuffer ( 7 ) #endif /* BSC_CELLXC */ #endif /* MPI */ !     Add contribution to stress from the derivative of the Jacobian of --- !     r->r' (strained r) in the integral of Vna*(rhoscf-rhoatm) if ( istr . eq . 1 ) then do i = 1 , 3 stress ( i , i ) = stress ( i , i ) + ( Enascf - Enaatm ) / volume enddo endif !     Stop time counter for SCF iteration part call timer ( 'DHSCF3' , 2 ) ! ---------------------------------------------------------------------- !     End of SCF iteration part ! ---------------------------------------------------------------------- if ( ifa . eq . 1 . or . istr . eq . 1 ) then ! ---------------------------------------------------------------------- ! Forces and stress : SCF contribution ! ---------------------------------------------------------------------- !       Start time counter for force calculation part call timer ( 'DHSCF4' , 1 ) !       Find contribution of partial-core-correction if ( npcc . eq . 1 ) then call reord ( rhopcc , rhopcc , nml , nsm , TO_CLUSTER ) call reord ( Vaux , Vaux , nml , nsm , TO_CLUSTER ) ! The partial core calculation only acts on ! the diagonal spin-components (no need to ! redistribute un-used elements) do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_CLUSTER ) enddo call PartialCoreOnMesh ( na , isa , ntpl , rhopcc , indxua , nsd , & dvol , volume , Vscf , Vaux , Fal , & stressl , ifa . ne . 0 , istr . ne . 0 ) call reord ( rhopcc , rhopcc , nml , nsm , TO_SEQUENTIAL ) call reord ( Vaux , Vaux , nml , nsm , TO_SEQUENTIAL ) ! ** see above do ispin = 1 , nsd call reord ( Vscf (:, ispin ), Vscf (:, ispin ), & nml , nsm , TO_SEQUENTIAL ) enddo if ( debug_dhscf ) then write ( * , debug_fmt ) Node , 'PartialCore' , & ( sqrt ( sum ( Vscf (:, ispin ) ** 2 )), ispin = 1 , nsd ) end if endif if ( harrisfun ) then !         Forhar deals internally with its own needs !         for distribution changes #ifndef BSC_CELLXC call forhar ( ntpl , nspin , nml , ntml , ntm , npcc , cell , #else /* BSC_CELLXC */ call forhar ( ntpl , nspin , nml , ntml , npcc , cell , #endif /* BSC_CELLXC */ & rhoatm , rhopcc , Vna , DRho , Vscf , Vaux ) !         Upon return, everything is UNIFORM, sequential form endif !     Transform spin density into sum and difference ! TODO NC/SO ! Should we perform local diagonalization? if ( nsd . eq . 2 ) then !$OMP parallel do default(shared), private(rhotot,ip) do ip = 1 , ntpl rhotot = DRho ( ip , 1 ) + DRho ( ip , 2 ) DRho ( ip , 2 ) = DRho ( ip , 2 ) - DRho ( ip , 1 ) DRho ( ip , 1 ) = rhotot enddo !$OMP end parallel do endif !       Find contribution of neutral-atom potential call reord ( Vna , Vna , nml , nsm , TO_CLUSTER ) call reord ( DRho , DRho , nml , nsm , TO_CLUSTER ) call NeutralAtomOnMesh ( na , isa , ntpl , Vna , indxua , dvol , & volume , DRho , Fal , stressl , & ifa . ne . 0 , istr . ne . 0 ) call reord ( DRho , DRho , nml , nsm , TO_SEQUENTIAL ) call reord ( Vna , Vna , nml , nsm , TO_SEQUENTIAL ) if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( Vscf_par , 1 , ntpl , 1 , nspin , & 'Vscf_par' , 'dhscf' ) do ispin = 1 , nspin fsrc => Vscf (:, ispin ) fdst => Vscf_par (:, ispin ) call distMeshData ( UNIFORM , fsrc , & QUADRATIC , fdst , TO_CLUSTER ) enddo !       Remember that Vaux contains everything except Vxc call re_alloc ( Vaux_par , 1 , ntpl , 'Vaux_par' , 'dhscf' ) call distMeshData ( UNIFORM , Vaux , & QUADRATIC , Vaux_par , TO_CLUSTER ) call dfscf ( ifa , istr , na , norb , nuo , nuotot , nmpl , nspin , & indxua , isa , iaorb , iphorb , & maxnd , numd , listdptr , listd , Dscf , datm , & Vscf_par , Vaux_par , dvol , volume , Fal , stressl ) call de_alloc ( Vaux_par , 'Vaux_par' , 'dhscf' ) call de_alloc ( Vscf_par , 'Vscf_par' , 'dhscf' ) if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif !       Stop time counter for force calculation part call timer ( 'DHSCF4' , 2 ) ! ---------------------------------------------------------------------- !       End of force and stress calculation ! ---------------------------------------------------------------------- endif !     We are in the UNIFORM distribution !     Rhoatm, Rhopcc and Vna are in UNIFORM dist, sequential form !     The index array endpht is in the QUADRATIC distribution !     Stop time counter call timer ( 'DHSCF' , 2 ) ! ---------------------------------------------------------------------- !     Free locally allocated memory ! ---------------------------------------------------------------------- call de_alloc ( Vaux , 'Vaux' , 'dhscf' ) call de_alloc ( Vscf , 'Vscf' , 'dhscf' ) call de_alloc ( DRho , 'DRho' , 'dhscf' ) #ifdef DEBUG call write_debug ( '    POS DHSCF' ) #endif !------------------------------------------------------------------------ END CONTAINS subroutine save_bader_charge () use meshsubs , only : ModelCoreChargeOnMesh #ifdef NCDF_4 use siesta_options , only : write_cdf use m_ncdf_siesta , only : cdf_save_grid #endif ! Auxiliary routine to output the Bader Charge ! real ( grid_p ), pointer :: BaderCharge (:) => null () call re_alloc ( BaderCharge , 1 , ntpl , name = 'BaderCharge' , & routine = 'dhscf' ) ! Find a model core charge by re-scaling the local charge call ModelCoreChargeOnMesh ( na , isa , ntpl , BaderCharge , indxua ) ! It comes out in clustered form, so we convert it call reord ( BaderCharge , BaderCharge , nml , nsm , TO_SEQUENTIAL ) do ispin = 1 , nsd BaderCharge ( 1 : ntpl ) = BaderCharge ( 1 : ntpl ) + DRho ( 1 : ntpl , ispin ) enddo #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'RhoBader' , 1 , ntml , & BaderCharge ) else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) end if #else call write_rho ( trim ( slabel ) // \".BADER\" , cell , $ ntm , nsm , ntpl , 1 , BaderCharge ) call write_grid_netcdf ( cell , ntm , 1 , ntpl , $ BaderCharge , \"BaderCharge\" ) #endif call de_alloc ( BaderCharge , name = 'BaderCharge' ) end subroutine save_bader_charge subroutine setup_analysis_options () !! For the analyze-charge-density-only case, !! avoiding any diagonalization use siesta_options , only : hirshpop , voropop use siesta_options , only : saverho , savedrho , saverhoxc use siesta_options , only : savevh , savevt , savevna use siesta_options , only : savepsch , savetoch want_partial_charges = ( hirshpop . or . voropop ) if ( saverho ) filesOut % rho = trim ( slabel ) // '.RHO' if ( savedrho ) filesOut % drho = trim ( slabel ) // '.DRHO' if ( saverhoxc ) filesOut % rhoxc = trim ( slabel ) // '.RHOXC' if ( savevh ) filesOut % vh = trim ( slabel ) // '.VH' if ( savevt ) filesOut % vt = trim ( slabel ) // '.VT' if ( savevna ) filesOut % vna = trim ( slabel ) // '.VNA' if ( savepsch ) filesOut % psch = trim ( slabel ) // '.IOCH' if ( savetoch ) filesOut % toch = trim ( slabel ) // '.TOCH' end subroutine setup_analysis_options subroutine save_ebs_density () !! Optional output of the \"band-structure energy density\", which !! is just the charge density weighted by the eigenvalues, i.e., !! using EDM instead of DM in rhoofd use sparse_matrices , only : Escf real ( grid_p ), pointer :: Ebs_dens (:,:) => null (), & Ebs_dens_quad (:,:) => null () !     Allocate memory for Ebs_dens using the UNIFORM data distribution call re_alloc ( Ebs_dens , 1 , ntpl , 1 , nspin , 'Ebs_dens' , 'dhscf' ) !     Switch to quadratic distribution for call to rhoofd if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call re_alloc ( ebs_dens_quad , 1 , ntpl , 1 , nspin , & 'Ebs_dens_quad' , 'dhscf' ) call rhoofd ( norb , nmpl , maxnd , numd , listdptr , listd , & nspin , Escf , Ebs_dens_quad , & nuo , nuotot , iaorb , iphorb , isa ) !     Ebs_dens_par is here in QUADRATIC, clustered form !     Set the UNIFORM distribution again and copy Ebs_dens to it if ( nodes . gt . 1 ) then call setMeshDistr ( UNIFORM , nsm , nsp , nml , nmpl , ntml , ntpl ) endif do ispin = 1 , nspin fsrc => Ebs_dens_quad (:, ispin ) fdst => Ebs_dens (:, ispin ) ! Sequential to be able to write it out ! if nodes==1, this call will just reorder call distMeshData ( QUADRATIC , fsrc , & UNIFORM , fdst , TO_SEQUENTIAL ) enddo call de_alloc ( Ebs_dens_quad , 'Ebs_dens_quad' , 'dhscf' ) #ifdef NCDF_4 if ( write_cdf ) then call cdf_save_grid ( trim ( slabel ) // '.nc' , 'Ebs_density' , $ nspin , ntml , Ebs_dens ) else call write_rho ( filesOut % ebs_dens , $ cell , ntm , nsm , ntpl , nspin , Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , Ebs_dens , & \"Ebs_density\" ) end if #else call write_rho ( filesOut % ebs_dens , cell , ntm , nsm , ntpl , nspin , $ Ebs_dens ) call write_grid_netcdf ( cell , ntm , nspin , ntpl , & Ebs_dens , \"Ebs_density\" ) #endif call de_alloc ( Ebs_dens , 'Ebs_dens' , 'dhscf' ) end subroutine save_ebs_density end subroutine dhscf","tags":"","loc":"proc/dhscf.html","title":"dhscf – SIESTA"},{"text":"public subroutine delk_wrapper(isigneikr, norb, maxnd, numd, listdptr, listd, nuo, nuotot, iaorb, iphorb, isa) Uses m_delk moreMeshSubs moreMeshSubs parallel mesh proc~~delk_wrapper~~UsesGraph proc~delk_wrapper delk_wrapper module~parallel parallel proc~delk_wrapper->module~parallel module~m_delk m_delk proc~delk_wrapper->module~m_delk module~mesh mesh proc~delk_wrapper->module~mesh module~moremeshsubs moreMeshSubs proc~delk_wrapper->module~moremeshsubs module~precision precision module~mesh->module~precision module~moremeshsubs->module~parallel module~moremeshsubs->module~precision alloc alloc module~moremeshsubs->alloc module~sys sys module~moremeshsubs->module~sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. This is a wrapper to call delk , using some of the module\n variables of m_dhscf , but from outside dhscf itself. The dhscf module variables used are: nmpl dvol nml nmpl ntml ntpl Some of them might be put somewhere else (mesh?) to allow some\n of the kitchen-sink functionality of dhscf to be made more modular.\n For example, this wrapper might live independently if enough mesh\n information is made available to it. Arguments Type Intent Optional Attributes Name integer :: isigneikr integer :: norb integer :: maxnd integer :: numd (nuo) integer :: listdptr (nuo) integer :: listd (maxnd) integer :: nuo integer :: nuotot integer :: iaorb (*) integer :: iphorb (*) integer :: isa (*) Calls proc~~delk_wrapper~~CallsGraph proc~delk_wrapper delk_wrapper proc~setmeshdistr setMeshDistr proc~delk_wrapper->proc~setmeshdistr proc~delk delk proc~delk_wrapper->proc~delk phi phi proc~delk->phi indxuo indxuo proc~delk->indxuo listsc listsc proc~delk->listsc dsin dsin proc~delk->dsin matrixmtooc matrixmtooc proc~delk->matrixmtooc endpht endpht proc~delk->endpht re_alloc re_alloc proc~delk->re_alloc xa xa proc~delk->xa listp2 listp2 proc~delk->listp2 indxua indxua proc~delk->indxua timer timer proc~delk->timer needdscfl needdscfl proc~delk->needdscfl dcos dcos proc~delk->dcos globaltolocalorb globaltolocalorb proc~delk->globaltolocalorb de_alloc de_alloc proc~delk->de_alloc numdl numdl proc~delk->numdl lstpht lstpht proc~delk->lstpht proc~rcut rcut proc~delk->proc~rcut listdlptr listdlptr proc~delk->listdlptr ipack ipack proc~delk->ipack listdl listdl proc~delk->listdl proc~chk chk proc~rcut->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code delk_wrapper Source Code subroutine delk_wrapper ( isigneikr , norb , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) !! This is a wrapper to call [[delk(proc)]], using some of the module !! variables of `m_dhscf`, but from outside `dhscf` itself. !! !! The dhscf module variables used are: !! !! * nmpl !! * dvol !! * nml !! * nmpl !! * ntml !! * ntpl !! !! Some of them might be put somewhere else (mesh?) to allow some !! of the kitchen-sink functionality of dhscf to be made more modular. !! For example, this wrapper might live independently if enough mesh !! information is made available to it. use m_delk , only : delk ! The real workhorse, similar to vmat use moreMeshSubs , only : setMeshDistr use moreMeshSubs , only : UNIFORM , QUADRATIC use parallel , only : Nodes use mesh , only : nsm , nsp integer :: isigneikr , & norb , nuo , nuotot , maxnd , & iaorb ( * ), iphorb ( * ), isa ( * ), & numd ( nuo ), & listdptr ( nuo ), listd ( maxnd ) ! ---------------------------------------------------------------------- ! Calculate matrix elements of exp(i \\vec{k} \\cdot \\vec{r}) ! ---------------------------------------------------------------------- if ( isigneikr . eq . 1 . or . isigneikr . eq . - 1 ) then if ( nodes . gt . 1 ) then call setMeshDistr ( QUADRATIC , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif call delk ( isigneikr , norb , nmpl , dvol , maxnd , & numd , listdptr , listd , & nuo , nuotot , iaorb , iphorb , isa ) if ( nodes . gt . 1 ) then !           Everything back to UNIFORM, sequential call setMeshDistr ( UNIFORM , nsm , nsp , & nml , nmpl , ntml , ntpl ) endif endif end subroutine delk_wrapper","tags":"","loc":"proc/delk_wrapper.html","title":"delk_wrapper – SIESTA"},{"text":"public subroutine delk(iexpikr, no, np, dvol, nvmax, numVs, listVsptr, listVs, nuo, nuotot, iaorb, iphorb, isa) Uses precision atmfuncs atm_types atomlist siesta_geom listsc_module mesh mesh mesh meshdscf meshdscf meshphi parallel alloc parallelsubs m_planewavematrixvar proc~~delk~~UsesGraph proc~delk delk listsc_module listsc_module proc~delk->listsc_module module~m_planewavematrixvar m_planewavematrixvar proc~delk->module~m_planewavematrixvar module~precision precision proc~delk->module~precision module~atmfuncs atmfuncs proc~delk->module~atmfuncs siesta_geom siesta_geom proc~delk->siesta_geom meshphi meshphi proc~delk->meshphi parallelsubs parallelsubs proc~delk->parallelsubs alloc alloc proc~delk->alloc atomlist atomlist proc~delk->atomlist module~parallel parallel proc~delk->module~parallel module~mesh mesh proc~delk->module~mesh meshdscf meshdscf proc~delk->meshdscf module~atm_types atm_types proc~delk->module~atm_types module~m_planewavematrixvar->module~precision module~atmfuncs->module~precision module~atmfuncs->module~atm_types spher_harm spher_harm module~atmfuncs->spher_harm module~radial radial module~atmfuncs->module~radial module~sys sys module~atmfuncs->module~sys module~mesh->module~precision module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Finds the matrix elements of a plane wave <\\phi_{\\mu}|e&#94;{(\\boldsymbol{iexpikr} \\: \\cdot \\: i \\vec{k} \\vec{r})}|\\phi_{\\nu}> First version written by J. Junquera in Feb. 2008 Adapted from an existing version of vmat after the parallelization\n designed by BSC in July 2011. Output complex(dp) delkmat (nvmax) :\n value of nonzero elements in each row\n of the matrix elements of exp(i*\\vec{k}\\vec{r}) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iexpikr Prefactor of the dot product between the\n the k-vector and the position-vector in exponent. It might be +1 or -1 integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of columns in C (local) real(kind=dp), intent(in) :: dvol Volume per mesh point integer, intent(in) :: nvmax First dimension of listV and Vs , and max\n number of nonzero elements in any row of delkmat integer, intent(in) :: numVs (nuo) Number of non-zero elements in a row of delkmat integer, intent(in) :: listVsptr (nuo) Pointer to the start of rows in listVs integer, intent(in) :: listVs (nvmax) List of non-zero elements of delkmat integer :: nuo integer :: nuotot integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms Calls proc~~delk~~CallsGraph proc~delk delk phi phi proc~delk->phi indxuo indxuo proc~delk->indxuo listsc listsc proc~delk->listsc dsin dsin proc~delk->dsin matrixmtooc matrixmtooc proc~delk->matrixmtooc endpht endpht proc~delk->endpht re_alloc re_alloc proc~delk->re_alloc xa xa proc~delk->xa listp2 listp2 proc~delk->listp2 indxua indxua proc~delk->indxua timer timer proc~delk->timer needdscfl needdscfl proc~delk->needdscfl dcos dcos proc~delk->dcos globaltolocalorb globaltolocalorb proc~delk->globaltolocalorb de_alloc de_alloc proc~delk->de_alloc numdl numdl proc~delk->numdl lstpht lstpht proc~delk->lstpht proc~rcut rcut proc~delk->proc~rcut listdlptr listdlptr proc~delk->listdlptr ipack ipack proc~delk->ipack listdl listdl proc~delk->listdl proc~chk chk proc~rcut->proc~chk proc~die die proc~chk->proc~die io_close io_close proc~die->io_close cmlfinishfile cmlfinishfile proc~die->cmlfinishfile io_assign io_assign proc~die->io_assign pxfabort pxfabort proc~die->pxfabort mpi_abort mpi_abort proc~die->mpi_abort pxfflush pxfflush proc~die->pxfflush Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Called by proc~~delk~~CalledByGraph proc~delk delk proc~delk_wrapper delk_wrapper proc~delk_wrapper->proc~delk Help × Graph Key Nodes of different colours represent the following: Graph Key Subroutine Subroutine Function Function Interface Interface Unknown Procedure Type Unknown Procedure Type Program Program This Page's Entity This Page's Entity Solid arrows point from a procedure to one which it calls. Dashed \n    arrows point from an interface to procedures which implement that interface.\n    This could include the module procedures in a generic interface or the\n    implementation in a submodule of an interface in a parent module. Contents Source Code delk Source Code subroutine delk ( iexpikr , no , np , dvol , nvmax , & numVs , listVsptr , listVs , & nuo , nuotot , iaorb , iphorb , isa ) !! author: J. Junquera !! date: February 2008 !! license: GNU GPL !! !! Finds the matrix elements of a plane wave !! <\\phi_{\\mu}|e&#94;{(\\boldsymbol{iexpikr} \\: \\cdot \\: i \\vec{k} \\vec{r})}|\\phi_{\\nu}> !! !! First version written by J. Junquera in Feb. 2008 !! !! Adapted from an existing version of `vmat` after the parallelization !! designed by BSC in July 2011. !! !!###Output !! `complex(dp) [[m_planewavematrixvar(module):delkmat]](nvmax)`: !! value of nonzero elements in each row !! of the matrix elements of  exp(i*\\vec{k}\\vec{r})  !  Modules use precision , only : dp , grid_p use atmfuncs , only : rcut , all_phi use atm_types , only : nsmax => nspecies use atomlist , only : indxuo , indxua use siesta_geom , only : xa use listsc_module , only : listsc use mesh , only : dxa , nsp , xdop , xdsp , ne , nem use mesh , only : cmesh , ipa , idop , nmsc , iatfold use mesh , only : meshLim use meshdscf , only : matrixMtoOC use meshdscf , only : needdscfl , listdl , numdl , nrowsdscfl , listdlptr use meshphi , only : directphi , endpht , lstpht , listp2 , phi use parallel , only : Nodes , node use alloc , only : re_alloc , de_alloc use parallelsubs , only : GlobalToLocalOrb use m_planewavematrixvar , only : delkmat , wavevector #ifdef MPI use mpi_siesta #endif #ifdef _OPENMP use omp_lib #endif integer , intent ( in ) :: iexpikr !! Prefactor of the dot product between the !! the k-vector and the position-vector in exponent. !! It might be +1 or -1 integer , intent ( in ) :: no !! Number of basis orbitals integer , intent ( in ) :: np !! Number of columns in `C` (local) integer , intent ( in ) :: nvmax !! First dimension of `listV` and `Vs`, and max !! number of nonzero elements in any row of `[[m_planewavematrixvar(module):delkmat]]` integer :: nuo , nuotot integer , intent ( in ) :: iaorb ( * ) !! Pointer to atom to which orbital belongs integer , intent ( in ) :: iphorb ( * ) !! Orbital index within each atom integer , intent ( in ) :: isa ( * ) !! Species index of all atoms integer , intent ( in ) :: numVs ( nuo ) !! Number of non-zero elements in a row of `[[m_planewavematrixvar(module):delkmat]]` integer , intent ( in ) :: listVsptr ( nuo ) !! Pointer to the start of rows in `listVs` integer , intent ( in ) :: listVs ( nvmax ) !! List of non-zero elements of `[[m_planewavematrixvar(module):delkmat]]` real ( dp ), intent ( in ) :: dvol !! Volume per mesh point ! Internal variables and arrays integer , parameter :: minloc = 1000 ! Min buffer size integer , parameter :: maxoa = 100 ! Max # of orb/atom integer :: i , ia , ic , ii , ijl , il , imp , ind , iop integer :: ip , iphi , io , is , isp , iu , iul integer :: ix , j , jc , jl , last , lasta , lastop integer :: maxloc , maxloc2 , nc , nlocal , nphiloc integer :: nvmaxl , triang , lenx , leny , lenz , lenxy logical :: ParallelLocal real ( dp ) :: Vij ( 2 ), r2sp , dxsp ( 3 ), VClocal ( 2 , nsp ) integer , pointer :: ilc (:), ilocal (:), iorb (:) real ( dp ), pointer :: DscfL (:,:), t_DscfL (:,:,:), Clocal (:,:) real ( dp ), pointer :: Vlocal (:,:), phia (:,:), r2cut (:) integer :: NTH , TID ! Variables to compute the matrix element of the plane wave ! (not included in the original vmat subroutine) integer :: irel , iua , irealim , inmp ( 3 ) real ( dp ) :: kxij , aux ( 2 ), dist ( 3 ), kpoint ( 3 ) real ( dp ) :: dxp ( 3 ), displaat ( 3 ) real ( dp ) :: dxpgrid ( 3 , nsp ) complex ( dp ), pointer :: delkmats (:), t_delkmats (:,:) #ifdef _TRACE_ integer :: MPIerror #endif #ifdef DEBUG call write_debug ( '    PRE delk' ) #endif #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 4 ) #endif !   Start time counter call timer ( 'delk' , 1 ) !   Initialize the matrix elements of exp(i*\\vec{k} \\vec{r}) !$OMP parallel workshare default(shared) delkmat (:) = 0.0_dp !$OMP end parallel workshare kpoint (:) = dble ( iexpikr ) * wavevector (:) !  For debugging !      kpoint(:) = 0.0_dp !  End debugging !   Find atomic cutoff radii nullify ( r2cut ) call re_alloc ( r2cut , 1 , nsmax , 'r2cut' , 'delk' ) r2cut = 0.0_dp do i = 1 , nuotot ia = iaorb ( i ) is = isa ( ia ) io = iphorb ( i ) r2cut ( is ) = max ( r2cut ( is ), rcut ( is , io ) ** 2 ) end do !   Set algorithm logical ParallelLocal = ( Nodes > 1 ) lenx = meshLim ( 2 , 1 ) - meshLim ( 1 , 1 ) + 1 leny = meshLim ( 2 , 2 ) - meshLim ( 1 , 2 ) + 1 lenz = meshLim ( 2 , 3 ) - meshLim ( 1 , 3 ) + 1 lenxy = lenx * leny !   Find value of maxloc maxloc2 = maxval ( endpht ( 1 : np ) - endpht ( 0 : np - 1 )) maxloc = maxloc2 + minloc maxloc = min ( maxloc , no ) triang = ( maxloc + 1 ) * ( maxloc + 2 ) / 2 if ( ParallelLocal ) then if ( nrowsDscfL > 0 ) then nvmaxl = listdlptr ( nrowsDscfL ) + numdl ( nrowsDscfL ) else nvmaxl = 1 end if end if !   Allocate local memory !$OMP parallel default(shared), & !$OMP&shared(NTH,t_DscfL,t_delkmats), & !$OMP&private(TID,last,delkmats,irealim), & !$OMP&private(ip,nc,nlocal,ic,imp,i,il,iu,iul,ii,ind,j,ijl,jl), & !$OMP&private(lasta,lastop,ia,is,iop,isp,ix,dxsp,r2sp,nphiloc,iphi,jc), & !$OMP&private(Vij,VClocal,DscfL,ilocal,ilc,iorb,Vlocal,Clocal,phia), & !$OMP&private(irel,inmp,dxp,dxpgrid,dist,kxij,iua,displaat,aux) !$OMP single #ifdef _OPENMP NTH = omp_get_num_threads ( ) #else NTH = 1 #endif !$OMP end single ! implicit barrier, IMPORTANT #ifdef _OPENMP TID = omp_get_thread_num ( ) + 1 #else TID = 1 #endif nullify ( Clocal , phia , ilocal , ilc , iorb , Vlocal ) !$OMP critical ! Perhaps the critical section is not needed, ! however it \"tells\" the OS to allocate per ! thread, possibly waiting for each thread to ! place the memory in the best position. allocate ( Clocal ( nsp , maxloc2 ) ) allocate ( ilocal ( no ) , ilc ( maxloc2 ) , iorb ( maxloc ) ) allocate ( Vlocal ( triang , 2 ) ) if ( DirectPhi ) allocate ( phia ( maxoa , nsp ) ) !$OMP end critical !$OMP single if ( ParallelLocal ) then nullify ( t_DscfL ) call re_alloc ( t_DscfL , 1 , nvmaxl , 1 , 2 , 1 , NTH , & 'DscfL' , 'delk' ) else if ( NTH > 1 ) then nullify ( t_delkmats ) call re_alloc ( t_delkmats , 1 , nvmax , 1 , NTH , & 'delkmats' , 'delk' ) end if end if !$OMP end single if ( ParallelLocal ) then DscfL => t_DscfL ( 1 : nvmaxl , 1 : 2 , TID ) DscfL ( 1 : nvmaxl , 1 : 2 ) = 0.0_dp else if ( NTH > 1 ) then delkmats => t_delkmats ( 1 : nvmax , TID ) else delkmats => delkmat end if end if !   Full initializations done only once ilocal ( 1 : no ) = 0 iorb ( 1 : maxloc ) = 0 Vlocal ( 1 : triang , 1 : 2 ) = 0.0_dp last = 0 !   Loop over grid points !$OMP do do ip = 1 , np !      Find number of nonzero orbitals at this point nc = endpht ( ip ) - endpht ( ip - 1 ) !      Find new required size of Vlocal nlocal = last do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) . eq . 0 ) nlocal = nlocal + 1 end do !      If overflooded, add Vlocal to delkmat and reinitialize it if ( nlocal > maxloc . and . last > 0 ) then if ( ParallelLocal ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) ! The variables we want to compute in this subroutine are complex numbers ! Here, when irealim =1 we refer to the real part, and ! when irealim = 2 we refer to the imaginary part DscfL ( ind ,:) = DscfL ( ind ,:) + Vlocal ( ijl ,:) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listsc ( i , iu , listdl ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) DscfL ( ind ,:) = DscfL ( ind ,:) + aux (:) * dVol end do end if end do else do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) call GlobalToLocalOrb ( iu , Node , Nodes , iul ) if ( i == iu ) then do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = listVs ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( Vlocal ( ijl , 1 ), Vlocal ( ijl , 2 ), kind = dp ) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numVs ( iul ) ind = listVsptr ( iul ) + ii j = listsc ( i , iu , listVs ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( aux ( 1 ), aux ( 2 ), kind = dp ) * dVol end do end if end do end if !         Reset local arrays do ii = 1 , last ilocal ( iorb ( ii )) = 0 end do iorb ( 1 : last ) = 0 ijl = ( last + 1 ) * ( last + 2 ) / 2 Vlocal ( 1 : ijl , 1 : 2 ) = 0.0_dp last = 0 end if !      Look for required orbitals not yet in Vlocal if ( nlocal > last ) then do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) if ( ilocal ( i ) == 0 ) then last = last + 1 ilocal ( i ) = last iorb ( last ) = i end if end do end if if ( DirectPhi ) then lasta = 0 lastop = 0 do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !            Localize the position of the mesh point irel = idop ( iop ) + ipa ( ia ) call ipack ( - 1 , 3 , nem , inmp , irel ) inmp (:) = inmp (:) + ( meshLim ( 1 ,:) - 1 ) inmp (:) = inmp (:) - 2 * ne (:) dxp (:) = cmesh (:, 1 ) * inmp ( 1 ) + & cmesh (:, 2 ) * inmp ( 2 ) + & cmesh (:, 3 ) * inmp ( 3 ) do isp = 1 , nsp dxpgrid (:, isp ) = dxp (:) + xdsp (:, isp ) end do !            Generate phi values if ( ia /= lasta . or . iop /= lastop ) then lasta = ia lastop = iop is = isa ( ia ) do isp = 1 , nsp dxsp (:) = xdsp (:, isp ) + xdop (:, iop ) - dxa (:, ia ) r2sp = sum ( dxsp ** 2 ) if ( r2sp < r2cut ( is ) ) then !$OMP critical call all_phi ( is , + 1 , dxsp , nphiloc , phia (:, isp ) ) !$OMP end critical else phia (:, isp ) = 0.0_dp end if end do end if iphi = iphorb ( i ) Clocal (:, ic ) = phia ( iphi ,:) !            Pre-multiply V and Clocal(,ic) Vij (:) = 0._dp do isp = 1 , nsp kxij = kpoint ( 1 ) * dxpgrid ( 1 , isp ) + & kpoint ( 2 ) * dxpgrid ( 2 , isp ) + & kpoint ( 3 ) * dxpgrid ( 3 , isp ) VClocal ( 1 , isp ) = dcos ( kxij ) * Clocal ( isp , ic ) VClocal ( 2 , isp ) = dsin ( kxij ) * Clocal ( isp , ic ) Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , ic ) end do !            ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) !            Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !               Loop over sub-points Vij (:) = 0.0_dp do isp = 1 , nsp Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) * 2._dp else Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) end if end do end do else !         Loop on first orbital of mesh point do ic = 1 , nc imp = endpht ( ip - 1 ) + ic i = lstpht ( imp ) il = ilocal ( i ) ia = iaorb ( i ) iop = listp2 ( imp ) ilc ( ic ) = il !            Localize the position of the mesh point irel = idop ( iop ) + ipa ( ia ) call ipack ( - 1 , 3 , nem , inmp , irel ) inmp (:) = inmp (:) + ( meshLim ( 1 ,:) - 1 ) inmp (:) = inmp (:) - 2 * ne (:) dxp (:) = cmesh (:, 1 ) * inmp ( 1 ) + & cmesh (:, 2 ) * inmp ( 2 ) + & cmesh (:, 3 ) * inmp ( 3 ) do isp = 1 , nsp dxpgrid (:, isp ) = dxp (:) + xdsp (:, isp ) end do !            Retrieve phi values Clocal (:, ic ) = phi (:, imp ) !            Pre-multiply V and Clocal(,ic) Vij (:) = 0._dp do isp = 1 , nsp kxij = kpoint ( 1 ) * dxpgrid ( 1 , isp ) + & kpoint ( 2 ) * dxpgrid ( 2 , isp ) + & kpoint ( 3 ) * dxpgrid ( 3 , isp ) VClocal ( 1 , isp ) = dcos ( kxij ) * Clocal ( isp , ic ) VClocal ( 2 , isp ) = dsin ( kxij ) * Clocal ( isp , ic ) Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , ic ) end do !            ic == jc, hence we simply do ijl = idx_ijl ( il , ilc ( ic )) Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) !            Loop on second orbital of mesh point do jc = 1 , ic - 1 jl = ilc ( jc ) !               Loop over sub-points Vij (:) = 0.0_dp do isp = 1 , nsp Vij (:) = Vij (:) + VClocal (:, isp ) * Clocal ( isp , jc ) end do ijl = idx_ijl ( il , jl ) if ( il == jl ) then Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) * 2._dp else Vlocal ( ijl ,:) = Vlocal ( ijl ,:) + Vij (:) end if end do end do end if end do !$OMP end do nowait !   Add final Vlocal to delkmat if ( ParallelLocal . and . last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) iul = NeedDscfL ( iu ) if ( i == iu ) then do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listdl ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) DscfL ( ind ,:) = DscfL ( ind ,:) + Vlocal ( ijl ,:) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numdl ( iul ) ind = listdlptr ( iul ) + ii j = listsc ( i , iu , listdl ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) DscfL ( ind ,:) = DscfL ( ind ,:) + aux (:) * dVol end do end if end do else if ( last > 0 ) then do il = 1 , last i = iorb ( il ) iu = indxuo ( i ) if ( i == iu ) then do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = listVs ( ind ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( Vlocal ( ijl , 1 ), Vlocal ( ijl , 2 ), kind = dp ) * dVol end do else ia = iaorb ( i ) iua = indxua ( ia ) do ix = 1 , 3 displaat ( ix ) = & ( iatfold ( 1 , ia ) * nmsc ( 1 )) * cmesh ( ix , 1 ) + & ( iatfold ( 2 , ia ) * nmsc ( 2 )) * cmesh ( ix , 2 ) + & ( iatfold ( 3 , ia ) * nmsc ( 3 )) * cmesh ( ix , 3 ) end do dist (:) = xa (:, iua ) - xa (:, ia ) - displaat (:) kxij = kpoint ( 1 ) * dist ( 1 ) + & kpoint ( 2 ) * dist ( 2 ) + & kpoint ( 3 ) * dist ( 3 ) do ii = 1 , numVs ( iu ) ind = listVsptr ( iu ) + ii j = listsc ( i , iu , listVs ( ind ) ) jl = ilocal ( j ) ijl = idx_ijl ( il , jl ) aux ( 1 ) = Vlocal ( ijl , 1 ) * dcos ( kxij ) - & Vlocal ( ijl , 2 ) * dsin ( kxij ) aux ( 2 ) = Vlocal ( ijl , 2 ) * dcos ( kxij ) + & Vlocal ( ijl , 1 ) * dsin ( kxij ) delkmats ( ind ) = delkmats ( ind ) + & cmplx ( aux ( 1 ), aux ( 2 ), kind = dp ) * dVol end do end if end do end if !$OMP barrier if ( ParallelLocal . and . NTH > 1 ) then !$OMP do collapse(2) do irealim = 1 , 2 do ind = 1 , nvmaxl do ii = 2 , NTH t_DscfL ( ind , irealim , 1 ) = t_DscfL ( ind , irealim , 1 ) + & t_DscfL ( ind , irealim , ii ) end do end do end do !$OMP end do else if ( NTH > 1 ) then !$OMP do do ind = 1 , nvmax do ii = 1 , NTH delkmat ( ind ) = delkmat ( ind ) + t_delkmats ( ind , ii ) end do end do !$OMP end do end if !   Free local memory deallocate ( Clocal , ilocal , ilc , iorb , Vlocal ) if ( DirectPhi ) deallocate ( phia ) !$OMP master if ( ParallelLocal ) then !      Redistribute Hamiltonian from mesh to orbital based distribution DscfL => t_DscfL ( 1 : nvmaxl , 1 : 2 , 1 ) call matrixMtoOC ( nvmaxl , nvmax , numVs , listVsptr , nuo , DscfL , delkmat ) call de_alloc ( t_DscfL , 'DscfL' , 'delk' ) else if ( NTH > 1 ) then call de_alloc ( t_delkmats , 'delkmats' , 'delk' ) end if !$OMP end master !$OMP end parallel call de_alloc ( r2cut , 'r2cut' , 'delk' ) #ifdef _TRACE_ call MPI_Barrier ( MPI_Comm_World , MPIerror ) call MPItrace_event ( 1000 , 0 ) #endif call timer ( 'delk' , 2 ) #ifdef DEBUG call write_debug ( '    POS delk' ) #endif contains !   In any case will the compiler most likely inline this !   small routine. So it should not pose any problem. pure function idx_ijl ( i , j ) result ( ij ) integer , intent ( in ) :: i , j integer :: ij if ( i > j ) then ij = i * ( i + 1 ) / 2 + j + 1 else ij = j * ( j + 1 ) / 2 + i + 1 end if end function idx_ijl end subroutine delk","tags":"","loc":"proc/delk.html","title":"delk – SIESTA"},{"text":"Used by module~~m_rhoofd~~UsedByGraph module~m_rhoofd m_rhoofd proc~dhscf dhscf proc~dhscf->module~m_rhoofd Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines rhoofd Subroutines public subroutine rhoofd (no, np, maxnd, numd, listdptr, listd, nspin, Dscf, rhoscf, nuo, nuotot, iaorb, iphorb, isa) Author P.Ordejon and J.M.Soler Date May 1995 License GNU GPL Finds the SCF density at the mesh points from the density matrix. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of mesh points integer, intent(in) :: maxnd First dimension of listd and Dscf , and\n maximum number of nonzero elements in\n any row of Dscf integer, intent(in) :: numd (nuo) Number of nonzero elemts in each row of Dscf integer, intent(in) :: listdptr (nuo) Pointer to start of rows in listd integer, intent(in) :: listd (maxnd) List of nonzero elements in each row of Dscf integer, intent(in) :: nspin Number of spin components real(kind=dp), intent(in) :: Dscf (:,:) real*8  Dscf(maxnd) - Rows of Dscf that are non-zero real(kind=grid_p), intent(out) :: rhoscf (nsp,np,nspin) SCF density at mesh points integer, intent(in) :: nuo Number of orbitals in unit cell locally integer, intent(in) :: nuotot Number of orbitals in unit cell in total integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms","tags":"","loc":"module/m_rhoofd.html","title":"m_rhoofd – SIESTA"},{"text":"Used by module~~m_setup_h0~~UsedByGraph module~m_setup_h0 m_setup_H0 proc~siesta_forces siesta_forces proc~siesta_forces->module~m_setup_h0 Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines setup_H0 Subroutines public subroutine setup_H0 (g2max) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(inout) :: g2max","tags":"","loc":"module/m_setup_h0.html","title":"m_setup_H0 – SIESTA"},{"text":"This module implements most of the functionality necessary to\n to mix the Fourier components of the charge density Theory As presented in Kresse and Furthmuller\n The most basic form is the Kerker mixing, with a parameter related\n to the Thomas Fermi screening wavevector.\n There is also a Pulay (DIIS) mixer. This module is not completely self-contained. It interacts with: dhscf_init : the allocation of the rhog arrays is done there once\n the mesh-related parameters are known. dhscf : It can optionally use rhog_in instead of Dscf as starting\n point. It also computes and stores rhog by default. compute_energies : Apart from correcting EKS when mixing the charge,\n it generates rhog from DM_out. siesta_forces : In transiesta runs the history is reset upon starting\n   transiesta setup_hamiltonian : it saves rhog_in. Uses precision class_dData1D class_Pair_dData1D class_Fstack_Pair_dData1D m_spin module~~m_rhog~~UsesGraph module~m_rhog m_rhog module~precision precision module~m_rhog->module~precision class_Fstack_Pair_dData1D class_Fstack_Pair_dData1D module~m_rhog->class_Fstack_Pair_dData1D class_dData1D class_dData1D module~m_rhog->class_dData1D m_spin m_spin module~m_rhog->m_spin class_Pair_dData1D class_Pair_dData1D module~m_rhog->class_Pair_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_rhog~~UsedByGraph module~m_rhog m_rhog proc~dhscf_init dhscf_init proc~dhscf_init->module~m_rhog proc~dhscf dhscf proc~dhscf->module~m_rhog proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~m_rhog proc~siesta_forces siesta_forces proc~siesta_forces->module~m_rhog proc~compute_energies compute_energies proc~compute_energies->module~m_rhog Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables rhog rhog_in g2 g2mask gindex star_index rhog_cutoff ng_diis g2_diis using_diis_for_rhog rg_in rg_diff q0sq q1sq rhog_stack jg0 Functions scalar_product Subroutines mix_rhog compute_charge_diff order_rhog resetRhog Variables Type Visibility Attributes Name Initial real(kind=grid_p), public, pointer :: rhog (:,:,:) => null() real(kind=grid_p), public, pointer :: rhog_in (:,:,:) => null() real(kind=dp), public, pointer :: g2 (:) => null() logical, public, pointer :: g2mask (:) => null() integer, public, pointer :: gindex (:) => null() integer, public, pointer :: star_index (:) => null() real(kind=dp), public :: rhog_cutoff integer, public :: ng_diis real(kind=dp), public, pointer :: g2_diis (:) => null() logical, public :: using_diis_for_rhog real(kind=dp), public, pointer :: rg_in (:) => null() real(kind=dp), public, pointer :: rg_diff (:) => null() real(kind=dp), public :: q0sq real(kind=dp), public :: q1sq type(Fstack_Pair_dData1D), public, save :: rhog_stack integer, public :: jg0 Functions private function scalar_product (a, b) result(sp) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: a (:) real(kind=dp), intent(in) :: b (:) Return Value real(kind=dp) Subroutines public subroutine mix_rhog (iscf) ! maybe           if (g2(j) <= certain cutoff) then Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf public subroutine compute_charge_diff (drhog) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(out) :: drhog public subroutine order_rhog (cell, n1, n2, n3, mesh, nsm) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: cell (3,3) integer, intent(in) :: n1 integer, intent(in) :: n2 integer, intent(in) :: n3 integer, intent(in) :: mesh (3) integer, intent(in) :: nsm public subroutine resetRhog (continuation) Arguments Type Intent Optional Attributes Name logical, intent(in), optional :: continuation If .true. we don't de-allocate anything, we only reset the history","tags":"","loc":"module/m_rhog.html","title":"m_rhog – SIESTA"},{"text":"Uses precision parallel sys alloc module~~moremeshsubs~~UsesGraph module~moremeshsubs moreMeshSubs module~parallel parallel module~moremeshsubs->module~parallel module~precision precision module~moremeshsubs->module~precision alloc alloc module~moremeshsubs->alloc module~sys sys module~moremeshsubs->module~sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~moremeshsubs~~UsedByGraph module~moremeshsubs moreMeshSubs proc~dhscf_init dhscf_init proc~dhscf_init->module~moremeshsubs proc~dhscf dhscf proc~dhscf->module~moremeshsubs module~m_forhar m_forhar proc~dhscf->module~m_forhar proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~moremeshsubs module~m_forhar->module~moremeshsubs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables UNIFORM QUADRATIC LINEAR TO_SEQUENTIAL TO_CLUSTER KEEP moduName maxDistr gp meshDistr meshCommu exteCommu tBuff1 tBuff2 Interfaces distMeshData Derived Types meshDisType meshCommType Subroutines initMeshDistr allocASynBuffer allocExtMeshDistr allocIpaDistr setMeshDistr resetMeshDistr distMeshData_rea distMeshData_rea distMeshData_int boxIntersection initMeshExtencil distExtMeshData gathExtMeshData splitwload reduce3Dto1D vecBisec reordMeshNumbering reordMeshNumbering compMeshComm Variables Type Visibility Attributes Name Initial integer, public, parameter :: UNIFORM = 1 integer, public, parameter :: QUADRATIC = 2 integer, public, parameter :: LINEAR = 3 integer, public, parameter :: TO_SEQUENTIAL = +1 alias to translation direction for reord procedure integer, public, parameter :: TO_CLUSTER = -1 alias to translation direction for reord procedure integer, public, parameter :: KEEP = 0 character(len=*), private, parameter :: moduName = 'moreMeshSubs' Name of the module. integer, private, parameter :: maxDistr = 5 Maximum number of data distribution that can be handled integer, private, parameter :: gp = grid_p Alias of the grid precision type( meshDisType ), private, target, save :: meshDistr (maxDistr) Contains information of the several data distributions type( meshCommType ), private, target, save :: meshCommu ((maxDistr*(maxDistr-1))/2) Contains all the communications to move among the\n several data distributions type( meshCommType ), private, target, save :: exteCommu (maxDistr,3) Contains all the needed communications to compute\n the extencil real(kind=grid_p), private, pointer :: tBuff1 (:) Memory buffer for asynchronous communications real(kind=grid_p), private, pointer :: tBuff2 (:) Memory buffer for asynchronous communications Interfaces public interface distMeshData Move data from vector fsrc , that uses distribution iDistr , to vector fdst , that uses distribution oDistr . It also re-orders a clustered\n data array into a sequential one and viceversa.\n If this is a sequencial execution, it only reorders the data. Read more… private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr Derived Types type, private :: meshDisType Private type to hold mesh distribution data. Components Type Visibility Attributes Name Initial integer, public :: nMesh (3) Number of mesh div. in each axis. integer, public, pointer :: box (:,:,:) Mesh box bounds of each node: box(1,iAxis,iNode)=lower bounds box(2,iAxis,iNode)=upper bounds integer, public, pointer :: indexp (:) integer, public, pointer :: idop (:) real(kind=dp), public, pointer :: xdop (:,:) integer, public, pointer :: ipa (:) type, private :: meshCommType Private type to hold communications to move data from one\n distribution to another. Components Type Visibility Attributes Name Initial integer, public :: ncom Number of needed communications integer, public, pointer :: src (:) Sources of communications integer, public, pointer :: dst (:) Destination of communications Subroutines public subroutine initMeshDistr (iDistr, oDistr, nm, wload) Computes a new data distribution and the communications needed to\n move data from/to the current distribution to the existing ones. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index of the input vector integer, intent(in) :: oDistr The new data distribution index integer, intent(in) :: nm (3) Number of Mesh divisions of each cell vector integer, intent(in), optional :: wload (*) Weights of every point of the mesh using the input distribut      !ion public subroutine allocASynBuffer (ndistr) Allocate memory buffers for asynchronous communications.\n It does nothing for synchronous communications. The output values are stored in the current module: tBuff1 : Buffer for distribution 1 tBuff2 : Buffer for other distributions Arguments Type Intent Optional Attributes Name integer :: ndistr Total number of distributions public subroutine allocExtMeshDistr (iDistr, nep, mop) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: nep integer, intent(in) :: mop public subroutine allocIpaDistr (iDistr, na) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: na public subroutine setMeshDistr (iDistr, nsm, nsp, nml, nmpl, ntml, ntpl) Fixes the new data limits and dimensions of the mesh to those of\n the data distribution iDistr . Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index of the input vector integer, intent(in) :: nsm Number of mesh sub-divisions in each direction integer, intent(in) :: nsp Number of sub-points of each mesh point integer, intent(out) :: nml (3) Local number of Mesh divisions in each cell vector integer, intent(out) :: nmpl Local number of Mesh divisions integer, intent(out) :: ntml (3) Local number of Mesh points in each cell vector integer, intent(out) :: ntpl Local number of Mesh points public subroutine resetMeshDistr (iDistr) Reset the data of the distribution iDistr .\n Deallocate associated arrays of the current distribution. Modifies data of the current module. Arguments Type Intent Optional Attributes Name integer, intent(in), optional :: iDistr Distribution index to be reset private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_rea (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr real(kind=grid_p), intent(in) :: fsrc (*) integer, intent(in) :: oDistr real(kind=grid_p), intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine distMeshData_int (iDistr, fsrc, oDistr, fdst, itr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr integer, intent(in) :: fsrc (*) integer, intent(in) :: oDistr integer, intent(out) :: fdst (*) integer, intent(in) :: itr private subroutine boxIntersection (ibox1, ibox2, obox, inters) Checks the three axis of the input boxes to see if there is\n intersection between the input boxes. If it exists, returns\n the resulting box. Arguments Type Intent Optional Attributes Name integer, intent(in) :: ibox1 (2,3) Input box integer, intent(in) :: ibox2 (2,3) Input box integer, intent(out) :: obox (2,3) Intersection between ibox1 and ibox2 logical, intent(out) :: inters TRUE , if there is an intersection. Otherwise FALSE . public subroutine initMeshExtencil (iDistr, nm) Compute the needed communications in order to send/receive the\n extencil (when the data is ordered in the distribution iDistr )\n The results are stored in the variable exteCommu (iDistr,1:3) of the current module. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: nm (3) Number of Mesh divisions in each cell vector public subroutine distExtMeshData (iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, DENS, BDENS) Send/receive the extencil information from the DENS matrix to the\n temporal array BDENS . Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: DENS (maxp,NSPIN) Electron density matrix real(kind=gp), intent(out) :: BDENS (BS,2*NN,NSPIN) Auxiliary arrays to store the extencil from other partitions public subroutine gathExtMeshData (iDistr, iaxis, BS, NSM, NN, NSPIN, maxp, NMeshG, BVXC, VXC) Send/receive the extencil information from the BVXC temporal array\n to the array VXC . Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iDistr Distribution index to be used integer, intent(in) :: iaxis Axe to be splitted integer, intent(in) :: BS Dimension of a plane in the current axe integer, intent(in) :: NSM Number of mesh sub-divisions in each direction integer, intent(in) :: NN Size of the extencil integer, intent(in) :: NSPIN Number of pollarizations integer, intent(in) :: maxp Total number of points integer, intent(in) :: NMeshG (3) Number of Mesh points in each cell vector real(kind=gp), intent(in) :: BVXC (BS,2*NN,NSPIN) Auxiliar array that contains the extencil of the\n exch-corr potential real(kind=gp), intent(out) :: VXC (maxp,NSPIN) Exch-corr potential private subroutine splitwload (Nodes, Node, nm, wload, iDistr, oDistr) Compute the limits of a new distribution, trying to split the load\n of the array wload . We use the nested disection algorithm in\n order to split the mesh in the 3 dimensions. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes Total number of nodes integer, intent(in) :: Node Current process ID (from 1 to Node) integer, intent(in) :: nm (3) Number of mesh sub-divisions in each direction integer, intent(in) :: wload (*) Weights of every point of the mesh. type( meshDisType ), intent(in) :: iDistr Input distribution type( meshDisType ), intent(out) :: oDistr Output distribution private subroutine reduce3Dto1D (iaxis, Ibox, Lbox, wload, lwload) Given a 3-D array, wload , we will make a reduction of its values\n to one of its dimensions ( iaxis ). Ibox gives the limits of the\n input array wload and Lbox gives the limits of the part that we\n want to reduce. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iaxis Axe to be reduced integer, intent(in) :: Ibox (2,3) Limits of the input array integer, intent(in) :: Lbox (2,3) Limits of the intersection that we want to reduce integer, intent(in) :: wload (*) 3-D array that we want to reduce to one of\n its dimensions integer(kind=i8b), intent(out) :: lwload (*) 1-D array. Reduction of the intersected part\n of wload private subroutine vecBisec (nval, values, nparts, pos, h1, h2) Bisection of the load associated to an array. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: nval Dimension of the input array integer(kind=i8b), intent(in) :: values (nval) Input array integer, intent(in) :: nparts Numbers of partitions that we want to make from\n the input array (in this call we only make one cut) integer, intent(out) :: pos Position of the cut integer(kind=i8b), intent(out) :: h1 Load of the first part integer(kind=i8b), intent(out) :: h2 Load of the second part private subroutine reordMeshNumbering (distr1, distr2) Read more… Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution private subroutine reordMeshNumbering (distr1, distr2) Read more… Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 First distribution type( meshDisType ), intent(out) :: distr2 Second distribution private subroutine compMeshComm (distr1, distr2, mcomm) Find the communications needed to transform one array that uses\n distribution distr1 to distribution distr2 Read more… Arguments Type Intent Optional Attributes Name type( meshDisType ), intent(in) :: distr1 Source distribution type( meshDisType ), intent(in) :: distr2 Destination distribution type( meshCommType ), intent(out) :: mcomm Communications needed","tags":"","loc":"module/moremeshsubs.html","title":"moreMeshSubs – SIESTA"},{"text":"Uses precision alloc parallel mesh siestaXC siestaXC siestaXC mesh moreMeshSubs moreMeshSubs moreMeshSubs module~~m_forhar~~UsesGraph module~m_forhar m_forhar alloc alloc module~m_forhar->alloc siestaXC siestaXC module~m_forhar->siestaXC module~moremeshsubs moreMeshSubs module~m_forhar->module~moremeshsubs module~precision precision module~m_forhar->module~precision module~parallel parallel module~m_forhar->module~parallel module~mesh mesh module~m_forhar->module~mesh module~moremeshsubs->alloc module~moremeshsubs->module~precision module~moremeshsubs->module~parallel module~sys sys module~moremeshsubs->module~sys module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_forhar~~UsedByGraph module~m_forhar m_forhar proc~dhscf dhscf proc~dhscf->module~m_forhar Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines forhar Subroutines public subroutine forhar (NTPL, NSPIN, NML, NTML, NPCC, CELL, RHOATM, RHOPCC, VNA, DRHOOUT, VHARRIS1, VHARRIS2) Author J.Junquera Date 09/00 Build the potentials needed for computing Harris forces: Read more… Arguments Type Intent Optional Attributes Name integer :: NTPL Number of Mesh Total Points in unit cell (including subpoints      !) locally. integer, intent(in) :: NSPIN Spin polarizations integer :: NML (3) integer :: NTML (3) integer, intent(in) :: NPCC Partial core corrections? ( 0 =no, 1 =yes) real(kind=dp), intent(in) :: CELL (3,3) Cell vectors real(kind=grid_p), intent(in) :: RHOATM (NTPL) Harris density at mesh points real(kind=grid_p), intent(in) :: RHOPCC (NTPL) Partial-core-correction density for xc real(kind=grid_p), intent(in) :: VNA (NTPL) Sum of neutral atoms potentials real(kind=grid_p), intent(inout) :: DRHOOUT (NTPL,NSPIN) Charge density at the mesh points in current step.\n The charge density that enters in forhar is Drho_{out} - Rho_{atm} . real(kind=grid_p), intent(inout), TARGET :: VHARRIS1 (NTPL,NSPIN) V_{na} + V_{Hartree}(DeltaRho_{in}) + V_{xc}(Rho_{in}) real(kind=grid_p), intent(inout) :: VHARRIS2 (NTPL) V_{na} + V_{Hartree}(Rho_{in}) +\n DV_{xc}(Rho_{in})/DRho_{in} * (Rho_{out}-Rho_{in}) Read more…","tags":"","loc":"module/m_forhar.html","title":"m_forhar – SIESTA"},{"text":"Parallelisation related global parameters Used by module~~parallel~~UsedByGraph module~parallel parallel proc~state_init state_init proc~state_init->module~parallel proc~dhscf dhscf proc~dhscf->module~parallel module~m_iorho m_iorho proc~dhscf->module~m_iorho module~m_forhar m_forhar proc~dhscf->module~m_forhar module~moremeshsubs moreMeshSubs proc~dhscf->module~moremeshsubs proc~mixers_print_block mixers_print_block proc~mixers_print_block->module~parallel proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~parallel proc~delk_wrapper->module~moremeshsubs proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~parallel proc~dhscf_init dhscf_init proc~dhscf_init->module~parallel proc~dhscf_init->module~moremeshsubs proc~mixers_init mixers_init proc~mixers_init->module~parallel proc~bye bye proc~bye->module~parallel module~m_iorho->module~parallel proc~compute_charge_diff compute_charge_diff proc~compute_charge_diff->module~parallel proc~mixing_scf_converged mixing_scf_converged proc~mixing_scf_converged->module~parallel proc~poison poison proc~poison->module~parallel proc~die die proc~die->module~parallel proc~mixing_step mixing_step proc~mixing_step->module~parallel proc~mixing_finalize mixing_finalize proc~mixing_finalize->module~parallel proc~dfscf dfscf proc~dfscf->module~parallel module~m_forhar->module~parallel module~m_forhar->module~moremeshsubs proc~mixers_print mixers_print proc~mixers_print->module~parallel module~moremeshsubs->module~parallel proc~order_rhog order_rhog proc~order_rhog->module~parallel proc~message message proc~message->module~parallel proc~compute_dm compute_dm proc~compute_dm->module~parallel proc~delk delk proc~delk->module~parallel proc~compute_energies compute_energies proc~compute_energies->module~parallel proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~parallel proc~state_analysis state_analysis proc~state_analysis->module~parallel proc~vmat vmat proc~vmat->module~parallel proc~siesta_forces siesta_forces proc~siesta_forces->module~parallel proc~rhoofd rhoofd proc~rhoofd->module~parallel proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->module~parallel proc~mixing_coeff mixing_coeff proc~mixing_coeff->module~parallel Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables SIESTA_group SIESTA_comm SIESTA_worker Node Nodes PEXSINodes BlockSize ProcessorY IOnode Interfaces operator(.PARCOUNT.) Functions parcount Subroutines parallel_init Variables Type Visibility Attributes Name Initial integer, public, save :: SIESTA_group integer, public, save :: SIESTA_comm logical, public, save :: SIESTA_worker = .false. integer, public, save :: Node = 0 integer, public, save :: Nodes = 1 integer, public, save :: PEXSINodes = 1 integer, public, save :: BlockSize = 24 This is the blocking factor used to divide up\n the arrays over the processes for the Scalapack\n routines. Setting this value is a compromise\n between dividing up the orbitals over the processors\n evenly to achieve load balancing and making the\n local work efficient. Typically a value of about\n 10 is good, but optimisation may be worthwhile.\n A value of 1 is very bad for any number of processors\n and a large value may also be less than ideal. integer, public, save :: ProcessorY = 1 Second dimension of processor grid in mesh point\n parallelisation - note that the first dimension\n is determined by the total number of processors\n in the current job. Also note that this number\n must be a factor of the total number of processors.\n Furthermore on many parallel machines (e.g. T3E)\n this number must also be a power of 2. logical, public, save :: IOnode = .true. Interfaces public interface operator(.PARCOUNT.) private elemental function parcount (Nodes, N) Convert a (positive) counter into a counter divisable by\n the number of Nodes. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes integer, intent(in) :: N Return Value integer Functions private elemental function parcount (Nodes, N) Convert a (positive) counter into a counter divisable by\n the number of Nodes. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: Nodes integer, intent(in) :: N Return Value integer Subroutines public subroutine parallel_init () Initializes Node, Nodes, and IOnode Arguments None","tags":"","loc":"module/parallel.html","title":"parallel – SIESTA"},{"text":"Contents Subroutines siesta_forces Subroutines public subroutine siesta_forces (istep) This subroutine represents central SIESTA operation logic. Read more… Arguments Type Intent Optional Attributes Name integer, intent(inout) :: istep","tags":"","loc":"module/m_siesta_forces.html","title":"m_siesta_forces – SIESTA"},{"text":"Uses precision class_dData1D class_Fstack_dData1D module~~m_mixing~~UsesGraph module~m_mixing m_mixing module~precision precision module~m_mixing->module~precision class_Fstack_dData1D class_Fstack_dData1D module~m_mixing->class_Fstack_dData1D class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_mixing~~UsedByGraph module~m_mixing m_mixing proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->module~m_mixing proc~state_init state_init proc~state_init->module~m_mixing module~m_mixing_scf m_mixing_scf proc~state_init->module~m_mixing_scf proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->module~m_mixing proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->module~m_mixing module~m_mixing_scf->module~m_mixing proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->module~m_mixing proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->module~m_mixing proc~siesta_forces siesta_forces proc~siesta_forces->module~m_mixing_scf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables MIX_LINEAR MIX_PULAY MIX_BROYDEN MIX_FIRE ACTION_MIX ACTION_RESTART ACTION_NEXT I_PREVIOUS_RES I_P_RESTART I_P_NEXT I_SVD_COND debug_mix debug_msg Interfaces mixing Derived Types tMixer Functions mix_method mix_method_variant mixing_ncoeff getstackval is_next current_itt stack_check norm Subroutines mixers_init mixer_init mixers_history_init mixers_reset mixers_print mixers_print_block mixing_init mixing_coeff mixing_calc_next mixing_finalize mixing_1d mixing_2d mixing_step inverse svd push_stack_data push_F update_F push_diff Variables Type Visibility Attributes Name Initial integer, public, parameter :: MIX_LINEAR = 1 integer, public, parameter :: MIX_PULAY = 2 integer, public, parameter :: MIX_BROYDEN = 3 integer, public, parameter :: MIX_FIRE = 4 integer, private, parameter :: ACTION_MIX = 0 integer, private, parameter :: ACTION_RESTART = 1 integer, private, parameter :: ACTION_NEXT = 2 integer, private, parameter :: I_PREVIOUS_RES = 0 integer, private, parameter :: I_P_RESTART = -1 integer, private, parameter :: I_P_NEXT = -2 integer, private, parameter :: I_SVD_COND = -3 logical, private :: debug_mix = .false. character(len=20), private :: debug_msg = 'mix:' Interfaces public interface mixing private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub Derived Types type, public :: tMixer Components Type Visibility Attributes Name Initial character(len=24), public :: name type(Fstack_dData1D), public, allocatable :: stack (:) integer, public :: m = MIX_PULAY integer, public :: v = 0 integer, public :: cur_itt = 0 integer, public :: start_itt = 0 integer, public :: n_hist = 2 integer, public :: n_itt = 0 integer, public :: restart = 0 integer, public :: restart_save = 0 integer, public :: action = ACTION_MIX type( tMixer ), public, pointer :: next => null() type( tMixer ), public, pointer :: next_conv => null() real(kind=dp), public :: w = 0._dp real(kind=dp), public, pointer :: rv (:) => null() integer, public, pointer :: iv (:) => null() Functions public function mix_method (str) result(m) Return the integer specification of the mixing type Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: str Return Value integer public function mix_method_variant (m, str) result(v) Return the variant of the mixing method Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: m character(len=*), intent(in) :: str Return Value integer private function mixing_ncoeff (mix) result(n) Function to retrieve the number of coefficients\n calculated in this iteration.\n This is so external routines can query the size\n of the arrays used. Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer private function getstackval (mix, sidx, hidx) result(d1) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix integer, intent(in) :: sidx integer, intent(in), optional :: hidx Return Value real(kind=dp),\n  pointer, (:) private function is_next (mix, method, next) result(bool) Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in), target :: mix integer, intent(in) :: method type( tMixer ), optional pointer :: next Return Value logical private function current_itt (mix) result(itt) Get current iteration count Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(in) :: mix Return Value integer private function stack_check (stack, n) result(check) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: stack integer, intent(in) :: n Return Value logical private function norm (n, x1, x2) Calculate the norm of two arrays Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: x1 (n) real(kind=dp), intent(in) :: x2 (n) Return Value real(kind=dp) Subroutines public subroutine mixers_init (prefix, mixers, Comm) Initialize a set of mixers by reading in fdf information.\n @param[in] prefix the fdf-label prefixes\n @param[pointer] mixers the mixers that are to be initialized\n @param[in] Comm @opt optional MPI-communicator Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), pointer :: mixers (:) integer, intent(in), optional :: Comm public subroutine mixer_init (mix) Initialize a single mixer depending on the preset\n options. Useful for external correct setup. Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix public subroutine mixers_history_init (mixers) Initialize all history for the mixers Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout), target :: mixers (:) public subroutine mixers_reset (mixers) Reset the mixers, i.e. clean everything Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mixers (:) public subroutine mixers_print (prefix, mixers) Print (to std-out) information regarding the mixers Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) public subroutine mixers_print_block (prefix, mixers) Print (to std-out) the fdf-blocks that recreate the mixer settings Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: prefix type( tMixer ), intent(in), target :: mixers (:) private subroutine mixing_init (mix, n, xin, F) Initialize the mixing algorithm Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) private subroutine mixing_coeff (mix, n, xin, F, coeff) Calculate the mixing coefficients for the\n current mixer Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), intent(inout) :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: coeff (:) private subroutine mixing_calc_next (mix, n, xin, F, xnext, coeff) Calculate the guess for the next iteration Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(out) :: xnext (n) real(kind=dp), intent(in) :: coeff (:) private subroutine mixing_finalize (mix, n, xin, F, xnext) Finalize the mixing algorithm Read more… Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in) :: xnext (n) private subroutine mixing_1d (mix, n, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n real(kind=dp), intent(in) :: xin (n) real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(inout) :: xnext (n) integer, intent(in), optional :: nsub private subroutine mixing_2d (mix, n1, n2, xin, F, xnext, nsub) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix integer, intent(in) :: n1 integer, intent(in) :: n2 real(kind=dp), intent(in) :: xin (n1,n2) real(kind=dp), intent(in) :: F (n1,n2) real(kind=dp), intent(inout) :: xnext (n1,n2) integer, intent(in), optional :: nsub private subroutine mixing_step (mix) Arguments Type Intent Optional Attributes Name type( tMixer ), pointer :: mix private subroutine inverse (n, A, B, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) integer, intent(out) :: info private subroutine svd (n, A, B, cond, info) Arguments Type Intent Optional Attributes Name integer, intent(in) :: n real(kind=dp), intent(in) :: A (n,n) real(kind=dp), intent(out) :: B (n,n) real(kind=dp), intent(in) :: cond integer, intent(out) :: info private subroutine push_stack_data (s_F, n) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n private subroutine push_F (s_F, n, F, fact) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) real(kind=dp), intent(in), optional :: fact private subroutine update_F (s_F, n, F) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_F integer, intent(in) :: n real(kind=dp), intent(in) :: F (n) private subroutine push_diff (s_rres, s_res, alpha) Arguments Type Intent Optional Attributes Name type(Fstack_dData1D), intent(inout) :: s_rres type(Fstack_dData1D), intent(in) :: s_res real(kind=dp), intent(in), optional :: alpha","tags":"","loc":"module/m_mixing.html","title":"m_mixing – SIESTA"},{"text":"Uses precision module~~m_compute_max_diff~~UsesGraph module~m_compute_max_diff m_compute_max_diff module~precision precision module~m_compute_max_diff->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_compute_max_diff~~UsedByGraph module~m_compute_max_diff m_compute_max_diff proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_max_diff Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables dDmax_current Interfaces compute_max_diff Subroutines compute_max_diff_2d compute_max_diff_1d Variables Type Visibility Attributes Name Initial real(kind=dp), public, save :: dDmax_current Temporary for storing the old maximum change Interfaces public interface compute_max_diff public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff Subroutines public subroutine compute_max_diff_2d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:,:) real(kind=dp), intent(in) :: X2 (:,:) real(kind=dp), intent(out) :: max_diff public subroutine compute_max_diff_1d (X1, X2, max_diff) Arguments Type Intent Optional Attributes Name real(kind=dp), intent(in) :: X1 (:) real(kind=dp), intent(in) :: X2 (:) real(kind=dp), intent(out) :: max_diff","tags":"","loc":"module/m_compute_max_diff.html","title":"m_compute_max_diff – SIESTA"},{"text":"Used by module~~m_setup_hamiltonian~~UsedByGraph module~m_setup_hamiltonian m_setup_hamiltonian proc~siesta_forces siesta_forces proc~siesta_forces->module~m_setup_hamiltonian Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines setup_hamiltonian Subroutines public subroutine setup_hamiltonian (iscf) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_setup_hamiltonian.html","title":"m_setup_hamiltonian – SIESTA"},{"text":"Used by module~~m_compute_dm~~UsedByGraph module~m_compute_dm m_compute_dm proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_dm Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables PreviousCallDiagon Subroutines compute_dm Variables Type Visibility Attributes Name Initial logical, public, save :: PreviousCallDiagon = .false. Subroutines public subroutine compute_dm (iscf) Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_compute_dm.html","title":"m_compute_dm – SIESTA"},{"text":"Uses class_Fstack_dData1D m_mixing module~~m_mixing_scf~~UsesGraph module~m_mixing_scf m_mixing_scf class_Fstack_dData1D class_Fstack_dData1D module~m_mixing_scf->class_Fstack_dData1D module~m_mixing m_mixing module~m_mixing_scf->module~m_mixing module~m_mixing->class_Fstack_dData1D module~precision precision module~m_mixing->module~precision class_dData1D class_dData1D module~m_mixing->class_dData1D Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_mixing_scf~~UsedByGraph module~m_mixing_scf m_mixing_scf proc~state_init state_init proc~state_init->module~m_mixing_scf proc~siesta_forces siesta_forces proc~siesta_forces->module~m_mixing_scf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables scf_mixs scf_mix MIX_SPIN_ALL MIX_SPIN_SPINOR MIX_SPIN_SUM MIX_SPIN_SUM_DIFF mix_spin Subroutines mixers_scf_init mixers_scf_print mixers_scf_print_block mixing_scf_converged mixers_scf_reset mixers_scf_history_init Variables Type Visibility Attributes Name Initial type( tMixer ), public, pointer :: scf_mixs (:) => null() type( tMixer ), public, pointer :: scf_mix => null() integer, public, parameter :: MIX_SPIN_ALL = 1 integer, public, parameter :: MIX_SPIN_SPINOR = 2 integer, public, parameter :: MIX_SPIN_SUM = 3 integer, public, parameter :: MIX_SPIN_SUM_DIFF = 4 integer, public :: mix_spin = MIX_SPIN_ALL Subroutines public subroutine mixers_scf_init (nspin, Comm) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in), optional :: Comm public subroutine mixers_scf_print (nspin) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin public subroutine mixers_scf_print_block () Arguments None public subroutine mixing_scf_converged (SCFconverged) Arguments Type Intent Optional Attributes Name logical, intent(inout) :: SCFconverged public subroutine mixers_scf_reset () Arguments None public subroutine mixers_scf_history_init () Arguments None","tags":"","loc":"module/m_mixing_scf.html","title":"m_mixing_scf – SIESTA"},{"text":"Used by module~~m_vmat~~UsedByGraph module~m_vmat m_vmat proc~dhscf dhscf proc~dhscf->module~m_vmat Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines vmat Subroutines public subroutine vmat (no, np, dvol, spin, V, nvmax, numVs, listVsptr, listVs, Vs, nuo, nuotot, iaorb, iphorb, isa) Author P.Ordejon Finds the matrix elements of the potential. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of columns in C (local) real(kind=dp), intent(in) :: dvol Volume per mesh point type(tSpin), intent(in) :: spin Spin configuration real(kind=grid_p), intent(in) :: V (nsp,np,spin%Grid) Value of the potential at the mesh points integer, intent(in) :: nvmax First dimension of listV and Vs , and maximum\n number of nonzero elements in any row of Vs integer, intent(in) :: numVs (nuo) Number of non-zero elements in a row of Vs integer, intent(in) :: listVsptr (nuo) Pointer to the start of rows in listVs integer, intent(in) :: listVs (nvmax) List of non-zero elements of Vs real(kind=dp), target :: Vs (nvmax,spin%H) Value of nonzero elements in each row\n of Vs to which the potential matrix\n elements are summed up integer, intent(in) :: nuo integer, intent(in) :: nuotot integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms","tags":"","loc":"module/m_vmat.html","title":"m_vmat – SIESTA"},{"text":"Precision handling Select precision of certain parts of the program\n These are set through preprocessor directives. The\n default behavior is to use single-precision variables\n for the values of the orbitals on the grid, the Broyden\n mixing auxiliary arrays, and the O(N) arrays, and\n double precision for the grid function arrays. Used by module~~precision~~UsedByGraph module~precision precision module~units units module~units->module~precision module~m_planewavematrixvar m_planewavematrixvar module~m_planewavematrixvar->module~precision module~radial radial module~radial->module~precision proc~dhscf_init dhscf_init proc~dhscf_init->module~precision module~mesh mesh proc~dhscf_init->module~mesh module~moremeshsubs moreMeshSubs proc~dhscf_init->module~moremeshsubs module~atmfuncs atmfuncs proc~dhscf_init->module~atmfuncs module~m_rhog m_rhog proc~dhscf_init->module~m_rhog module~m_compute_max_diff m_compute_max_diff module~m_compute_max_diff->module~precision module~mesh->module~precision proc~compute_energies compute_energies proc~compute_energies->module~precision module~m_dhscf m_dhscf proc~compute_energies->module~m_dhscf proc~compute_energies->module~m_rhog proc~compute_charge_diff compute_charge_diff proc~compute_charge_diff->module~precision proc~poison poison proc~poison->module~precision module~atm_types atm_types module~atm_types->module~precision module~atm_types->module~radial proc~rhooda rhooda proc~rhooda->module~precision proc~rhooda->module~mesh proc~rhooda->module~atmfuncs proc~dfscf dfscf proc~dfscf->module~precision proc~dfscf->module~mesh proc~dfscf->module~atm_types proc~dfscf->module~atmfuncs module~m_forhar m_forhar module~m_forhar->module~precision module~m_forhar->module~mesh module~m_forhar->module~moremeshsubs proc~reord reord proc~reord->module~precision module~moremeshsubs->module~precision module~m_dhscf->module~precision proc~mixers_scf_init mixers_scf_init proc~mixers_scf_init->module~precision module~m_mixing m_mixing proc~mixers_scf_init->module~m_mixing proc~compute_dm compute_dm proc~compute_dm->module~precision proc~compute_dm->module~units module~m_mixing->module~precision proc~delk delk proc~delk->module~precision proc~delk->module~m_planewavematrixvar proc~delk->module~mesh proc~delk->module~atm_types proc~delk->module~atmfuncs module~m_iorho m_iorho module~m_iorho->module~precision proc~dhscf dhscf proc~dhscf->module~precision proc~dhscf->module~units proc~dhscf->module~mesh proc~dhscf->module~m_forhar proc~dhscf->module~moremeshsubs proc~dhscf->module~m_iorho proc~dhscf->module~atmfuncs proc~dhscf->module~m_rhog module~atmfuncs->module~precision module~atmfuncs->module~radial module~atmfuncs->module~atm_types proc~mix_rhog mix_rhog proc~mix_rhog->module~precision proc~vmat vmat proc~vmat->module~precision proc~vmat->module~mesh proc~vmat->module~atm_types proc~vmat->module~atmfuncs module~m_rhog->module~precision proc~siesta_forces siesta_forces proc~siesta_forces->module~precision proc~siesta_forces->module~units proc~siesta_forces->module~m_compute_max_diff proc~siesta_forces->module~m_rhog module~m_mixing_scf m_mixing_scf proc~siesta_forces->module~m_mixing_scf proc~rhoofd rhoofd proc~rhoofd->module~precision proc~rhoofd->module~mesh proc~rhoofd->module~atm_types proc~rhoofd->module~atmfuncs proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~units proc~siesta_analysis->module~m_dhscf proc~state_init state_init proc~state_init->module~units proc~state_init->module~m_mixing proc~state_init->module~m_mixing_scf proc~allocextmeshdistr allocExtMeshDistr proc~allocextmeshdistr->module~mesh proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->module~mesh proc~setmeshdistr setMeshDistr proc~setmeshdistr->module~mesh proc~mixers_scf_history_init mixers_scf_history_init proc~mixers_scf_history_init->module~m_mixing proc~allocasynbuffer allocASynBuffer proc~allocasynbuffer->module~mesh proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->module~mesh proc~mixers_scf_print_block mixers_scf_print_block proc~mixers_scf_print_block->module~m_mixing proc~mixers_scf_reset mixers_scf_reset proc~mixers_scf_reset->module~m_mixing proc~allocipadistr allocIpaDistr proc~allocipadistr->module~mesh module~m_mixing_scf->module~m_mixing proc~setup_h0 setup_H0 proc~setup_h0->module~m_dhscf proc~setup_h0->module~atmfuncs proc~mixers_scf_print mixers_scf_print proc~mixers_scf_print->module~m_mixing proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~m_dhscf proc~setup_hamiltonian->module~atmfuncs proc~setup_hamiltonian->module~m_rhog proc~state_analysis state_analysis proc~state_analysis->module~units proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~mesh proc~delk_wrapper->module~moremeshsubs var panmoduleprecisionUsedByGraph = svgPanZoom('#moduleprecisionUsedByGraph', {zoomEnabled: true,controlIconsEnabled: true, fit: true, center: true,}); Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables i8b sp dp broyden_p broyden_p grid_p phi_grid_p grid_p phi_grid_p grid_p phi_grid_p on_p on_p ts_p Variables Type Visibility Attributes Name Initial integer, public, parameter :: i8b = selected_int_kind(18) integer, public, parameter :: sp = selected_real_kind(6, 30) integer, public, parameter :: dp = selected_real_kind(14, 100) integer, public, parameter :: broyden_p = dp integer, public, parameter :: broyden_p = sp integer, public, parameter :: grid_p = sp integer, public, parameter :: phi_grid_p = sp integer, public, parameter :: grid_p = dp integer, public, parameter :: phi_grid_p = dp integer, public, parameter :: grid_p = dp integer, public, parameter :: phi_grid_p = sp integer, public, parameter :: on_p = dp integer, public, parameter :: on_p = sp integer, public, parameter :: ts_p = dp","tags":"","loc":"module/precision.html","title":"precision – SIESTA"},{"text":"Define various unit conversion factors from internal units. Internally, siesta works with: length: Bohr energy: Rydberg. time: femtosecond The easy way to make sense of units conversion: real(dp), parameter :: Bohr   = 1.0_dp real(dp), parameter :: Rydberg = 1.0_dp real(dp), parameter :: Femtosecond = 1.0_dp Ang = Bohr / 0.529177 eV = Rydberg / 13.60580 Joule = eV / 1.6e-19_dp Meter = Ang / 1.0e-10_dp Pascal = Joule/Meter**2 kBar  = Pascal * 1.0e4 Ryd&#94;-1 (time) = fs/0.04837769 .... and so on. Uses precision module~~units~~UsesGraph module~units units module~precision precision module~units->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~units~~UsedByGraph module~units units proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~units proc~state_init state_init proc~state_init->module~units proc~dhscf dhscf proc~dhscf->module~units proc~siesta_forces siesta_forces proc~siesta_forces->module~units proc~compute_dm compute_dm proc~compute_dm->module~units proc~state_analysis state_analysis proc~state_analysis->module~units Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables Ang eV kBar GPa Kelvin Debye amu Ryd_time pi deg Variables Type Visibility Attributes Name Initial real(kind=dp), public, parameter :: Ang = 1._dp/0.529177_dp real(kind=dp), public, parameter :: eV = 1._dp/13.60580_dp real(kind=dp), public, parameter :: kBar = 1._dp/1.47108e5_dp real(kind=dp), public, parameter :: GPa = kBar*10 real(kind=dp), public, parameter :: Kelvin = eV/11604.45_dp real(kind=dp), public, parameter :: Debye = 0.393430_dp real(kind=dp), public, parameter :: amu = 2.133107_dp real(kind=dp), public, parameter :: Ryd_time = 1._dp/0.04837769_dp real(kind=dp), public, parameter :: pi = 3.14159265358979323846264338327950288419716939937510_dp \\pi to 50 digits real(kind=dp), public, parameter :: deg = pi/180.0_dp","tags":"","loc":"module/units.html","title":"units – SIESTA"},{"text":"To have both reading and writing, allowing a change in the\n values of variables used as array dimensions, is not very\n safe. Thus, the old iorho has been split in three routines: write_rho : Writes grid magnitudes to file read_rho : Reads grid magnitudes from file check_rho : Checks the appropriate dimensions for reading. The magnitudes are saved in SINGLE PRECISION (sp), regardless\n of the internal precision used in the program. Historically,\n the internal precision has been \"single\". Uses precision parallel sys alloc mpi_siesta parallel parallelsubs module~~m_iorho~~UsesGraph module~m_iorho m_iorho module~precision precision module~m_iorho->module~precision parallelsubs parallelsubs module~m_iorho->parallelsubs alloc alloc module~m_iorho->alloc module~parallel parallel module~m_iorho->module~parallel mpi_siesta mpi_siesta module~m_iorho->mpi_siesta module~sys sys module~m_iorho->module~sys Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_iorho~~UsedByGraph module~m_iorho m_iorho proc~dhscf dhscf proc~dhscf->module~m_iorho Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables fform Subroutines write_rho read_rho check_rho Variables Type Visibility Attributes Name Initial character(len=*), public, parameter :: fform = \"unformatted\" Subroutines public subroutine write_rho (fname, cell, mesh, nsm, maxp, nspin, rho) Author J.Soler Date July 1997 Writes the electron density or potential at the mesh points. Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname File name real(kind=dp), intent(in) :: cell (3,3) integer, intent(in) :: mesh (3) Number of mesh divisions of each lattice vector integer, intent(in) :: nsm Number of sub-mesh points per mesh point (not used in this version) integer, intent(in) :: maxp integer, intent(in) :: nspin real(kind=grid_p), intent(in) :: rho (maxp,nspin) public subroutine read_rho (fname, cell, mesh, nsm, maxp, nspin, rho) Author J.Soler Date July 1997 Reads the electron density or potential at the mesh points. Read more… Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname File name for input or output real(kind=dp), intent(out) :: cell (3,3) Lattice vectors integer, intent(out) :: mesh (3) Number of mesh divisions of each lattice vector integer, intent(in) :: nsm Number of sub-mesh points per mesh point (not used in this version) integer, intent(in) :: maxp First dimension of array rho integer, intent(in) :: nspin Second dimension of array rho real(kind=grid_p), intent(out) :: rho (maxp,nspin) Electron density public subroutine check_rho (fname, maxp, nspin, nsm, found, overflow) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: fname integer, intent(inout) :: maxp Required first dimension of array rho, equal to mesh(1)*mesh(2)*mesh(3) integer, intent(inout) :: nspin Number of spin polarizations (1 or 2) integer, intent(in) :: nsm Number of sub-mesh points per mesh point (not used in this version) logical, intent(out) :: found Were data found? logical, intent(out) :: overflow True if maxp or nspin were changed","tags":"","loc":"module/m_iorho.html","title":"m_iorho – SIESTA"},{"text":"Uses write_subs module~~m_siesta_analysis~~UsesGraph module~m_siesta_analysis m_siesta_analysis write_subs write_subs module~m_siesta_analysis->write_subs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines siesta_analysis Subroutines public subroutine siesta_analysis (relaxd) Check that we are converged in geometry,\n if strictly required,\n before carrying out any analysis. Read more… Arguments Type Intent Optional Attributes Name logical :: relaxd","tags":"","loc":"module/m_siesta_analysis.html","title":"m_siesta_analysis – SIESTA"},{"text":"In this module we define all the variables required\n to compute the matrix elements of a plane wave in a basis of\n numerical atomic orbitals. Uses precision module~~m_planewavematrixvar~~UsesGraph module~m_planewavematrixvar m_planewavematrixvar module~precision precision module~m_planewavematrixvar->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_planewavematrixvar~~UsedByGraph module~m_planewavematrixvar m_planewavematrixvar proc~delk delk proc~delk->module~m_planewavematrixvar Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables delkmatgen delkmat wavevector Variables Type Visibility Attributes Name Initial complex(kind=dp), public, pointer, save :: delkmatgen (:,:) The matrix elements of a planewave are not\n self-consistent. They can be computed\n once for a given k-point and stored in\n delkmatgen. The first index refers to the number of\n different k-points for which the planewave\n will be computed. The second index is the index of the\n sparse matrix. This pointer has to be allocated in\n the calling routine. complex(kind=dp), public, pointer, save :: delkmat (:) Matrix elements of a plane wave <\\phi_{\\mu}|e&#94;{( \\boldsymbol{isigneikr} \\: \\cdot \\:\n i \\: \\cdot \\: \\vec{kptpw} \\: \\cdot \\: \\vec{r} )}|\\phi_{\\nu}> This pointer has to be allocated in the calling routine. This is a sparse matrix, whose structure\n follows the same scheme as the\n hamiltonian or overlap matrix elements. real(kind=dp), public :: wavevector (3) Wave vector of the plane wave.","tags":"","loc":"module/m_planewavematrixvar.html","title":"m_planewavematrixvar – SIESTA"},{"text":"Used by module~~m_compute_energies~~UsedByGraph module~m_compute_energies m_compute_energies proc~siesta_forces siesta_forces proc~siesta_forces->module~m_compute_energies Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines compute_energies Subroutines public subroutine compute_energies (iscf) This routine computes the Harris energy from E_KS(DM_in): Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iscf","tags":"","loc":"module/m_compute_energies.html","title":"m_compute_energies – SIESTA"},{"text":"Hard-wired parameters (to be eliminated in the future) Contents Variables nzetmx nkbmx nsmx nsemx ntbmax lmaxd lmx2 nrmax maxos Variables Type Visibility Attributes Name Initial integer, public, parameter :: nzetmx = 200 Maximum number of PAOs or polarization orbitals\n with the same angular  momentum and\n for the same species. integer, public, parameter :: nkbmx = 4 Maximum number of Kleinman-Bylander projectors\n for each angular momentum For the off-site SO calculation plus semicore states\n there will be at least 4 KBs for each l angular momentum\n (for each l shell we have J = l +/- 1/2 ) integer, public, parameter :: nsmx = 2 Maximum number of semicore shells for each angular\n momentum present in the atom ( for normal atom nsmx=0 ) integer, public, parameter :: nsemx = 1+nsmx integer, public, parameter :: ntbmax = 500 Maximum number of points in the tables defining\n orbitals, projectors and local neutral-atom\n pseudopotential. integer, public, parameter :: lmaxd = 4 Maximum angular momentum for both orbitals and projectors. integer, public, parameter :: lmx2 = (lmaxd+1)*(lmaxd+1) integer, public, parameter :: nrmax = 20000 Maximum number of points in the functions read\n from file '.vps' or '.psf' (this number is\n determined by the parameter nrmax in the\n program ATOM, which generates the files with\n the pseudopotential information). The number\n of points in the grid can be redefined if the\n pseudopotential is reparametrized. nrmax = 20000 is a typical safe value in this case. integer, public, parameter :: maxos = 2*nzetmx*lmx2*nsemx","tags":"","loc":"module/atmparams.html","title":"atmparams – SIESTA"},{"text":"Termination and messaging routines, MPI aware Used by module~~sys~~UsedByGraph module~sys sys proc~dhscf_init dhscf_init proc~dhscf_init->module~sys module~atmfuncs atmfuncs proc~dhscf_init->module~atmfuncs module~moremeshsubs moreMeshSubs proc~dhscf_init->module~moremeshsubs proc~state_init state_init proc~state_init->module~sys module~atmfuncs->module~sys proc~dfscf dfscf proc~dfscf->module~sys proc~dfscf->module~atmfuncs module~moremeshsubs->module~sys proc~dhscf dhscf proc~dhscf->module~sys proc~dhscf->module~atmfuncs proc~dhscf->module~moremeshsubs module~m_iorho m_iorho proc~dhscf->module~m_iorho module~m_forhar m_forhar proc~dhscf->module~m_forhar proc~siesta_forces siesta_forces proc~siesta_forces->module~sys proc~poison poison proc~poison->module~sys proc~compute_dm compute_dm proc~compute_dm->module~sys proc~rhoofd rhoofd proc~rhoofd->module~sys proc~rhoofd->module~atmfuncs proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~sys proc~setup_hamiltonian->module~atmfuncs proc~check_cohp check_cohp proc~check_cohp->module~sys module~m_iorho->module~sys proc~setup_h0 setup_H0 proc~setup_h0->module~atmfuncs module~m_forhar->module~moremeshsubs proc~vmat vmat proc~vmat->module~atmfuncs proc~rhooda rhooda proc~rhooda->module~atmfuncs proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~moremeshsubs proc~delk delk proc~delk->module~atmfuncs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines message die bye Subroutines public subroutine message (level, str) Prints a message string if node==0 Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: level One of INFO, WARNING, FATAL character(len=*), intent(in) :: str public subroutine die (str) Prints an error message and calls MPI_Abort Arguments Type Intent Optional Attributes Name character(len=*), intent(in), optional :: str public subroutine bye (str) Prints an error message and calls MPI_Finalize Arguments Type Intent Optional Attributes Name character(len=*), intent(in), optional :: str","tags":"","loc":"module/sys.html","title":"sys – SIESTA"},{"text":"Uses precision xml interpolation interpolation module~~radial~~UsesGraph module~radial radial xml xml module~radial->xml module~precision precision module~radial->module~precision interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~radial~~UsedByGraph module~radial radial module~atm_types atm_types module~atm_types->module~radial module~atmfuncs atmfuncs module~atmfuncs->module~radial module~atmfuncs->module~atm_types proc~dhscf_init dhscf_init proc~dhscf_init->module~atmfuncs proc~setup_h0 setup_H0 proc~setup_h0->module~atmfuncs proc~dfscf dfscf proc~dfscf->module~atm_types proc~dfscf->module~atmfuncs proc~vmat vmat proc~vmat->module~atm_types proc~vmat->module~atmfuncs proc~dhscf dhscf proc~dhscf->module~atmfuncs proc~rhooda rhooda proc~rhooda->module~atmfuncs proc~rhoofd rhoofd proc~rhoofd->module~atm_types proc~rhoofd->module~atmfuncs proc~delk delk proc~delk->module~atm_types proc~delk->module~atmfuncs proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~atmfuncs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Derived Types rad_func Functions rad_rvals Subroutines reset_rad_func rad_alloc rad_get rad_setup_d2 rad_zero radial_read_ascii radial_dump_ascii radial_dump_xml Derived Types type, public :: rad_func Components Type Visibility Attributes Name Initial integer, public :: n real(kind=dp), public :: cutoff real(kind=dp), public :: delta real(kind=dp), public, pointer :: f (:) => null() Actual data real(kind=dp), public, pointer :: d2 (:) => null() 2nd derivative Functions private function rad_rvals (func) result(r) Arguments Type Intent Optional Attributes Name type( rad_func ), intent(in) :: func Return Value real(kind=dp),\n  dimension(:), pointer Subroutines public subroutine reset_rad_func (func) Arguments Type Intent Optional Attributes Name type( rad_func ) :: func public subroutine rad_alloc (func, n) Sets the 'size' n of the arrays and allocates f and d2. Arguments Type Intent Optional Attributes Name type( rad_func ), intent(inout) :: func integer, intent(in) :: n public subroutine rad_get (func, r, fr, dfdr) Arguments Type Intent Optional Attributes Name type( rad_func ), intent(in) :: func real(kind=dp), intent(in) :: r real(kind=dp), intent(out) :: fr real(kind=dp), intent(out) :: dfdr public subroutine rad_setup_d2 (func, yp1, ypn) Set up second derivative in a radial function Arguments Type Intent Optional Attributes Name type( rad_func ), intent(inout) :: func real(kind=dp), intent(in) :: yp1 real(kind=dp), intent(in) :: ypn public subroutine rad_zero (func) Arguments Type Intent Optional Attributes Name type( rad_func ), intent(inout) :: func public subroutine radial_read_ascii (op, lun, yp1, ypn) Arguments Type Intent Optional Attributes Name type( rad_func ) :: op integer :: lun real(kind=dp), intent(in) :: yp1 real(kind=dp), intent(in) :: ypn public subroutine radial_dump_ascii (op, lun, header) Arguments Type Intent Optional Attributes Name type( rad_func ) :: op integer :: lun logical, intent(in), optional :: header public subroutine radial_dump_xml (op, lun) Arguments Type Intent Optional Attributes Name type( rad_func ) :: op integer :: lun","tags":"","loc":"module/radial.html","title":"radial – SIESTA"},{"text":"Used by module~~siesta_options~~UsedByGraph module~siesta_options siesta_options proc~dhscf_init dhscf_init proc~dhscf_init->module~siesta_options proc~state_init state_init proc~state_init->module~siesta_options proc~setup_h0 setup_H0 proc~setup_h0->module~siesta_options proc~mix_rhog mix_rhog proc~mix_rhog->module~siesta_options proc~compute_energies compute_energies proc~compute_energies->module~siesta_options proc~dhscf dhscf proc~dhscf->module~siesta_options proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~siesta_options proc~siesta_forces siesta_forces proc~siesta_forces->module~siesta_options proc~compute_dm compute_dm proc~compute_dm->module~siesta_options proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~siesta_options proc~check_cohp check_cohp proc~check_cohp->module~siesta_options proc~state_analysis state_analysis proc~state_analysis->module~siesta_options Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables dp compat_pre_v4_DM_H mix_after_convergence recompute_H_after_scf compat_pre_v4_dynamics mix_scf_first mix_charge mixH h_setup_only chebef dumpcharge fire_mix fixspin init_anti_ferro initdmaux allow_dm_reuse allow_dm_extrapolation change_kgrid_in_md negl noeta new_diagk outlng pulfile RelaxCellOnly RemoveIntraMolecularPressure savehs savevh savevna savevt savedrho saverho saverhoxc save_ebs_dens savepsch savetoch savebader usesaveddata usesavecg usesavelwf usesavedm usesavedmloc usesavexv usesavezm writeig writbk writmd writpx writb writec write_coop write_GRAPHVIZ w90_processing w90_write_mmn w90_write_amn w90_write_eig w90_write_unk hasnobup hasnobdown hasnob nobup nobdown nob writef writek writic varcel do_pdos write_tshs_history write_hs_history writedm write_dm_at_end_of_cycle writeH write_H_at_end_of_cycle writedm_cdf writedm_cdf_history writedmhs_cdf writedmhs_cdf_history read_charge_cdf read_deformation_charge_cdf save_initial_charge_density analyze_charge_density_only atmonly harrisfun muldeb spndeb orbmoms split_sr_so converge_FreeE tolerance_FreeE converge_Eharr tolerance_Eharr converge_EDM tolerance_EDM converge_DM dDtol converge_H dHtol broyden_optim fire_optim struct_only use_struct_file bornz SCFMustConverge GeometryMustConverge want_domain_decomposition want_spatial_decomposition hirshpop voropop partial_charges_at_every_geometry partial_charges_at_every_scf_step monitor_forces_in_scf minim_calc_eigenvalues writetdwf extrapol_H_tdks td_elec_dyn etot_time eigen_time dip_time tdsavewf tdsaverho td_inverse_linear ntdsaverho itded ntded ntded_sub td_dt rstart_time totime ia1 ia2 ianneal idyn ifinal ioptlwf iquench isolve istart DM_history_depth maxsav broyden_maxit mullipop ncgmax nkick nmove nscf min_nscf pmax neigwanted level call_diagon_default call_diagon_first_step beta bulkm charnet rijmin dm_normalization_tol normalize_dm_during_scf dt dx dxmax eta etol ftol g2cut mn mpr occtol rcoor rcoorcp rmax_bonds strtol taurelax temp tempinit threshold tp total_spin tt wmix wmixkick sname SOLVE_DIAGON SOLVE_ORDERN SOLVE_TRANSI SOLVE_MINIM SOLVE_PEXSI MATRIX_WRITE SOLVE_CHESS Variables Type Visibility Attributes Name Initial integer, private, parameter :: dp = selected_real_kind(10, 100) logical, public :: compat_pre_v4_DM_H General switch logical, public :: mix_after_convergence Mix DM or H even after convergence logical, public :: recompute_H_after_scf Update H while computing forces logical, public :: compat_pre_v4_dynamics logical, public :: mix_scf_first Mix first SCF step? logical, public :: mix_charge New: mix fourier components of rho logical, public :: mixH Mix H instead of DM logical, public :: h_setup_only H Setup only logical, public :: chebef Compute the chemical potential in ordern? logical, public :: dumpcharge Write electron density? logical, public :: fire_mix SCF mixing with FIRE method logical, public :: fixspin Keep the total spin fixed? logical, public :: init_anti_ferro Antiferro spin ordering in initdm? logical, public :: initdmaux Re-initialize DM when auxiliary supercell changes? logical, public :: allow_dm_reuse Allow re-use of the previous geometry DM ? (with possible extrapolation) logical, public :: allow_dm_extrapolation Allow the extrapolation of previous geometries' DM ? logical, public :: change_kgrid_in_md Allow k-point grid to change in MD calculations logical, public :: negl Neglect hamiltonian matrix elements without overlap? logical, public :: noeta Use computed chemical potential instead of eta in ordern? logical, public :: new_diagk Use new diagk routine with file storage of eigenvectors? logical, public :: outlng Long output in the output file? logical, public :: pulfile Use file to store Pulay info in pulayx? (Obsolete) logical, public :: RelaxCellOnly Relax only lattice vectors, not atomic coordinates logical, public :: RemoveIntraMolecularPressure Remove molecular virial contribution to p logical, public :: savehs Write file with Hamiltonian electrostatic potential? logical, public :: savevh Write file with Hartree electrostatic potential? logical, public :: savevna Write file with neutral-atom potential? logical, public :: savevt Write file with total effective potential? logical, public :: savedrho Write file with diff. between SCF and atomic density? logical, public :: saverho Write file with electron density? logical, public :: saverhoxc Write file with electron density including nonlinear core correction? logical, public :: save_ebs_dens Write file with band structure energy density? logical, public :: savepsch Write file with ionic (local pseudopotential) charge? logical, public :: savetoch Write file with total charge? logical, public :: savebader Write file with charge for Bader analysis? logical, public :: usesaveddata Default for UseSavedData flag logical, public :: usesavecg Use continuation file for CG geometry relaxation? logical, public :: usesavelwf Use continuation file for Wannier functions? logical, public :: usesavedm Use cont. file for density matrix? logical, public :: usesavedmloc Temporary to keep usesavedm value logical, public :: usesavexv Use cont. file for atomic positions and velocities? logical, public :: usesavezm Use cont. file for Z-matrix? logical, public :: writeig Write eigenvalues? logical, public :: writbk Write k vectors of bands? logical, public :: writmd logical, public :: writpx Write atomic coordinates at every geometry step? logical, public :: writb Write band eigenvalues? logical, public :: writec Write atomic coordinates at every geometry step? logical, public :: write_coop Write information for COOP/COHP analysis ? integer, public :: write_GRAPHVIZ logical, public :: w90_processing Will we call the interface with Wannier90 logical, public :: w90_write_mmn Write the Mmn matrix for the interface with Wannier logical, public :: w90_write_amn Write the Amn matrix for the interface with Wannier logical, public :: w90_write_eig Write the eigenvalues or the interface with Wannier logical, public :: w90_write_unk Write the unks for the interface with Wannier logical, public :: hasnobup Is the number of bands with spin up for \n   wannierization defined? logical, public :: hasnobdown Is the number of bands with spin down for \n   wannierization defined? logical, public :: hasnob Is the number of bands for wannierization defined?\n   (for non spin-polarized calculations). integer, public :: nobup Number of bands with spin up for wannierization integer, public :: nobdown Number of bands with spin down for wannierization integer, public :: nob Number of bands for wannierization\n   (for non spin-polarized calculations). logical, public :: writef Write atomic forces at every geometry step? logical, public :: writek Write the k vectors of the BZ integration mesh? logical, public :: writic Write the initial atomic ccordinates? logical, public :: varcel Change unit cell during relaxation or dynamics? logical, public :: do_pdos Compute the projected density of states? logical, public :: write_tshs_history Write the MD track of Hamiltonian and overlap matrices in transiesta format logical, public :: write_hs_history Write the MD track of Hamiltonian and overlap matrices logical, public :: writedm Write file with density matrix? logical, public :: write_dm_at_end_of_cycle Write DM at end of SCF cycle? (converged or not) logical, public :: writeH Write file with Hamiltonian? (in \"DM\" format) logical, public :: write_H_at_end_of_cycle logical, public :: writedm_cdf Write file with density matrix in netCDF form? logical, public :: writedm_cdf_history Write file with SCF history of DM in netCDF form? logical, public :: writedmhs_cdf Write file with DM_in, H, DM_out, and S in netCDF form? logical, public :: writedmhs_cdf_history Write file with SCF history in netCDF form? logical, public :: read_charge_cdf Read charge density from file in netCDF form? logical, public :: read_deformation_charge_cdf Read deformation charge density from file in netCDF form? logical, public :: save_initial_charge_density Just save the initial charge density used logical, public :: analyze_charge_density_only Exit dhscf after processing charge logical, public :: atmonly Set up pseudoatom information only? logical, public :: harrisfun Use Harris functional? logical, public :: muldeb Write Mulliken populations at every SCF step? logical, public :: spndeb Write spin-polarization information at every SCF step? logical, public :: orbmoms Write orbital moments? logical, public :: split_sr_so logical, public :: converge_FreeE free Energy conv. to finish SCF iteration? real(kind=dp), public :: tolerance_FreeE Free-energy tolerance logical, public :: converge_Eharr to finish SCF iteration? real(kind=dp), public :: tolerance_Eharr Harris tolerance logical, public :: converge_EDM to finish SCF iteration? real(kind=dp), public :: tolerance_EDM Tolerance in change of EDM elements to finish SCF iteration logical, public :: converge_DM to finish SCF iteration? real(kind=dp), public :: dDtol Tolerance in change of DM elements to finish SCF iteration logical, public :: converge_H to finish SCF iteration? real(kind=dp), public :: dHtol Tolerance in change of H elements to finish SCF iteration logical, public :: broyden_optim Use Broyden method to optimize geometry? logical, public :: fire_optim Use FIRE method to optimize geometry? logical, public :: struct_only Output initial structure only? logical, public :: use_struct_file Read structural information from a special file? logical, public :: bornz Calculate Born polarization charges? logical, public :: SCFMustConverge Do we have to converge for each SCF calculation? logical, public :: GeometryMustConverge Do we have to converge the relaxation? logical, public :: want_domain_decomposition Use domain decomposition for orbitals logical, public :: want_spatial_decomposition Use spatial decomposition for orbitals logical, public :: hirshpop Perform Hirshfeld population analysis? logical, public :: voropop Perform Voronoi population analysis? logical, public :: partial_charges_at_every_geometry logical, public :: partial_charges_at_every_scf_step logical, public :: monitor_forces_in_scf Compute forces and stresses at every step logical, public :: minim_calc_eigenvalues Use diagonalization at the end of each MD step to find eigenvalues for OMM logical, public :: writetdwf To write the wavefuctions at the end of SCF. These\n would serve as the initial states for time evolution\n of KS states in TD-DFT. logical, public :: extrapol_H_tdks Extrapolate Hamiltonian within Crank-Nicolson integration? logical, public :: td_elec_dyn To do TDDFT calculation on second run logical, public :: etot_time Write Etot vs time during TDDFT logical, public :: eigen_time Write instataneous energy of the electronic states in TDDFT logical, public :: dip_time Write dipol moment againstan time in TDDFT logical, public :: tdsavewf To save the wavefunctions at the end of a calculation for restart./ logical, public :: tdsaverho To save TD-Rho after a given number of time steps logical, public :: td_inverse_linear Matrix inversion option? integer, public :: ntdsaverho Each number of steps TD-Rho is saved. integer, public :: itded a TDDFT counterpart of iscf integer, public :: ntded Number of TDED steps in each MD iteration. \n Or total number of TDED steps in an only electron calcuation\n (MD.FinalTimeStep = 1) integer, public :: ntded_sub Number of TDED sub-steps extrapolate H is applied to TDKS states. real(kind=dp), public :: td_dt Time step in electron dynamics. In case of doing electron dyanmics \n with MD, the dt in MD would be dt = td_dt x ntded real(kind=dp), public :: rstart_time Restart time real(kind=dp), public :: totime Total time including the restart time mainly for plotting integer, public :: ia1 Atom index integer, public :: ia2 Atom index integer, public :: ianneal Annealing option read in redata and passed to anneal integer, public :: idyn Geommetry relaxation/dynamics option integer, public :: ifinal Last geommetry iteration step for some types of dynamics integer, public :: ioptlwf Order-N functional option read in redata used in ordern integer, public :: iquench Quenching option, read in redata, used in dynamics routines integer, public :: isolve Option to find density matrix: 0=>diag, 1=>order-N integer, public :: istart First geommetry iteration step for certain types of dynamics integer, public :: DM_history_depth Number of previous density matrices used in extrapolation and reuse integer, public :: maxsav Number of previous density matrices used in Pulay mixing integer, public :: broyden_maxit Max. iterations in Broyden geometry relaxation integer, public :: mullipop Option for Mulliken population level of detail integer, public :: ncgmax Max. number of conjugate gradient steps in order-N minim. integer, public :: nkick Period between 'kick' steps in SCF iteration integer, public :: nmove Number of geometry iterations integer, public :: nscf Number of SCF iteration steps integer, public :: min_nscf Minimum number of SCF iteration steps integer, public :: pmax integer, public :: neigwanted Wanted number of eigenstates (per k point) integer, public :: level Option for allocation report level of detail integer, public :: call_diagon_default Default number of SCF steps for which to use diagonalization before OMM integer, public :: call_diagon_first_step Number of SCF steps for which to use diagonalization before OMM (first MD step) real(kind=dp), public :: beta Inverse temperature for Chebishev expansion. real(kind=dp), public :: bulkm Bulk modulus real(kind=dp), public :: charnet Net electric charge real(kind=dp), public :: rijmin Min. permited interatomic distance without warning real(kind=dp), public :: dm_normalization_tol Threshold for DM normalization mismatch error logical, public :: normalize_dm_during_scf Whether we normalize the DM real(kind=dp), public :: dt Time step in dynamics real(kind=dp), public :: dx Atomic displacement used to calculate Hessian matrix real(kind=dp), public :: dxmax Max. atomic displacement allowed during geom. relaxation real(kind=dp), public :: eta (2) Chemical-potential param. Read by redata used in ordern real(kind=dp), public :: etol Relative tol. in CG minim, read by redata, used in ordern real(kind=dp), public :: ftol Force tolerance to stop geometry relaxation real(kind=dp), public :: g2cut Required planewave cutoff of real-space integration mesh real(kind=dp), public :: mn Mass of Nose thermostat real(kind=dp), public :: mpr Mass of Parrinello-Rahman variables real(kind=dp), public :: occtol Occupancy threshold to build DM real(kind=dp), public :: rcoor Cutoff radius of Localized Wave Functions in ordern real(kind=dp), public :: rcoorcp Cutoff radius to find Fermi level by projection in ordern real(kind=dp), public :: rmax_bonds Cutoff length for bond definition real(kind=dp), public :: strtol Stress tolerance in relaxing the unit cell real(kind=dp), public :: taurelax Relaxation time to reach desired T and P in anneal real(kind=dp), public :: temp real(kind=dp), public :: tempinit Initial ionic temperature read in redata real(kind=dp), public :: threshold Min. size of arrays printed by alloc_report real(kind=dp), public :: tp Target pressure. Read in redata. Used in dynamics routines real(kind=dp), public :: total_spin Total spin used in spin-polarized calculations real(kind=dp), public :: tt Target temperature. Read in redata. Used in dynamics rout. real(kind=dp), public :: wmix Mixing weight for DM in SCF iteration real(kind=dp), public :: wmixkick Mixing weight for DM in special 'kick' SCF steps character(len=164), public :: sname System name, used to initialise read integer, public, parameter :: SOLVE_DIAGON = 0 integer, public, parameter :: SOLVE_ORDERN = 1 integer, public, parameter :: SOLVE_TRANSI = 2 integer, public, parameter :: SOLVE_MINIM = 3 integer, public, parameter :: SOLVE_PEXSI = 4 integer, public, parameter :: MATRIX_WRITE = 5 integer, public, parameter :: SOLVE_CHESS = 6","tags":"","loc":"module/siesta_options.html","title":"siesta_options – SIESTA"},{"text":"Used by module~~m_dfscf~~UsedByGraph module~m_dfscf m_dfscf module~m_dhscf m_dhscf module~m_dhscf->module~m_dfscf proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~m_dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~m_dhscf proc~setup_h0 setup_H0 proc~setup_h0->module~m_dhscf proc~compute_energies compute_energies proc~compute_energies->module~m_dhscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines dfscf Subroutines public subroutine dfscf (ifa, istr, na, no, nuo, nuotot, np, nspin, indxua, isa, iaorb, iphorb, maxnd, numd, listdptr, listd, Dscf, Datm, Vscf, Vatm, dvol, VolCel, Fal, Stressl) Author P.Ordejon, J.M.Soler, and J.Gale Adds the SCF contribution to atomic forces and stress. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: ifa Are forces required? ( 1 =yes, 0 =no) integer, intent(in) :: istr Is stress required? ( 1 =yes, 0 =no) integer, intent(in) :: na Number of atoms integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: nuo Number of orbitals in unit cell (local) integer, intent(in) :: nuotot Number of orbitals in unit cell (global) integer, intent(in) :: np Number of mesh points (total is nsp*np) integer, intent(in) :: nspin spin%Grid (i.e., min(nspin,4) ) nspin=1 => Unpolarized, nspin=2 => polarized nspin=4 => Noncollinear spin/SOC integer, intent(in) :: indxua (na) Index of equivalent atom in unit cell integer, intent(in) :: isa (na) Species index of each atom integer, intent(in) :: iaorb (no) Atom to which orbitals belong integer, intent(in) :: iphorb (no) Index of orbital within its atom integer, intent(in) :: maxnd First dimension of Dscf integer, intent(in) :: numd (nuo) Number of nonzero elemts in each row of Dscf integer, intent(in) :: listdptr (nuo) Pointer to start of row in listd integer, intent(in) :: listd (maxnd) List of nonzero elements of Dscf real(kind=dp), intent(in), target :: Dscf (:,:) Dscf(maxnd,*) : Value of nonzero elemens of density matrix.\n It is used in \"hermitified\" here. real(kind=dp), intent(in) :: Datm (nuotot) real(kind=grid_p), intent(in) :: Vscf (nsp,np,nspin) Value of SCF potential at the mesh points real(kind=grid_p), intent(in) :: Vatm (nsp,np) Value of Harris potential (Hartree potential\n of sum of atomic desities) at mesh points. Notice single precision of Vscf and Vatm. real(kind=dp), intent(in) :: dvol Volume per mesh point real(kind=dp), intent(in) :: VolCel Unit cell volume real(kind=dp), intent(inout) :: Fal (3,*) Atomic forces (contribution added on output) real(kind=dp), intent(inout) :: Stressl (9) Stress tensor","tags":"","loc":"module/m_dfscf.html","title":"m_dfscf – SIESTA"},{"text":"Derived types for orbitals, KB projectors, and LDA+U projectors Storage of orbital and projector real-space tables and other\n characteristics These parameters are over-dimensioned, but there is no storage\n penalty, as the real information is packed and indexed. Uses precision radial module~~atm_types~~UsesGraph module~atm_types atm_types module~precision precision module~atm_types->module~precision module~radial radial module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~atm_types~~UsedByGraph module~atm_types atm_types proc~delk delk proc~delk->module~atm_types module~atmfuncs atmfuncs proc~delk->module~atmfuncs proc~vmat vmat proc~vmat->module~atm_types proc~vmat->module~atmfuncs module~atmfuncs->module~atm_types proc~dfscf dfscf proc~dfscf->module~atm_types proc~dfscf->module~atmfuncs proc~rhoofd rhoofd proc~rhoofd->module~atm_types proc~rhoofd->module~atmfuncs proc~dhscf_init dhscf_init proc~dhscf_init->module~atmfuncs proc~dhscf dhscf proc~dhscf->module~atmfuncs proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~atmfuncs proc~rhooda rhooda proc~rhooda->module~atmfuncs proc~setup_h0 setup_H0 proc~setup_h0->module~atmfuncs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables maxnorbs maxn_pjnl maxn_orbnl maxnprojs nspecies npairs species elec_corr Derived Types species_info Variables Type Visibility Attributes Name Initial integer, public, parameter :: maxnorbs = 100 Maximum number of nlm orbitals integer, public, parameter :: maxn_pjnl = 20 Maximum number of projectors (not counting different \"m\" copi      !es) integer, public, parameter :: maxn_orbnl = 200 Maximum number of nl orbitals (not counting different \"m\" cop      !ies)\n Now very large to accommodate filteret basis sets integer, public, parameter :: maxnprojs = 100 Maximum number of nlm projectors integer, public, save :: nspecies integer, public, save :: npairs type( species_info ), public, target, allocatable, save :: species (:) type( rad_func ), public, allocatable, target, save :: elec_corr (:) Derived Types type, public :: species_info Species_info: Consolidate all the pieces of information in on      !e place Components Type Visibility Attributes Name Initial character(len=2), public :: symbol character(len=20), public :: label integer, public :: z Atomic number real(kind=dp), public :: mass real(kind=dp), public :: zval Valence charge real(kind=dp), public :: self_energy Electrostatic self-energy integer, public :: n_orbnl = 0 num of nl orbs integer, public :: lmax_basis = 0 basis l cutoff integer, public, dimension(maxn_orbnl) :: orbnl_l l of each nl orb integer, public, dimension(maxn_orbnl) :: orbnl_n n of each nl orb integer, public, dimension(maxn_orbnl) :: orbnl_z z of each nl orb logical, public, dimension(maxn_orbnl) :: orbnl_ispol is it a pol. orb? real(kind=dp), public, dimension(maxn_orbnl) :: orbnl_pop population of nl orb (total of 2l+1 components) logical, public :: lj_projs = .false. integer, public :: n_pjnl = 0 num of \"nl\" projs integer, public :: lmax_projs = 0 l cutoff for projs integer, public, dimension(maxn_pjnl) :: pjnl_l l of each nl proj real(kind=dp), public, dimension(maxn_pjnl) :: pjnl_j j of each nl proj integer, public, dimension(maxn_pjnl) :: pjnl_n n of each nl proj real(kind=dp), public, dimension(maxn_pjnl) :: pjnl_ekb energy of integer, public :: norbs = 0 integer, public, dimension(maxnorbs) :: orb_index integer, public, dimension(maxnorbs) :: orb_n integer, public, dimension(maxnorbs) :: orb_l integer, public, dimension(maxnorbs) :: orb_m integer, public, dimension(maxnorbs) :: orb_gindex real(kind=dp), public, dimension(maxnorbs) :: orb_pop population of nl orb integer, public :: nprojs = 0 integer, public, dimension(maxnprojs) :: pj_index integer, public, dimension(maxnprojs) :: pj_n integer, public, dimension(maxnprojs) :: pj_l real(kind=dp), public, dimension(maxnprojs) :: pj_j integer, public, dimension(maxnprojs) :: pj_m integer, public, dimension(maxnprojs) :: pj_gindex integer, public :: n_pjldaunl = 0 num of \"nl\" projs. Not counting the \"m copies\" integer, public :: lmax_ldau_projs = 0 l cutoff for LDA+U proj integer, public, dimension(maxn_pjnl) :: pjldaunl_l l of each nl proj integer, public, dimension(maxn_pjnl) :: pjldaunl_n n of each nl proj Here, n is not the principal\n quantum number, but a sequential\n index from 1 to the total\n number of projectors for that l. In the case of LDA+U projectors,\n It is always equal to 1. real(kind=dp), public, dimension(maxn_pjnl) :: pjldaunl_U U of each nl projector real(kind=dp), public, dimension(maxn_pjnl) :: pjldaunl_J J of each nl projector integer, public :: nprojsldau = 0 Total number of LDA+U proj. Counting the \"m copies\" (including the (2l + 1) factor). integer, public, dimension(maxnprojs) :: pjldau_index integer, public, dimension(maxnprojs) :: pjldau_n integer, public, dimension(maxnprojs) :: pjldau_l integer, public, dimension(maxnprojs) :: pjldau_m integer, public, dimension(maxnprojs) :: pjldau_gindex type( rad_func ), public, dimension(:), pointer :: orbnl type( rad_func ), public, dimension(:), pointer :: pjnl type( rad_func ), public, dimension(:), pointer :: pjldau type( rad_func ), public :: vna integer, public :: vna_gindex = 0 type( rad_func ), public :: chlocal type( rad_func ), public :: reduced_vlocal logical, public :: there_is_core type( rad_func ), public :: core logical, public :: read_from_file","tags":"","loc":"module/atm_types.html","title":"atm_types – SIESTA"},{"text":"This file contains a set of routines which provide all the information\n about the basis set, pseudopotential, atomic mass, etc... of all the\n chemical species present in the calculation. The routines contained in this file can only be called after they\n are initialized by calling the subroutine 'atom' for all the\n different chemical species in the calculation: Uses precision sys atm_types radial spher_harm module~~atmfuncs~~UsesGraph module~atmfuncs atmfuncs module~atm_types atm_types module~atmfuncs->module~atm_types module~precision precision module~atmfuncs->module~precision spher_harm spher_harm module~atmfuncs->spher_harm module~radial radial module~atmfuncs->module~radial module~sys sys module~atmfuncs->module~sys module~atm_types->module~precision module~atm_types->module~radial module~radial->module~precision xml xml module~radial->xml interpolation interpolation module~radial->interpolation Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~atmfuncs~~UsedByGraph module~atmfuncs atmfuncs proc~dhscf_init dhscf_init proc~dhscf_init->module~atmfuncs proc~setup_h0 setup_H0 proc~setup_h0->module~atmfuncs proc~dfscf dfscf proc~dfscf->module~atmfuncs proc~vmat vmat proc~vmat->module~atmfuncs proc~dhscf dhscf proc~dhscf->module~atmfuncs proc~rhooda rhooda proc~rhooda->module~atmfuncs proc~rhoofd rhoofd proc~rhoofd->module~atmfuncs proc~delk delk proc~delk->module~atmfuncs proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~atmfuncs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables spp op pp func message max_l max_ilm tiny20 tiny12 Functions floating IZOFIS ZVALFIS LABELFIS LMXKBFIS LOMAXFIS MASSFIS NKBFIS NOFIS UION RCORE RCHLOCAL orb_gindex kbproj_gindex ldau_gindex vna_gindex ATMPOPFIO CNFIGFIO LOFIO MOFIO ZETAFIO rcut SYMFIO POL EPSKB NZTFL NKBL_FUNC Subroutines chk vna_sub psch chcore_sub phiatm rphiatm all_phi psover Variables Type Visibility Attributes Name Initial type( species_info ), public, pointer :: spp type( rad_func ), public, pointer :: op type( rad_func ), public, pointer :: pp type( rad_func ), public, pointer :: func character(len=79), private :: message integer, private, parameter :: max_l = 5 integer, private, parameter :: max_ilm = (max_l+1)*(max_l+1) real(kind=dp), public, parameter :: tiny20 = 1.e-20_dp real(kind=dp), public, parameter :: tiny12 = 1.e-12_dp Functions public function floating (is) Returns .true. if the species is really a \"fake\" one,\n intended to provide some floating orbitals. Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value logical public function IZOFIS (is) result(izofis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value integer Atomic number public function ZVALFIS (is) result(zvalfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value real(kind=dp) Valence charge public function LABELFIS (is) result(labelfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value character(len=20) Atomic label public function LMXKBFIS (is) result(lmxkbfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Species index Return Value integer Maximum ang mom of the KB projectors public function LOMAXFIS (is) result(lomaxfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer public function MASSFIS (is) result(massfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) public function NKBFIS (is) result(nkbfis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer public function NOFIS (is) result(nofis) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer public function UION (is) result(uion) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) public function RCORE (is) result(rcore) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) public function RCHLOCAL (is) result(rchlocal) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value real(kind=dp) public function orb_gindex (is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer public function kbproj_gindex (is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer public function ldau_gindex (is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer public function vna_gindex (is) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is Return Value integer public function ATMPOPFIO (is, io) result(atmpopfio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value real(kind=dp) public function CNFIGFIO (is, io) result(cnfigfio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value integer public function LOFIO (is, io) result(lofio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io * * * OUTPUT * * * * * ****\n   INTEGER LOFIO  : Quantum number L of orbital or KB projector Return Value integer public function MOFIO (is, io) result(mofio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io * * * OUTPUT * * * * * ****\n   INTEGER MOFIO  : Quantum number m of orbital or KB projector Return Value integer public function ZETAFIO (is, io) result(zetafio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io * * * OUTPUT * * * * * ****\n   INTEGER ZETAFIO  : Zeta number of orbital Return Value integer public function rcut (is, io) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value real(kind=dp) public function SYMFIO (is, io) result(symfio) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value character(len=20) public function POL (is, io) result(pol) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value logical public function EPSKB (is, io) result(epskb) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io Return Value real(kind=dp) public function NZTFL (is, l) result(nztfl) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: l Return Value integer private function NKBL_FUNC (is, l) result(nkbl_func) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: l Return Value integer Subroutines private subroutine chk (name, is) Arguments Type Intent Optional Attributes Name character(len=*), intent(in) :: name integer, intent(in) :: is private subroutine vna_sub (is, r, v, grv) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: v real(kind=dp), intent(out) :: grv (3) public subroutine psch (is, r, ch, grch) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: ch real(kind=dp), intent(out) :: grch (3) public subroutine chcore_sub (is, r, ch, grch) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: ch real(kind=dp), intent(out) :: grch (3) public subroutine phiatm (is, io, r, phi, grphi) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io real(kind=dp), intent(in) :: r (3) real(kind=dp), intent(out) :: phi real(kind=dp), intent(out) :: grphi (3) public subroutine rphiatm (is, io, r, phi, dphidr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: io real(kind=dp), intent(in) :: r real(kind=dp), intent(out) :: phi real(kind=dp), intent(out) :: dphidr public subroutine all_phi (is, it, r, nphi, phi, grphi) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is integer, intent(in) :: it real(kind=dp), intent(in) :: r (3) integer, intent(out) :: nphi real(kind=dp), intent(out) :: phi (:) real(kind=dp), intent(out), optional :: grphi (:,:) public subroutine psover (is1, is2, r, energ, dedr) Arguments Type Intent Optional Attributes Name integer, intent(in) :: is1 integer, intent(in) :: is2 real(kind=dp), intent(in) :: r real(kind=dp), intent(out) :: energ real(kind=dp), intent(out) :: dedr","tags":"","loc":"module/atmfuncs.html","title":"atmfuncs – SIESTA"},{"text":"Stores quantities that are connected with the mesh Uses precision module~~mesh~~UsesGraph module~mesh mesh module~precision precision module~mesh->module~precision Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~mesh~~UsedByGraph module~mesh mesh proc~dhscf_init dhscf_init proc~dhscf_init->module~mesh proc~allocextmeshdistr allocExtMeshDistr proc~allocextmeshdistr->module~mesh proc~distmeshdata_rea distMeshData_rea proc~distmeshdata_rea->module~mesh proc~dfscf dfscf proc~dfscf->module~mesh module~m_forhar m_forhar module~m_forhar->module~mesh proc~setmeshdistr setMeshDistr proc~setmeshdistr->module~mesh proc~vmat vmat proc~vmat->module~mesh proc~allocasynbuffer allocASynBuffer proc~allocasynbuffer->module~mesh proc~distmeshdata_rea~2 distMeshData_rea proc~distmeshdata_rea~2->module~mesh proc~dhscf dhscf proc~dhscf->module~mesh proc~dhscf->module~m_forhar proc~rhooda rhooda proc~rhooda->module~mesh proc~allocipadistr allocIpaDistr proc~allocipadistr->module~mesh proc~rhoofd rhoofd proc~rhoofd->module~mesh proc~delk delk proc~delk->module~mesh proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~mesh Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables idop ipa dxa xdop xdsp mop ne nem nmsc nmuc nusc meshLim nmeshg nsm nsp cmesh rcmesh indexp iatfold Variables Type Visibility Attributes Name Initial integer, public, pointer, save :: idop (:) idop(mop) : Extended-mesh-index displacement of points\n within a sphere of radius rmax . integer, public, pointer, save :: ipa (:) => null() ipa(na) : Mesh cell in which atom is real(kind=dp), public, pointer, save :: dxa (:,:) => null() dxa(3,na) : Atom position within mesh-cell real(kind=dp), public, pointer, save :: xdop (:,:) xdop(3,mop) : Vector to mesh points within rmax real(kind=dp), public, pointer, save :: xdsp (:,:) xdsp(3,nsp) : Vector to mesh sub-points.\n Allocated in dhscf_init . integer, public, save :: mop Maximum number of non-zero orbital points integer, public, save :: ne (3) Number of mesh-Extension intervals in each direction integer, public, save :: nem (3) Extended-mesh divisions in each direction integer, public, save :: nmsc (3) Mesh divisions of each supercell vector integer, public, save :: nmuc (3) Mesh points in each unit cell vector integer, public, save :: nusc (3) Number of unit cells in each supercell dir integer, public, save :: meshLim (2,3) Upper an lower limits of the mesh in every process My processor's box of mesh points: myBox(1,:) : lower bounds myBox(2,:) : upper bounds integer, public, save :: nmeshg (3) Total number of mesh points in each direction integer, public, save :: nsm Number of mesh sub-divisions in each direction integer, public, save :: nsp Number of sub-points of each mesh point real(kind=dp), public, save :: cmesh (3,3) Mesh-cell vectors real(kind=dp), public, save :: rcmesh (3,3) Reciprocal mesh-cell vectors (WITHOUT 2*\\pi factor) integer, public, pointer, save :: indexp (:) => null() indexp(nep) : Translation from extended to normal mesh index integer, public, pointer, save :: iatfold (:,:) => null() iatfold(3,na) : Supercell vector that keeps track of the\n of the folding of the atomic coordinates in the mesh","tags":"","loc":"module/mesh.html","title":"mesh – SIESTA"},{"text":"Used by module~~m_state_init~~UsedByGraph module~m_state_init m_state_init proc~siesta_forces siesta_forces proc~siesta_forces->module~m_state_init Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines state_init check_cohp Subroutines public subroutine state_init (istep) Arguments Type Intent Optional Attributes Name integer :: istep private subroutine check_cohp () Arguments None","tags":"","loc":"module/m_state_init.html","title":"m_state_init – SIESTA"},{"text":"Uses write_subs module~~m_state_analysis~~UsesGraph module~m_state_analysis m_state_analysis write_subs write_subs module~m_state_analysis->write_subs Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_state_analysis~~UsedByGraph module~m_state_analysis m_state_analysis proc~siesta_forces siesta_forces proc~siesta_forces->module~m_state_analysis Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines state_analysis Subroutines public subroutine state_analysis (istep) Arguments Type Intent Optional Attributes Name integer :: istep","tags":"","loc":"module/m_state_analysis.html","title":"m_state_analysis – SIESTA"},{"text":"To facilitate the communication among dhscf_init and dhscf ,\n some arrays that hold data which do not change during the SCF loop\n have been made into module variables Some others are scratch, such as nmpl , ntpl , etc Uses precision m_dfscf module~~m_dhscf~~UsesGraph module~m_dhscf m_dhscf module~precision precision module~m_dhscf->module~precision module~m_dfscf m_dfscf module~m_dhscf->module~m_dfscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Used by module~~m_dhscf~~UsedByGraph module~m_dhscf m_dhscf proc~siesta_analysis siesta_analysis proc~siesta_analysis->module~m_dhscf proc~setup_hamiltonian setup_hamiltonian proc~setup_hamiltonian->module~m_dhscf proc~setup_h0 setup_H0 proc~setup_h0->module~m_dhscf proc~compute_energies compute_energies proc~compute_energies->module~m_dhscf Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Variables rhopcc rhoatm Vna Uharrs IsDiag spiral shape nml ntml npcc nmpl ntpl bcell cell dvol field rmax scell G2mesh debug_dhscf debug_fmt Subroutines dhscf_init dhscf delk_wrapper Variables Type Visibility Attributes Name Initial real(kind=grid_p), public, pointer :: rhopcc (:) real(kind=grid_p), public, pointer :: rhoatm (:) real(kind=grid_p), public, pointer :: Vna (:) real(kind=dp), public :: Uharrs Harris energy logical, public :: IsDiag logical, public :: spiral character(len=10), public :: shape integer, public :: nml (3) integer, public :: ntml (3) integer, public :: npcc integer, public :: nmpl integer, public :: ntpl real(kind=dp), public :: bcell (3,3) real(kind=dp), public :: cell (3,3) real(kind=dp), public :: dvol real(kind=dp), public :: field (3) real(kind=dp), public :: rmax real(kind=dp), public :: scell (3,3) real(kind=dp), public :: G2mesh = 0.0_dp logical, public :: debug_dhscf = .false. character(len=*), public, parameter :: debug_fmt = '('' -- dhscf : Node '',i3,tr1,a,4(tr1,e20.7))' Subroutines public subroutine dhscf_init (nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ucell, mscell, g2max, ntm, maxnd, numd, listdptr, listd, datm, Fal, stressl) Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin integer, intent(in) :: norb integer, intent(in) :: iaorb (norb) integer, intent(in) :: iphorb (norb) integer, intent(in) :: nuo integer, intent(in) :: nuotot integer, intent(in) :: nua integer, intent(in) :: na integer, intent(in) :: isa (na) real(kind=dp), intent(in) :: xa (3,na) integer, intent(in) :: indxua (na) real(kind=dp), intent(in) :: ucell (3,3) integer, intent(in) :: mscell (3,3) real(kind=dp), intent(inout) :: g2max integer, intent(inout) :: ntm (3) integer, intent(in) :: maxnd integer, intent(in) :: numd (nuo) integer, intent(in) :: listdptr (nuo) integer, intent(in) :: listd (maxnd) real(kind=dp), intent(in) :: datm (norb) real(kind=dp), intent(inout) :: Fal (3,nua) real(kind=dp), intent(inout) :: stressl (3,3) public subroutine dhscf (nspin, norb, iaorb, iphorb, nuo, nuotot, nua, na, isa, xa, indxua, ntm, ifa, istr, iHmat, filesOut, maxnd, numd, listdptr, listd, Dscf, datm, maxnh, Hmat, Enaatm, Enascf, Uatm, Uscf, DUscf, DUext, Exc, Dxc, dipol, stress, Fal, stressl, use_rhog_in, charge_density_only) Author J.M. Soler Date August 1996 Calculates the self-consistent field contributions to Hamiltonian\n matrix elements, total energy and atomic forces. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: nspin Number of different spin polarisations: nspin=1 => Unpolarized, nspin=2 => polarized nspin=4 => Noncollinear spin or spin-orbit. integer, intent(in) :: norb Total number of basis orbitals in supercell integer, intent(in) :: iaorb (norb) Atom to which each orbital belongs integer, intent(in) :: iphorb (norb) Orbital index (within atom) of each orbital integer, intent(in) :: nuo Number of orbitals in a unit cell in this node integer, intent(in) :: nuotot Number of orbitals in a unit cell integer, intent(in) :: nua Number of atoms in unit cell integer, intent(in) :: na Number of atoms in supercell integer, intent(in) :: isa (na) Species index of all atoms in supercell real(kind=dp), intent(in) :: xa (3,na) Atomic positions of all atoms in supercell integer, intent(in) :: indxua (na) Index of equivalent atom in unit cell integer, intent(inout) :: ntm (3) Number of mesh divisions of each cell\n vector, including subgrid. integer, intent(in) :: ifa Switch which fixes whether the SCF contrib: to atomic forces is calculated and added to fa. integer, intent(in) :: istr Switch which fixes whether the SCF contrib: to stress is calculated and added to stress. integer, intent(in) :: iHmat Switch which fixes whether the Hmat matrix\n elements are calculated or not. type(filesOut_t), intent(inout) :: filesOut Output file names (If blank => not saved) integer, intent(in) :: maxnd First dimension of listd and Dscf integer, intent(in) :: numd (nuo) Number of nonzero density-matrix\n elements for each matrix row integer, intent(in) :: listdptr (nuo) Pointer to start of rows of density-matrix integer, intent(in) :: listd (*) listd(maxnd) : Nonzero-density-matrix-element column\n indexes for each matrix row real(kind=dp), intent(in) :: Dscf (:,:) Dscf(maxnd,h_spin_dim) : SCF density-matrix elements real(kind=dp), intent(in) :: datm (norb) Harris density-matrix diagonal elements (atomic occupation charges of orbitals) integer, intent(in) :: maxnh First dimension of listh and Hmat real(kind=dp), intent(in) :: Hmat (:,:) Hmat(maxnh,h_spin_dim) : Hamiltonian matrix in sparse form, to which are added the matrix elements <ORB_I | DeltaV | ORB_J> , where DeltaV = Vna + Vxc(SCF) + Vhartree(RhoSCF-RhoHarris) real(kind=dp), intent(out) :: Enaatm Integral of Vna * rhoatm real(kind=dp), intent(out) :: Enascf Integral of Vna * rhoscf real(kind=dp), intent(out) :: Uatm Harris hartree electron-interaction energy real(kind=dp), intent(out) :: Uscf SCF hartree electron-interaction energy real(kind=dp), intent(out) :: DUscf Electrostatic (Hartree) energy of (rhoscf - rhoatm) density real(kind=dp), intent(out) :: DUext Interaction energy with external electric field real(kind=dp), intent(out) :: Exc SCF exchange-correlation energy real(kind=dp), intent(out) :: Dxc SCF double-counting correction to Exc Dxc = integral of ( (epsxc - Vxc) * Rho ) All energies in Rydbergs real(kind=dp), intent(out) :: dipol (3) Electric dipole (in a.u.)\n only when the system is a molecule real(kind=dp) :: stress (3,3) real(kind=dp), intent(inout) :: Fal (3,nua) Atomic forces, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative\n of (Enascf - Enaatm + DUscf + Exc) with\n respect to atomic positions, in Ry/Bohr.\n Contributions local to this node. real(kind=dp), intent(inout) :: stressl (3,3) Stress tensor, to which the SCF contribution\n is added by this routine when ifa=1 .\n The SCF contribution is minus the derivative of (Enascf - Enaatm + DUscf + Exc) / volume with respect to the strain tensor, in Ry.\n Contributions local to this node. logical, intent(in), optional :: use_rhog_in logical, intent(in), optional :: charge_density_only public subroutine delk_wrapper (isigneikr, norb, maxnd, numd, listdptr, listd, nuo, nuotot, iaorb, iphorb, isa) This is a wrapper to call delk , using some of the module\n variables of m_dhscf , but from outside dhscf itself. Read more… Arguments Type Intent Optional Attributes Name integer :: isigneikr integer :: norb integer :: maxnd integer :: numd (nuo) integer :: listdptr (nuo) integer :: listd (maxnd) integer :: nuo integer :: nuotot integer :: iaorb (*) integer :: iphorb (*) integer :: isa (*)","tags":"","loc":"module/m_dhscf.html","title":"m_dhscf – SIESTA"},{"text":"Used by module~~m_delk~~UsedByGraph module~m_delk m_delk proc~delk_wrapper delk_wrapper proc~delk_wrapper->module~m_delk Help × Graph Key Nodes of different colours represent the following: Graph Key Module Module Submodule Submodule Subroutine Subroutine Function Function Program Program This Page's Entity This Page's Entity Solid arrows point from a submodule to the (sub)module which it is\n    descended from. Dashed arrows point from a module or program unit to \n    modules which it uses. Contents Subroutines delk Subroutines public subroutine delk (iexpikr, no, np, dvol, nvmax, numVs, listVsptr, listVs, nuo, nuotot, iaorb, iphorb, isa) Author J. Junquera Date February 2008 License GNU GPL Finds the matrix elements of a plane wave <\\phi_{\\mu}|e&#94;{(\\boldsymbol{iexpikr} \\: \\cdot \\: i \\vec{k} \\vec{r})}|\\phi_{\\nu}> Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: iexpikr Prefactor of the dot product between the\n the k-vector and the position-vector in exponent. It might be +1 or -1 integer, intent(in) :: no Number of basis orbitals integer, intent(in) :: np Number of columns in C (local) real(kind=dp), intent(in) :: dvol Volume per mesh point integer, intent(in) :: nvmax First dimension of listV and Vs , and max\n number of nonzero elements in any row of delkmat integer, intent(in) :: numVs (nuo) Number of non-zero elements in a row of delkmat integer, intent(in) :: listVsptr (nuo) Pointer to the start of rows in listVs integer, intent(in) :: listVs (nvmax) List of non-zero elements of delkmat integer :: nuo integer :: nuotot integer, intent(in) :: iaorb (*) Pointer to atom to which orbital belongs integer, intent(in) :: iphorb (*) Orbital index within each atom integer, intent(in) :: isa (*) Species index of all atoms","tags":"","loc":"module/m_delk.html","title":"m_delk – SIESTA"},{"text":"SIESTA is both a method and its computer program implementation, to perform efficient electronic structure calculations and ab initio molecular dynamics simulations of molecules and solids. SIESTA's efficiency stems from the use of strictly localized basis sets and from the implementation of linear-scaling algorithms which can be applied to suitable systems. A very important feature of the code is that its accuracy and cost can be tuned in a wide range, from quick exploratory calculations to highly accurate simulations matching the quality of other approaches, such as plane-wave and all-electron methods. This is a documentation project for SIESTA's source code. Unlike the underlying\nprinciples of SIESTA and its user options, the codebase was not well documented,\nuntil recently, and here we make an attempt to do so. This project is in active stage of development, and thus should not be considered a reliable source of information. Please, consult the manual on SIESTA's official repository .","tags":"","loc":"page//index.html","title":"Program Overview – SIESTA"},{"text":"The general logic of the SIESTA's operation cycle is performed by siesta_forces . After initialization that includes setup of non-scf part of Hamiltonian setup_H0 (see subsections below) as long as initialization of\nparameters for MPI and TranSiesta, SIESTA enters the main Self-Consistent Field loop, that can be schematically represented: Within each SCF cycle normally we have setup_hamiltonian and compute_dm executed subsequently. \nOne of the two convergence criteria is checked at the beginning and\nat the end of each scf cycle. Hamiltonian setup DM computation Check for Convergence Results analysis step","tags":"","loc":"page/01_calculation_flow/index.html","title":"SIESTA Calculation Flow – SIESTA"},{"text":"","tags":"","loc":"page/01_calculation_flow/01_hamiltonian_setup.html","title":"Hamiltonian setup – SIESTA"},{"text":"Performed in the compute_dm subroutine. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/02_dm_computation.html","title":"DM computation – SIESTA"},{"text":"Performed in the compute_max_diff subroutine. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/03_check_convergence.html","title":"SCF Check for Convergence – SIESTA"},{"text":"Results analysis is performed in siesta_analysis and state_analysis routines. Todo Add docs, expand description.","tags":"","loc":"page/01_calculation_flow/04_analyze_results.html","title":"SCF Results analysis – SIESTA"}]}